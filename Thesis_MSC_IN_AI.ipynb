{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/icsd13152/Thesis_MSC_in_AI_AuthorVerif/blob/main/Thesis_MSC_IN_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4UCq75a1-x-"
      },
      "source": [
        "# Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWOhMItkLJZ0",
        "outputId": "66b5acfd-f27b-4ac9-a1c4-2b3347f6375a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.9/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2.27.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.13.1+cu116)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (2022.10.31)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (1.26.104)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch-pretrained-bert) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.104 in /usr/local/lib/python3.9/dist-packages (from boto3->pytorch-pretrained-bert) (1.29.104)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pytorch-pretrained-bert) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.104->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.104->boto3->pytorch-pretrained-bert) (1.16.0)\n",
            "2023-04-01 08:52:03.741360: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-01 08:52:03.798438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-01 08:52:04.795772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_metric_learning in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_metric_learning) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pytorch_metric_learning) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pytorch_metric_learning) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pytorch_metric_learning) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->pytorch_metric_learning) (4.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch_metric_learning) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch_metric_learning) (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pytorch_metric_learning) (1.10.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "!export PYTORCH_CUDA_ALLOC_CONF=garbage_collection_threshold:0.7,max_split_size_mb:128\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import numpy as np\n",
        "# import spacy\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import f1_score\n",
        "!pip install transformers\n",
        "!pip install pytorch-pretrained-bert\n",
        "# !python -m spacy download en_core_web_trf\n",
        "!python -m spacy download en_core_web_lg\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "!pip install -qU transformers sentence-transformers\n",
        "from sentence_transformers.losses.ContrastiveLoss import SiameseDistanceMetric\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "# from sentence_transformers.losses import ContrastiveLoss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "# from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from transformers import BertTokenizer, RobertaModel,BertModel,BertTokenizerFast,T5Model\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "import random\n",
        "import matplotlib as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import gc\n",
        "from sklearn.manifold import TSNE\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "!pip install pytorch_metric_learning\n",
        "import  pytorch_metric_learning\n",
        "from pytorch_metric_learning import losses,distances\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtG7nrlQjMZH"
      },
      "source": [
        "## Implementation Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugJRyrW-P_f_",
        "outputId": "d38e70d4-89de-4136-ce48-41e53d4a1496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nWfaUZPEQNC5"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/drive/MyDrive/Thesis'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50tRdzIUF8DA",
        "outputId": "adbe93b5-51bb-4ca5-a43b-9f6e91fb8cd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7HNwE-jfTJh",
        "outputId": "4cf146ad-3dcb-485a-b243-8d7b9d54f381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "# bertModel= RobertaModel.from_pretrained('roberta-base',output_hidden_states = True)\n",
        "bertModel= BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True)#,output_hidden_states = True\n",
        "# bertModel= T5Model.from_pretrained('t5-base',output_hidden_states = True)\n",
        "bertModel.train()\n",
        "# bertModel = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2,output_hidden_states = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aKKMCUUTC7c",
        "outputId": "e7653cb9-f2e0-4aab-ca75-c301ce7a810b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WuEEaluTiRZU"
      },
      "outputs": [],
      "source": [
        "# Learning rate (Adam): 5e-5, 3e-5, 2e-5 based on authors\n",
        "# loss_margin = 0.5\n",
        "learning_rate = 2e-5\n",
        "epsilon = 1e-8\n",
        "batch_size = 16\n",
        "n_epochs = 2\n",
        "num_warmup_steps = 0\n",
        "num_training_steps = 52623 #len(data)*n_epochs\n",
        "patience = 1\n",
        "# fully_connected_layer_units = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CyO6xZ339rnS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osaQ1dlat9vi"
      },
      "outputs": [],
      "source": [
        "def createPairsWithSEPToken(ids1,masks1,ids2,masks2,label,combinations='one-by-one'):\n",
        "    rlabels = []\n",
        "    rIds = []\n",
        "    rMasks = []\n",
        "    if combinations=='one-by-one':\n",
        "       for idx in range(len(ids1)):\n",
        "           if idx <= len(ids2)-1:\n",
        "             rIds.append(torch.cat((ids1[idx],ids2[idx]),0))\n",
        "             rMasks.append(torch.cat((masks1[idx],masks2[idx]),0))\n",
        "             rlabels.append(label)\n",
        "           elif idx >= len(ids2):\n",
        "             i = random.randint(0, len(ids2)-1)\n",
        "             rIds.append(torch.cat((ids1[idx],ids2[i]),0))\n",
        "             rMasks.append(torch.cat((masks1[idx],masks2[i]),0))\n",
        "             rlabels.append(label)  \n",
        "    elif combinations=='all':\n",
        "       for idx in range(len(ids1)):\n",
        "          for j in range(len(ids2)):\n",
        "              rIds.append(torch.cat((ids1[idx],ids2[j]),0))\n",
        "              rMasks.append(torch.cat((masks1[idx],masks2[j]),0))\n",
        "              rlabels.append(label)\n",
        "\n",
        "    return rIds, rMasks, rlabels\n",
        "\n",
        "def createSeperatePairs(ids1,masks1,ids2,masks2,label,combinations='one-by-one'):\n",
        "    rlabels = []\n",
        "    rIds = []\n",
        "    rMasks = []\n",
        "    rIds2 = []\n",
        "    rMasks2 = []\n",
        "    if combinations=='one-by-one':\n",
        "       for idx in range(len(ids1)):\n",
        "           if idx <= len(ids2)-1:\n",
        "             rIds.append(ids1[idx])\n",
        "             rMasks.append(masks1[idx])\n",
        "             rIds2.append(ids2[idx])\n",
        "             rMasks2.append(masks2[idx])\n",
        "             rlabels.append(label)\n",
        "           elif idx >= len(ids2):\n",
        "             i = random.randint(0, len(ids2)-1)\n",
        "             rIds.append(ids1[idx])\n",
        "             rMasks.append(masks1[idx])\n",
        "             rIds2.append(ids2[i])\n",
        "             rMasks2.append(masks2[i])\n",
        "             rlabels.append(label)  \n",
        "    elif combinations=='all':\n",
        "       for idx in range(len(ids1)):\n",
        "          \n",
        "          for j in range(len(ids2)):\n",
        "              rIds.append(ids1[idx]) \n",
        "              rMasks.append(masks1[idx])\n",
        "              rIds2.append(ids2[j]) \n",
        "              rMasks2.append(masks2[j])\n",
        "              rlabels.append(label)\n",
        "\n",
        "    return rIds, rMasks, rIds2, rMasks2,rlabels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0q8urx50QEXe"
      },
      "source": [
        "# Read Data PAN 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF-T20ueQFU7"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import RobertaTokenizer\n",
        "# import emojis\n",
        "import spacy\n",
        "import random\n",
        "from nltk.probability import FreqDist\n",
        "from tqdm import tqdm\n",
        "##################\n",
        "# Class for corpus\n",
        "##################\n",
        "class Corpus(object):\n",
        "\n",
        "    def __init__(self, dict_dataset):\n",
        "\n",
        "        # define tokenizer\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "        # self.tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "        # raw dataset\n",
        "        self.dict_dataset_raw = dict_dataset\n",
        "\n",
        "        self.dict_dataset_per_auth_ids= {}\n",
        "        self.dict_dataset_per_auth_masks= {}\n",
        "        self.dict_dataset_per_auth_ids2= {}\n",
        "        self.dict_dataset_per_auth_masks2= {}\n",
        "        self.dict_dataset_per_auth_idsFirst= {}\n",
        "        self.dict_dataset_per_auth_masksFirst= {}\n",
        "        self.dict_positives_pairs = {}\n",
        "        self.dict_negatives_pairs = {}\n",
        "        self.dict_All_pairs = {}\n",
        "        self.dict_All_pairs_shuffle = {}\n",
        "        self.usedAuthors = {}\n",
        "        self.dict_pairs_per_author = {}\n",
        "        self.ner = spacy.load('en_core_web_lg')\n",
        "        print(self.ner.pipe_names)\n",
        "        self.ner.disable_pipes('tagger', 'parser','tok2vec', 'parser', 'attribute_ruler', 'lemmatizer')\n",
        "        print(self.ner.pipe_names)\n",
        "        self.list_of_non_freq_words = []\n",
        "    \n",
        "\n",
        "    def getNonFrequentWordsPerDoc(self,doc,threshold=5):\n",
        "        words = nltk.word_tokenize(doc)\n",
        "        fdist = FreqDist(words)\n",
        "\n",
        "        for word,freq in fdist.items():\n",
        "            if freq <= threshold:\n",
        "                # if len(word) > 4:\n",
        "                self.list_of_non_freq_words.append(word)\n",
        "\n",
        "    def replaceTokensWith(self,text,symbol='2'):\n",
        "        for token in self.list_of_non_freq_words:\n",
        "            text = text.replace(token,symbol)\n",
        "        return text\n",
        "\n",
        "\n",
        "            \n",
        "    def paddingChunksToMaxlen(self,IdsChunks,masksChunks,isSecond = False,maxLen = 126):#listMasksChunks\n",
        "        listIdsChunks = list()\n",
        "        listMasksChunks = list()\n",
        "\n",
        "        for i in range(len(IdsChunks)):\n",
        "\n",
        "            pad_len = maxLen  - IdsChunks[i].shape[0]\n",
        "\n",
        "            tmplistids = IdsChunks[i].tolist()\n",
        "            if len(tmplistids) < maxLen:\n",
        "                if i > 0: #from second element\n",
        "                    temp = IdsChunks[i-1].tolist()\n",
        "                    lenToadd = maxLen - len(tmplistids)\n",
        "                    # print(\"=================================\")\n",
        "                    # print(\"prev = \",str(len(temp)))\n",
        "                    # print(\"curr = \", str(len(tmplistids)))\n",
        "\n",
        "                    if len(temp) >= lenToadd:\n",
        "\n",
        "                        tmplistids = temp[len(temp)-lenToadd:len(temp)] + tmplistids\n",
        "                        pad_len = maxLen  - len(tmplistids)\n",
        "                    else:\n",
        "                        tmplistids = temp + tmplistids\n",
        "                        pad_len = maxLen  - len(tmplistids)\n",
        "                else:\n",
        "                    tmplistids = tmplistids\n",
        "                    pad_len = maxLen  - len(tmplistids)\n",
        "            elif len(tmplistids) == maxLen:\n",
        "                tmplistids = tmplistids\n",
        "                pad_len = maxLen  - len(tmplistids)\n",
        "            # if isSecond == False:\n",
        "            tmplistids.insert(0,101) #101 for BERT 0 for roberta\n",
        "            tmplistids.insert(len(tmplistids),102) #102 for Bert 2 for roberta\n",
        "            # print(\"after = \",str(len(tmplistids)))\n",
        "            # print(\"=================================\")\n",
        "            tmplistmasks = masksChunks[i].tolist()#listmasks[i].tolist()\n",
        "            if len(tmplistmasks) < maxLen:\n",
        "                if i > 0: #from second element\n",
        "                    temp = masksChunks[i-1].tolist()\n",
        "                    lenToadd = maxLen - len(tmplistmasks)\n",
        "                    if len(temp) >= lenToadd:\n",
        "                        tmplistmasks = temp[len(temp)-lenToadd:len(temp)] + tmplistmasks\n",
        "                    else:\n",
        "                        tmplistmasks = temp + tmplistmasks\n",
        "                else:\n",
        "                    tmplistmasks = tmplistmasks\n",
        "\n",
        "            elif len(tmplistmasks) == maxLen:\n",
        "                tmplistmasks = tmplistmasks\n",
        "            # if isSecond == False:\n",
        "            tmplistmasks.insert(0,1)\n",
        "            tmplistmasks.insert(len(tmplistmasks),1)\n",
        "            # print(tmplistids)\n",
        "            # if len(tmplistids) > 128:\n",
        "            listIdsChunks.append(torch.LongTensor(tmplistids))\n",
        "            listMasksChunks.append(torch.LongTensor(tmplistmasks))\n",
        "            del tmplistmasks, tmplistids\n",
        "\n",
        "            if pad_len > 0:\n",
        "\n",
        "                listIdsChunks[i] =  F.pad(listIdsChunks[i], (0,pad_len), \"constant\", 0) # 0 for bert 1 for roberta\n",
        "                listMasksChunks[i] =  F.pad(listMasksChunks[i], (0,pad_len), \"constant\", 0)\n",
        "        del IdsChunks, masksChunks, pad_len\n",
        "        # gc.collect()\n",
        "        return listIdsChunks ,listMasksChunks\n",
        "\n",
        "    def createRandomSetence(self,listOfSetences):\n",
        "        tmpList = []\n",
        "        for x in listOfSetences:\n",
        "            tmpValue = random.choice(listOfSetences)\n",
        "            tmpList.append(tmpValue)\n",
        "        sete = ' '.join(tmpList)\n",
        "        return sete\n",
        "\n",
        "    def chunkingTextsBasedOnBert(self,text):\n",
        "       \n",
        "\n",
        "\n",
        "        # setences1 = nltk.sent_tokenize(text)\n",
        "        # setences2 = nltk.sent_tokenize(row['Text2'])\n",
        "        # # print(setences1)\n",
        "        # set1 = self.createRandomSetence(setences1)\n",
        "        # # print(set1)\n",
        "        # set2 = createRandomSetence(setences2)\n",
        "       \n",
        "        encoded_dict = self.tokenizer.encode_plus(\n",
        "                                        text ,                    # Sentence to encode.\n",
        "                                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                                        # max_length = 512,           # Pad & truncate all sentences.\n",
        "                                        # pad_to_max_length = True,\n",
        "                                        # truncation = True,\n",
        "                                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                        )\n",
        "\n",
        "        # print(encoded_dict['input_ids'][0])\n",
        "        tensorsIdList1,tensorsMaskList1 = self.paddingChunksToMaxlen(encoded_dict['input_ids'][0].split(510),encoded_dict['attention_mask'][0].split(510),False,510)\n",
        "        # tensorsIdList1,tensorsMaskList1 = encoded_dict['input_ids'][0],encoded_dict['attention_mask'][0]\n",
        "        # del encoded_dict\n",
        "\n",
        "\n",
        "        # tensorsIdList1,tensorsMaskList1 = encoded_dict['input_ids'][0],encoded_dict['attention_mask'][0]\n",
        "        # del encoded_dict\n",
        "\n",
        "        return tensorsIdList1, tensorsMaskList1\n",
        "        # return encoded_dict['input_ids'][0],encoded_dict['attention_mask'][0]\n",
        "\n",
        "    def maskNumbers(self,text, symbol='1'):\n",
        "        x = re.sub('[0-9]', symbol,text)\n",
        "        return x\n",
        "    def maskEntities(self,text,entities = ['GPE','FAC','PERSON','ORG','PRODUCT']):\n",
        "\n",
        "      #GPE = Countries and Geo Location\n",
        "      #FAC = Building airports etc\n",
        "      #PERSON = Name of person\n",
        "      #PRODCUT = products\n",
        "      #ORG = Organization\n",
        "      docText1 = self.ner(text)\n",
        "      for ent in docText1.ents:\n",
        "        if ent.label_ == 'PERSON':\n",
        "              #  data[\"Text1\"] = data[\"Text1\"].apply(lambda row: row.replace(ent.text, 'David') ,meta=('Text1', 'object'))\n",
        "               text = text.replace(ent.text, 'David')\n",
        "        if ent.label_ == 'GPE':\n",
        "               text = text.replace(ent.text, 'Mexico')\n",
        "\n",
        "      return text\n",
        "    def preprocess_doc(self, doc):\n",
        "        doc = self.maskNumbers(doc)\n",
        "        # doc = self.maskEntities(doc)\n",
        "        # self.getNonFrequentWordsPerDoc(doc,threshold=5)\n",
        "        # doc = self.replaceTokensWith(doc)\n",
        "        return doc.strip()\n",
        "\n",
        "    def parse_dictionary_v2(self):\n",
        "        cnt = 0\n",
        "        # authors\n",
        "        print(\"Parse per author...for \",str(len(list(self.dict_dataset_raw.keys()))))\n",
        "        for a in tqdm(self.dict_dataset_raw.keys()):\n",
        "            # fandom categories\n",
        "            listDocsPerAuthor = []\n",
        "            listMasksPerAuthor = []\n",
        "            for i, docs in enumerate(self.dict_dataset_raw[a]):\n",
        "                print(docs)\n",
        "                inputIDs1First,attn_masks1First,inputIDs1,attn_masks1 = self.chunkingTextsBasedOnBert(docs)\n",
        "                flat_IDs = []\n",
        "                flat_Masks = []\n",
        "                    # flat_IDs2 = []\n",
        "                    # flat_Masks2 = []\n",
        "                # for item in inputIDs1:\n",
        "                #     flat_IDs.append(item)\n",
        "                # for item in attn_masks1:\n",
        "                #     flat_Masks.append(item)\n",
        "\n",
        "                listDocsPerAuthor.append(flat_IDs)\n",
        "                listMasksPerAuthor.append(flat_Masks)\n",
        "                if a not in self.dict_dataset_per_auth_ids.keys():\n",
        "                    self.dict_dataset_per_auth_ids[a] = []\n",
        "                if a not in self.dict_dataset_per_auth_masks.keys():\n",
        "                    self.dict_dataset_per_auth_masks[a] = []\n",
        "\n",
        "            self.dict_dataset_per_auth_ids[a] = listDocsPerAuthor  # authID:{[x1,x2,x3],[y1,y2...yn],...}\n",
        "            self.dict_dataset_per_auth_masks[a] = listMasksPerAuthor\n",
        "\n",
        "    def parse_dictionary(self,isval=False):\n",
        "        cnt = 0\n",
        "        # authors\n",
        "        print(\"Parse per author...for \",str(len(list(self.dict_dataset_raw.keys()))))\n",
        "        for a in tqdm(self.dict_dataset_raw.keys()):\n",
        "            # fandom categories\n",
        "            listDocsPerAuthor = []\n",
        "            listMasksPerAuthor = []\n",
        "            # print(a)\n",
        "            # for f in self.dict_dataset_raw[a].keys():\n",
        "            # documents\n",
        "            # print(f)\n",
        "            # print(self.dict_dataset_raw[a])\n",
        "            itemid = 0\n",
        "\n",
        "            for i, docs in enumerate(self.dict_dataset_raw[a]):\n",
        "                        # processed_doc1 = self.preprocess_doc(docs,True)\n",
        "                        # processed_doc2 = self.preprocess_doc(docs)\n",
        "                        if len(docs) < 256:\n",
        "                           print(a) \n",
        "                           break\n",
        "                        processed_doc2 = self.preprocess_doc(docs)\n",
        "                     \n",
        "                        inputIDs1,attn_masks1 = self.chunkingTextsBasedOnBert(processed_doc2)\n",
        "                        # flat_IDs = []\n",
        "                        # flat_Masks = []\n",
        "                        # print(inputIDs1)\n",
        "                        # for item in inputIDs1:\n",
        "                        #     flat_IDs.append(item)\n",
        "                        # for item in attn_masks1:\n",
        "                        #     flat_Masks.append(item)\n",
        "                      \n",
        "                        # listDocsPerAuthor.append(flat_IDs)\n",
        "                        # listMasksPerAuthor.append(flat_Masks)\n",
        "                        if a not in self.dict_dataset_per_auth_ids.keys():\n",
        "                          self.dict_dataset_per_auth_ids[a] = {}\n",
        "                        if itemid not in self.dict_dataset_per_auth_ids[a].keys():\n",
        "                          self.dict_dataset_per_auth_ids[a][itemid] = {}\n",
        "                        \n",
        "                        if a not in self.dict_dataset_per_auth_ids2.keys():\n",
        "                          self.dict_dataset_per_auth_ids2[a] = {}\n",
        "                        if itemid not in self.dict_dataset_per_auth_ids2[a].keys():\n",
        "                          self.dict_dataset_per_auth_ids2[a][itemid] = []\n",
        "                        \n",
        "                        if a not in self.dict_dataset_per_auth_masks.keys():\n",
        "                          self.dict_dataset_per_auth_masks[a] = {}\n",
        "                        if itemid not in self.dict_dataset_per_auth_masks[a].keys():\n",
        "                          self.dict_dataset_per_auth_masks[a][itemid] = {}\n",
        "                        if a not in self.dict_dataset_per_auth_masks2.keys():\n",
        "                          self.dict_dataset_per_auth_masks2[a] = {}\n",
        "                        if itemid not in self.dict_dataset_per_auth_masks2[a].keys():\n",
        "                          self.dict_dataset_per_auth_masks2[a][itemid] = []\n",
        "                        \n",
        "                        # for item in listDocsPerAuthor:\n",
        "                        #     for x in item:\n",
        "                        # counter1 = 0\n",
        "                        # for x in inputIDs1:\n",
        "                        #     counter1+=1\n",
        "                        #     if counter1 % 2 == 0 or counter1 == 1:\n",
        "                        self.dict_dataset_per_auth_ids[a][itemid]= inputIDs1\n",
        "                            # else:\n",
        "                            #    self.dict_dataset_per_auth_ids2[a][itemid].append(x)  \n",
        "                            #self.dict_dataset_per_auth_ids[a][itemid] = inputIDs1\n",
        "                        # for item in listMasksPerAuthor:\n",
        "                        #     for x in item:\n",
        "                        # counter2 = 0\n",
        "                        # for x in attn_masks1:\n",
        "                        #     counter2+=1\n",
        "                        #     if counter2 % 2 == 0 or counter2 == 1:\n",
        "                        self.dict_dataset_per_auth_masks[a][itemid]=attn_masks1\n",
        "                            # else:\n",
        "                              #  self.dict_dataset_per_auth_masks2[a][itemid].append(x) \n",
        "                        # self.dict_dataset_per_auth_masks[a][itemid] = attn_masks1\n",
        "                        itemid+=1       \n",
        "            if isval==False:\n",
        "              cnt+=1\n",
        "              if cnt > 35001:\n",
        "                break\n",
        "            else:\n",
        "              cnt+=1\n",
        "              if cnt == 10531:\n",
        "                break\n",
        "            \n",
        "            # for item in listDocsPerAuthor:\n",
        "                # counter1 = 1\n",
        "                        \n",
        "                # for x in item:\n",
        "                #     counter1+=1\n",
        "                    \n",
        "                #     if counter1 % 2 != 0:\n",
        "                #         self.dict_dataset_per_auth_ids[a].append(x)\n",
        "                #     else:\n",
        "                #         if len(item)<2:\n",
        "                #            self.dict_dataset_per_auth_ids[a].append(x) \n",
        "                        \n",
        "                       \n",
        "            # for item in listMasksPerAuthor:\n",
        "                        \n",
        "                # counter2 = 1\n",
        "                # for x in item:\n",
        "                #   counter2+=1\n",
        "                #   if counter2 % 2 != 0:\n",
        "                #        self.dict_dataset_per_auth_masks[a].append(x)\n",
        "                #   else:\n",
        "                #        if len(item)<2:\n",
        "                #            self.dict_dataset_per_auth_masks[a].append(x) \n",
        "            \n",
        "            # cnt+=1\n",
        "            # if cnt==116:\n",
        "            #   break\n",
        "            \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370,
          "referenced_widgets": [
            "30a6a50dd2944a7bafe7f332b118658b",
            "f9e93eabfd8a48d4824da8b1b21073d1",
            "a25329cb016e4d1e96248d61a9435e89",
            "f602361e43574417ace59246ba975d8a",
            "9fda2a68db4b47299e85a89dd7c5437a",
            "be9bd95f149c4ab1a638b4f75107b4f0",
            "8b2ae757dd6b497682bb671a392417d5",
            "8c4ea841b04b4364bc84f7e31548fa8a",
            "a07f64a9ba864e7faba1ba38a1fd2224",
            "3194dc2f5e4d49d4aaae2f55aed82c2b",
            "7a0025a147124158a7f003eb3dae1c65",
            "efed44431ebc435688b3eb0e46eae96b",
            "17930393d0ff4b2186dcd8217673d0c8",
            "fff7628804744f54a5359f381dc192be",
            "be45071aa3ac47b1a77b0191c4a36b6e",
            "0d037b45aa644edfbe493fc43cb2de68",
            "af4db6e4a20a48aeb9a2076c54107b33",
            "3845dbd442ae4bd18924a378c7f77148",
            "353776018a3048f8a92fc4498b44fd36",
            "7158da9b773740c290a6afff8024a373",
            "c8dadda9377a46edbfd562c778702f51",
            "f0c49329a5224525b90d964842e558c3",
            "8d49a7d3652941389d62dbc8a0d6c164",
            "9265ef91f4044b9c8ccfe2f8a2b3cf44",
            "2919a03c29de4629943a3d36a65d9fca",
            "3933840c51654e76ac71679e8814008a",
            "48e722c84ca740308fce0332ad4db56e",
            "9fa8cfbc351e4bd1941e5eb82ed78bf1",
            "d6b7000f324d4eac9dfa399d1895a556",
            "cdcafddb16b44dc4afaf720582c4a707",
            "fa0d7d1bf18d4f9198ba82d378f9eaa5",
            "f7a0978c9f484ab18396e073d2e8f5cf",
            "ac3ea17989f84f77976656bb5e94e708"
          ]
        },
        "id": "y6ZUXyOZSSXC",
        "outputId": "df588b3a-40ba-431d-b3fe-8c77c5a38889"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "52655\n",
            "42124\n",
            "10531\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30a6a50dd2944a7bafe7f332b118658b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efed44431ebc435688b3eb0e46eae96b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d49a7d3652941389d62dbc8a0d6c164",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "['ner']\n",
            "Parse per author...for  42124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/42124 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (5453 > 512). Running this sequence through the model will result in indexing errors\n",
            " 83%|████████▎ | 35001/42124 [28:36<05:49, 20.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
            "['ner']\n",
            "Parse per author...for  10531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10531 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4984 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|█████████▉| 10530/10531 [13:14<00:00, 13.25it/s]\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "datasets = [base_path+'/PAN20/PAN20_per_author_doc'\n",
        "             # 'dict_author_fandom_doc_val'\n",
        "            ]\n",
        "\n",
        "\n",
        "def shuffledict(big_dict):\n",
        "    keys = list(big_dict.keys())\n",
        "    \n",
        "    random.shuffle(keys)\n",
        "\n",
        "    Shuffled = dict()\n",
        "\n",
        "    for key in keys:\n",
        "        if key not in Shuffled.keys():\n",
        "            Shuffled[key] = {}\n",
        "        Shuffled[key]= big_dict[key]\n",
        "\n",
        "    return Shuffled\n",
        "\n",
        "def train_val_split(shuffledDict):\n",
        "\n",
        "\n",
        "    tmp_train = {}\n",
        "    tmp_val = {}\n",
        "    for k,v in shuffledDict.items():\n",
        "        \n",
        "        if len(v) >= 2:\n",
        "            n = int(0.85 * len(v))\n",
        "            if k not in tmp_train.keys():\n",
        "                tmp_train[k] = {}\n",
        "                if k not in tmp_val.keys():\n",
        "                    tmp_val[k] = {}\n",
        "\n",
        "                tmp_train[k] = v[:n]\n",
        "                tmp_val[k]=v[n:]\n",
        "        elif len(v) == 1:\n",
        "            if k not in tmp_train.keys():\n",
        "                tmp_train[k] = {}\n",
        "            tmp_train[k] = v\n",
        "\n",
        "    return tmp_train,tmp_val\n",
        "\n",
        "def train_val_split_perAuthor(shuffledDict):\n",
        "    tmp_train = {}\n",
        "    tmp_val = {}\n",
        "    n = int(0.80 * len(list(shuffledDict.keys())))\n",
        "    keys = list(shuffledDict.keys())\n",
        "    train_keys = keys[:n]\n",
        "    val_keys = keys[n:]\n",
        "    for key in train_keys:\n",
        "        if key not in shuffledDict.keys():\n",
        "            tmp_train[key] = {}\n",
        "        tmp_train[key]= shuffledDict[key]\n",
        "    for key in val_keys:\n",
        "        if key not in shuffledDict.keys():\n",
        "            tmp_val[key] = {}\n",
        "        tmp_val[key]= shuffledDict[key]\n",
        "\n",
        "    return tmp_train,tmp_val\n",
        "\n",
        "\n",
        "\n",
        "for dataset in datasets:\n",
        "\n",
        "\n",
        "    ######\n",
        "    # load\n",
        "    ######\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    print(len(list(dict_dataset.keys())))\n",
        "    shuffled_data = shuffledict(dict_dataset)\n",
        "    train,val = train_val_split_perAuthor(shuffled_data)\n",
        "    print(len(list(train.keys())))\n",
        "    print(len(list(val.keys())))\n",
        "    ############\n",
        "    # preprocess\n",
        "    ############\n",
        "    corpus = Corpus(dict_dataset=train)\n",
        "    corpus.parse_dictionary(False)\n",
        "    # corpus.create_pairs_per_author()#create_anchor_batches()#generatePairs()\n",
        "    # corpus.generateLastDict()\n",
        "    #######\n",
        "    # store\n",
        "    #######\n",
        "    with open(\"PAN20_512_Notrunc_perAuth_uncased_train_ids_list\", 'wb') as f:\n",
        "        pickle.dump(corpus.dict_dataset_per_auth_ids, f)\n",
        "    with open(\"PAN20_512_Notrunc_perAuth_uncased_train_masks_list\", 'wb') as f:\n",
        "        pickle.dump(corpus.dict_dataset_per_auth_masks, f)\n",
        "    # with open(\"PAN20_512_Notrunc_perAuth_onlyNums_cased_train_ids_list2\", 'wb') as f:\n",
        "    #     pickle.dump(corpus.dict_dataset_per_auth_ids2, f)\n",
        "    # with open(\"PAN20_512_Notrunc_perAuth_onlyNums_cased_train_masks_list2\", 'wb') as f:\n",
        "    #     pickle.dump(corpus.dict_dataset_per_auth_masks2, f)\n",
        "    # with open( \"PAN20_neg_pairs_512_train_overlapping\", 'wb') as f:\n",
        "    #     pickle.dump(corpus.dict_negatives_pairs, f)\n",
        "    del corpus\n",
        "    corpusval = Corpus(dict_dataset=val)\n",
        "    corpusval.parse_dictionary(True)\n",
        "    # corpusval.create_pairs_per_author_v2()#generatePairs()\n",
        "    # # corpusval.generateLastDict()\n",
        "    #\n",
        "    with open( \"PAN20_512_Notrunc_perAuth_uncased_val_ids_list\", 'wb') as f:\n",
        "        pickle.dump(corpusval.dict_dataset_per_auth_ids, f)\n",
        "    with open( \"PAN20_512_Notrunc_perAuth_uncased_val_masks_list\", 'wb') as f:\n",
        "        pickle.dump(corpusval.dict_dataset_per_auth_masks, f)\n",
        "    # with open( \"PAN20_512_Notrunc_perAuth_onlyNums_cased_val_ids_list2\", 'wb') as f:\n",
        "    #     pickle.dump(corpusval.dict_dataset_per_auth_ids2, f)\n",
        "    # with open( \"PAN20_512_Notrunc_perAuth_onlyNums_cased_val_masks_list2\", 'wb') as f:\n",
        "    #     pickle.dump(corpusval.dict_dataset_per_auth_masks2, f)\n",
        "    # with open(\"PAN20_train_512_trunc_cased_222\", 'wb') as f:\n",
        "    #     pickle.dump(corpus.dict_pairs_per_author, f)#dict_pairs_per_author\n",
        "    # # with open( \"PAN20_neg_pairs_512_train_overlapping\", 'wb') as f:\n",
        "    # #     pickle.dump(corpus.dict_negatives_pairs, f)\n",
        "    # del corpus\n",
        "    # corpusval = Corpus(dict_dataset=val)\n",
        "    # corpusval.parse_dictionary()\n",
        "    # corpusval.create_pairs_per_author()#create_anchor_batches()#generatePairs()\n",
        "    # # # corpusval.generateLastDict()    \n",
        "\n",
        "    # with open( \"PAN20_val_512_trunc_cased_222\", 'wb') as f:\n",
        "    #     pickle.dump(corpusval.dict_pairs_per_author, f)\n",
        "    # with open( \"PAN20_neg_pairs_512_val_overlapping\", 'wb') as f:\n",
        "    #     pickle.dump(corpusval.dict_negatives_pairs, f)\n",
        "    # with open( dataset + \"_processed_texts_test\", 'wb') as f:\n",
        "    #     pickle.dump(corpus.dict_dataset_per_auth_ids, f)\n",
        "    del corpusval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "go3-VvtjS2c1",
        "outputId": "2da6c503-f829-440c-d90a-1c6dfe56e562"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Thesis/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_val_masks_list'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shutil.copy('PAN20_512_Notrunc_perAuth_uncased_train_ids_list','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "shutil.copy('PAN20_512_Notrunc_perAuth_uncased_train_masks_list','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "shutil.copy('PAN20_512_Notrunc_perAuth_uncased_val_ids_list','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "shutil.copy('PAN20_512_Notrunc_perAuth_uncased_val_masks_list','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "\n",
        "# shutil.copy('PAN20_512_Notrunc_perAuth_onlyNums_cased_train_ids_list2','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "# shutil.copy('PAN20_512_Notrunc_perAuth_onlyNums_cased_train_masks_list2','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "# shutil.copy('PAN20_512_Notrunc_perAuth_onlyNums_cased_val_ids_list2','/content/drive/My Drive/Thesis/PAN20/ids_masks')\n",
        "# shutil.copy('PAN20_512_Notrunc_perAuth_onlyNums_cased_val_masks_list2','/content/drive/My Drive/Thesis/PAN20/ids_masks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vox8efTsY1e6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "\n",
        "\n",
        "def splitDataset(mydata_pos,mydata_neg):\n",
        "    n_pos = int(0.80 * len(mydata_pos.keys()))\n",
        "    n_neg = int(0.80 * len(mydata_neg.keys()))\n",
        "    data_train_pos = dict(list(mydata_pos.items())[:n_pos])\n",
        "    data_train_neg = dict(list(mydata_neg.items())[:n_neg])\n",
        "\n",
        "    data_val_pos = dict(list(mydata_pos.items())[n_pos:])\n",
        "    data_val_neg = dict(list(mydata_neg.items())[n_neg:])\n",
        "    # masks_val = dict(list(masks.items())[:n])\n",
        "\n",
        "    return data_train_pos,data_train_neg,data_val_pos,data_val_neg\n",
        "\n",
        "def generateLastDict(per_author_dataset_pos,per_author_dataset_neg):\n",
        "        generalID = 0\n",
        "        dict_All_pairs = dict()\n",
        "        for posid in per_author_dataset_pos.keys():\n",
        "            generalID+=1\n",
        "            if generalID not in dict_All_pairs.keys():\n",
        "                dict_All_pairs[generalID] = per_author_dataset_pos[posid]\n",
        "            # dict_All_pairs[generalID].append()\n",
        "        for negid in per_author_dataset_neg.keys():\n",
        "            generalID+=1\n",
        "            if generalID not in dict_All_pairs.keys():\n",
        "                dict_All_pairs[generalID] = per_author_dataset_neg[negid]\n",
        "            # dict_All_pairs[generalID].append()\n",
        "        return dict_All_pairs   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnHxNdfQWrDF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def convertdictToPandas(myDictIds,myDictMasks):\n",
        "    data = pd.DataFrame(columns=['authId','textId','chunkId','Text','Mask','Label'],index=range(sum([len(myDictIds[a][k]) for a in myDictIds.keys() for k in myDictIds[a].keys()])))\n",
        "    lab = 0\n",
        "    idx = 0\n",
        "    for auth in myDictIds.keys():\n",
        "        \n",
        "        # if len(list(myDictIds[auth].keys()))<2:\n",
        "        #     print(\"Texts < 2\")\n",
        "        #     print(auth)\n",
        "        #     print(myDictIds[auth].keys())\n",
        "        for tId in myDictIds[auth].keys():\n",
        "            \n",
        "            \n",
        "            if (len(myDictIds[auth][tId])<2):\n",
        "                print(\"chunks < 2\")\n",
        "                print(auth)\n",
        "                print(myDictIds[auth][tId])\n",
        "            for chId in range(0,len(myDictIds[auth][tId])):\n",
        "                \n",
        "                data.loc[idx].authId = auth\n",
        "                data.loc[idx].textId = tId\n",
        "                data.loc[idx].chunkId = chId\n",
        "                data.loc[idx].Text = myDictIds[auth][tId][chId]\n",
        "                data.loc[idx].Mask = myDictMasks[auth][tId][chId]\n",
        "                data.loc[idx].Label = lab\n",
        "                idx+=1\n",
        "                \n",
        "        lab+=1\n",
        "    return data\n",
        "train_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_onlyNums_uncased_train_ids_list'\n",
        "train_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_onlyNums_uncased_train_masks_list'\n",
        "val_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_onlyNums_uncased_val_ids_list'\n",
        "val_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_onlyNums_uncased_val_masks_list'\n",
        "trainIds,valIds,trainMasks,valMasks = getPickleFileInDict(train_ids),getPickleFileInDict(val_ids),getPickleFileInDict(train_masks),getPickleFileInDict(val_masks)\n",
        "\n",
        "data = convertdictToPandas(trainIds,trainMasks)\n",
        "dataval = convertdictToPandas(valIds,valMasks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo-ZJVgO-2G9",
        "outputId": "1b9708a1-e609-43c6-e1af-176bb5e23f85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "authId\n",
            "3315101    [tensor(101), tensor(2002), tensor(6760), tens...\n",
            "3315101    [tensor(101), tensor(2006), tensor(1996), tens...\n",
            "3315101    [tensor(101), tensor(22822), tensor(12171), te...\n",
            "3315101    [tensor(101), tensor(1037), tensor(10257), ten...\n",
            "3315101    [tensor(101), tensor(1999), tensor(1037), tens...\n",
            "                                 ...                        \n",
            "419103     [tensor(101), tensor(1029), tensor(1000), tens...\n",
            "419103     [tensor(101), tensor(1010), tensor(1037), tens...\n",
            "419103     [tensor(101), tensor(23960), tensor(1012), ten...\n",
            "419103     [tensor(101), tensor(1010), tensor(2025), tens...\n",
            "419103     [tensor(101), tensor(7224), tensor(1012), tens...\n",
            "Name: Text, Length: 124964, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# data = data.set_index(['authId'])\n",
        "# print(data.isnull().attributesvalues.any())\n",
        "print(getattr(data, 'Text'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oja-EN66WTlo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dict_per_auth_ids,\n",
        "                 \n",
        "                 \n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file (string): the path to the IMDB62 Dataset txt file\n",
        "        \"\"\"\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of texts.\n",
        "        self.per_author_dataset = dict_per_auth_ids\n",
        "        self.length = len(self.per_author_dataset)\n",
        "        self.authors = self.per_author_dataset.authId.unique().tolist()\n",
        "        self.per_author_dataset = self.per_author_dataset.set_index(['authId'])\n",
        "    def __len__(self):\n",
        "        #return sum([len(self.per_author_dataset[a]) for a in self.per_author_dataset.keys()])\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        n_auth = len(self.authors)\n",
        "        \n",
        "        auth = self.authors[idx%n_auth]\n",
        "\n",
        "        # print(auth)\n",
        "        textid1,textid2 = self.per_author_dataset.loc[auth].sample(2).textId.tolist()\n",
        "        \n",
        "        counter = 0\n",
        "        while textid1 == textid2:\n",
        "            counter+=1\n",
        "            if counter > 5:\n",
        "               break \n",
        "            textid2 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "        # print()\n",
        "        # print(auth)\n",
        "        if textid1 == textid2:\n",
        "            tmpDataframe = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"]\n",
        "            while len(tmpDataframe)<=2:\n",
        "                  i = random.choice(range(0,n_auth))\n",
        "                  auth = self.authors[i%n_auth]\n",
        "                  textid1 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "                  textid2 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "                  counter = 0\n",
        "                  print(auth)\n",
        "                  while textid1 == textid2:\n",
        "                        counter+=1\n",
        "                        if counter > 5:\n",
        "                            break \n",
        "                  textid2 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "                  tmpDataframe = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"]\n",
        "                  \n",
        "            anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "            anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "            replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "            replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "        else:\n",
        "            anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "            anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "            replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "            replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "\n",
        "        # if len(self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"])>2:\n",
        "\n",
        "            \n",
        "        # else:\n",
        "        #     i = random.choice(range(0,n_auth))\n",
        "        #     auth = self.authors[i%n_auth]\n",
        "        #     textid1 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "        #     textid2 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "        #     anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "        #     anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "        #     replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "        #     replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "        # if hasattr(self.per_author_dataset[self.per_author_dataset.textId == textid2].iloc[auth].sample(1),'Text'):\n",
        "        #     try:\n",
        "        #       anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].iloc[auth].sample(1).Text.tolist()[0]\n",
        "        #       anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].iloc[auth].sample(1).Mask.tolist()[0]\n",
        "        #       replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].iloc[auth].sample(1).Text.tolist()[0]\n",
        "        #       replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].iloc[auth].sample(1).Mask.tolist()[0]\n",
        "        #     except AttributeError:\n",
        "        #       print( self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth].sample(1))\n",
        "        #       # i = random.choice(range(0,n_auth))\n",
        "        #       # auth = self.authors[i%n_auth]\n",
        "        #       # textid1 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "        #       # textid2 = self.per_author_dataset.loc[auth].sample(1).textId.tolist()[0]\n",
        "        #       # anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth].sample(1).Text.tolist()[0]\n",
        "        #       # anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].loc[auth].sample(1).Mask.tolist()[0]\n",
        "             \n",
        "        #       # replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth].sample(1).Text.tolist()[0]\n",
        "        #       # replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth].sample(1).Mask.tolist()[0]\n",
        "        # else:\n",
        "        #     textid1 = self.per_author_dataset.iloc[auth].sample(1).textId.tolist()[0]\n",
        "        #     textid2 = self.per_author_dataset.iloc[auth].sample(1).textId.tolist()[0]\n",
        "        #     anchor = self.per_author_dataset[self.per_author_dataset.textId == textid1].iloc[auth].sample(1).Text.tolist()[0]\n",
        "        #     anchorMask = self.per_author_dataset[self.per_author_dataset.textId == textid1].iloc[auth].sample(1).Mask.tolist()[0]\n",
        "        #     replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].iloc[auth].sample(1).Text.tolist()[0]\n",
        "        #     replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].iloc[auth].sample(1).Mask.tolist()[0]\n",
        "      \n",
        "        cnt=0\n",
        "    \n",
        "        while torch.equal(anchor,replica):\n",
        "              cnt+=1\n",
        "             \n",
        "              if cnt > 50:\n",
        "                 break \n",
        "              replica = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Text\"].sample(1).tolist()[0]\n",
        "              replicaMask = self.per_author_dataset[self.per_author_dataset.textId == textid2].loc[auth,\"Mask\"].sample(1).tolist()[0]\n",
        "        label = self.per_author_dataset.loc[auth].sample(1).Label.tolist()[0]\n",
        "        \n",
        "        return anchor,anchorMask, replica,replicaMask,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrFFzb9IjQw7",
        "outputId": "f80f71fc-36cf-41b3-b44a-e71edc1ec70c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2202\n",
            "1122\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset_train = AuthorshipDataset(data)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=64,shuffle=True)\n",
        "\n",
        "dataset_val = AuthorshipDataset(dataval)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=64,shuffle=True)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PaKtHsK9j3EX",
        "outputId": "b79d69f6-db50-44ac-a574-a4c898f38db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "tensor([[  101,  2665, 11344,  ...,  2008, 14085,   102],\n",
            "        [  101,  2174,  1010,  ...,  2311,  1998,   102],\n",
            "        [  101,  1045,  2001,  ...,  1006,  1045,   102],\n",
            "        ...,\n",
            "        [  101,  2021,  1045,  ...,  1998,  3147,   102],\n",
            "        [  101,  1000,  2054,  ...,  1996,  3460,   102],\n",
            "        [  101,  2135,  2004,  ...,  2021,  2017,   102]])\n",
            "2\n",
            "tensor([[ 101, 2008, 4933,  ..., 9655, 1012,  102],\n",
            "        [ 101, 8937, 1029,  ..., 3342, 2101,  102],\n",
            "        [ 101, 4390, 1012,  ..., 1012, 1045,  102],\n",
            "        ...,\n",
            "        [ 101, 2617, 1010,  ..., 2023, 1012,  102],\n",
            "        [ 101, 2014, 2050,  ..., 2016, 2001,  102],\n",
            "        [ 101, 1997, 1996,  ..., 1037, 2146,  102]])\n",
            "3\n",
            "tensor([[  101,  1996, 23160,  ...,   999,  1066,   102],\n",
            "        [  101,  2129,  2002,  ...,  1037,  2261,   102],\n",
            "        [  101,  2058,  1996,  ...,  2032,  2067,   102],\n",
            "        ...,\n",
            "        [  101, 13675, 10929,  ...,  2008,  8673,   102],\n",
            "        [  101,  1000,  2292,  ...,  2017,  1000,   102],\n",
            "        [  101,  6865,  2006,  ...,  2002,  3844,   102]])\n",
            "4\n",
            "tensor([[  101,  1996,  8975,  ...,  1000,  1996,   102],\n",
            "        [  101,  1012,  4593,  ...,  1010,  2027,   102],\n",
            "        [  101, 22168,  2011,  ..., 25804,  1010,   102],\n",
            "        ...,\n",
            "        [  101,  2145,  2042,  ...,  2172,  1012,   102],\n",
            "        [  101,  2579,  1999,  ...,  4066,  1997,   102],\n",
            "        [  101, 12812,  1012,  ...,  2145,  2009,   102]])\n",
            "5\n",
            "tensor([[  101,  2633,  3876,  ...,  1000,  2026,   102],\n",
            "        [  101, 11113,  9711,  ...,  1011,  4566,   102],\n",
            "        [  101,  1012,  2104,  ...,  2619,  2842,   102],\n",
            "        ...,\n",
            "        [  101,  1996,  3829,  ...,  2000,  2191,   102],\n",
            "        [  101,  7204,  2000,  ..., 27930,  2001,   102],\n",
            "        [  101,  1037,  2797,  ...,  2001,  2000,   102]])\n",
            "6\n",
            "tensor([[  101,  2008,  2003,  ...,  3335,  3848,   102],\n",
            "        [  101,  2356,  2034,  ...,  2272,   999,   102],\n",
            "        [  101,  3182,  1012,  ...,  1056,  8572,   102],\n",
            "        ...,\n",
            "        [  101,  2979,  2041,  ...,  2428,  3480,   102],\n",
            "        [  101,  5186,  3928,  ..., 14704,  1012,   102],\n",
            "        [  101,  2210,  4942,  ...,  7842, 27390,   102]])\n",
            "7\n",
            "tensor([[  101,  5604,  2129,  ...,  4067,  2017,   102],\n",
            "        [  101,  1000,  9061,  ...,  1000,  1055,   102],\n",
            "        [  101,  2023,  2052,  ...,  1012,  1000,   102],\n",
            "        ...,\n",
            "        [  101,  1996,  7120,  ...,  3900, 24728,   102],\n",
            "        [  101,  1010,  1000,  ...,  1012,  1012,   102],\n",
            "        [  101,  3062,  4333,  ...,  1045,  1000,   102]])\n",
            "8\n",
            "tensor([[  101,  1011, 12411,  ..., 19960, 10362,   102],\n",
            "        [  101,  1000,  2222,  ...,  2466,   999,   102],\n",
            "        [  101,  1029,  1000,  ...,  2016,  2052,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  1055,  ...,  1998,  2016,   102],\n",
            "        [  101,  1012,  4415,  ...,  1000,  1000,   102],\n",
            "        [  101,  1010,  2304,  ...,  7293,  1012,   102]])\n",
            "9\n",
            "tensor([[  101,  2242,  2007,  ..., 13653,  1000,   102],\n",
            "        [  101,  7210,  2153,  ...,  1000,  1055,   102],\n",
            "        [  101,  1011,  2879,  ...,  2054,  2018,   102],\n",
            "        ...,\n",
            "        [  101,  1045,  2001,  ...,  1000,  2053,   102],\n",
            "        [  101,  1012,  1012,  ...,  2229,  1998,   102],\n",
            "        [  101,  2288,  1999,  ...,  2017,  2330,   102]])\n",
            "10\n",
            "tensor([[  101,  2005,  2085,  ..., 27205,  1998,   102],\n",
            "        [  101,  3134,  1010,  ...,  2002, 10290,   102],\n",
            "        [  101,  1037,  2868,  ...,  3592,  2012,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  1056,  ...,  2074,  2425,   102],\n",
            "        [  101,  2006, 16349,  ...,  2817,  1012,   102],\n",
            "        [  101,  1012,  2016,  ...,  4248,  2007,   102]])\n",
            "11\n",
            "tensor([[  101,  6476,  1996,  ...,  1000,  1045,   102],\n",
            "        [  101,  3327,  3114,  ...,  4582,  2039,   102],\n",
            "        [  101,  2123,  1000,  ...,  2128,  4550,   102],\n",
            "        ...,\n",
            "        [  101,  2014,  1000,  ...,  2304, 11094,   102],\n",
            "        [  101,  2540,  2001,  ...,  2114,  8672,   102],\n",
            "        [  101,  1000,  1056,  ...,  2019,  4495,   102]])\n",
            "12\n",
            "tensor([[  101,  8529,  2072,  ...,  1000,  1000,   102],\n",
            "        [  101, 15379,  2015,  ...,  2217,  1012,   102],\n",
            "        [  101,  2002,  5489,  ...,  2000,  2022,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  1056,  ...,  2031,  4895,   102],\n",
            "        [  101,  2026,  7804,  ...,  2054,  2003,   102],\n",
            "        [  101,  7842,  3211,  ..., 21195,  2242,   102]])\n",
            "13\n",
            "tensor([[  101,  2005,  1037,  ...,  2359,  2000,   102],\n",
            "        [  101,  2002, 19700,  ...,  5251,  1012,   102],\n",
            "        [  101, 15747,  1997,  ...,  1998,  5293,   102],\n",
            "        ...,\n",
            "        [  101,  2129,  2000,  ...,   999,  2002,   102],\n",
            "        [  101,  1012,  4098,  ...,  2031,  2000,   102],\n",
            "        [  101,  9154,  1012,  ...,  2002,  2071,   102]])\n",
            "14\n",
            "tensor([[  101,  7710,  2056,  ...,  1998,  2027,   102],\n",
            "        [  101,  2010,  2159,  ..., 13954,  1997,   102],\n",
            "        [  101,  1996,  2332,  ...,  2078,  1012,   102],\n",
            "        ...,\n",
            "        [  101,  3266,  2000,  ...,  1999,  1996,   102],\n",
            "        [  101,  2027,  2411,  ...,  2017,  1000,   102],\n",
            "        [  101,  2001,  1996,  ...,  2304,  1012,   102]])\n",
            "15\n",
            "tensor([[  101,  8328,  1000,  ...,  2023,  2453,   102],\n",
            "        [  101,  2017,  5959,  ...,  2052,  3786,   102],\n",
            "        [  101,  1012,  1000,  ..., 23210,  1997,   102],\n",
            "        ...,\n",
            "        [  101,  1037,  2353,  ...,  3214,  2008,   102],\n",
            "        [  101,  2101,  1000,  ...,  2305,  2035,   102],\n",
            "        [  101,  2009,  2001,  ...,  2033,  1010,   102]])\n",
            "16\n",
            "tensor([[ 101, 1000, 6721,  ..., 3596, 1997,  102],\n",
            "        [ 101, 2009, 1012,  ..., 2042, 2188,  102],\n",
            "        [ 101, 2356, 7135,  ..., 2002, 2357,  102],\n",
            "        ...,\n",
            "        [ 101, 1000, 1055,  ..., 2755, 2017,  102],\n",
            "        [ 101, 2611, 2056,  ..., 1012, 2197,  102],\n",
            "        [ 101, 2043, 2014,  ..., 2001, 3294,  102]])\n",
            "17\n",
            "tensor([[  101,  1000,  2017,  ...,  1011,  6007,   102],\n",
            "        [  101,  2062,  1010,  ...,  2023,   999,   102],\n",
            "        [  101,  3300,  1010,  ...,  2000, 12342,   102],\n",
            "        ...,\n",
            "        [  101,  1996,  5430,  ..., 14526, 14526,   102],\n",
            "        [  101,  2007,  5142,  ...,  2126,  2027,   102],\n",
            "        [  101,  2002,  2179,  ...,  2190,  3033,   102]])\n",
            "18\n",
            "tensor([[  101,  4775,  1045,  ..., 17665,  7365,   102],\n",
            "        [  101,  2354,  2055,  ...,  1037,  8364,   102],\n",
            "        [  101,  2009,  1012,  ...,  2025,  2157,   102],\n",
            "        ...,\n",
            "        [  101,  3281,  1012,  ...,  3201,  2005,   102],\n",
            "        [  101,  2000,  1996,  ...,  2019,  8144,   102],\n",
            "        [  101,  2017,  2056,  ...,  2060,  1012,   102]])\n",
            "19\n",
            "tensor([[  101,  2024,  2017,  ...,  6069,  2175,   102],\n",
            "        [  101,  2210,  2978,  ...,  1012,  4365,   102],\n",
            "        [  101,  2907,  1997,  ...,  1056,  2031,   102],\n",
            "        ...,\n",
            "        [  101,  1010,  2017,  ...,  2017,  5838,   102],\n",
            "        [  101,  1012,  1000,  ..., 18087,  2098,   102],\n",
            "        [  101,  1012, 27126,  ...,  6802, 10097,   102]])\n",
            "20\n",
            "tensor([[  101,  1012,  2138,  ...,  1012,  1000,   102],\n",
            "        [  101,  6336,  2010,  ...,  2010, 21243,   102],\n",
            "        [  101,  2158, 12671,  ...,  1000,  2054,   102],\n",
            "        ...,\n",
            "        [  101,  3070,  3909,  ...,  2017,  2071,   102],\n",
            "        [  101,  2954,  1029,  ...,  1996, 13785,   102],\n",
            "        [  101,  1996,  2365,  ...,  2699,  2000,   102]])\n",
            "21\n",
            "tensor([[  101,  1000,  7210,  ...,  2009,  4165,   102],\n",
            "        [  101,  2227,  1012,  ...,  2033,  2003,   102],\n",
            "        [  101,  2014,  5600,  ...,  2034,  1011,   102],\n",
            "        ...,\n",
            "        [  101,  6886,  7624,  ...,  2032,  1996,   102],\n",
            "        [  101,  2017,  1998,  ...,  2017,  4011,   102],\n",
            "        [  101,  2023,  2518,  ..., 11514,  3366,   102]])\n",
            "22\n",
            "tensor([[  101,  1012,  1012,  ...,  4299,  2000,   102],\n",
            "        [  101,  2009,  1998,  ...,  2021,  2035,   102],\n",
            "        [  101,  1000,  5916,  ..., 12075,  2011,   102],\n",
            "        ...,\n",
            "        [  101,  2061,  8873,  ...,  2054,  2001,   102],\n",
            "        [  101,  1012,  1000,  ..., 20786,  1010,   102],\n",
            "        [  101,  1010,  1000,  ...,  2159,  2855,   102]])\n",
            "23\n",
            "tensor([[  101,  1010,  1000,  ...,  2227,  1999,   102],\n",
            "        [  101,  1000, 15068,  ..., 13164,  9864,   102],\n",
            "        [  101,  2256,  4363,  ...,  2079,  2000,   102],\n",
            "        ...,\n",
            "        [  101,  4180,  2000,  ...,  1000,  1000,   102],\n",
            "        [  101,  1012,  1012,  ...,  2014,  3648,   102],\n",
            "        [  101,  7182,  3815,  ...,  5510,  1012,   102]])\n",
            "24\n",
            "tensor([[  101,  6376,  2010,  ...,  1000,  2016,   102],\n",
            "        [  101,  1012,  2057,  ..., 14526,  2487,   102],\n",
            "        [  101,  2918, 15725,  ...,  2428,  2397,   102],\n",
            "        ...,\n",
            "        [  101,  2388,  1010,  ...,  3391,  2729,   102],\n",
            "        [  101,  4204,  8038,  ...,  5731,  1012,   102],\n",
            "        [  101, 22788,  2046,  ...,  1012,  1005,   102]])\n",
            "25\n",
            "tensor([[  101,  4728,  1010,  ...,  2369,  2032,   102],\n",
            "        [  101,  2013,  2037,  ...,  2911,  1012,   102],\n",
            "        [  101,  1010,  1998,  ..., 24297,  1029,   102],\n",
            "        ...,\n",
            "        [  101,  3087,  2077,  ...,  2134,  1000,   102],\n",
            "        [  101, 11530,  3726,  ...,  1999,  1996,   102],\n",
            "        [  101,  2000,  1037,  ...,  1000,  1000,   102]])\n",
            "26\n",
            "tensor([[ 101, 2919, 1029,  ..., 2008, 2002,  102],\n",
            "        [ 101, 1997, 2010,  ..., 1996, 2613,  102],\n",
            "        [ 101, 1996, 2543,  ..., 6700, 2004,  102],\n",
            "        ...,\n",
            "        [ 101, 2000, 2022,  ..., 4091, 1012,  102],\n",
            "        [ 101, 4283, 2061,  ..., 1012, 1000,  102],\n",
            "        [ 101, 2002, 2409,  ..., 1996, 3341,  102]])\n",
            "27\n",
            "tensor([[  101,  1997,  1996,  ...,  1998,  1996,   102],\n",
            "        [  101,  1012, 10097,  ..., 10535,  1998,   102],\n",
            "        [  101,  3043,  2007,  ...,  4122,  2000,   102],\n",
            "        ...,\n",
            "        [  101,  2010,  2606,  ..., 15106,  1997,   102],\n",
            "        [  101,  1000,  2008,  ...,  3984,  1045,   102],\n",
            "        [  101, 14899,  7698,  ...,  2006,  1996,   102]])\n",
            "28\n",
            "tensor([[  101,  5785,  2030,  ...,  2000,  3275,   102],\n",
            "        [  101,  4668,  1010,  ...,  2341,  1045,   102],\n",
            "        [  101,  2009,  1012,  ...,  2001,  2178,   102],\n",
            "        ...,\n",
            "        [  101,  2157,  1010,  ...,  1056,  2074,   102],\n",
            "        [  101,  2849,  1012,  ...,  1012,  1000,   102],\n",
            "        [  101,  1011, 10872,  ...,  1000,  1056,   102]])\n",
            "29\n",
            "tensor([[  101,  2269,  6051,  ...,  2013,  2010,   102],\n",
            "        [  101,  2790,  2066,  ...,  4931,  1010,   102],\n",
            "        [  101,  3246,  1012,  ..., 28667, 29313,   102],\n",
            "        ...,\n",
            "        [  101,  2731,  1012,  ...,  1029,  1000,   102],\n",
            "        [  101, 20403,  1998,  ...,  2729,  1997,   102],\n",
            "        [  101,  1055,  1037,  ...,  3457,  3062,   102]])\n",
            "30\n",
            "tensor([[  101,  3046,  2000,  ...,  1045,  2156,   102],\n",
            "        [  101,  1000,  1055,  ..., 10346,  1012,   102],\n",
            "        [  101,  4063,  2063,  ...,  1066,  1066,   102],\n",
            "        ...,\n",
            "        [  101,  2022,  6230,  ...,  2040,  2024,   102],\n",
            "        [  101,  1012,  2130,  ...,  2132,  1010,   102],\n",
            "        [  101, 11968, 11649,  ...,  2081,  2172,   102]])\n",
            "31\n",
            "tensor([[  101,  2046,  2026,  ...,  1045,  2018,   102],\n",
            "        [  101,  2357,  2010,  ...,  2033,  1029,   102],\n",
            "        [  101,  2009,  2001,  ...,  1012,  1012,   102],\n",
            "        ...,\n",
            "        [  101, 21281,  2015,  ...,  8434,  4596,   102],\n",
            "        [  101,  4699,  1999,  ...,  2040,  2027,   102],\n",
            "        [  101,  2079,  2001,  ...,  1012,  2027,   102]])\n",
            "32\n",
            "tensor([[  101,  1045,  2020,  ...,  1998,  3959,   102],\n",
            "        [  101, 11118,  2213,  ..., 25033,  1011,   102],\n",
            "        [  101,  2023,  2001,  ...,  1045,  1000,   102],\n",
            "        ...,\n",
            "        [  101,  1055,  2157,  ...,  1012, 23883,   102],\n",
            "        [  101,  1997,  1996,  ...,  1011,  1000,   102],\n",
            "        [  101, 14196,  2023,  ...,  2042, 14855,   102]])\n",
            "33\n",
            "tensor([[  101,  2010,  2402,  ...,  1055, 11174,   102],\n",
            "        [  101,  1000,  2040,  ...,  2812,  2011,   102],\n",
            "        [  101,  1012,  1005,  ...,  1000,  1055,   102],\n",
            "        ...,\n",
            "        [  101,  2893,  3730,  ...,  8507,  2001,   102],\n",
            "        [  101,  2118,  1012,  ...,  4092,  1012,   102],\n",
            "        [  101,  7453,  9301,  ...,  2046,  1996,   102]])\n",
            "34\n",
            "tensor([[  101,  2025,  2000,  ...,  2017, 12489,   102],\n",
            "        [  101,  1010,  1000,  ...,  2227,  2004,   102],\n",
            "        [  101,  2134,  1000,  ...,  1010,  2025,   102],\n",
            "        ...,\n",
            "        [  101,  2025,  5599,  ...,  1000,  1012,   102],\n",
            "        [  101,  1000,  2624,  ...,  2505,  1012,   102],\n",
            "        [  101,  1998,  7104,  ...,  2009, 10076,   102]])\n",
            "35\n",
            "tensor([[  101,  4859, 12298,  ...,  4851,  6656,   102],\n",
            "        [  101, 17111,  2080,  ..., 13627,  2014,   102],\n",
            "        [  101,  2008,  2261,  ...,  1998,  8537,   102],\n",
            "        ...,\n",
            "        [  101,  1012,  2383,  ...,  2055,  2073,   102],\n",
            "        [  101,  2023,  2391,  ...,  2675,  1012,   102],\n",
            "        [  101,  3945,  2005,  ..., 21038,  4741,   102]])\n",
            "36\n",
            "tensor([[  101,  2035,  4941,  ...,  1010,  4820,   102],\n",
            "        [  101, 20754,  8235,  ...,  2071,  2514,   102],\n",
            "        [  101,  2005,  5599,  ...,  2005,  1996,   102],\n",
            "        ...,\n",
            "        [  101,  1045,  2123,  ...,  1012,  2002,   102],\n",
            "        [  101,  1037, 18386,  ...,  1012,  8482,   102],\n",
            "        [  101,  2060,  1012,  ...,  2227,  1012,   102]])\n",
            "37\n",
            "tensor([[  101, 11005,  1012,  ...,  6551, 16903,   102],\n",
            "        [  101,  2611, 16688,  ...,  1012,  1000,   102],\n",
            "        [  101, 25212,  2243,  ...,  2014,  5458,   102],\n",
            "        ...,\n",
            "        [  101,  3062,  2067,  ...,  2008,  2097,   102],\n",
            "        [  101,  1056,  2908,  ...,  7603,  1012,   102],\n",
            "        [  101,  3350,  2004,  ...,  3954,  2018,   102]])\n",
            "38\n",
            "tensor([[ 101, 2312, 5445,  ..., 1012, 1045,  102],\n",
            "        [ 101, 2044, 2008,  ..., 2009, 1000,  102],\n",
            "        [ 101, 2441, 1010,  ..., 3170, 1012,  102],\n",
            "        ...,\n",
            "        [ 101, 9715, 1012,  ..., 2014, 3086,  102],\n",
            "        [ 101, 1056, 3543,  ..., 4046, 1996,  102],\n",
            "        [ 101, 2017, 2048,  ..., 1012, 2633,  102]])\n",
            "39\n",
            "tensor([[  101, 10236,  2039,  ...,  2031,  4771,   102],\n",
            "        [  101,  4571,  4263,  ...,  8916,  2068,   102],\n",
            "        [  101,  2402,  3040,  ...,  2010,  3086,   102],\n",
            "        ...,\n",
            "        [  101,  1010,  1000,  ...,  2113,  2053,   102],\n",
            "        [  101,  1996,  2048,  ...,  2063,  1012,   102],\n",
            "        [  101, 13876,  2545,  ...,  1010,  1000,   102]])\n",
            "40\n",
            "tensor([[  101,  1996,  6616,  ...,  2409,  2033,   102],\n",
            "        [  101,  1056,  2031,  ...,  2339,  2017,   102],\n",
            "        [  101,  1055,  2868,  ...,  2053,  4168,   102],\n",
            "        ...,\n",
            "        [  101, 16717, 12717,  ...,  9875,  2638,   102],\n",
            "        [  101,  3103, 14707,  ...,   999, 10556,   102],\n",
            "        [  101,  2000,  4148,  ...,  5959,  4426,   102]])\n",
            "41\n",
            "tensor([[  101,  2046,  2048,  ...,  2228,  1045,   102],\n",
            "        [  101, 10206,  1012,  ...,  1045,  2768,   102],\n",
            "        [  101,  8744,  2075,  ...,  4439,  2033,   102],\n",
            "        ...,\n",
            "        [  101,  2172,  1012,  ...,  1996,  3086,   102],\n",
            "        [  101,  2126,  2006,  ...,  1012,  1012,   102],\n",
            "        [  101,  1010,  2115,  ...,  2033,  1012,   102]])\n",
            "42\n",
            "tensor([[  101,  9369,  1010,  ...,  1997,  1996,   102],\n",
            "        [  101,  1000,  3159,  ...,  2132,  1012,   102],\n",
            "        [  101,  1000,  2085,  ...,  2317,  4091,   102],\n",
            "        ...,\n",
            "        [  101,  2015,  2024,  ..., 22889,  8490,   102],\n",
            "        [  101,  2131,  2041,  ...,  1000,  1055,   102],\n",
            "        [  101,  2123,  1000,  ...,  1010,  1998,   102]])\n",
            "43\n",
            "tensor([[  101,  1996,  4432,  ...,  2043,  5109,   102],\n",
            "        [  101,  1012,  1005,  ...,  4593,  2027,   102],\n",
            "        [  101,  1010, 11865,  ...,  2130,  2004,   102],\n",
            "        ...,\n",
            "        [  101,  2005,  2032,  ...,  2438,  1012,   102],\n",
            "        [  101,  1045,  3856,  ...,  2012,  2149,   102],\n",
            "        [  101,  1000,  2128,  ...,  2056,  5629,   102]])\n",
            "44\n",
            "tensor([[  101,  1040,  2175,  ...,  2046,  1037,   102],\n",
            "        [  101, 16124,  1012,  ...,  3114,  2002,   102],\n",
            "        [  101,  1006,  3524,  ...,  2004,  2002,   102],\n",
            "        ...,\n",
            "        [  101,  2222,  2031,  ...,  3008,  1010,   102],\n",
            "        [  101,  2085,  1010,  ...,  2370,  1010,   102],\n",
            "        [  101,  2000,  1996,  ..., 21354,  1012,   102]])\n",
            "45\n",
            "tensor([[  101,  1012,  2002,  ...,  1011,  1066,   102],\n",
            "        [  101,  2356,  4862,  ...,  2080,   999,   102],\n",
            "        [  101,  1056,  3404,  ...,  1000,  1055,   102],\n",
            "        ...,\n",
            "        [  101,  2039,  1012,  ...,  1012,  1000,   102],\n",
            "        [  101,  2010,  2051,  ...,  2232,  2023,   102],\n",
            "        [  101,  2016, 11206,  ...,  2359,  2000,   102]])\n",
            "46\n",
            "tensor([[  101,  1012,  2017,  ...,  2601, 23867,   102],\n",
            "        [  101,  2307,   999,  ...,   999,  2003,   102],\n",
            "        [  101,  1996,  2217,  ...,  2085,  2026,   102],\n",
            "        ...,\n",
            "        [  101, 17387,  2205,  ...,  2557,  1012,   102],\n",
            "        [  101,  2023,  2003,  ...,  6206,  9590,   102],\n",
            "        [  101,  3409,  1998,  ...,  2005, 15774,   102]])\n",
            "47\n",
            "tensor([[  101,  1000,  2092,  ..., 18232,  1012,   102],\n",
            "        [  101,  2105,  1996,  ...,  2145,  2431,   102],\n",
            "        [  101,  6518,  1997,  ...,  1997, 19135,   102],\n",
            "        ...,\n",
            "        [  101,  1996,  2160,  ...,  1029,  1000,   102],\n",
            "        [  101,  2043,  1045,  ...,  1000,  1040,   102],\n",
            "        [  101,  3560, 19119,  ...,  1011,  2124,   102]])\n",
            "48\n",
            "tensor([[  101,  6016,  2014,  ..., 14969,  1998,   102],\n",
            "        [  101,  2016,  2018,  ...,  1996,  2390,   102],\n",
            "        [  101,  1000,  2054,  ..., 15572,  1012,   102],\n",
            "        ...,\n",
            "        [  101,  6108,  2094,  ...,  1012,  5517,   102],\n",
            "        [  101,  1000,  2053,  ...,  2594, 20954,   102],\n",
            "        [  101,  6155, 23235,  ...,  4904,  1010,   102]])\n",
            "49\n",
            "tensor([[ 101, 2159, 1012,  ..., 7224, 2015,  102],\n",
            "        [ 101, 1000, 1055,  ..., 3160, 2016,  102],\n",
            "        [ 101, 2043, 1045,  ..., 1056, 9190,  102],\n",
            "        ...,\n",
            "        [ 101, 3241, 1998,  ..., 2090, 6014,  102],\n",
            "        [ 101, 1010, 3383,  ..., 2152, 1998,  102],\n",
            "        [ 101, 1998, 2011,  ..., 2041,  999,  102]])\n",
            "50\n",
            "tensor([[  101,  2002,  2245,  ...,  2043,  2008,   102],\n",
            "        [  101,  2058,  2153,  ...,  1056,  2215,   102],\n",
            "        [  101,  2007,  1037,  ...,  2026, 13941,   102],\n",
            "        ...,\n",
            "        [  101,  2043,  2027,  ...,   999,  1007,   102],\n",
            "        [  101, 19863,  9057,  ...,  1996,  3437,   102],\n",
            "        [  101,  2011,  1996,  ...,  1012,  1045,   102]])\n",
            "51\n",
            "tensor([[  101,  2017,  2215,  ..., 21183,  2063,   102],\n",
            "        [  101,  4741,  2005,  ...,  1056,  2017,   102],\n",
            "        [  101,  2075,  2370,  ...,  2129,  2106,   102],\n",
            "        ...,\n",
            "        [  101,  2327,  3238,  ...,  3114,  1012,   102],\n",
            "        [  101,  2138,  2027,  ...,  1056,  2514,   102],\n",
            "        [  101,   999,  2129,  ...,  1999,  2014,   102]])\n",
            "52\n",
            "tensor([[ 101, 3975, 2039,  ..., 1000, 2054,  102],\n",
            "        [ 101, 2085, 1012,  ..., 9273, 1010,  102],\n",
            "        [ 101, 1010, 4843,  ..., 3902, 1010,  102],\n",
            "        ...,\n",
            "        [ 101, 2044, 2082,  ..., 3819, 1012,  102],\n",
            "        [ 101, 1045, 3984,  ..., 2115, 3739,  102],\n",
            "        [ 101, 2077, 2032,  ..., 1000, 2339,  102]])\n",
            "53\n",
            "tensor([[  101,  2729,  1997,  ...,  1045,  2113,   102],\n",
            "        [  101,  2354,  2008,  ...,  3531,  1012,   102],\n",
            "        [  101,  6820,  2912,  ...,  3086,  1029,   102],\n",
            "        ...,\n",
            "        [  101,  1010,  2016,  ...,  2000,  4952,   102],\n",
            "        [  101,  5125,  2481,  ...,  1010,  2009,   102],\n",
            "        [  101,  2801,  1997,  ...,  1010, 22308,   102]])\n",
            "54\n",
            "tensor([[ 101, 1997, 2035,  ..., 2408, 1996,  102],\n",
            "        [ 101, 2128, 2183,  ..., 2595, 1012,  102],\n",
            "        [ 101, 6603, 2054,  ..., 2074, 2175,  102],\n",
            "        ...,\n",
            "        [ 101, 2091, 1996,  ..., 1012, 2202,  102],\n",
            "        [ 101, 4933, 2169,  ..., 2012, 2560,  102],\n",
            "        [ 101, 2043, 1045,  ..., 2064, 3305,  102]])\n",
            "55\n",
            "tensor([[  101, 13899,  1998,  ...,  1012,  2481,   102],\n",
            "        [  101,  1998, 16097,  ...,  1012,  2004,   102],\n",
            "        [  101,  1012,  2016,  ...,  1012, 26708,   102],\n",
            "        ...,\n",
            "        [  101,  2002,  2134,  ...,  3589,  1010,   102],\n",
            "        [  101,  1012,  4283,  ...,  1049,  3201,   102],\n",
            "        [  101,  2598,  1998,  ...,  2409,  9079,   102]])\n",
            "56\n",
            "tensor([[ 101, 1000, 1055,  ..., 3180, 2154,  102],\n",
            "        [ 101, 4191, 1012,  ..., 5790, 2005,  102],\n",
            "        [ 101, 2341, 1012,  ..., 2849, 1998,  102],\n",
            "        ...,\n",
            "        [ 101, 2168, 3632,  ..., 2115, 3460,  102],\n",
            "        [ 101, 2356, 1010,  ..., 1000, 6174,  102],\n",
            "        [ 101, 2002, 2109,  ..., 1000, 1056,  102]])\n",
            "57\n",
            "tensor([[  101,  2311,  1029,  ...,  2131,  1999,   102],\n",
            "        [  101,  1000,  2128,  ...,  2007,  2060,   102],\n",
            "        [  101,  1012,  4312,  ...,  3663,  1012,   102],\n",
            "        ...,\n",
            "        [  101,  3733,  1029,  ...,  2004,  2027,   102],\n",
            "        [  101,  2113,  1012,  ...,  1012,  1033,   102],\n",
            "        [  101, 24785,  2000,  ...,  2341,  3844,   102]])\n",
            "58\n",
            "tensor([[ 101, 1010, 2021,  ..., 1012, 1000,  102],\n",
            "        [ 101, 2412, 2777,  ..., 1010, 1998,  102],\n",
            "        [ 101, 2035, 1999,  ..., 2026, 4524,  102],\n",
            "        ...,\n",
            "        [ 101, 4121, 4799,  ..., 2464, 2014,  102],\n",
            "        [ 101, 3105, 2017,  ..., 1012, 1037,  102],\n",
            "        [ 101, 1012, 1037,  ..., 7388, 1012,  102]])\n",
            "59\n",
            "tensor([[  101,  2062,  2335,  ..., 24646, 14795,   102],\n",
            "        [  101,  9445,  1012,  ...,  3468,  1012,   102],\n",
            "        [  101,  2008,  3697,  ...,  5223,  5223,   102],\n",
            "        ...,\n",
            "        [  101, 13801,  3856,  ...,  1998,  2633,   102],\n",
            "        [  101,  1045,  2074,  ..., 20042,  2066,   102],\n",
            "        [  101,  5679,  1029,  ...,  2330,  3869,   102]])\n",
            "60\n",
            "tensor([[  101,  1999,  1000,  ...,  1996,  4690,   102],\n",
            "        [  101,  2023,  2521,  ...,  2242,  2003,   102],\n",
            "        [  101,  2767,  1012,  ...,  2843,  1997,   102],\n",
            "        ...,\n",
            "        [  101,  1025,  2012,  ...,  1012,  2009,   102],\n",
            "        [  101, 22966,  4933,  ...,  5870,  2018,   102],\n",
            "        [  101,  2383,  1996,  ...,  1996,  2547,   102]])\n",
            "61\n",
            "tensor([[  101,  1000,  1056,  ...,  3389,  4576,   102],\n",
            "        [  101,  2000, 27764,  ...,  2812,  2666,   102],\n",
            "        [  101,  3495,  2104,  ...,  1996, 18821,   102],\n",
            "        ...,\n",
            "        [  101,  1012,  2002,  ...,  5281,  2077,   102],\n",
            "        [  101,  2007,   999,  ...,  1045,  2180,   102],\n",
            "        [  101,  2033,  1000,  ...,  1996,  2482,   102]])\n",
            "62\n",
            "tensor([[  101, 18405,  3641,  ...,  6099,  1000,   102],\n",
            "        [  101,  2033,  2000,  ...,  2711,  1010,   102],\n",
            "        [  101,  2023,  2210,  ...,  2014,  1029,   102],\n",
            "        ...,\n",
            "        [  101,  2699,  2021,  ...,  2171,   999,   102],\n",
            "        [  101,  2071,  2022,  ...,  1045,  2097,   102],\n",
            "        [  101,  1998,  1037,  ...,  2021,  2008,   102]])\n",
            "63\n",
            "tensor([[  101,  1000,  2009,  ...,  1000, 10556,   102],\n",
            "        [  101,  3654,  4046,  ...,  1000,  1045,   102],\n",
            "        [  101,  2057,  2012,  ...,  2485,  2000,   102],\n",
            "        ...,\n",
            "        [  101,  2275,  3064,  ..., 11111,  1010,   102],\n",
            "        [  101,  2008,  2018,  ...,  2000,  1996,   102],\n",
            "        [  101,  2073,  2002,  ...,  6609,   999,   102]])\n",
            "64\n",
            "tensor([[  101,  1000,  1045,  ...,  2014, 11111,   102],\n",
            "        [  101,  2073,  2016,  ...,  2057,  2514,   102],\n",
            "        [  101,  6012,  2066,  ...,  5586,  2054,   102],\n",
            "        ...,\n",
            "        [  101,  1996,  3947,  ...,  2000,  2868,   102],\n",
            "        [  101,  2129,  2521,  ...,  1012,  2498,   102],\n",
            "        [  101,  1000,  2059,  ...,  1045,  3855,   102]])\n",
            "65\n",
            "tensor([[  101, 14421,  2000,  ...,  2100,  1998,   102],\n",
            "        [  101,  2295,  2016,  ...,  4999,  2054,   102],\n",
            "        [  101,  7656,  2357,  ...,  2347,  1000,   102],\n",
            "        ...,\n",
            "        [  101,  3407, 15060,  ...,  1029,  1000,   102],\n",
            "        [  101,  2006,  6318,  ...,  2012,  2033,   102],\n",
            "        [  101, 14046,  1012,  ...,  2053,   999,   102]])\n",
            "66\n",
            "tensor([[  101,  1996,  2793,  ...,  1012,  1012,   102],\n",
            "        [  101,  3786,  2032,  ...,  1012,  5125,   102],\n",
            "        [  101,  2002,  2018,  ...,  2027,  4741,   102],\n",
            "        ...,\n",
            "        [  101,  4084,  2000,  ...,  5506,  2012,   102],\n",
            "        [  101,  1045,  2428,  ...,  1012,  1000,   102],\n",
            "        [  101, 29179,  1998,  ...,  4562,  2009,   102]])\n",
            "67\n",
            "tensor([[  101,  3046,  1998,  ...,  2012,  1996,   102],\n",
            "        [  101,  2031,  1010,  ..., 10963,  2014,   102],\n",
            "        [  101, 15544,  2078,  ...,  1010,  1000,   102],\n",
            "        ...,\n",
            "        [  101,  5956,  3262,  ...,  1024,  1000,   102],\n",
            "        [  101,  1000,  1055,  ...,  2001,  2785,   102],\n",
            "        [  101,  2411,  2038,  ...,  1010,  6016,   102]])\n",
            "68\n",
            "tensor([[  101,  1010,  2365,  ...,  1010, 20521,   102],\n",
            "        [  101,  2000,  2037,  ...,  2002,  2768,   102],\n",
            "        [  101,  3480,  1012,  ..., 10930, 25032,   102],\n",
            "        ...,\n",
            "        [  101,  1010,  2146,  ...,  2179,  1012,   102],\n",
            "        [  101,  2056,  2004,  ...,   999,   999,   102],\n",
            "        [  101,  1996,  5108,  ...,  1996, 17078,   102]])\n",
            "69\n",
            "tensor([[  101,  1000,  2893,  ...,  2681,  2033,   102],\n",
            "        [  101,  2066,  1037,  ...,  2200,  2574,   102],\n",
            "        [  101,  2196,  2165,  ...,  1997,  2154,   102],\n",
            "        ...,\n",
            "        [  101,  7231, 14545,  ...,  1010,  2130,   102],\n",
            "        [  101,  3093,  1998,  ...,  2000,  2298,   102],\n",
            "        [  101,  3631,  2041,  ...,  2012,  1996,   102]])\n",
            "70\n",
            "tensor([[  101, 18900,  2239,  ..., 14547,  1010,   102],\n",
            "        [  101,  1997,  2026,  ...,  1998,  2387,   102],\n",
            "        [  101, 14688,  2000,  ...,  1040,  3582,   102],\n",
            "        ...,\n",
            "        [  101,  2031,  2042,  ...,  2003,  2054,   102],\n",
            "        [  101,  2007,  2115,  ...,  1012,  2002,   102],\n",
            "        [  101,  2013, 23244,  ..., 12541,  9307,   102]])\n",
            "71\n",
            "tensor([[  101,  2958,  1012,  ...,  9078,  1010,   102],\n",
            "        [  101,  2790,  2061,  ...,  1045,  2018,   102],\n",
            "        [  101,  2150, 15560,  ...,  1012,  2292,   102],\n",
            "        ...,\n",
            "        [  101,  2054,  2002,  ...,  1000,  4918,   102],\n",
            "        [  101, 16242,  1010,  ...,  2069,  1037,   102],\n",
            "        [  101,  2035,  1037,  ...,  2790,  2000,   102]])\n",
            "72\n",
            "tensor([[  101,  2023,  2003,  ...,  2052,  5256,   102],\n",
            "        [  101,  1000,  1056,  ...,  1996, 17060,   102],\n",
            "        [  101,  2376, 21397,  ..., 15736,  1012,   102],\n",
            "        ...,\n",
            "        [  101,  2043,  2027,  ...,  2243,  2481,   102],\n",
            "        [  101,  1012,  1000,  ...,  4242,  1998,   102],\n",
            "        [  101,  2026,  2611,  ...,  5657,  2389,   102]])\n",
            "73\n",
            "tensor([[  101,  1010, 27503,  ...,  1011,  1000,   102],\n",
            "        [  101, 22975,  2232,  ...,  3782,  2038,   102],\n",
            "        [  101,  1010, 24599,  ...,  2008,  1996,   102],\n",
            "        ...,\n",
            "        [  101,  1996, 12727,  ...,  2031,  2042,   102],\n",
            "        [  101,  1012,  1012,  ...,  2074,  2018,   102],\n",
            "        [  101,  2002,  2481,  ...,  8619,  1997,   102]])\n",
            "74\n",
            "tensor([[  101,  7929,  1010,  ...,  1010,  2016,   102],\n",
            "        [  101,  4666,  2041,  ...,  2008,  2154,   102],\n",
            "        [  101, 17466,  2979,  ...,  3198, 19957,   102],\n",
            "        ...,\n",
            "        [  101,  2017,  2052,  ...,  2061,  2008,   102],\n",
            "        [  101,  2012,  2026,  ...,  1000,  1012,   102],\n",
            "        [  101,  3148,  1012,  ...,  7967,  1011,   102]])\n",
            "75\n",
            "tensor([[  101, 10623,  2989,  ...,  4777, 10203,   102],\n",
            "        [  101,  2091,  1996,  ...,  1000,  2002,   102],\n",
            "        [  101,  1000,  2428,  ...,  2017,   999,   102],\n",
            "        ...,\n",
            "        [  101, 15344,  2814,  ...,  1024,  2175,   102],\n",
            "        [  101,  3182,  2014,  ...,  2003,  1037,   102],\n",
            "        [  101,  2023,  1012,  ...,  2026,  2166,   102]])\n",
            "76\n",
            "tensor([[  101,  1045,  1000,  ...,  2027,  2031,   102],\n",
            "        [  101,  2023,  1012,  ...,  6592,  1029,   102],\n",
            "        [  101,  1998,  4627,  ...,  2002,  2056,   102],\n",
            "        ...,\n",
            "        [  101,  2203, 25121,  ...,  1010,  2798,   102],\n",
            "        [  101,  2022,  1029,  ...,  1000,  1045,   102],\n",
            "        [  101,  3066,  1010,  ...,  1012,  1000,   102]])\n",
            "77\n",
            "tensor([[  101,  2023,  3957,  ...,  3047,  1029,   102],\n",
            "        [  101,  2178,  2154,  ...,  1008, 20605,   102],\n",
            "        [  101,  1012,  4067,  ...,  2005,  2082,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  2017,  ...,  1012,  2010,   102],\n",
            "        [  101,  1997,  2068,  ...,  2070,  1997,   102],\n",
            "        [  101,  7877,  1010,  ...,  2012,  1996,   102]])\n",
            "78\n",
            "tensor([[  101,  1012,  1000,  ...,  2057,  2442,   102],\n",
            "        [  101,  1012,  1000,  ..., 15560,  1007,   102],\n",
            "        [  101, 15978,  1012,  ...,  2592,  2006,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  4165,  ...,  2657,  1012,   102],\n",
            "        [  101,  4405,  1000,  ...,  1045,  2481,   102],\n",
            "        [  101,  2159,  1012,  ...,  2228,  1012,   102]])\n",
            "79\n",
            "tensor([[  101, 21468,  2046,  ...,  3859,  1012,   102],\n",
            "        [  101,  8117,  2532,  ...,  1000, 15384,   102],\n",
            "        [  101, 10000,  8271,  ..., 10000,  2018,   102],\n",
            "        ...,\n",
            "        [  101,  2001,  3564,  ...,  2037,  5656,   102],\n",
            "        [  101,  1012,  2320,  ...,  1996, 28938,   102],\n",
            "        [  101,  2039,  1037,  ...,  4895,  7368,   102]])\n",
            "80\n",
            "tensor([[  101,  3046, 13053,  ...,  2821, 18243,   102],\n",
            "        [  101,  1029,  1000,  ...,  1058,  1058,   102],\n",
            "        [  101,  1012,  1000,  ...,  2626,  1037,   102],\n",
            "        ...,\n",
            "        [  101,  2296,  4535,  ...,  2033,  4755,   102],\n",
            "        [  101,  2132,  1010,  ...,  2021,  3402,   102],\n",
            "        [  101,  1012,  2320,  ...,  1029,  1000,   102]])\n",
            "81\n",
            "tensor([[  101,  1011,  2138,  ...,  2000,  3153,   102],\n",
            "        [  101,  1000,  2222,  ..., 19487,  1012,   102],\n",
            "        [  101,  2477,  1012,  ...,  2347,  1000,   102],\n",
            "        ...,\n",
            "        [  101,  1045,  2134,  ...,  2009,  2001,   102],\n",
            "        [  101,  2046,  1996,  ...,  1045,  1000,   102],\n",
            "        [  101,  5306,  2225,  ...,  2017,  1000,   102]])\n",
            "82\n",
            "tensor([[  101,  2066,  1996,  ...,  1010,  1037,   102],\n",
            "        [  101, 18243,  4115,  ...,  1012,  2014,   102],\n",
            "        [  101,  2009,  2211,  ...,  2876,  1056,   102],\n",
            "        ...,\n",
            "        [  101,  1012,  1012,  ...,  1000,  7710,   102],\n",
            "        [  101,  1997,  1996,  ..., 28057,  1012,   102],\n",
            "        [  101, 13878,  1010,  ...,  1010,  1000,   102]])\n",
            "83\n",
            "tensor([[  101,  2019, 17471,  ...,  1056,  2228,   102],\n",
            "        [  101,  1045,  2123,  ...,  1005,  9342,   102],\n",
            "        [  101,  2135,  4359,  ...,  2064,  2467,   102],\n",
            "        ...,\n",
            "        [  101,  2371,  2066,  ...,  2026,  5615,   102],\n",
            "        [  101,  1010,  1045,  ...,  2100,  6868,   102],\n",
            "        [  101,  1996,  2417,  ...,  2156,  1010,   102]])\n",
            "84\n",
            "tensor([[  101,  3959,  2002,  ...,  3477, 10259,   102],\n",
            "        [  101,  2041,  1996,  ...,  2046,  1996,   102],\n",
            "        [  101,  2009,  2001,  ...,  2293,  1997,   102],\n",
            "        ...,\n",
            "        [  101,  1000,  5629,  ...,  1012,  1012,   102],\n",
            "        [  101,  1037,  2117,  ...,  1000,  9318,   102],\n",
            "        [  101,  1000, 26114,  ...,  2017,  2145,   102]])\n",
            "85\n",
            "tensor([[  101,  2004,  1996,  ...,  2068,  1037,   102],\n",
            "        [  101,  2009,  2069,  ...,  1012,  2028,   102],\n",
            "        [  101,  2014,  4301,  ...,  3427,  1996,   102],\n",
            "        ...,\n",
            "        [  101, 21552,  2875,  ...,  3464,  1997,   102],\n",
            "        [  101,  2008,  1000,  ...,  3460,  1012,   102],\n",
            "        [  101, 16191,  2007,  ...,  1007,  1000,   102]])\n",
            "86\n",
            "tensor([[  101,  2044,  3038,  ...,  1037,  2300,   102],\n",
            "        [  101,  2071,  2130,  ...,  1000,  1049,   102],\n",
            "        [  101, 18068,  2091,  ..., 13433,  2615,   102],\n",
            "        ...,\n",
            "        [  101,  4062,  2356,  ...,  1012,  5477,   102],\n",
            "        [  101,  2051,  1010,  ...,  9955,  2046,   102],\n",
            "        [  101,  2508,  1998,  ...,  1000,  1055,   102]])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f7cdb2bf30b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manchorMask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplicaMask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mstep\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-0da3dbe58a55>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0manchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextid1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0manchorMask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextid1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mreplica\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextid2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mreplicaMask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_author_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextId\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtextid2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "step = 0\n",
        "for anchor,anchorMask, replica,replicaMask,label in train_dataloader:\n",
        "    step+=1\n",
        "    print(step)\n",
        "    print(anchor)\n",
        "    # print(replica)\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2xB3WrO6gge"
      },
      "source": [
        "##Actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hbWmCsh3uBb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dict_per_auth_ids,\n",
        "                 dict_per_auth_masks,\n",
        "                 pan22\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file (string): the path to the IMDB62 Dataset txt file\n",
        "        \"\"\"\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of texts.\n",
        "        self.per_author_dataset = dict_per_auth_ids\n",
        "        self.per_author_dataset_masks = dict_per_auth_masks\n",
        "        # self.base_rate = base_rate\n",
        "        self.authors = list(self.per_author_dataset.keys())\n",
        "        self.PAN22 = pan22\n",
        "    def __len__(self):\n",
        "        #return sum([len(self.per_author_dataset[a]) for a in self.per_author_dataset.keys()])\n",
        "        if self.PAN22: \n",
        "          return sum([len(self.per_author_dataset[a][k][j]) for a in self.per_author_dataset.keys() for k in self.per_author_dataset[a].keys() for j in self.per_author_dataset[a][k].keys()])\n",
        "        else:\n",
        "          return sum([len(self.per_author_dataset[a][k]) for a in self.per_author_dataset.keys() for k in self.per_author_dataset[a].keys()])\n",
        "    def __getitem__(self, idx):\n",
        "        n_auth = len(self.authors)\n",
        "        \n",
        "        auth = self.authors[idx%n_auth]\n",
        "\n",
        "        # print(auth)\n",
        "        textid1 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "        textid2 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "\n",
        "        counter = 0\n",
        "        while textid1 == textid2:\n",
        "            counter+=1\n",
        "            if counter > 5:\n",
        "               break \n",
        "            textid2 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "        \n",
        "        if self.PAN22==True:\n",
        "          type1 = random.choice(list(self.per_author_dataset[auth][textid1].keys()))\n",
        "          type2 = random.choice(list(self.per_author_dataset[auth][textid2].keys()))\n",
        "          \n",
        "          cnt = 0\n",
        "          while type1 == type2:\n",
        "              cnt+=1\n",
        "              if cnt == 10:\n",
        "                  break\n",
        "              type2 = random.choice(list(self.per_author_dataset[auth][textid2].keys()))\n",
        "          chunkid1 = random.choice(range(0,len(self.per_author_dataset[auth][textid1][type1])))\n",
        "          chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2][type2])))\n",
        "          text1 = self.per_author_dataset[auth][textid1][type1][chunkid1]\n",
        "          mask1 = self.per_author_dataset_masks[auth][textid1][type1][chunkid1]\n",
        "          text2 = self.per_author_dataset[auth][textid2][type2][chunkid2]\n",
        "          mask2 = self.per_author_dataset_masks[auth][textid2][type2][chunkid2]\n",
        "          cnt=0\n",
        "          # print(text1)\n",
        "          while torch.equal(text1,text2):\n",
        "                cnt+=1\n",
        "              \n",
        "                if cnt > 50:\n",
        "                  break\n",
        "                chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2][type2])))    \n",
        "                text2 = self.per_author_dataset[auth][textid2][type2][chunkid2]\n",
        "                mask2 = self.per_author_dataset_masks[auth][textid2][type2][chunkid2]\n",
        "        else:\n",
        "            chunkid1 = random.choice(range(0,len(self.per_author_dataset[auth][textid1])))\n",
        "            chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2])))\n",
        "            text1 = self.per_author_dataset[auth][textid1][chunkid1]\n",
        "            mask1 = self.per_author_dataset_masks[auth][textid1][chunkid1]\n",
        "            text2 = self.per_author_dataset[auth][textid2][chunkid2]\n",
        "            mask2 = self.per_author_dataset_masks[auth][textid2][chunkid2]\n",
        "            cnt=0\n",
        "            # print(text1)\n",
        "            while torch.equal(text1,text2):\n",
        "                  cnt+=1\n",
        "                \n",
        "                  if cnt > 50:\n",
        "                    break\n",
        "                  chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2])))    \n",
        "                  text2 = self.per_author_dataset[auth][textid2][chunkid2]\n",
        "                  mask2 = self.per_author_dataset_masks[auth][textid2][chunkid2]\n",
        "        return text1,mask1,text2,mask2    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN585Kk35mYy",
        "outputId": "dccca918-b21e-48d8-88aa-9aabcc2d141c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5204\n",
            "682\n"
          ]
        }
      ],
      "source": [
        "# train_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_train_ids_list'\n",
        "# train_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_train_masks_list'\n",
        "# val_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_val_ids_list'\n",
        "# val_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_val_masks_list'\n",
        "\n",
        "train_ids = base_path+'/PAN22/train/dict_auth_type_pan22_train_PAN22_256_train_overlap_T5_memo-essays_POS_ids'\n",
        "train_masks = base_path+'/PAN22/train/dict_auth_type_pan22_train_PAN22_256_train_overlap_T5_memo-essays_POS_masks'\n",
        "val_ids = base_path+'/PAN22/val/dict_auth_type_pan22_train_PAN22_256_val_overlap_T5_memo-essays_POS_ids'\n",
        "val_masks = base_path+'/PAN22/val/dict_auth_type_pan22_train_PAN22_256_val_overlap_T5_memo-essays_POS_masks'\n",
        "# train_ids = base_path+'/PAN15/PAN15_128_train_uncased_ids'\n",
        "# train_masks = base_path+'/PAN15/PAN15_128_train_uncased_masks'\n",
        "# val_ids = base_path+'/PAN15/PAN15_128_val_uncased_ids'\n",
        "# val_masks = base_path+'/PAN15/PAN15_128_val_uncased_masks'\n",
        "\n",
        "trainIds,valIds,trainMasks,valMasks = getPickleFileInDict(train_ids),getPickleFileInDict(val_ids),getPickleFileInDict(train_masks),getPickleFileInDict(val_masks)\n",
        "\n",
        "dataset_train = AuthorshipDataset(trainIds,trainMasks,True)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16,shuffle=True)\n",
        "\n",
        "dataset_val = AuthorshipDataset(valIds,valMasks,True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=16,shuffle=True)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzhlNyjV56bk",
        "outputId": "cdb5b4a4-42f1-41e5-ec8c-1d4b950d3b9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "tensor([[ 101, 7632, 1010,  ...,    0,    0,    0],\n",
            "        [ 101, 2065, 2017,  ..., 6643, 1012,  102],\n",
            "        [ 101, 7632, 1016,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 1045, 2031,  ..., 2990, 1012,  102],\n",
            "        [ 101, 2106, 2025,  ..., 8145, 2951,  102],\n",
            "        [ 101, 1012, 5713,  ..., 2920, 1999,  102]])\n"
          ]
        }
      ],
      "source": [
        "step = 0\n",
        "for anchor,anchorMask, replica,replicaMask in train_dataloader:\n",
        "    step+=1\n",
        "    print(step)\n",
        "    print(anchor)\n",
        "    break\n",
        "    # print(replica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWcU6YTHrGzY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dict_per_auth_ids,\n",
        "                 dict_per_auth_masks,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file (string): the path to the IMDB62 Dataset txt file\n",
        "        \"\"\"\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of texts.\n",
        "        self.per_author_dataset = dict_per_auth_ids\n",
        "        self.per_author_dataset_masks = dict_per_auth_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        #return sum([len(self.per_author_dataset[a]) for a in self.per_author_dataset.keys()])\n",
        "        return sum([len(self.per_author_dataset[a][k]) for a in self.per_author_dataset.keys() for k in self.per_author_dataset[a].keys()])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we want this to work with contrastive, so sample on the author level\n",
        "        auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "       \n",
        "        if np.random.uniform() < self.base_rate:\n",
        "            # this is a same_author sample\n",
        "            # make sure the auth has multiple samples\n",
        "            cnt=0\n",
        "            while len(list(self.per_author_dataset[auth1].keys())) < 2:\n",
        "                cnt+=1\n",
        "                # if cnt>100:\n",
        "                #    break \n",
        "                auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "            i1 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "            i2 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "            # i1= random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "            # i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "           \n",
        "            while i1==i2: #not chunks from same text\n",
        "               \n",
        "              #  counter+=1\n",
        "              #  i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "               i2 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "               \n",
        "            ii1 = random.choice(range(0,len(self.per_author_dataset[auth1][i1])))\n",
        "            ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "            # cnt = 0\n",
        "            # while ii1!=ii2:\n",
        "            #     cnt+=1\n",
        "            #     if cnt > 50:\n",
        "            #        break\n",
        "            #     if ii1 >= len(self.per_author_dataset[auth1][i2]):\n",
        "            #        if ii1 == len(self.per_author_dataset[auth1][i1])-1:\n",
        "            #           ii2 = len(self.per_author_dataset[auth1][i2])-1\n",
        "            #        else:\n",
        "            #           ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "            #        break\n",
        "            #     ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "    \n",
        "            text1 =  self.per_author_dataset[auth1][i1][ii1]\n",
        "            text2 = self.per_author_dataset[auth1][i2][ii2]\n",
        "            mask1 =  self.per_author_dataset_masks[auth1][i1][ii1]\n",
        "            mask2 = self.per_author_dataset_masks[auth1][i2][ii2]\n",
        "           \n",
        "            # make sure the texts are different\n",
        "            counter = 0\n",
        "            im_confused_counter = 0\n",
        "            auths_tried = 0\n",
        "            while torch.equal(text1,text2):\n",
        "                \n",
        "                \n",
        "                i2 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "                ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "                text2 = self.per_author_dataset[auth1][i2][ii2]\n",
        "                mask2 = self.per_author_dataset_masks[auth1][i2][ii2]\n",
        "                counter += 1\n",
        "                if counter > 100:\n",
        "                    # these texts are the same, get a different author\n",
        "                    while len(list(self.per_author_dataset[auth1].keys())) < 2:\n",
        "                        \n",
        "                        auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "                    auths_tried += 1\n",
        "                    # i1 = i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "                    i1 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "                    i2 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "                    ii1 = random.choice(range(0,len(self.per_author_dataset[auth1][i1])))\n",
        "                    ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "                    # while ii1!=ii2:\n",
        "\n",
        "                    #     if ii1 >= len(self.per_author_dataset[auth1][i2]):\n",
        "                    #       if ii1 == len(self.per_author_dataset[auth1][i1])-1:\n",
        "                    #          ii2 = len(self.per_author_dataset[auth1][i2])-1\n",
        "                    #       else:\n",
        "                    #          ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "                          \n",
        "                    #       break\n",
        "                       \n",
        "                    #     ii2 = random.choice(range(0,len(self.per_author_dataset[auth1][i2])))\n",
        "                    text1 =  self.per_author_dataset[auth1][i1][ii1]\n",
        "                    text2 = self.per_author_dataset[auth1][i2][ii2]\n",
        "                    mask1 =  self.per_author_dataset_masks[auth1][i1][ii1]\n",
        "                    mask2 = self.per_author_dataset_masks[auth1][i2][ii2]\n",
        "                    if auths_tried > 100:\n",
        "                        text1 =  self.per_author_dataset[auth1][i1][ii1]\n",
        "                        text2 = self.per_author_dataset[auth1][i2][ii2]\n",
        "                        mask1 =  self.per_author_dataset_masks[auth1][i1][ii1]\n",
        "                        mask2 = self.per_author_dataset_masks[auth1][i2][ii2]\n",
        "                        break\n",
        "                        #assert False, \"we've got problems, can't find a different text from same author\"\n",
        "                    counter = 0\n",
        "                if im_confused_counter > 1000:\n",
        "                    \n",
        "                    \n",
        "                    assert False, \"we've got problems, stuck in this same-author loop again.\"\n",
        "                im_confused_counter += 1\n",
        "            auth2 = auth1\n",
        "            label = 1\n",
        "\n",
        "        else:\n",
        "            # this is a different author sample\n",
        "            auth2 = auth1\n",
        "            while auth1 == auth2:\n",
        "                \n",
        "                auth2 = random.choice(list(self.per_author_dataset.keys()))\n",
        "            # now get a text from both authors\n",
        "            # i1 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "            # i2 = random.choice(range(0,len(self.per_author_dataset[auth2])))\n",
        "            i1 = random.choice(list(self.per_author_dataset[auth1].keys()))\n",
        "            i2 = random.choice(list(self.per_author_dataset[auth2].keys()))\n",
        "            ii1 = random.choice(range(0,len(self.per_author_dataset[auth1][i1])))\n",
        "            ii2 = random.choice(range(0,len(self.per_author_dataset[auth2][i2])))\n",
        "            # cnt=0\n",
        "            # while ii1!=ii2:\n",
        "            #       cnt+=1\n",
        "            #       if cnt > 50:\n",
        "            #         break\n",
        "            #       if ii1 >= len(self.per_author_dataset[auth2][i2]):\n",
        "            #          if ii1 == len(self.per_author_dataset[auth1][i1])-1:\n",
        "            #             ii2 = len(self.per_author_dataset[auth2][i2])-1\n",
        "            #          else:\n",
        "            #             ii2 = random.choice(range(0,len(self.per_author_dataset[auth2][i2])))\n",
        "            #          break\n",
        "            text1 =  self.per_author_dataset[auth1][i1][ii1]\n",
        "            text2 = self.per_author_dataset[auth2][i2][ii2]\n",
        "            mask1 =  self.per_author_dataset_masks[auth1][i1][ii1]\n",
        "            mask2 = self.per_author_dataset_masks[auth2][i2][ii2]\n",
        "            label = 0\n",
        "        return text1,mask1,text2,mask2,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Qel8eePFLsj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dict_per_auth_ids,\n",
        "                 dict_per_auth_masks,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file (string): the path to the IMDB62 Dataset txt file\n",
        "        \"\"\"\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of texts.\n",
        "        self.per_author_dataset = dict_per_auth_ids\n",
        "        self.per_author_dataset_masks = dict_per_auth_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return sum([len(self.per_author_dataset[a]) for a in self.per_author_dataset.keys()])\n",
        "    def createPairsWithSEPToken(self,text1,mask1,text2,mask2):\n",
        "        tmplistids1 = text1.tolist()\n",
        "        tmplistids1.insert(0,101) #101 for BERT\n",
        "        tmplistids1.insert(len(tmplistids1),102)\n",
        "        tmpMasks1 = mask1.tolist()\n",
        "        tmpMasks1.insert(0,1)\n",
        "        tmpMasks1.insert(len(tmpMasks1),1)\n",
        "        tokenType1 = torch.LongTensor()\n",
        "        tokenType1 =  F.pad(tokenType1, (0,len(tmplistids1)), \"constant\", 0)\n",
        "\n",
        "\n",
        "        tmplistids2 = text2.tolist()\n",
        "        tmplistids2.insert(len(tmplistids2),102)\n",
        "        tmpMasks2 = mask2.tolist()\n",
        "        tmpMasks2.insert(len(tmpMasks2),1)\n",
        "        tokenType2 = torch.LongTensor()\n",
        "        tokenType2 =  F.pad(tokenType2, (0,len(tmplistids2)), \"constant\", 1)\n",
        "        \n",
        "        finalIds = tmplistids1 + tmplistids2\n",
        "        finalMasks = tmpMasks1 + tmpMasks2\n",
        "\n",
        "\n",
        "        finalIds = torch.LongTensor(finalIds)\n",
        "        finalMasks = torch.LongTensor(finalMasks)\n",
        "        finalTypes = torch.cat((tokenType1,tokenType2)) \n",
        "        return finalIds, finalMasks, finalTypes\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # we want this to work with contrastive, so sample on the author level\n",
        "        auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "       \n",
        "        if np.random.uniform() < self.base_rate:\n",
        "            # this is a same_author sample\n",
        "            # make sure the auth has multiple samples\n",
        "            while len(self.per_author_dataset[auth1]) < 2:\n",
        "                auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "            i1= i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "\n",
        "            text1 =  self.per_author_dataset[auth1][i1]\n",
        "            text2 = self.per_author_dataset[auth1][i2]\n",
        "            mask1 =  self.per_author_dataset_masks[auth1][i1]\n",
        "            mask2 = self.per_author_dataset_masks[auth1][i2]\n",
        "           \n",
        "            # make sure the texts are different\n",
        "            counter = 0\n",
        "            im_confused_counter = 0\n",
        "            auths_tried = 0\n",
        "            while torch.equal(text1,text2):\n",
        "                i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "                text2 = self.per_author_dataset[auth1][i2]\n",
        "                mask2 = self.per_author_dataset_masks[auth1][i2]\n",
        "                counter += 1\n",
        "                if counter > 100:\n",
        "                    # these texts are the same, get a different author\n",
        "                    while len(self.per_author_dataset[auth1]) < 2:\n",
        "                        auth1 = random.choice(list(self.per_author_dataset.keys()))\n",
        "                    auths_tried += 1\n",
        "                    i1 = i2 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "                    text1 =  self.per_author_dataset[auth1][i1]\n",
        "                    text2 = self.per_author_dataset[auth1][i2]\n",
        "                    mask1 =  self.per_author_dataset_masks[auth1][i1]\n",
        "                    mask2 = self.per_author_dataset_masks[auth1][i2]\n",
        "                    if auths_tried > 50:\n",
        "                        assert False, \"we've got problems, can't find a different text from same author\"\n",
        "                    counter = 0\n",
        "                if im_confused_counter > 10000:\n",
        "                    \n",
        "                    \n",
        "                    assert False, \"we've got problems, stuck in this same-author loop again.\"\n",
        "                im_confused_counter += 1\n",
        "            auth2 = auth1\n",
        "            label = 1\n",
        "            texts, masks, types = self.createPairsWithSEPToken(text1,mask1,text2,mask2)\n",
        "        else:\n",
        "            # this is a different author sample\n",
        "            auth2 = auth1\n",
        "            while auth1 == auth2:\n",
        "                \n",
        "                auth2 = random.choice(list(self.per_author_dataset.keys()))\n",
        "            # now get a text from both authors\n",
        "            i1 = random.choice(range(0,len(self.per_author_dataset[auth1])))\n",
        "            i2 = random.choice(range(0,len(self.per_author_dataset[auth2])))\n",
        "            text1 =  self.per_author_dataset[auth1][i1]\n",
        "            text2 = self.per_author_dataset[auth2][i2]\n",
        "            mask1 =  self.per_author_dataset_masks[auth1][i1]\n",
        "            mask2 = self.per_author_dataset_masks[auth2][i2]\n",
        "            label = 0\n",
        "            texts, masks, types = self.createPairsWithSEPToken(text1,mask1,text2,mask2)\n",
        "        return texts,masks,types,label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ssNriZBscv2",
        "outputId": "e342f99c-2591-462b-f2c5-35762fc11c60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11553\n",
            "3500\n"
          ]
        }
      ],
      "source": [
        "train_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_train_ids_list'\n",
        "train_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_train_masks_list'\n",
        "val_ids = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_val_ids_list'\n",
        "val_masks = base_path+'/PAN20/ids_masks/PAN20_512_Notrunc_perAuth_uncased_val_masks_list'\n",
        "\n",
        "trainIds,valIds,trainMasks,valMasks = getPickleFileInDict(train_ids),getPickleFileInDict(val_ids),getPickleFileInDict(train_masks),getPickleFileInDict(val_masks)\n",
        "\n",
        "dataset_train = AuthorshipDataset(trainIds,trainMasks)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=64,shuffle=True)\n",
        "\n",
        "dataset_val = AuthorshipDataset(valIds,valMasks)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=64,shuffle=True)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDhA0_vJD29y",
        "outputId": "4de0164b-5cde-4606-8d7c-cd48ad661ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('4120014', '6867335', '2249613', '1431093', '1482188', '871132', '2022622', '2022622', '974478', '1995659', '1082342', '4126346', '53107', '602070', '1579870', '2272395')\n",
            "tensor([[  101,  1122,  1171,  ...,  1140,   119,   102],\n",
            "        [  101,  1167,  6595,  ..., 12714,   119,   102],\n",
            "        [  101,   119,  1753,  ..., 26224,   119,   102],\n",
            "        ...,\n",
            "        [  101,  6805, 17404,  ...,  1696,  1173,   102],\n",
            "        [  101,  4455, 15391,  ...,  1451,  1285,   102],\n",
            "        [  101,  1105,  3826,  ...,  5036,  1183,   102]])\n",
            "====\n",
            "tensor([[ 101,  119,  119,  ...,  117, 3522,  102],\n",
            "        [ 101, 1103, 4583,  ..., 1142, 1159,  102],\n",
            "        [ 101, 1131, 1156,  ..., 1149, 1303,  102],\n",
            "        ...,\n",
            "        [ 101,  188, 3759,  ..., 1283,  119,  102],\n",
            "        [ 101, 1658, 4371,  ...,  117,  146,  102],\n",
            "        [ 101, 2220, 1194,  ..., 1231, 1136,  102]])\n",
            "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
            "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
            "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
            "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
            "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
            "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
            "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
            "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
            "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
            "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
            "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
            "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
            "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
            "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
            "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
            "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
            "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
            "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
            "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
            "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
            "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
            "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
            "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
            "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
            "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
            "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
            "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
            "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
            "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
            "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
            "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
            "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
            "        504, 505, 506, 507, 508, 509, 510, 511])\n"
          ]
        }
      ],
      "source": [
        "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "for text1,mask1, text2,mask2,a1 in train_dataloader:\n",
        "    print(a1)\n",
        "    print(text1)\n",
        "    print(\"====\")\n",
        "  \n",
        "    print(text2)\n",
        "    labels = torch.arange(0,512)\n",
        "    print(labels)\n",
        "    break\n",
        "# for k,v in trainIds.items():\n",
        "#     print(v)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS4GbwWOZGE4"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "      def __init__(self,\n",
        "                 data_pos,\n",
        "                 data_neg,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_author_dataset_pos = data_pos\n",
        "        self.per_author_dataset_neg = data_neg\n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "        self.dict_All_pairs = generateLastDict(self.per_author_dataset_pos,self.per_author_dataset_neg)\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)     \n",
        "      def __len__(self):\n",
        "  \n",
        "        return sum([len(x) for x in self.dict_All_pairs.values()])\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          # posid = random.choice(list(self.per_author_dataset_pos.keys()))\n",
        "          # negid = random.choice(list(self.per_author_dataset_neg.keys()))\n",
        "          # if np.random.uniform() <= self.base_rate:\n",
        "          x = self.dict_All_pairs[idx]\n",
        "            #  print(x)\n",
        "          text1 = x[0][0]\n",
        "          mask1 = x[0][1]\n",
        "          text2 = x[0][2]\n",
        "          mask2 = x[0][3]\n",
        "          label = x[0][4]\n",
        "          # else:\n",
        "          #    x = self.per_author_dataset_neg[negid]\n",
        "             \n",
        "          #    text1 = x[0][0]\n",
        "          #    mask1 = x[0][1]\n",
        "          #    text2 = x[0][2]\n",
        "          #    mask2 = x[0][3]\n",
        "          #    label = x[0][4] \n",
        "         \n",
        "          return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "Fq7OOdEIZIfG",
        "outputId": "f47d0daf-44e0-45af-f7ca-3190ef5d404d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3b8316352cdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/PAN20/PAN20_neg_pairs_512_val'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_train_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_train_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_val_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_val_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(data_train_pos[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_train_neg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-acbe456f9041>\u001b[0m in \u001b[0;36mgetPickleFileInDict\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdict_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Thesis/PAN20/PAN20_pos_pairs_512_train'"
          ]
        }
      ],
      "source": [
        "train_pos = base_path+'/PAN20/PAN20_pos_pairs_512_train'\n",
        "train_neg = base_path+'/PAN20/PAN20_neg_pairs_512_train'\n",
        "val_pos = base_path+'/PAN20/PAN20_pos_pairs_512_val'\n",
        "val_neg = base_path+'/PAN20/PAN20_neg_pairs_512_val'\n",
        "\n",
        "data_train_pos,data_train_neg,data_val_pos,data_val_neg = getPickleFileInDict(train_pos),getPickleFileInDict(train_neg),getPickleFileInDict(val_pos),getPickleFileInDict(val_neg)\n",
        "# print(data_train_pos[1])\n",
        "dataset_train = MyDataset(data_train_pos,data_train_neg,0.5)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=32,shuffle=True)\n",
        "\n",
        "dataset_val = MyDataset(data_val_pos,data_val_neg,0.5)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=32,shuffle=True)\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZJfR2SORM2o"
      },
      "outputs": [],
      "source": [
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_pairs):\n",
        "        self.text_pairs = text_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(list(self.text_pairs.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "     \n",
        "          text_pair = self.text_pairs[idx]\n",
        "          # print(text_pair)\n",
        "          text1 = text_pair[0][0]\n",
        "          mask1 = text_pair[0][1]\n",
        "          text2 = text_pair[0][2]\n",
        "          mask2 = text_pair[0][3]\n",
        "          label = text_pair[0][4]\n",
        "\n",
        "          return text1,mask1, text2,mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzqudYKqRTop",
        "outputId": "8488041d-3fde-40c5-c025-b26ff593697f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3213\n",
            "392\n"
          ]
        }
      ],
      "source": [
        "train = base_path+'/PAN20/PAN20_train_512_trunc_cased_222'#train/dict_per_auth_type_docs_pan22_PAN22_256_train_clean_overlap_NER_sms-email_uncased'#PAN20_train_512_trunc_cased_22'\n",
        "val = base_path+'/PAN20/PAN20_val_512_trunc_cased_222'#val/dict_per_auth_type_docs_pan22_PAN22_256_val_clean_overlap_NER_sms-email_uncased'#PAN20_val_512_trunc_cased_22'\n",
        "train,val = getPickleFileInDict(train),getPickleFileInDict(val)\n",
        "\n",
        "dataset_train = AuthorshipDataset(train)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=64,shuffle=False)\n",
        "\n",
        "dataset_val = AuthorshipDataset(val)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=64,shuffle=False)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MObqWp5hCb8R",
        "outputId": "bd152517-62db-4247-fcfa-426195f79351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 101, 1109, 1602,  ..., 2772, 1105,  102],\n",
            "        [ 101, 1109, 1602,  ..., 2772, 1105,  102],\n",
            "        [ 101, 1760, 8679,  ..., 1702, 1166,  102],\n",
            "        ...,\n",
            "        [ 101, 3516,  107,  ..., 1104,  170,  102],\n",
            "        [ 101,  107, 1573,  ..., 1173, 1598,  102],\n",
            "        [ 101,  107, 1573,  ..., 1173, 1598,  102]])\n",
            "====\n",
            "tensor([[  101, 26247,  2316,  ...,  3189,   119,   102],\n",
            "        [  101,   107,  3446,  ..., 15112,   132,   102],\n",
            "        [  101,   119, 19599,  ...,  1108,   170,   102],\n",
            "        ...,\n",
            "        [  101,   107,  2119,  ...,  1131,   107,   102],\n",
            "        [  101, 14934,  2420,  ...,  4185,   119,   102],\n",
            "        [  101, 23376,  1105,  ...,  1105,  6061,   102]])\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n"
          ]
        }
      ],
      "source": [
        "for text1,mask1, text2,mask2, label in train_dataloader:\n",
        "    print(text1)\n",
        "    print(\"====\")\n",
        "    print(text2)\n",
        "    print(label)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZQPMC1H9fSd"
      },
      "source": [
        "# Read Data PAN 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh1EPpVTAZbX"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "\n",
        "\n",
        "def splitDataset(mydata_pos,mydata_neg):\n",
        "    n_pos = int(0.80 * len(mydata_pos.keys()))\n",
        "    n_neg = int(0.80 * len(mydata_neg.keys()))\n",
        "    data_train_pos = dict(list(mydata_pos.items())[:n_pos])\n",
        "    data_train_neg = dict(list(mydata_neg.items())[:n_neg])\n",
        "\n",
        "    data_val_pos = dict(list(mydata_pos.items())[n_pos:])\n",
        "    data_val_neg = dict(list(mydata_neg.items())[n_neg:])\n",
        "    # masks_val = dict(list(masks.items())[:n])\n",
        "\n",
        "    return data_train_pos,data_train_neg,data_val_pos,data_val_neg\n",
        "\n",
        "def generateLastDict(per_author_dataset_pos,per_author_dataset_neg):\n",
        "        generalID = 0\n",
        "        dict_All_pairs = dict()\n",
        "        for posid in per_author_dataset_pos.keys():\n",
        "            generalID+=1\n",
        "            if generalID not in dict_All_pairs.keys():\n",
        "                dict_All_pairs[generalID] = per_author_dataset_pos[posid]\n",
        "            # dict_All_pairs[generalID].append()\n",
        "        for negid in per_author_dataset_neg.keys():\n",
        "            generalID+=1\n",
        "            if generalID not in dict_All_pairs.keys():\n",
        "                dict_All_pairs[generalID] = per_author_dataset_neg[negid]\n",
        "            # dict_All_pairs[generalID].append()\n",
        "        return dict_All_pairs    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g8xbTI99kkD"
      },
      "outputs": [],
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "      def __init__(self,\n",
        "                 data_pos,\n",
        "                 data_neg,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_author_dataset_pos = data_pos\n",
        "        self.per_author_dataset_neg = data_neg\n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "        self.dict_All_pairs = generateLastDict(self.per_author_dataset_pos,self.per_author_dataset_neg)\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)     \n",
        "      def __len__(self):\n",
        "  \n",
        "        return sum([len(x) for x in self.dict_All_pairs.values()])\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          posid = random.choice(list(self.per_author_dataset_pos.keys()))\n",
        "          negid = random.choice(list(self.per_author_dataset_neg.keys()))\n",
        "          if np.random.uniform() <= self.base_rate:\n",
        "             x = self.per_author_dataset_pos[posid]\n",
        "            #  print(x)\n",
        "             text1 = x[0][0]\n",
        "             mask1 = x[0][1]\n",
        "             text2 = x[0][2]\n",
        "             mask2 = x[0][3]\n",
        "             label = x[0][4]\n",
        "          else:\n",
        "             x = self.per_author_dataset_neg[negid]\n",
        "             \n",
        "             text1 = x[0][0]\n",
        "             mask1 = x[0][1]\n",
        "             text2 = x[0][2]\n",
        "             mask2 = x[0][3]\n",
        "             label = x[0][4] \n",
        "         \n",
        "          return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUox24UbHEKE",
        "outputId": "0873f190-1522-42e9-eb4f-50043f50d4ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1015\n",
            "67\n"
          ]
        }
      ],
      "source": [
        "train_pos = base_path+'/PAN22/train/Closet_Same_text_type/dict_per_auth_type_docs_pan22_PAN22_train_pos_pairs_256_auth_type_balance_clean_Nooverlapping'\n",
        "train_neg = base_path+'/PAN22/train/Closet_Same_text_type/dict_per_auth_type_docs_pan22_PAN22_train_neg_pairs_256_auth_type_balance_clean_Nooverlapping'\n",
        "\n",
        "val_pos = base_path+'/PAN22/train/Closet_Same_text_type/dict_per_auth_type_docs_pan22_PAN22_val_pos_pairs_256_auth_type_balance_clean_Nooverlapping'\n",
        "val_neg = base_path+'/PAN22/train/Closet_Same_text_type/dict_per_auth_type_docs_pan22_PAN22_val_neg_pairs_256_auth_type_balance_clean_Nooverlapping'\n",
        "\n",
        "# train_pos = base_path+'/PAN20/PAN20_pos_pairs_512_train'\n",
        "# train_neg = base_path+'/PAN20/PAN20_neg_pairs_512_train'\n",
        "# val_pos = base_path+'/PAN20/PAN20_pos_pairs_512_val'\n",
        "# val_neg = base_path+'/PAN20/PAN20_neg_pairs_512_val'\n",
        "\n",
        "data_train_pos,data_train_neg,data_val_pos,data_val_neg = getPickleFileInDict(train_pos),getPickleFileInDict(train_neg),getPickleFileInDict(val_pos),getPickleFileInDict(val_neg)\n",
        "# print(data_train_pos[1])\n",
        "dataset_train = MyDataset(data_train_pos,data_train_neg,0.5)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=32,shuffle=True)\n",
        "\n",
        "dataset_val = MyDataset(data_val_pos,data_val_neg,0.5)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=32,shuffle=True)\n",
        "# print(shuffled_ids)\n",
        "# for x in dataset_train:\n",
        "#    print(x)\n",
        "#    break\n",
        "# i = 0\n",
        "# for text1,mask1, text2,mask2, label in train_dataloader:\n",
        "#     # print(text1[0])\n",
        "    \n",
        "#     print(tokenizer.convert_ids_to_tokens(text1[0], skip_special_tokens=False))\n",
        "#     print(tokenizer.convert_ids_to_tokens(text2[0], skip_special_tokens=False))\n",
        "#     print(tokenizer.convert_ids_to_tokens(text1[1], skip_special_tokens=False))\n",
        "#     print(tokenizer.convert_ids_to_tokens(text2[1], skip_special_tokens=False))\n",
        "#     print(label[0])\n",
        "#     print(label[1])\n",
        "#     print(\"=====================================================\")\n",
        "    \n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EIsU64SdsVU"
      },
      "outputs": [],
      "source": [
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_pairs):\n",
        "        self.text_pairs = text_pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(list(self.text_pairs.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "          text_pair = self.text_pairs[idx]\n",
        "          # print(text_pair)\n",
        "          text1 = text_pair[0][0]\n",
        "          mask1 = text_pair[0][1]\n",
        "          text2 = text_pair[0][2]\n",
        "          mask2 = text_pair[0][3]\n",
        "          label = text_pair[0][4]\n",
        "\n",
        "          return text1,mask1, text2,mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JdPSfTVdzOa",
        "outputId": "7582236f-ebc2-46d7-bf49-7d4c861be124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2714\n",
            "337\n"
          ]
        }
      ],
      "source": [
        "train = base_path+'/PAN22/val/dict_per_auth_type_docs_pan22_PAN22_350_train_clean_overlap_NER_almostSameType_diffAuthor'\n",
        "val = base_path+'/PAN22/val/dict_per_auth_type_docs_pan22_PAN22_350_val_clean_overlap_NER_almostSameType_diffAuthor'\n",
        "train,val = getPickleFileInDict(train),getPickleFileInDict(val)\n",
        "\n",
        "dataset_train = AuthorshipDataset(train)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=32,shuffle=True)\n",
        "\n",
        "dataset_val = AuthorshipDataset(val)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=32,shuffle=True)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8jy4wthdqe5"
      },
      "outputs": [],
      "source": [
        "class MyDataset2(torch.utils.data.Dataset):\n",
        "      def __init__(self,\n",
        "                 data,\n",
        "                 \n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_author_dataset = data\n",
        "        \n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "        \n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)     \n",
        "      def __len__(self):\n",
        "        counter = 0\n",
        "        for a in  self.per_author_dataset.keys():\n",
        "            for l in self.per_author_dataset[a].keys():\n",
        "                counter += len(self.per_author_dataset[a][l]) \n",
        "        return counter\n",
        "        # return sum([len(self.per_author_dataset[x]) for x in self.per_author_dataset.keys()])\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          author = random.choice(list(self.per_author_dataset.keys()))\n",
        "          # x = self.per_author_dataset[author]\n",
        "          if np.random.uniform() <= self.base_rate:\n",
        "             if 1 in self.per_author_dataset[author].keys():\n",
        "                x = self.per_author_dataset[author][1]\n",
        "                i = random.choice(range(0,len(x)))\n",
        "               \n",
        "                text1 = x[i][0]\n",
        "                mask1 = x[i][1]\n",
        "                text2 = x[i][2]\n",
        "                mask2 = x[i][3]\n",
        "                label = x[i][4]\n",
        "             else:\n",
        "                x = self.per_author_dataset[author][0]\n",
        "                i = random.choice(range(0,len(x)))\n",
        "                \n",
        "                text1 = x[i][0]\n",
        "                mask1 = x[i][1]\n",
        "                text2 = x[i][2]\n",
        "                mask2 = x[i][3]\n",
        "                label = x[i][4]\n",
        "          else:\n",
        "             if 0 in self.per_author_dataset[author].keys():\n",
        "                x = self.per_author_dataset[author][0]\n",
        "                i = random.choice(range(0,len(x)))\n",
        "                \n",
        "                text1 = x[i][0]\n",
        "                mask1 = x[i][1]\n",
        "                text2 = x[i][2]\n",
        "                mask2 = x[i][3]\n",
        "                label = x[i][4]\n",
        "             else:\n",
        "                x = self.per_author_dataset[author][1]\n",
        "                i = random.choice(range(0,len(x)))\n",
        "               \n",
        "                text1 = x[i][0]\n",
        "                mask1 = x[i][1]\n",
        "                text2 = x[i][2]\n",
        "                mask2 = x[i][3]\n",
        "                label = x[i][4]\n",
        "          # print(x)\n",
        "          # text1 = x[i][0]\n",
        "          # mask1 = x[i][1]\n",
        "          # text2 = x[i][2]\n",
        "          # mask2 = x[i][3]\n",
        "          # label = x[i][4]\n",
        "         \n",
        "         \n",
        "          return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "gIQJ24m4eOJR",
        "outputId": "6c3a17ab-c898-4155-aecf-a7386f06ed91"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9acbe9a6acda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/PAN22/train/per_author/dict_per_auth_type_docs_pan22_PAN22_val_256_Nooverlapping_pairs_Clean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataset_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyDataset2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MyDataset2' is not defined"
          ]
        }
      ],
      "source": [
        "dataset_train = base_path+'/PAN22/train/per_author/dict_per_auth_type_docs_pan22_PAN22_train_256_Nooverlapping_pairs_Clean'\n",
        "dataset_val = base_path+'/PAN22/train/per_author/dict_per_auth_type_docs_pan22_PAN22_val_256_Nooverlapping_pairs_Clean'\n",
        "dataset_train,dataset_val = getPickleFileInDict(dataset_train),getPickleFileInDict(dataset_val)\n",
        "dataset_train = MyDataset2(dataset_train,0.5)\n",
        "dataset_val = MyDataset2(dataset_val,0.5)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=64,shuffle=True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=64,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WlSFye7e3vV",
        "outputId": "26c65479-01b1-402d-cbe1-6200b1a739a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "986\n",
            "226\n"
          ]
        }
      ],
      "source": [
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))  \n",
        "\n",
        "# for text1,mask1, text2,mask2, label in validation_dataloader:\n",
        "#   print(label)\n",
        "#   break\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TFf9c0luO1L"
      },
      "outputs": [],
      "source": [
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "      def __init__(self,\n",
        "                 data,\n",
        "                 \n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_pair_dataset = data\n",
        "        \n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "       \n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)     \n",
        "      def __len__(self):\n",
        "  \n",
        "        return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "          author = random.choice(list(self.per_pair_dataset.keys()))\n",
        "          i = random.choice(range(0,len(self.per_pair_dataset[author])))\n",
        "          x = self.per_pair_dataset[author][i]\n",
        "          text1 = x[0]\n",
        "          mask1 = x[1]\n",
        "          text2 = x[2]\n",
        "          mask2 = x[3]\n",
        "          label = x[4]\n",
        "         \n",
        "          return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lylzzyysuw8g"
      },
      "outputs": [],
      "source": [
        "dataset_test = base_path+'/PAN22/dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_Noclean_pairid'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "dataset_test = getPickleFileInDict(dataset_test)\n",
        "dataset_test = MyDatasetTest(dataset_test,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=64,shuffle=True)\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI7_4y1Kx1DX"
      },
      "outputs": [],
      "source": [
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data_pos,\n",
        "                 data2_neg,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_pair_dataset = data_pos\n",
        "        self.per_pair_dataset2 = data2_neg\n",
        "\n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)\n",
        "    def __len__(self):\n",
        "\n",
        "        # return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "        return len(list(self.per_pair_dataset.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # i = random.choice(list(self.per_pair_dataset.keys()))\n",
        "        # i = random.choice(range(0,len(self.per_pair_dataset[author])))\n",
        "        if np.random.uniform() >= self.base_rate:\n",
        "            x = self.per_pair_dataset[idx]\n",
        "          \n",
        "            text1 = x[0].squeeze(0)\n",
        "            mask1 = x[1].squeeze(0)\n",
        "            text2 = x[2].squeeze(0)\n",
        "            mask2 = x[3].squeeze(0)\n",
        "            label = x[4]\n",
        "        else:\n",
        "            x = self.per_pair_dataset2[idx]\n",
        "            \n",
        "            text1 = x[0].squeeze(0)\n",
        "            mask1 = x[1].squeeze(0)\n",
        "            text2 = x[2].squeeze(0)\n",
        "            mask2 = x[3].squeeze(0)\n",
        "            label = x[4]\n",
        "\n",
        "        return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUbjagcex_7G",
        "outputId": "5ca158ad-2cba-4180-904d-ccc97789803b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "164\n"
          ]
        }
      ],
      "source": [
        "dataset_test_pos = base_path+'/PAN22/dict_per_pairID_pan22_test_PAN22_350_test_clean_NER_pos'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "dataset_test_neg = base_path+'/PAN22/dict_per_pairID_pan22_test_PAN22_350_test_clean_NER_neg'\n",
        "dataset_test_pos = getPickleFileInDict(dataset_test_pos)\n",
        "dataset_test_neg = getPickleFileInDict(dataset_test_neg)\n",
        "dataset_test = MyDatasetTest(dataset_test_pos,dataset_test_neg,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=32,shuffle=True)\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvwlpqfMb73c"
      },
      "source": [
        "# Read test Data PAN 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "PcZAohUQb66a",
        "outputId": "ff38f889-d555-40e9-e268-1ecd8d65265a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a552425e80b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/PAN20/dict_per_pairID_pan20_test'\u001b[0m \u001b[0;31m#PAN20_per_author_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m \u001b[0mdict_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPickleFileInDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpusTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'getPickleFileInDict' is not defined"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import re\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "# from transformers import BertTokenizerFast\n",
        "# import emojis\n",
        "# import demoji\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "##################\n",
        "# Class for corpus\n",
        "##################\n",
        "class CorpusTest(object):\n",
        "\n",
        "    def __init__(self, dict_dataset,trunc = True):\n",
        "\n",
        "        # define tokenizer\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
        "        # raw dataset\n",
        "        self.dict_dataset_raw = dict_dataset\n",
        "\n",
        "        self.dict_dataset_per_auth_ids= {}\n",
        "        self.dict_dataset_per_auth_ids2= {}\n",
        "        self.dict_dataset_per_auth_masks= {}\n",
        "        self.dict_positives_pairs = {}\n",
        "        self.dict_negatives_pairs = {}\n",
        "        self.dict_All_pairs = {}\n",
        "        self.dict_All_pairs_shuffle = {}\n",
        "        self.usedAuthors = {}\n",
        "        self.DoTrunc = trunc\n",
        "    ####################\n",
        "    # doc pre-processing\n",
        "    ####################\n",
        "    def removeHTMLtags(self,text):\n",
        "\n",
        "\n",
        "        x = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '',text)\n",
        "        x2 = re.sub('\\s\\s', '',x)\n",
        "        x3 = re.sub('\\t', ' ',x2)\n",
        "        return x3.strip()\n",
        "    def maskNumbers(self,text, symbol='1'):\n",
        "        x = re.sub('[0-9]', symbol,text)\n",
        "        return x\n",
        "\n",
        "    # def maskEmoticons(self,text, symbol='0'):\n",
        "    #     new_list1 = emojis.get(text)\n",
        "    #     xx = ''\n",
        "    #     for x in new_list1:\n",
        "\n",
        "    #         xx = text.replace(x,'0 ')\n",
        "\n",
        "    #     dem = demoji.findall(xx)\n",
        "    #     for item in dem.keys():\n",
        "    #         xx = xx.replace(item, symbol)\n",
        "\n",
        "    #     if xx=='':\n",
        "    #         return text\n",
        "    #     else:\n",
        "    #         return xx\n",
        "\n",
        "    def paddingChunksToMaxlen(self,IdsChunks,masksChunks,isSecond = False,maxLen = 126):#listMasksChunks\n",
        "        listIdsChunks = list()\n",
        "        listMasksChunks = list()\n",
        "\n",
        "        for i in range(len(IdsChunks)):\n",
        "\n",
        "            pad_len = maxLen  - IdsChunks[i].shape[0]\n",
        "\n",
        "            tmplistids = IdsChunks[i].tolist()\n",
        "            if isSecond == False:\n",
        "                tmplistids.insert(0,101)\n",
        "            tmplistids.insert(len(tmplistids),102)\n",
        "\n",
        "            tmplistmasks = masksChunks[i].tolist()\n",
        "            if isSecond == False:\n",
        "                tmplistmasks.insert(0,1)\n",
        "            tmplistmasks.insert(len(tmplistmasks),1)\n",
        "            # print(tmplistids)\n",
        "            listIdsChunks.append(torch.LongTensor(tmplistids))\n",
        "            listMasksChunks.append(torch.LongTensor(tmplistmasks))\n",
        "            del tmplistmasks, tmplistids\n",
        "\n",
        "            if pad_len > 0:\n",
        "\n",
        "                listIdsChunks[i] =  F.pad(listIdsChunks[i], (0,pad_len), \"constant\", 0)\n",
        "                listMasksChunks[i] =  F.pad(listMasksChunks[i], (0,pad_len), \"constant\", 0)\n",
        "        del IdsChunks, masksChunks, pad_len\n",
        "        # gc.collect()\n",
        "        return listIdsChunks ,listMasksChunks\n",
        "\n",
        "    def createRandomSetence(self,listOfSetences):\n",
        "        tmpList = []\n",
        "        for x in listOfSetences:\n",
        "            tmpValue = random.choice(listOfSetences)\n",
        "            tmpList.append(tmpValue)\n",
        "        sete = ' '.join(tmpList)\n",
        "        return sete\n",
        "\n",
        "    def chunkingTextsBasedOnBert(self,text,Dotruncate=True):\n",
        "        input_ids=[]\n",
        "        attention_masks=[]\n",
        "        if Dotruncate:\n",
        "           encoded_dict = self.tokenizer.encode_plus(\n",
        "                                        text ,                    # Sentence to encode.\n",
        "                                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                                        max_length = 512,           # Pad & truncate all sentences.\n",
        "                                        pad_to_max_length = True,\n",
        "                                        truncation = True,\n",
        "                                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                                        return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                        )\n",
        "           return encoded_dict['input_ids'],encoded_dict['attention_mask']\n",
        "\n",
        "        # setences1 = nltk.sent_tokenize(row['Text1'])\n",
        "        # setences2 = nltk.sent_tokenize(row['Text2'])\n",
        "        # # print(setences1)\n",
        "        # set1 = createRandomSetence(setences1)\n",
        "        # # print(set1)\n",
        "        # set2 = createRandomSetence(setences2)\n",
        "        else:\n",
        "            encoded_dict = self.tokenizer.encode_plus(\n",
        "                                            text ,                    # Sentence to encode.\n",
        "                                            add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                                            # max_length = 512,           # Pad & truncate all sentences.\n",
        "                                            # pad_to_max_length = True,\n",
        "                                            # truncation = False,\n",
        "                                            return_attention_mask = True,   # Construct attn. masks.\n",
        "                                            return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                                            )\n",
        "\n",
        "            # print(encoded_dict['input_ids'][0])\n",
        "            tensorsIdList1,tensorsMaskList1 = self.paddingChunksToMaxlen(encoded_dict['input_ids'][0].split(254),encoded_dict['attention_mask'][0].split(254),False,254)\n",
        "\n",
        "\n",
        "\n",
        "            return tensorsIdList1, tensorsMaskList1\n",
        "    def getNonFrequentWordsPerDoc(self,doc,threshold=5):\n",
        "        words = doc.split()#nltk.word_tokenize(doc)\n",
        "        fdist = nltk.FreqDist(words)\n",
        "        list_of_non_freq_words = []\n",
        "        for word,freq in fdist.items():\n",
        "            if freq <= threshold:\n",
        "                # if len(word) > 4:\n",
        "                list_of_non_freq_words.append(word)\n",
        "        for token in list_of_non_freq_words:\n",
        "            doc = doc.replace(token,'2')\n",
        "        return doc\n",
        "\n",
        "    def preprocess_doc(self, doc,isPAN20=False):\n",
        "        doc = self.maskNumbers(doc)\n",
        "        # doc = self.getNonFrequentWordsPerDoc(doc,threshold=5)\n",
        "        if isPAN20 == False:\n",
        "           doc = self.removeHTMLtags(doc)\n",
        "          #  doc = self.maskEmoticons(doc)\n",
        "\n",
        "        # tokenize doc\n",
        "        # tokens = list(self.tokenizer(doc))\n",
        "        return doc.strip()\n",
        "\n",
        "    ################\n",
        "    # split data set\n",
        "    ################\n",
        "    def parse_dictionary(self):\n",
        "        posidx = 0\n",
        "        negidx = 0\n",
        "        genid = 0\n",
        "        # authors\n",
        "        for a in tqdm(self.dict_dataset_raw.keys()):\n",
        "            # fandom categories\n",
        "            listDocsPerAuthor = []\n",
        "            listMasksPerAuthor = []\n",
        "            # print(a)\n",
        "            # for f in self.dict_dataset_raw[a].keys():\n",
        "                # documents\n",
        "                # print(f)\n",
        "            # print(self.dict_dataset_raw[a])\n",
        "            for i, docs in enumerate(self.dict_dataset_raw[a]):\n",
        "                processed_doc1 = self.preprocess_doc(docs[0],True)\n",
        "                processed_doc2 = self.preprocess_doc(docs[1],True)\n",
        "                inputIDs1,attn_masks1 = self.chunkingTextsBasedOnBert(processed_doc1,self.DoTrunc)\n",
        "                inputIDs2,attn_masks2 = self.chunkingTextsBasedOnBert(processed_doc2,self.DoTrunc)\n",
        "                label = docs[2]\n",
        "                # print(label)\n",
        "                # flat_IDs = []\n",
        "                # flat_Masks = []\n",
        "                # flat_IDs2 = []\n",
        "                # flat_Masks2 = []\n",
        "                # for item in inputIDs1:\n",
        "                #     flat_IDs.append(item)\n",
        "                # for item in attn_masks1:\n",
        "                #     flat_Masks.append(item)\n",
        "                # for item in inputIDs2:\n",
        "                #     flat_IDs2.append(item)\n",
        "                # for item in attn_masks2:\n",
        "                #     flat_Masks2.append(item)\n",
        "\n",
        "\n",
        "            #     listDocsPerAuthor.append([flat_IDs,flat_IDs2])\n",
        "            #     listMasksPerAuthor.append([flat_Masks,flat_Masks2])\n",
        "            # if a not in self.dict_dataset_per_auth_ids.keys():\n",
        "            #     self.dict_dataset_per_auth_ids[a] = {}\n",
        "            # if a not in self.dict_dataset_per_auth_masks.keys():\n",
        "            #     self.dict_dataset_per_auth_masks[a] = {}\n",
        "            # # print(len(listDocsPerAuthor))\n",
        "            # self.dict_dataset_per_auth_ids[a] = listDocsPerAuthor  # authID:{[x1,x2,x3],[y1,y2...yn],...}\n",
        "            # self.dict_dataset_per_auth_masks[a] = listMasksPerAuthor\n",
        "            if label == 1:\n",
        "                if posidx not in self.dict_dataset_per_auth_ids.keys():\n",
        "                    self.dict_dataset_per_auth_ids[posidx] = []\n",
        "                self.dict_dataset_per_auth_ids[posidx]=[inputIDs1,attn_masks1,inputIDs2,attn_masks2,label] #listDocsPerAuthor  # authID:{[x1,x2,x3],[y1,y2...yn],...}\n",
        "                genid+=1\n",
        "                posidx+=1\n",
        "            elif label == 0:\n",
        "                if negidx not in self.dict_dataset_per_auth_ids2.keys():\n",
        "                    self.dict_dataset_per_auth_ids2[negidx] = []\n",
        "                self.dict_dataset_per_auth_ids2[negidx]=[inputIDs1,attn_masks1,inputIDs2,attn_masks2,label] #listDocsPerAuthor  # authID:{[x1,x2,x3],[y1,y2...yn],...}\n",
        "                genid+=1\n",
        "                negidx+=1\n",
        "    # def generatePairs(self):\n",
        "    #     pairIDpos = 0\n",
        "    #     pairIDneg = 0\n",
        "    #     if self.DoTrunc:\n",
        "    #        for id in tqdm(self.dict_dataset_per_auth_ids.keys()):\n",
        "                \n",
        "test = base_path+'/PAN20/dict_per_pairID_pan20_test' #PAN20_per_author_doc          \n",
        "dict_dataset = getPickleFileInDict(test)\n",
        "corpus = CorpusTest(dict_dataset)\n",
        "corpus.parse_dictionary()\n",
        "with open( \"PAN20_pos_pairs_512_cased_train\", 'wb') as f:\n",
        "        pickle.dump(corpus.dict_dataset_per_auth_ids, f)\n",
        "with open( \"PAN20_neg_pairs_512_cased_test\", 'wb') as f:\n",
        "        pickle.dump(corpus.dict_dataset_per_auth_ids2, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CkkyKGWegWIz",
        "outputId": "ccb72d09-43e6-4612-b31c-3b9a571dabb7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Thesis/PAN20/PAN20_neg_pairs_512_cased_test'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.copy('PAN20_pos_pairs_512_cased_test','/content/drive/My Drive/Thesis/PAN20')\n",
        "shutil.copy('PAN20_neg_pairs_512_cased_test','/content/drive/My Drive/Thesis/PAN20')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN9WTLR9gt0V"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def shuffledict(mergeDict):\n",
        "        keys = list(mergeDict.keys())\n",
        "        random.shuffle(keys)\n",
        "\n",
        "        Shuffled = dict()\n",
        "\n",
        "        for key in keys:\n",
        "            if key not in Shuffled.keys():\n",
        "                Shuffled[key] = {}\n",
        "            Shuffled[key]= mergeDict[key]\n",
        "\n",
        "        return Shuffled\n",
        "def mergeDicts(per_pair_dataset,per_pair_dataset2):\n",
        "        index = 0\n",
        "        mergeDict = {}\n",
        "        for k1 in per_pair_dataset.keys():\n",
        "            if index not in mergeDict.keys():\n",
        "               mergeDict[index] = []\n",
        "            mergeDict[index] = per_pair_dataset[k1] \n",
        "            index+=1\n",
        "        for k2 in per_pair_dataset2.keys():\n",
        "            if index not in mergeDict.keys():\n",
        "               mergeDict[index] = []\n",
        "            mergeDict[index] = per_pair_dataset2[k2] \n",
        "            index+=1\n",
        "        \n",
        "        mergeDict = shuffledict(mergeDict)\n",
        "        return mergeDict\n",
        "\n",
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data_pos,\n",
        "                 data2_neg,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_pair_dataset = data_pos\n",
        "        self.per_pair_dataset2 = data2_neg\n",
        "        \n",
        "        self.mergeDict = mergeDicts(self.per_pair_dataset,self.per_pair_dataset2)\n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)\n",
        "    \n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "        return len(list(self.mergeDict.keys()))#+len(list(self.per_pair_dataset2.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # i = random.choice(list(self.per_pair_dataset.keys()))\n",
        "        # i = random.choice(range(0,len(self.per_pair_dataset[author])))\n",
        "        # if np.random.uniform() >= self.base_rate:\n",
        "            # if idx in self.per_pair_dataset.keys():\n",
        "              \n",
        "        x = self.mergeDict[idx]\n",
        "            \n",
        "        text1 = x[0].squeeze(0)\n",
        "        mask1 = x[1].squeeze(0)\n",
        "        text2 = x[2].squeeze(0)\n",
        "        mask2 = x[3].squeeze(0)\n",
        "        label = x[4]\n",
        "        # else:\n",
        "        #     # if idx in self.per_pair_dataset2.keys():\n",
        "              \n",
        "        #       x = self.per_pair_dataset2[idx]\n",
        "              \n",
        "        #       text1 = x[0].squeeze(0)\n",
        "        #       mask1 = x[1].squeeze(0)\n",
        "        #       text2 = x[2].squeeze(0)\n",
        "        #       mask2 = x[3].squeeze(0)\n",
        "        #       label = x[4]\n",
        "\n",
        "        return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5ai9CLBgxiW",
        "outputId": "e550ee1b-b038-4718-dcd4-1b71f35dff21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "224\n"
          ]
        }
      ],
      "source": [
        "dataset_test_pos = base_path+'/PAN20/PAN20_pos_pairs_512_cased_test'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "dataset_test_neg = base_path+'/PAN20/PAN20_neg_pairs_512_cased_test'\n",
        "dataset_test_pos = getPickleFileInDict(dataset_test_pos)\n",
        "dataset_test_neg = getPickleFileInDict(dataset_test_neg)\n",
        "dataset_test = MyDatasetTest(dataset_test_pos,dataset_test_neg,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=64,shuffle=True)\n",
        "print(len(test_dataloader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IwxSRmnM7gi"
      },
      "outputs": [],
      "source": [
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data_pos,\n",
        "                \n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_pair_dataset = data_pos\n",
        "      \n",
        "        \n",
        "        \n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)\n",
        "    \n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "        return len(list(self.per_pair_dataset.keys()))#+len(list(self.per_pair_dataset2.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id = random.choice(list(self.per_pair_dataset.keys()))      \n",
        "        x = self.per_pair_dataset[id]   \n",
        "        print(x)\n",
        "        text1 = x[0][0]\n",
        "        mask1 = x[0][1]\n",
        "        text2 = x[0][2]\n",
        "        mask2 = x[0][3]\n",
        "        label = x[0][4]\n",
        "\n",
        "        return text1, mask1, text2, mask2, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98f-UEGgNN1l",
        "outputId": "736fe3cb-7945-4da4-da9a-f4acac94857d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "101\n"
          ]
        }
      ],
      "source": [
        "dataset_test = base_path+'/PAN20/PAN20_512_Notrunc_perPairid_AllChunks_onlyNums_cased_test'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "\n",
        "dataset_test = getPickleFileInDict(dataset_test)\n",
        "\n",
        "dataset_test = MyDatasetTest(dataset_test,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=1,shuffle=False)\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr-4akBVEJ3D",
        "outputId": "0bc19217-2e11-47d6-e97e-5d7f67ced929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[tensor([  101,  1230,  6566,  2411,  1608,  1117,  2209,  1106, 21722,   117,\n",
            "         1679, 24271,  1117,  4585,  1105,  7963,  1117,  1319,  4346,  1106,\n",
            "         3542,   119, 21722,   107,   188,  4346,  1899,  1117,  1105,  1103,\n",
            "         1160, 25144,  1114,   170,  4900, 17159,  1115,  8293,  1149,  1852,\n",
            "         3772,   119,   107,  3969,  1128,  1107,  2630,   106,   107, 21722,\n",
            "         6104,   117, 13769,  1103,  4282,  1104,  1117,  1319,  6275,   119,\n",
            "         1109,  1656,  1104, 21722,   107,   188,  4346,  2411,  4601,  1146,\n",
            "         1114,   170,  1783,  1183,  9556,  1115,  2569,  1103,  2049,  1104,\n",
            "         1117, 26632,  3059,  1166,   119, 21722,  3691,  1256,  5747,  1105,\n",
            "         2873,  1103, 12604,  1399,  1171,   170,  4600,  1995,  1623,  1121,\n",
            "         1147,  1560,  1700,  1114,   170,  1353,  7552,  1121,  1117,  4346,\n",
            "         6187,  7111,  3375,   119, 21722,  4860,  1113,  1117,  4257,  1173,\n",
            "         1976,  1866,  1146,  1106,  2407,  1471,  1111,  1330,  2035,   119,\n",
            "         1124,  1350,  1120,  1117,  6566,   117,  1150,  1471,  1108, 12699,\n",
            "         1158,  1117, 26715,   119,  1249,  1770,  1112,  1119,  3390,  1257,\n",
            "         1852,  1103, 25731,  9349, 25469,  4621,  1117,  3437,  1108,  2355,\n",
            "          117,  1119,  1108,  1678,   170,  4197,   119,  1731,   119,   119,\n",
            "          119,  1293,  1674,  1119,  1138,   119,   119,   119,  1115,  4346,\n",
            "          119,   119,   119,   136, 21722,  1354,   119,  1124,  1125,  2331,\n",
            "         1562,  1103,   176, 16481,  8671,  6275,  1115,  1117, 19114,  1208,\n",
            "          192, 12350,  1174,  1219,  1117,  2321,  1114,  1727, 18045,  1656,\n",
            "         1103, 17784, 17564,  1160,  1201,  1196,   119,  1135,  1108, 11178,\n",
            "         1103,  1269,  4346,  1115,  1103,  9445,  4753,  5122, 23665, 18484,\n",
            "         1471,  1125,  1215,  1106,  2321,  1103,  3420,  5266,  1104,  1103,\n",
            "         5725,  1362,  1165,  1119, 10474,  2433,  1222,  1172,   119, 21722,\n",
            "         1238,   107,   189,  1138,  1277,  1159,  1106,  1965,  1117,  3774,\n",
            "         1112,  1117,  3437,  1108,  1770,  1113,  1103,  2035,  1254,   119,\n",
            "        21722,  2925,  1117,  1268,  1981,  1105,  1122, 17531, 14963,  1229,\n",
            "          170,  7483,  1193,  1981,  2578,  2200,  1807,  1122,   119,  1230,\n",
            "         6566,  1814,  1117,  4346,  1213,  1105,  1793,  1106, 26632, 21722,\n",
            "         1121,  1103,  1268,  1334,   119, 21722,  2347,  1103,  4346,  1118,\n",
            "         1157,  6275,  1606,  1103,   188, 26426,  1874,   118,  1981,   117,\n",
            "         1454,  1213,  1105,  3885,  1117,  3437,  1481,  1140,  1114,  1117,\n",
            "         1103,  9927,  3220,  1107,  1117,  1268,  1981,   119,  1230,  6048,\n",
            "         6964,  1107,  2286,  1586,  1105,  7040,  1117,  4346,  1154,  1103,\n",
            "         1747,  1106,  3345,  1117, 11550,   119,  1135,  9626,   170,  1415,\n",
            "         3245,  1324,  1154,  1103,  4033,  1112,  1122,  2795,  1117,  2303,\n",
            "          119,   107,  2091,  1136,  8306,  1115,  1134,  1128,  2834,  2147,\n",
            "          119,   119,   119,   107,  1117,  3437,  1163,   117,  4239,  1117,\n",
            "         4346,  1149,  1104,  1103,  1747,   119,  1124,  1316,  1117,  1286,\n",
            "         1981,  1107,  1524,  1104,  1140,  1105, 12114,  1210,  4840, 12604,\n",
            "         1106, 16858,  1117,  1404,   119,  1124,  5850,  1103, 12604,  1120,\n",
            "        21722,  1173,  1310,  3179,  2019,  1140,   119,   107,  1327,  1202,\n",
            "         1128,  1928,  1115,   146,  1169,   107,   189,  2147,   119,   119,\n",
            "          119,   136,   107, 21722,  1163, 19353, 13789,  1193,   119,  1124,\n",
            "         1316,  1117, 27324,  1981,  1107,  1524,  1104,  1117,  1339,  1105,\n",
            "         8173,  1122,  1154,   170,  7374,   119, 21722,  3583,  1117,  4346,\n",
            "         1105, 26551,  1122,  1106,  1117,  1334,  1229, 24386,  1117,  7374,\n",
            "         1154,  1103,  1586,   119,  3929,  1104,  1117, 20580,  1313,   117,\n",
            "          170,  2878,  6746, 26124,  1181,  2019,  1140,   119,  1124,  1533,\n",
            "         1117,  1289,  1105,  2347,  1103, 24485,  4231,  1107,  1117,  7374,\n",
            "          119,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,   113,   138,   120,   151,   131,   146,  1108,  2034,  1280,\n",
            "         1106,  1202,  1142,  1155,  1107,  1141,  6073,  1133,   146,  2023,\n",
            "         2269,  1105,  1122,  2207,  1146,  2039,  1105,  2039,  1103,  1748,\n",
            "          146,  1355,   119,  1332,   146,  1408, 18031,  1103,  1509,  5039,\n",
            "          117,   146,  1879,  1106,  3325,  1122,  1154,  1160,  9611,  1106,\n",
            "         1712,  1614,  1120,   170,  1363,  2251,   119,   146,  1125,  1142,\n",
            "         1642,  1107,  1713,  1165,   146,  1148,  1845,  9060,   122,  1314,\n",
            "         1214,  1133,   146,  1879,  1136,  1106,  6799,  1122,  1235,   146,\n",
            "         1125,  1307,  1194,  9060,   122,  1254,  9961,  1107,  1103,  2174,\n",
            "          113,   146,  1108,  1167,  7676,  1113,  2033,  1139,  1148, 20497,\n",
            "         1665,   117,   107, 17723,   107,   188, 13212,   107,   117,  1149,\n",
            "         1112,   146,  1464,  1167,  7572, 13241,   114,   119,   146,   107,\n",
            "          182,  5171,   146,  1225,  3074,  1272,   146,  1400,   170,  2640,\n",
            "         1106,  1505,  3302,  1111,  1398,  1148,  1105,  1208,   117,   146,\n",
            "         1169, 18831,  9193,  1121,  1115,  1342,  1154,  1142,  1642,   113,\n",
            "         1191,  1128,  3983,   107,   189,  3535,   132,  1142,  1642,  2274,\n",
            "         1282,  1170,  1103,   122,  3276,  1692,  1104,  1115,  1342,   117,\n",
            "          107, 11336, 19698,   117,  1105, 12128, 20230,   107,   117,   170,\n",
            "         1864,   146,  1238,   107,   189,  1221,  1103,  1148,  1159,   146,\n",
            "         1408,  2269,  1142,  1642,   114,   119,  1573,  1208,   146,  1169,\n",
            "         1329,  2650,  1105,  4928,  3050,  1121,  1115,  1342,  1105,  1112,\n",
            "          170,  2006,   132,   146,  1176,  1187,  1142,  1642,  1110,  1280,\n",
            "         1277,  1167,  1190,  1103,  1148,  1159,   146,  1148,  1354,  1122,\n",
            "         1146,   119,   114, 17723,   107,   188,  7025,   113,  2943,   122,\n",
            "          114,  1351,  1429,   117,   122,   131,  1429, 14123,   140, 14201,\n",
            "        12842,  4556,  2161, 11949,  1944,   107,   156, 19556, 22861,  6258,\n",
            "         2036,   106,   106,   106,   106,   107,  1332,  2617,  1454,  1113,\n",
            "         1103,  4204,  1103,  1395, 12773,  1114,  1103,  3807,  1104,  1242,\n",
            "         1234, 11500,   107,  3774,   107,  1120,  1103,  1269,  1159,   119,\n",
            "         1258,  1103,  4900,  1104,  1103,  1721,  4307,  1228,   117,   146,\n",
            "         1261,   170,  1440,  1213,  1103,  2885,  1105,  4379,  1103,  6065,\n",
            "         1107,  1103,  1395,   119,  9996,  1213,  1103,  1395,   146,  1486,\n",
            "         1672,  2021,  4675,  1105,  4876,  1121,  1103,  5096, 27727,   107,\n",
            "          188,  3060,  3366,  1213,  1164,   122,  1668,  7072,  1105,   170,\n",
            "          171,  9435,  2105,  1952,  1107,  1103,  1171,   119,   146,  3037,\n",
            "         1242,  1104,  1172,  1121,  1139,  1159,  1112,  2534,  5096, 27727,\n",
            "          119,  3841,  1172,   117,   146,  1486,  4387,  5137,  1105,  5876,\n",
            "        14919,   117,  1150,  1125,   170,  1353, 23305, 23982,  1113,   170,\n",
            "        14051, 20437,  1377,   119,  9187,   144, 17167, 10061,  1108,  1175,\n",
            "         1112,  1218,  1105,  1119,  1108, 21593,  1103,   171,  9435,  2105,\n",
            "         1952,  1114,  1632, 12430,   119,  1109, 10572,  2021,  2705,  1105,\n",
            "         1117,  1676,  1127,  2288,  1113,  1103,  1286,  1334,  1104,  1103,\n",
            "         1395,  1105,  1256,  1103,  4398,  1108,  1621,  1103,  6065,   119,\n",
            "         1212,  1103,  1268,  1334,  1104,  1103,  2885,   117,   146,  1486,\n",
            "         6343,  5445,  2288,  3338,   170,  1685,  1590,  1105,   170,  1376,\n",
            "         1873,   119,  1262,  2288,  1114,  1172,  1108,   119,   119,   119,\n",
            "          107, 17723,   106,   107,   170,  4509,  1490,  1270,   119,   138,\n",
            "        11009,  1873,  1338,  1919,  1166,  1105,  1522,  1143,   170,  8363,\n",
            "          119,  1153,  1108,  3351,   170,  1653,  8074,  4920,  1114,  1672,\n",
            "        18607,  1113,  1122,  1105,  1125,   170,  1383,  1104,  3152,   118,\n",
            "         6805,  7537,  1113,  1123,  1246,   119,   107, 18653,  1161,   106,\n",
            "          146,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,  1109,  4231,  1119,  1125,  1270,  1106,  1140,  1108,   170,\n",
            "        24561,  1174, 24181, 15634,   119, 21722,  7185,  1103,  2262,  6275,\n",
            "         1107,  1524,  1104,  1117,  1404,  1105,  1107,  1141,  5307,  4018,\n",
            "          117,  8362, 21581, 21549,  1122,  1106,  7063,   170,  5220,   118,\n",
            "        13247,  6275,   119,  1109,  4346, 15510,  1106,  1117,  1981,  1105,\n",
            "         2411,  3609,  1140,  1157,  1540,   119,   138,  1843,  2308, 20420,\n",
            "         1205,  1117,  1981,  1121,  1103,  4346,  1105,  1194,  1117,  1404,\n",
            "          119, 21722,   107,   188,  1257,  1454,  3999,  2221,  1105,   170,\n",
            "        27324, 11295,  4840,  1691,  1807,  1140,   119,   107,  3435,   106,\n",
            "          107,  1119,  6031,   117,  1117,  1490,  1781,  1113,   170,  1304,\n",
            "         4719,   118,  9209,  1231,  4121,  3169,  1891,   119,  1109,  1299,\n",
            "         6925,  1117,  4840, 12604,  1852,  3195,  1142,  9047,   119, 21722,\n",
            "         1231,  6163, 20175,  1118, 21147,  1158,  1210,  9754,  7483, 11939,\n",
            "         1104,  1117,  1319,   117, 22620, 18469,  1121,  1103, 11295,  4840,\n",
            "         1115,  1108,  2935,  3338,  1140,   119,  1109,  1565,  4840, 11939,\n",
            "        25144,  1107,  2286,  1586,  1105, 11670,  1141,  1330,   119, 21722,\n",
            "         1517,  1254,  4601,  1103,  8198,  9280,  1114,  1241,  1117, 12604,\n",
            "         1120,  1103,  2407,   119,  1109,  1160,  1899,  1107,   170,  1248,\n",
            "        18751,  1114, 21722,  4905,  1117,  1319, 12604,  1107,  1524,  1104,\n",
            "         1117,  1339,   119,  1124,  1108,  1339,   118,  1106,   118,  1339,\n",
            "         1114,  1117, 19114,   117,  1117,  5725,   118,  8471,  1257, 20013,\n",
            "          119, 21722,  1310,  1106, 11079,  1103,  4282,  1104,  1117,  2425,\n",
            "         4621,  1106,  2965,  1122,  1146,  1106,  1660,  1471,  1103,  4316,\n",
            "         1133,  1196,  1119,  1180,   117,  1117,  3437,  2873,  1228,  1105,\n",
            "         1149,  1104,  1103,   188, 15903,  7213,   119,  1124, 18507,  1171,\n",
            "          170,  1603,  2462,  1105,  1173,  3356,  1117,  4346,  2019, 21722,\n",
            "          119,   107,  6682,   119,   119,   119,  1268,  1175,   119,   119,\n",
            "          119,   107,  1117, 19114,  1163,   117,  9256, 21722,  1228,   118,\n",
            "         3542,   119,  1124,  2411, 27615,  1160,  1167,  4840, 12604,  1173,\n",
            "         5850,  1172,  1120,   148, 12577,  1663,  1105, 23612,  8584,   117,\n",
            "         1150,  1125,  1151,  2903,  1103,  2321,  8362, 10787,   119, 21722,\n",
            "         1350,  1120,  1117, 14120,  1676,  1105,  1488,  1173,  1171,  1120,\n",
            "         1117,  6566,   119,   107,   160,  1324,   118,   119,   119,   119,\n",
            "         1128,  1169,   107,   189,   106,   107, 21722,  1163,   117,  1208,\n",
            "         7820,  1206,  5542,  1103,  2147,  1105,  8547,  1117,  1266,   119,\n",
            "          107,  9918,  1166,  1115,  4346,  1208,  1137,   146,  1209,   119,\n",
            "          119,   119,   107,  1103,  1299,  1163, 28048,  1774,  1106,  2049,\n",
            "        21722,   107,   188,  1289,   119, 21722,   107,   188,  1257, 14568,\n",
            "         1171,  1105,  5275,  1206,  1117,  3437,  1105,  1117,  1676,   119,\n",
            "          107,  1192,   119,   119,   119,  1128,  7320,  1488,  1104,   170,\n",
            "         7979,   119,   119,   119,   107, 21722,  1163,   117,  1260, 24653,\n",
            "         1193,   119,  1124,  1125,  1576,  1149,  1104,  6665,   119,  1124,\n",
            "         6964,  1103, 24181, 15634,  1173,  7113,  1122,  6275,   118,  1148,\n",
            "         1154,  1103,  1747,   119,  1109, 11295,  4509,  9898,  1165,  1119,\n",
            "         1519,  1301,  1104,  1103,  4346,  1105,  1103,  8198,  1540,  1115,\n",
            "         1166, 24144,  1140, 25149,   119,  1109,  3437, 12604,  1399,  4267,\n",
            "        20080, 11682,  1181,  1117,  4840, 12604,  1105,  2045,  2019, 21722,\n",
            "         1106,  6268,  1103, 24181, 15634,   119, 21722,  1350,  1120,  1103,\n",
            "         1747,  1105,   176,  7729,  1117,  3307,  1112,  1103,  1299,  2580,\n",
            "         1485,   117,  1515,  1106,  4392,  1117,  2257,  3326,   119,  1109,\n",
            "         1299,  1865,  1103,  4346,  1121,  1103,  1747,  1105, 14408,  1103,\n",
            "        24561,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1238,   107,   189,  5363,  1106,  1267,  1128,  1303,   106,\n",
            "          107,   146,  1163,   117,  3753,  1106,  1267,  1139,  2104,  1108,\n",
            "         1675,   119,   107,  1192,  1238,   107,   189,  1587,  1143,  1128,\n",
            "         1127,  1107,  1411,  1142,  1989,   106,   107,   107,  1828,   119,\n",
            "         5445,  1270,  1143,  1314,  1989,  1105,  1455,  1191,   146,  1180,\n",
            "         1435,  1106,  1411,   119,  1124,  1163,  1119,  1125,  2919,   170,\n",
            "         1710,  1106,  8294,  1240,  1836,  1105,  4205,  1191,   146,  1108,\n",
            "         1907,   119,   146,  2347,  1103,  1148,  3043,  1121,  1980,  1106,\n",
            "         1129,  1303,   117,   107, 18653,  1161,  1500,  1143,   119,   146,\n",
            "         1350,  1481,  1123,  1105,  1486,  6343,  2368,  1143,   170,  8892,\n",
            "         2944,  4003,   119,   107,  3435,  1166,  1303,   132,   146,   107,\n",
            "         1396,  1400,   170,  2337,  1234,   146,  1328,  1128,  1106,  2283,\n",
            "          119,   107,  1650,  1142,  1553,  1103,  1710,  1125,  1541,  4690,\n",
            "         1223,  1236,  1112,  1103,  6065,  1127,  2547,  1106,  8422,  1213,\n",
            "         1103,  2094,  1166,  1120,  1103,   171,  9435,  2105,   119,   146,\n",
            "         1723, 18653,  1161,   113,  1150,  1108, 11519,  4239,  1143,  1118,\n",
            "         1103,  6703,   114,  1166,  1106,  6343,  1105,  1117,  2053,   119,\n",
            "         8125, 16534,  6343,  1529,   170,  1873,  1150,  1350,  1106,  1129,\n",
            "         1164, 18653,  1161,   107,   188,  1425,   119,  1153,  1108,  3351,\n",
            "         8208,  5387,  7264,  1105,  1125,  1113,  1199,  5283, 22604,   119,\n",
            "         1430,  3459,  1127,  2426,  7212,  1105,  1152,  1593,  1691,  1112,\n",
            "         1191,  1122,  1127,  1199,  2076,  1104, 11580,   119,  1430,  1843,\n",
            "         1602,  1716,  1108,   171, 12809,  4774,  1120,  1103,  1499,  1104,\n",
            "         1123,  1246,  1105,  1103,  1832,  1108,  1263,  1536,  1106,  1576,\n",
            "         1205,  1123,  1171,   119,  9996,  3338,  1123,  1108,   170,  1685,\n",
            "         1873,  4462,  9279,  1133,  1125,  3058,  1716,  1115,  1108,  4353,\n",
            "         1154,   170,  3170,   118,  1176,  7125,   119,   107,  1636,  1132,\n",
            "         1828,   119,  5445,   107,   188,  2053,   117,   107, 18653,  1161,\n",
            "         1310,  1103,  4784,  1116,   119,   107,  1188,  1110,   107,  9738,\n",
            "        11907,  1183,   107,   117,   107,  1131,  1163,   117,  6903,  1106,\n",
            "         1103,  2214,  1104,  1103,  1160,  2636,   119,   107,  1153,  6618,\n",
            "         1828,   119,  5445,  1120,  1117,  1644,  1701,   119,   107,  1153,\n",
            "         1680,  1149,  1106,  5854,  1139,  1289,   119,  1153,  1125,   170,\n",
            "         1541,  3999,  2003,  1105,  1108,  1304,  3258,  2019,  1143,   119,\n",
            "          107,  1262,  1142,  1110,   107,  7585, 11907,  1183,   107,   117,\n",
            "         9738,   107,   188,  5009,   117,   107, 18653,  1161,  1598,   117,\n",
            "        10404,  1139,  2209,  2019,  1103,  1168,  1873,   119,  1153, 12076,\n",
            "         1193,  2387,  1196, 13520,  1158,  1481,  1123,  5009,   119,   107,\n",
            "          146,  3319,  1128,   107,  1231,   107, 17723,   107,   117,   107,\n",
            "         9738,  1310,   119,   107,   146,  1767,  1155,  1164,  1128,  1121,\n",
            "        18653,  1161,   119,  1153,  1541,  3093,  1106,  1440,  1146,  1106,\n",
            "         1128,   119,   107,   146,  1350,  1166,  1120, 18653,  1161,  1150,\n",
            "         1125,  1113,   170,  1992,  5207,   119,   107,  3350,  1500,  1143,\n",
            "         1164,  1240,  1692,   119,   146,  3683,   146,  1180,  1138,  1151,\n",
            "         1175,  1133,   146,  1125,  1640,  1608,  1106,  1139,  1491,   119,\n",
            "          146,   107,   182,  2816,  1106,  2100,  1115,  2534,   144,  2861,\n",
            "         1400,  1184,  1108,  1909,  1106,  1140,   132,  1184,  1119,  1225,\n",
            "         4234,  2566,  9210,   119,   146,   107,  1396,  1178,  1151,  1171,\n",
            "         1303,  1111,   170,  2370,   119,  3350,  1163,  1195,  1127,  1280,\n",
            "         1106,  1138,   170,  1710,  1105,  1290,   146,  1567,  3512,  1177,\n",
            "          146,  1198,  1125,  1106,  1435,   106,  1284,  1814,  7585,  1183,\n",
            "         1373,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,   119,  1124,  1608,  1117,  4346,  1106,  1157, 24561,  1105,\n",
            "         1173,  2045,  1763,  1103,  2378, 21722,   119,   107,  2353,  1540,\n",
            "         1110,  3254,  2354,  1810,  2165,   117,   107,  1119,  1163,  1106,\n",
            "        21722,  1112,  1119,  2045,  1118,   119, 21722,  3358,  1117,  1246,\n",
            "         1106,  1339,  1117,  6048,   107,   188,  1171,   119,   107,  2907,\n",
            "          119,   119,   119,   146, 12529,  1111,  1781,  1216, 27735,  5252,\n",
            "          119,   119,   119,  1133,   146,  1834,  1142,  4346,   119,   119,\n",
            "          119,  1175,  1110,   170,  1167,  3021,  2187,  1120,  1289,  1105,\n",
            "          146,  1180,  1136,  1321,  1251,  9820,   119,   119,   119, 10737,\n",
            "         1143,   119,   119,   119,   107,   107,  2009,   136,  1192,  3623,\n",
            "         1143,  1105,  1139,  1266,   119,   119,   119,  1128,  1274,   107,\n",
            "          189, 10026, 19827,   119,   119,   119,   107, 21722,  1163,  1114,\n",
            "        18875,  1107,  1117,  1490,   119,   107,  5055,  1240, 21087,  4346,\n",
            "         1105,  1301,   119,   119,   119,   107,  1556,  1115,   117,  1103,\n",
            "         1299,  9321, 21359,  1513,  4342,  1174,  1283,   119,   107, 21722,\n",
            "          106,   107,   148, 12577,  1663,  6104,  1170,  1103, 19114,  1108,\n",
            "         2065,   119,   107,   148, 12577,  1663,   106,   107, 21722,  3028,\n",
            "          119,  1124,  2434,  1117,  4346,  1105,  6169,  1106,  1117,  1676,\n",
            "          107,   188,  1334,  1106,  7238,  1123,   119,  1124,  1508,  1117,\n",
            "         1981,  1213,  1123,  3221,  1112,  1131,  2795,  1154,  3632,  2355,\n",
            "         1147,  1488,   117,  1150,  1108,  6675,  9733,   119, 21722,   107,\n",
            "          188,  1319,  1257,  1310,  1106,  1218,  1146,  1114,  3632,  1104,\n",
            "         8730,  1115,  1117,  1676,  1105,  1488,  1127,  8362,  7111,  4611,\n",
            "         1107,  1103, 10676,   119,   107,   148, 12577,  1663,   117,   146,\n",
            "          107,   182,  1177,  2959,   119,   119,   119,   107,   107, 21722,\n",
            "          119,   119,   119,  1157,  3008,   117,  1112,  1263,   119,   119,\n",
            "          119,  1112,  1263,  1112,  1128,  3920,   107,   189,  2644,   119,\n",
            "          119,   119,  1105, 23612,  8584,  1110,  2914,   119,   119,   119,\n",
            "          107,  1131,  1163,   117,  1253,  5121,   119,   138,  1374,  2005,\n",
            "         1224,   117,  1170,  1152,  1125,  3035,  1147,  1488,  1205,   117,\n",
            "         1152, 14758,  1103,  3290,  1106,  1147,  1313,   119,  1109,  1282,\n",
            "         1187, 21722,  1125,  7573,  1194,  1103,  2095,  1108,  1208,  5490,\n",
            "         1106,  1103,  1796,  1105,  1103,  3664,  1108,  4058,  3966,   119,\n",
            "          107,  2048, 21722,   119,   119,   119,  1440,  1184,  2171,   119,\n",
            "          119,   119,   107,   148, 12577,  1663,  1163,   117,  1253, 10613,\n",
            "         1120,  1103,  3290,  1694,  1106,  1147,  1313,  1118,  1103,  2147,\n",
            "          119,   107,  3458,  1313,  1110,   170,  6477,   119,   119,   119,\n",
            "         1157,  1280,  1106,  1321,  2277,  1106,  8239,  1142,  3290,   119,\n",
            "        21722,   136,   107,  1153,  1455,   117,  2840, 10459, 21722,  1445,\n",
            "          107,   189,  2520,  1171,  1106,  1123,   119,  1153,  1454,  1213,\n",
            "         1105,  1350,  1171,  1154,  1103,  6219,  2020,  1154,  1103, 10552,\n",
            "          119,  1430,  2252,  1338,  3179,  1205,  1114,   170,  4920,  1166,\n",
            "         1117,  2342,  1105,  1117,  4346,  1107,  1289,   119,   107, 21722,\n",
            "          117,  1184,  1132,  1128,  1833,   136,   107,   107,   146,  1169,\n",
            "          107,   189,  1519,  1142,  2484,   117,   148, 12577,  1663,   119,\n",
            "          119,   119,   107,  1119,  1500,  1123,   119,   107,  1124,  3623,\n",
            "         1412,  1313,  1105,  1261,  1103,  4346,  9406,  1286,  1107,  1139,\n",
            "         1920,   119,  8696,   146,  1138,  1199,  3243,  1106,  2367,  1115,\n",
            "         2564,  1991,   119,   119,   119,   107,   107, 21722,   117,  1274,\n",
            "          107,   189,  1202,  1142,   119,   119,   119,  1122,   107,   188,\n",
            "         1136,  3869,  1122,   119,   119,   119,  1341,  1164, 23612,  8584,\n",
            "          119,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1315,  1290,  1131,  2144,   107,   189,  1243,  1149,  1536,\n",
            "          117,   107,  1131,  1598,   119,  1153,  1316,  7585,   107,   188,\n",
            "         1289,  1112,  1131,  2910,   119,   146,  2471,  1107,  3311,   119,\n",
            "          107,  8652,   119,   119,   119,  9738,   117,  1240,  1314,  1271,\n",
            "          119,   119,   119,  1225,  1128,  1474,  1122,  1108,   107, 11907,\n",
            "         1183,   107,   136,   107,   146,  1455,  1107,  2593,   119,  1153,\n",
            "         2471,   119,   107,  2372,  1128,   119,   119,   119,  3566,  2272,\n",
            "         1106,   107,  9103, 11907,  1183,   107,   136,   107,   107,  2814,\n",
            "          119,   119,   119,  1131,   107,   188,  1139,   119,   119,   119,\n",
            "         2214,  2104,   117,   107,  9738,  1163,   117,  1123,  1490,  1976,\n",
            "         2898,  6531,   119,   107,  2048,   106,  1337,   107,   188,  5426,\n",
            "          119,   146,  1355,  1106,  1644,  1278,  1114,  1123,   117,   107,\n",
            "          146,  5133,   119,   146,  3535,  1139,  2304,  1125,  1189,  1123,\n",
            "         8362, 21018,  1183,   119,  6343,  1125,   170,  1440,  1104,  4517,\n",
            "         1107,  1117,  1257,   119,   107,  2048,   119,   119,   119,  9294,\n",
            "         1185,   117,   146,   107,   182,  2959,   119,   146,  9424,   119,\n",
            "          119,   119,   107,   146,  1976,  1896,   117,  2840, 10459,  1184,\n",
            "          146,  1125,  1163,   119,  1337,   107,   188,  1268,   132,   146,\n",
            "         1108,  1256,  1103,  1141,  1150,  3346,  7726, 10403,  4189,  1112,\n",
            "         1103, 16810,  1107,  1103,  1692,  1187,  1131,  1452,   117,   146,\n",
            "         1354,   119,   107,  1302,   117,  1185,  1122,   107,   188, 15354,\n",
            "          119,   119,   119,   107,  1131,  1163,   117,  1774,  1106,  1294,\n",
            "         1143,  1631,  1618,   119,  1153,  1976,  2014,  1103,  2548,   119,\n",
            "          107,  1135,  1108,  3505,  1106,  2283,  1240,  2104,   117,   107,\n",
            "         1131,  1500,  1143,   119,   146,  1350,  1171,  1166,  1120, 18653,\n",
            "         1161,  1150, 19829,  2387,  1254,   119,   107,  2160,   119,   119,\n",
            "          119,   107,   146,  1163,   117, 10810,  1123,  1166,  1123,  2342,\n",
            "          119,   107,  1153,   107,   188, 14485,  1106,  1561,   170, 21330,\n",
            "         7482,   119,   119,   119,  1105,   146, 13938,  1106,  1129,  1175,\n",
            "         1111,  1123,   119,   107,  1335,  1142,  1553,  9738,  1882,  4742,\n",
            "        11353,   119,   146,  3535,  1131,  2023,   188,  2605, 17242,  1103,\n",
            "         1586,   119,   107,  1409,  1128,   107,  1325,  9107,  1143,   117,\n",
            "          146,  1541,  1328,  1106,  2222,  1139,  1289,  1120,  1115,   171,\n",
            "         9435,  2105,   106,   107,  1131,  1163,   117,  3219,  1123,  2209,\n",
            "         2019,  1103,  2094,   119,   107,  3435,  1113,   117,  7585,  1183,\n",
            "          117,  1519,   107,   188,  1301,  1243,  1380,  1106,  3940,   106,\n",
            "          107,  1131,  1163,  1106,  1123,  5009,   119,   107,  2091,  1152,\n",
            "         1138, 22620,  5084, 12266,   117,  1422,  5668,  9738,   136,   107,\n",
            "         7585,  1455,  1123,  5009,   117,  1702, 15496,   119,   107,   146,\n",
            "         1341,  1177,   117,  7585,  1183,   119,  2421,   107,   188,  1301,\n",
            "         1525,  1149,   117,   107,  9738,  1163,  1112,  1131,  1261,  7585,\n",
            "         1114,  1123,  1166,  1106,  1103,   171,  9435,  2105,   119,   107,\n",
            "          146,  5340,   117,   146,   107,   182,  2851,  5790,   117,   107,\n",
            "        18653,  1161,  1896,   119,   107,   146,   107,  1396,  1458,  1106,\n",
            "         2222,  1199,  1104,  1115,   179,  6592,  3457,  9323,  1290,   146,\n",
            "         1486,  1122,   119,   107,   107,  1573,   117,  6980,   119, 17685,\n",
            "          119,   119,   119,  1293,  1138,  1128,  1151,  1290,  1128,  1127,\n",
            "         1308,   136,   107,  6343,  1455,  1517, 18653,  1161,  1125,  1286,\n",
            "         1111,  1103,   171,  9435,  2105,  1952,   119,   107,   146,   107,\n",
            "         1396,  1151,  1833,  1218,   119, 21981,  4395,  1149,  1114,  1103,\n",
            "         6768,  1933,  1205,  1120,  1103,  2021,  2853,   117,   107,   146,\n",
            "         5133,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,   119,   119,   107,  1131,  1163,   117,  1774,  1106,  7627,\n",
            "         1123,  2252,  1136,  1106,  6799,  1103,  1299,  1107,  2221,   119,\n",
            "        21722,  1868,  1117, 21297,  1981,  1194,  1123,  1263,  1716,   117,\n",
            "         1134, 17531,  1114,   170,  3345,  8561,   119,   107,   146,  1138,\n",
            "         1142,   117,   148, 12577,  1663,   119,   119,   119,  1122,  1209,\n",
            "         3244,  1143,   119,   119,   119,  1274,   107,   189,  3994,  1164,\n",
            "         1143,   119,   146,   107,  1325,  1129,  2914,   119,   146,  2023,\n",
            "         1139,  4437,  1106,  3244,  1128,  1314,  1159,  1105,   146,   107,\n",
            "          182,  1543,  1115,  1269,  4437,  1208,   119,   107,  1124,  2045,\n",
            "         1149,  1103,  1524,  1442,  1105,  1865,  1117,  9580,  1149,  1104,\n",
            "         1103,  7419,   119, 21722,  5002,  1117,  4920,  1113,  1105,  4309,\n",
            "         1117,  4346,  1106,  1117,  1171,   119,   148, 12577,  1663,  1723,\n",
            "         1140,  1149,  1104,  1103,  1442,   119, 21722,  8243,  1117, 23473,\n",
            "         1105,  1173,  1508,  1113,   170, 10815,   119,  1124,  5998,  2135,\n",
            "         1117,  8295,  1105,  1231,  1964,  5790,  1157,  2395,   119,   148,\n",
            "        12577,  1663,  1350,  1154,  1117,  1257,  1194,  1117, 10815,  1105,\n",
            "         1119,  1608,  1103,  1440,  1196,  4581,  1149,  1105, 24485,  1205,\n",
            "         1103,  1480,   118, 12923,  1174,  4083,   119,  1247,  1144,  1106,\n",
            "         1129,   170,  2255,  1119,   107,   173,  5622,  1143,  1149,   119,\n",
            "          119,   119,  2108,  1111,  1115,  4346,   119,   119,   119,  1262,\n",
            "         1119,  1108,  4004, 23665, 18484,   107,   188,  4346,   119,   119,\n",
            "          119,   146,  1169,  1178,  1435,  1146,  1114,  1141,  7108,  1115,\n",
            "         2228,  2305,   119,   119,   119,  1105,   146,  1138,  1106,  1525,\n",
            "         1149,  1191,   146,   107,   182,  1268,   119,   119,   119, 21722,\n",
            "         1354,  1112,  1119,  7850,  1205,  1103,  1812,   117,  1117,  4920,\n",
            "        17332,  1107,  1103,  3223,   119,  1124,  1231,  1964,  5790,  1117,\n",
            "         2395,  1105, 16112,  1154,  1103,  1480,   119,  1706,  4108, 16752,\n",
            "         6105, 17226,   119,   119,   119,   113,   138,   120,   151,   131,\n",
            "          146,  2197,  1106,  2760,  1142,  1642,  1112,  1263,  1112,   146,\n",
            "         1169,  1712,  1909,  1146,  1114,  4133,   119,  1249,  1122,  1110,\n",
            "          117,  1103,  1236,   146,   107,  1396,  3390,  1149,  1103,  1642,\n",
            "         1110,  1912,  1104, 18110,  1177,   146,  1274,   107,   189,  1221,\n",
            "         1293,  1263,   146,  1169,  1301,  1443,  2479,  1315, 15736,   119,\n",
            "         3278,  1159,   117,  1122,  1209,  1322,  1146,  1781,  1103,  7281,\n",
            "         1104,  1210,  1472,  1827,  1104,  2458,  1105,   146,  1336,  1660,\n",
            "         1296,  1141,  1157,  1319,  1554,   118,  6192,  3660,  1642,   119,\n",
            "          114,  8201,  1318, 15586,   122,   131, 16805,  6626,  2943,   122,\n",
            "          138, 14936,  8229, 21359,  1513,  4342,  1174,  1154,  1103, 10629,\n",
            "         1104,   170,  3532, 14995,  3804, 26533,  1107,  1103, 11408,   119,\n",
            "         1556,  1140,  1119,  2446,  1103,  4346,  1104, 23665, 18484,  1113,\n",
            "         1117,  1171,  1112,  1218,  1112,  1117, 24181, 15634, 14680, 21943,\n",
            "         1186,  1107,  1117,  1286,  1289,   119,  1230,  2221,  4920, 19812,\n",
            "         1107,  1103,  6812, 10647,  1112,  1119,  1261,   170,  1440,  1120,\n",
            "         1117, 11342,   119, 16805,   107,   188,  7947,  3856,   119,   119,\n",
            "          119,  1119,  1354,   119,  1124,  1209,  1435,   119,   119,   119,\n",
            "         2028,   119,   119,   119,  1124,  2045,  2019,  1103,  1415,  4122,\n",
            "         3581,  1120,  1103,  1524,  1104,  1103,  3804,   119,  1130,  1103,\n",
            "         8492,  1104,   170,  1248,   117,  1119,  3583,  1105,  1173,  6901,\n",
            "         1117, 24181, 15634,   119,  1249,  1103, 24181, 15634,  1608,  1106,\n",
            "         1157, 24561,   117,  1103,  4122,  3581,  3325,  1107,  1160,  1105,\n",
            "         1173,  2204,  3966,   117,  1515,  1151,  2195,  1107,   170, 14119,\n",
            "         1118,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,   119,   107,   146,   107,   182,  5171,  1128,  1105, 18653,\n",
            "         1161,  6182,  1143,  1106,  3593,  1106,  1103,  4066,   119,   146,\n",
            "         2010,   107,   189,  1138,  1793,  1122,  4303,   119,   119,   119,\n",
            "         9294,  1105,  4268,  1840,  1143,   107, 17723,   107,   119,  1284,\n",
            "          107,  1231,  2053,   132,   146,  9353,  1106,  3968,  1103,  4698,\n",
            "         4233,   119,   107,   107,  1124,  1324,   117,   146,   107,  1325,\n",
            "         2676,  1115,   117,   107,  1119, 10072,   119,   107,  2119,   117,\n",
            "          146,  1450,  1115,  1128,  1125,   170, 11858,  1692,   119,   146,\n",
            "         1445,   107,   189,  1280,  1106,  1519,  1128,  3465,  1107,  7237,\n",
            "         1191,  1128,  1125,   170,  2640,  1106,  1294,  1614,  1268,   117,\n",
            "          107,  1119,  1500,  1143,   119,   107,  2966,   146,  5737,  9738,\n",
            "         1165,   146,  1814,  1146,   119,   119,   119,  9103,   136,   107,\n",
            "          146,  1455,  1140,   119,   107,  1302,   117,   146,  1341,  1131,\n",
            "          107,   188,  3008,   119,  3473,   117,  1614,  1138,  1151,  2785,\n",
            "        10987,  1111,  1123,  3055,   119,   119,   119,   107,  6343,  1163,\n",
            "          117,  1117,  1490, 13161,  1228,   119,   107,  2966,  1380,  3333,\n",
            "          136,   107,   146,  1455,   119,   107,  2814,   119,   119,   119,\n",
            "         1131,  1108,   119,   119,   119,  4601,  1114,  3513,   119,   119,\n",
            "          119,   107,  1119,  3716,   119,   107,  1135,  2171,  1165,  1195,\n",
            "         3891,  1123,  1313,  1491,   117, 23209, 11098,   119,  1337,   107,\n",
            "          188,  1725,  7585,  1116,  1110,  6218,  1114,  1366,   132,  1123,\n",
            "         1534,  1793,  1106,  4207,  1123, 12585,   117,  9738,   117,  1107,\n",
            "          170,  3513,  1692,  1105,  1122,  1108,  1814,  1106,  1609,  1219,\n",
            "         1103,  3443,   119,   146,  1108,  1123,  6507,  1107,  1115,  1692,\n",
            "          119,   119,   119,  1122,  1445,   107,   189,  1126,  3123,  1692,\n",
            "          119,   119,   119,  4882,   146,  1125,  1106,  2239,  1114,  8615,\n",
            "        13189,  1161,  3262, 14812, 10841,   119,   119,   119,   107,   107,\n",
            "         3262, 14812, 10841,   136,  2181,  1131,  2272,  1106,  5096, 27727,\n",
            "        20882,   136,   107,   146,  1455,   119,   107,  1153,   107,   188,\n",
            "         1117,  1797,   119,   119,   119,  1131,  1338,  1106,  1142,  1583,\n",
            "         1114,  1199,  6061,  7972,  4928,  1107,  1713,  1106,  1243,  1143,\n",
            "         1171,   119,  1153,   107,   188,   170, 14221, 12734,  1158,  5250,\n",
            "         3309,  4873,  1120,  1178,  1425,  1429,   119,   119,   119,  4882,\n",
            "         1131,  7407,  1106, 15005,  2256,  1150,   119,   119,   119,  1218,\n",
            "          117,  1131,  7407,  1106, 15005,  2256,   119,   107,   107,  2160,\n",
            "          117,   146,  1341,   146,   107,  1396,  1767,  1104,  1123,   119,\n",
            "         1153,   107,   188,  1151,  6218,  1107,  1860,  1105,  1144,   170,\n",
            "         3264,  1647,   119,   119,   119,  1218,   117,  4547,  1235,  1131,\n",
            "         1899,  1128,   117,   107,   146,  1163, 27912,  1193,   119,   107,\n",
            "         1337,  1108,   170, 15372,  1692,   119,  7585,  1116,   107,  1534,\n",
            "         1108,  1177,  4293,  1146,  1107,  1123, 24007,  2197,  1105,  1131,\n",
            "         1541,  1238,   107,   189,  1920,  1184,  1156,  3333,   119,   119,\n",
            "          119,   146,  1631,  2959,  1111,  7585,  1116,   117,  1131,  2144,\n",
            "          107,   189,  1256,  1221,  1184,  2171,   119,  1135,  1108,  1145,\n",
            "         1304,  5283,   119,   119,   119,  5403,  1515,  1774,  1106,  6472,\n",
            "          170,  7230,  1165,  1103,  1558,  6171,  1110,  4840,  3094,  1158,\n",
            "          117,   107,  6343,  1163,  1114,  1126,  1586,  1104,  1107, 19091,\n",
            "         7222,  1107,  1117,  1490,   119,   107,  7258,  3094,  1158,   119,\n",
            "          119,   119,  9738,   107,   188,   170,  6170,  1776,   136,  1337,\n",
            "         1156,  4137,  1103, 22604,   117,   107,   146,  1163,   119,  1650,\n",
            "         1142,  1553,   132,  9738,   117,  7585,  1105, 18653,  1161,  1127,\n",
            "         1909,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101, 14680, 21943,  1186,   107,   188,  9605,   118,  3613, 26632,\n",
            "          119,  1109,  2221,   118, 14439, 12604,  1399,  2242,  1103,  1514,\n",
            "         2885,  1104,  1103,  3804,   119,  1230, 10139, 10001,  1107,  1103,\n",
            "         4542,  1104,  1103,  1480,   118, 12923,  1174,  3804,   119,  7935,\n",
            "          117,  1119,  5162,  1107,  1103,  2243,  1104,  1103,  2885,  1112,\n",
            "         1191,  1119,  2637,  1380,   119,   107,  1220,   107,  1231,  1303,\n",
            "          119,   119,   119,   107,  1119,  1163,  1106,  1471,   119,  1124,\n",
            "         2494,  1310,  4619, 14680, 21943,  1186,  1256,  1463,  1122,  1691,\n",
            "         1112,  1191,  1185,  1141,  1133,  1140,  1108,  1175,   119,  4914,\n",
            "         5183,   117,   170,  4632,   188,  8167, 24324, 17547,  1194,  1103,\n",
            "        12816,  1183, 22942,  1104,  1103,  3804,  2885,   119,   138, 26117,\n",
            "         2482, 25242,  1149,  1104,  1103,  4542,  1105,  3867,  1106,  3963,\n",
            "         1103,  1299,  1118,  3774,   119,  1252,  1112,  1122,  1225,   117,\n",
            "         1103, 12604,  1399,  1125,  1117,  4621,  3106,  3795,  1105,  1125,\n",
            "        13699,  1122,  1481,  1140,   117,  6755,  1114,  1103, 19508,  3919,\n",
            "        11922,  2861,   119,   138,  4295,  5354,  1104,  2489,  8293,  1149,\n",
            "         1165,  1103,  4346,  1899,  1157,  4551,  1105,  1112,  1122,  1225,\n",
            "          117,  1103,  1395,  1108,  2840,  4941,  1146,  1118,  1103, 22572,\n",
            "         5709, 26041,  1120,  1103,  1499,  1104,  1103,  2885,   119,  1109,\n",
            "        12604,  1399,  2494,  1454,  1117,  1246,  1481,  1140,  1106,  1267,\n",
            "         1184,  1119,  1125, 13699,   119,  1212,  1103,  1168,  1322,  1104,\n",
            "         1117, 24181, 15634,   117,   170,  1769,  7874,   118,  1176,  1217,\n",
            "         1114,  3999,  1894,  1257,  1108, 24034, 18121,  1194,  1157, 14701,\n",
            "          117, 15571,  1103,  4346,  6852,  1114,  1157, 19659, 10318,  1493,\n",
            "          119,  1109,  1217,  1691,  1106,  1129,  1189,  1104,  5805,  6464,\n",
            "         1112,  1157,  1404,  1108,   170,   182,  2149,  3781,  1602,  1105,\n",
            "         1691,  1106,  1129,  1593,  3245, 13169,  1656,  1157,  1404,   119,\n",
            "         1109, 12604,  1399,  7113,  1103,  4346,  1154,  1103,  6093,  1256,\n",
            "         1748,  1105,  2416,  1122,  1106,  2702,  1166,  1107,  2489,   119,\n",
            "         1599,  1107,   170,  6746,   117,  1119,  6367,  1117,  4346,  1105,\n",
            "         1310,  1106,  7253,  1122,  1171,  1154,  1157, 24561,   119,  1332,\n",
            "         1103,   107, 13440,   107,  1104,  1103, 24181, 15634, 16348,  1154,\n",
            "         1157, 24561,  8293,  1149,   117,  1103,  6464,  8839, 12755,  1154,\n",
            "          170, 23609, 13002,  1104,  1602,  6161,  1229,  1157,  1473,  5354,\n",
            "         1680,  1103,  8993,  1104,  1103,  2885,   119,  1249,  1770,  1112,\n",
            "         1103, 23609, 13002,  9898,   117,  1421,  1167,  1602, 23609, 13002,\n",
            "         1116,  2578,  2200,  1105,  1330,  1421,  6464,  7207,  3152,  1149,\n",
            "         1104,  1172,  1781,  1103,  1282,  1104,  1147,  4984,  9304,  8767,\n",
            "         5123,   119,  1220,  1976,  4405,  1103,  1299,  1107,  2221,  1105,\n",
            "         8475,  1147, 16094,  3307,   119,  2397,  1894,  1257,  3597,  1147,\n",
            "        23186,  1105,  1147, 11222,  1127,  1112,  4295,  1112, 14726,   119,\n",
            "          107,  3046,  1104,  1128,   136,  1192,  1138,  1139, 13532,   119,\n",
            "          119,   119,   107,  1103,  1299,  1107,  2221,  1163, 28048,   119,\n",
            "         2677,  1104,  1103,  7207, 19070,  1120,  1103, 12604,  1399,  1133,\n",
            "         1119,  2566, 21359,  1513,  4342,  1174, 11316,  1149,  1104,  2079,\n",
            "         1165,  1152, 27581,   119,  1124,  3182,  1117, 24181, 15634,  1105,\n",
            "         1173,  1691,  1106,  3282,  1122,  1133,  1173,  1238,   107,   189,\n",
            "         3373,  1122,  1149,  1155,  1103,  1236,   119,  1252,  1112,  1119,\n",
            "         1225,   117,   170,  8611, 15775,  1691,  1107,  1524,  1104,  1140,\n",
            "         1112,  2114,  1104, 18324,  1691,  1439,  1122,   119,  2994,  6746,\n",
            "         3387,  1114,  1141,  1104,  1103,  6464,  7207,   117, 23548,  4404,\n",
            "         1172,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1171,  1166,  1106,  1412,  1952,   119,   107,  2160,   119,\n",
            "          119,   119,   146,   107,   182,  1612,  1128,  1767,  1164,  1103,\n",
            "        26624,   118,   122,  1692,   119,  1249,   170, 16810,   146,   107,\n",
            "          182,  1612,  1122,   107,   188,  1435,  1146,  1107,  1103,  1763,\n",
            "          119,   119,   119,   107,   107,  2096,  1736,   132,  1115,  1108,\n",
            "         1103, 15881,  1692,  5336,  7726, 10403,  4189,   107,   188,  1401,\n",
            "         6813,   119,   119,   119,  1208,  1115,   146,  1341,  1164,  1122,\n",
            "          117,   146,  9148,  3455,  1380,  1164,  4840,  3094,  1158,  1107,\n",
            "         1115,  1692,   119,   107,   107,  1337,   107,   188,  1725,  1122,\n",
            "          107,   188,  1177, 15881,   119,  1109,  1825,  1150,  1982,  1115,\n",
            "         3094,  1158,  1108,  9738,   107,   188,  1534,   117, 19323,   119,\n",
            "         1337,   107,   188,  1184,  1103,  1692,   146,  7607,  1107,  1123,\n",
            "         1108,  8663,  1213,   119,   119,   119,  1122,   107,   188,  1912,\n",
            "         1104, 18110,   132,   146,   107,  1325,  1587,  1128,  1164,  1122,\n",
            "         1199,  1168,  1159,   117,   107,  6343,  1845,   119,   107,   146,\n",
            "         1341,   146,   107,   173,  1176,  1106,  2100,  1164,  1115,   119,\n",
            "          119,   119,   107,   146,  1163,  1114,   170,  2003,   119,  8540,\n",
            "         1213,  1103,  1395,   117,   146,  3535,  1380,  1108,  1821, 14788,\n",
            "          119,   107, 14072,  1104,  7726, 10403,  4189,   119,   119,   119,\n",
            "         1110,  1119,  1909,   136,   107,   146,  1455,   119,   107,  7066,\n",
            "          119,   119,   119, 14044,   119,   119,   119,   107,  6343,   188,\n",
            "        20284, 18583,   119,  1124,  1350,  1176,  1119,  1445,   107,   189,\n",
            "         6062,  2520,  1164,  1122,   119,   107,   146,  1274,   107,   189,\n",
            "         1341,  1119,   107,   188,  1280,  1106,  1129,  1303,   119,   119,\n",
            "          119,  1185,  1141,  1169,   119,   119,   119,  1525,  1140,   119,\n",
            "          119,   119,   107,  1119,   188, 16156,  7655,  1149,  1544,   118,\n",
            "        21898,  1193,   119,   146,  6901,  1115,  1119,  1445,   107,   189,\n",
            "         1675,  1120, 11337,   144,  2861,   107,   188,  3443,  1719,   117,\n",
            "         2693,  1139, 11458,  1111,  1140,  1106,  1129,  1675,   119,   107,\n",
            "         2048,   119,   119,   119,   107,   146,  1163, 17132,   117,  2422,\n",
            "         1104,  1126,  9107,  1106,  1849,  1103,  2548,   119,   107,  2119,\n",
            "          117,  2654,  1195,  1431,  1243,  1199,  2094,  1173,   119,  1135,\n",
            "         3093,  1412,  1710,  1110,  2613,   119,   119,   119,   107,   146,\n",
            "        13325,  2019,  9738,  1105, 18653,  1161,   117,  1150,  1127,  1241,\n",
            "         1107,  1103,  2243,  1104,   188,  7535, 15615,  1158,   170,  1185,\n",
            "         5412,  1513,   119,  6343,  2387,  1105,  1195,  1241,  2917,  2019,\n",
            "         1103,  4014,   171,  9435,  2105,   119,  1109,  1710,  1120,  1139,\n",
            "         1952,  4927,  1126,  2327,  4014,  1487,   119,  9738,  1500,  1143,\n",
            "         1164,  1123,  1313,  1491,  1104, 23209, 11098,  1105,  1256,  2910,\n",
            "         1164,  1123,  2104,   119,   146,  3560,  2385,   170,  1974,  1164,\n",
            "         1103, 23209, 11098,  4076,  1158,  7882,  2605,  3530,  1121,  1123,\n",
            "          119, 18653,  1161,  1105,  9738,  3166,  1106,  1243,  1373,  2385,\n",
            "         1218,   132,   146,  3319, 18653,  1161,  1834,  1800,  1123,  1425,\n",
            "         1107,  1123,  1297,   119, 18653,  1161,  1500,  2490,  1120,  1103,\n",
            "         1952,  1164,  1184,  1297,  1108,  1176,  1107,  1980,  1105,  1184,\n",
            "         1131,  1125,  1562,  1105,  1694,   119,  7585,  1882,  1106,  7311,\n",
            "         2135,  1123,  1451,  1937,   119,   119,   119,  1131,  1108,  1304,\n",
            "         1107, 27110,  8588,  1105,  8193,  1111,  1126,   122,   118,  1214,\n",
            "         1385,   119,   146,  1145,  3004,  4387,  5137,  1105,  5876, 14919,\n",
            "          170,  3143,  1120,   170,  8480,  1952,   119,  1220,  1127,  1241,\n",
            "         1702,  1977,  1106,  1147,  1862,  1112, 23641,   132,  1152,  1127,\n",
            "         1383,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,  6677,   119,  1448,  1104,  1103,  2735,  1160,  7207,  3867,\n",
            "         1106,  2364,  3708,  1113,  1103,  1299,   107,   188, 15879,  1105,\n",
            "         1793,  1106, 17279,  1140,  1121,  1103,  1334,   119,  1438,   117,\n",
            "         1112,  1770,  1112,  1119, 11168,  1103,  4018,   117,  1117,  1257,\n",
            "         4707,  1106,  1103,  2223,  6464,  6093,  1105,  1119, 12114,   170,\n",
            "         4840,  4346,  1106, 19717,  1103, 19114,  1107,  2286,   118,  1815,\n",
            "          119,  1135,  3885,  1103,  1217,  1154,  1103,  2095,   117,  1134,\n",
            "         1103, 12604,  1399,  1723,  1146,  1118, 21359,  1513,  4342,  1158,\n",
            "         1166,  1106,  1103,  6093,  1105,  4416,  1122,  1228,  1114,  1160,\n",
            "         3613, 26632,  1279,  1121,  1117, 14680, 21943,  1186,  4346,   119,\n",
            "         1986,  1178,  3544,  1114,  1141,  1167,  8050, 10704,  3113,   117,\n",
            "         1119,  1454,  1117,  2209,  1106,  1103,  2735,  8839,   119,  1135,\n",
            "         1519,  1149,   170,  4632, 13818,  1112,  1191, 14563,  1157,  7676,\n",
            "          119,   107,   146,  1138,  1185,  1159,  1111,  1638,   119,   119,\n",
            "          119,   107,  1103, 12604,  1399, 27629, 13153,  1112,  1119, 11161,\n",
            "         1117,  1263,  4920,   119,  1109,  6464,  6093,  1261,  1117,  4506,\n",
            "         1105, 14568,  1166,  1106,  1103, 12604,  1399,  1107,   170, 26534,\n",
            "         2042,   118,  1176,  4844,   117,   171, 15516,  1158,  1106,  1157,\n",
            "         1286,  1105,  1268,  1120,  1632,  2420,   119, 12118,  8057,  5305,\n",
            "          117,  1103, 12604,  1399,  3583,  1103,  4346,  1104, 23665, 18484,\n",
            "         1105,  1899,  1103,  6093,  1112,  1122,  2085,  1107,  1524,  1104,\n",
            "         1140,  1114,   170, 16605,  1158,  7113,   119,  1135,  3885,  1103,\n",
            "         6093,  1171,  1506,  1103,  3804,  2885,  1105,  1122, 10458, 19409,\n",
            "         1106,   170,  1831,  1107,  1103,  2057,   119,  9251,  4316,  1104,\n",
            "         1103,  1721,  3113, 20727,   117,  1103,  8198, 12604,  1399, 21359,\n",
            "         1513,  4342,  1174,  1807,  1103,  6093,  1173,  1338,  1205,  1114,\n",
            "          170,  4672,  9008, 26632,  1106,  1322,  1103,  2321,   119,  1124,\n",
            "         1519,  1149,   170,  6106,  1112,  1119,  1866,  1146,  1105,  1608,\n",
            "         1117,  4346,  1106,  1117,  1171,   119,  2543,  1159,   119,   119,\n",
            "          119,  1330,  1282,   119,   119,   119,   107,  2750,  2106,   117,\n",
            "         1828,   119,  5007,   106,   107,   170,  1685,  1873,  1163,  1106,\n",
            "          170,  4130, 11953,   119,   107,  8667,   117, 14989,   117,   107,\n",
            "         1119,  3028,   119,  1124,  7294,  1171,  1117,  1653,  1716,  1105,\n",
            "         1350,  1205,  1103, 16118,  1979,  2984, 11949,   118,  2262,  2472,\n",
            "          119,  1135,  1108,  2286,   118,  2106,  1105,  1103,  6001,  1629,\n",
            "         1108,   170,  3983,  1111,  2239,   118,  1525,  1468,  1105,  1934,\n",
            "         7571, 11609,   119,   107,  2777,  1132,  1128,  1280,  2052,   136,\n",
            "          107,   107,   146,   107,   182,  2033,  1199,  8162,  1111,  1139,\n",
            "         4113,   117,   107,  1131,  5133,   117, 20710,  1193,   119,   107,\n",
            "         2966,  1128,  1243,  1251,  1167, 17186,  1116,   136,   107,   107,\n",
            "         1753,  1870,  1133,   146,  2197,  1106,  1294,  1330,  3868,  1149,\n",
            "         1745,  1106,  1440,  1111,  1199,   119,   146,   107,  1325,  2676,\n",
            "         1106,  2498,  1141,  1171,  1111,  1128,   117,   107,  1119,  3028,\n",
            "          119,  1556,  1115,  1131, 18002,  1205,  1103,  1884, 15114,  6078,\n",
            "         2472,   119,  2098,  6929,  1142,  1411,  1110,  1177, 10795,  1104,\n",
            "         1234,  1176,  1143,   117,  1119,  1354,   119,  4222,  1201,  2403,\n",
            "         1119,  1125,  1691,  1107,  1103,  1411,  1114,  1185,  1231,  2528,\n",
            "         4838,  5796,  1104,  1150,  1119,  1108,  1137,  1184,  1125,  2171,\n",
            "         1106,  1140,   119,  1398,  1119,  1180,  9148,  2988,  1106,  7190,\n",
            "         1107,  1103,  1411,  1104, 10597,  3925,  1108,  1119,  1125, 25580,\n",
            "         1146,  1113,   170, 13366,  4640,   119,  1731,  1119,  1125,  4690,\n",
            "         1175,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1106,  3295,  4019,  1103,  1397,  1989,   119,  5876,  1107,\n",
            "         2440,  1108, 15496,  1164,  1909,  1171,  1106,  1103,  2049,   119,\n",
            "         1153,  1163,  1131,  1105,  4387,  1127,  1256,  2033,  1171,  1103,\n",
            "         4158,  1152,  3749,  1196,  2534,   144,  2861,  2856,  1241,  1104,\n",
            "         1172,  1112, 23641,   119,   146,  1108,  2816,  1111,  1147,  2244,\n",
            "         1105,  1500,  1172,   146,  5608,  1172,  1103,  1436,  1111,  1103,\n",
            "         2174,   119,  1258,  1429,  1904,  1104,  5497,  1105, 25132,   113,\n",
            "         1378,   170, 13108, 20392,   114,   117,  2617,  1866,  1146,  1120,\n",
            "         1103,  1952,  1397,  1106,  1366,  1105, 10316,  1117,  4077,  2525,\n",
            "          119,   107,  7025,   117,  6243,  1128,  1177,  1277,  1111,  7410,\n",
            "         1303,  3568,   117,   107,  1119,  1310,   119,   107,  1135,   107,\n",
            "          188,  1151,   170, 15194,  4687,  1106,  1138,  1128,  1155,  1303,\n",
            "         1142,  3440,  1106,  8294,  1103,  1958,  1104,  1103,  1763,  1374,\n",
            "         1552,   119,   146,  1538,  6243,  1828,   119,  6343,  5445,  1111,\n",
            "         4035, 12959,  1158,  1142,  1243,   118,  1487,  1105,   146,  1341,\n",
            "         1103, 11491,  1577,   107,   189,  1138,  1151,  1251,  1618,   119,\n",
            "          107,  1109,  6065,  1107,  1103,  1395, 12647, 15554,  4902,  2617,\n",
            "         1196,  1119,  3589,  1174,  1172,  1205,   119,   107,  1438,  1142,\n",
            "         1480,  2762,   107,   189,  2335,   119,  1130,  1901,  1106, 14118,\n",
            "         1103,  2280,  1104,  1412,  6768,   117, 10079,  1103,  1836,  1104,\n",
            "         6980,   119, 17685,  1105, 24793,  1103, 15468,  1104,  4387,  5137,\n",
            "         1105,  5876, 14919,  1112, 23641,   132,  1175,  1108,  1330,  2255,\n",
            "          146,  1458,  1106,  2498,  1366,  1487,  3568,   119,   107,  2617,\n",
            "         1680,  1166,  1103,  1952,  1105,  3015,  1146,   170,  1353,  5439,\n",
            "        22073,   119,   107, 17723,   117,  1156,  1128,  4268,  1435,  1166,\n",
            "          136,   107,  1119, 13325,  2019,  1143,   119,   146,  1350,  1213,\n",
            "          117,  1136,  7805,  1184,  1108,  1909,  1397,  1196, 20098,  4703,\n",
            "         1121,  1103,  1952,  1105,  8320,  2617,   119,   107, 17723,   117,\n",
            "         1112,   146,   107,   182,  1612,  1128,  1221,  1195,  1138,  1151,\n",
            "          188, 19091,  3970,  1106,   107,  1815,  1113,   107,  1121,  1103,\n",
            "         1958,  1104,  1103,  1763,  1160,  1201,   119,   146,  1631,  1112,\n",
            "         1103,  1207,  2534,  5096, 27727,   132,  1226,  1104,  1115,  4019,\n",
            "        17808,  1113,  1139,  3221,  1112,  1218,   119,  1249,  1216,   117,\n",
            "         1112,  1139,  1148,  2496,  1112,  2534,  5096, 27727,   117,   146,\n",
            "         1821, 19458,  1128,   117, 17723, 17685,   117,  1435,  1171,  1106,\n",
            "         1103,  5096, 27727,   107,   188,  3060,   117,   107,  1119,  1163,\n",
            "         1106,  1143,   119,  1124,  3541,  1143,  1103,  5439, 22073,   117,\n",
            "         1134,  4049,   170,  1207, 10890,   172,  7340,  8985,  1143,  1112,\n",
            "          170, 16810,   119,   146,  2373,  1103,  2269,  1113,  1103, 10890,\n",
            "          119,   146,  1125,  1562,  1122,  1242,  1551,  1991,  1112,  2534,\n",
            "         5096, 27727,  1133,  1138,  1309,  1460,  1141,  1290,  1139, 15468,\n",
            "         1112,  2534,  5096, 27727,  1125,  1435,  1121, 11337,   144,  2861,\n",
            "         2626,   119,  1135,  1108,  1637,  1107,  1126, 19870,  1947,  1114,\n",
            "         1103,  5096, 27727,   107,   188,  3060,  9438, 23245,  2135,  1103,\n",
            "         1499,   131,  1188, 10890,  1303,  2665, 14255, 23085,  1852,  1103,\n",
            "        26204,   164, 17723, 17685,  1103,  8969,  1105,  2266,  1112,   107,\n",
            "         1574,  5096, 27727,   107,   119,  1109,  2510,  1144,  2602,  1103,\n",
            "         2912,  1105,  7864,  3238,  1106,  2867,  1103,  1331,  1112, 16810,\n",
            "         1105,  1110,  1682,  1106,  3564,  1149,  1103, 15337,  1104, 14221,\n",
            "        12734,  1158,  6507,  1107,   170,  2175,  1104,  1644,   119,  1188,\n",
            "        10890,  1144,  1151,  4092,  1105,  1878,  1118,  2534,  5096, 27727,\n",
            "          164,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,  1137,  1184,  1125, 14715, 23709,  2331,  1108, 14673,  1120,\n",
            "         1436,   119,  1124, 13668,  1154,  1103,  1411,  1105,  1678,  1146,\n",
            "         4662,  1120,  1103,  1313,  1104,   170,  1469,  6800,   119,  1109,\n",
            "         1271,  1119,  1522,  1108,   107,  3274,  5007,   107,   117,   170,\n",
            "         1271,  1119,  1125, 27615,  1165,  1455,   119,  1109,  6800,  1108,\n",
            "         1126,  9808,  1590,  1150,  2085,  1283,   170,  1374,  1808,  1170,\n",
            "         1117,  4870,   119,  2577,  5694,   117,  1131,  3175,  1123, 17186,\n",
            "         1116,  4130,  1106,  1103,  1299,  1105,  1119,  1125,  1151,  3709,\n",
            "         1122,  1518,  1290,   119,  3100,   146,  1518,  1221,  1150,   146,\n",
            "         1821,   136,  1124,  1455,  1471,   119,  1124,  1350,  2019,  1103,\n",
            "         3901,  1105,  2542,  1112,  1103,  8435,  3733,  1107,   119,  1249,\n",
            "         1119,  2376,  1103,  2221,  3901,   117,   170,   188, 24657,  5354,\n",
            "        17547,  1103,  9441,  2472,   119,  1124,  1976,  1350,  1205,  1103,\n",
            "         2472,  1105, 10676,  1108, 14044,  4455,  1916,  1621,  1103,  3515,\n",
            "         1104,  1234,   119,   107,  1327,   107,   188,  1280,  1113,   136,\n",
            "          107,  1119,  6031,   117,  5277,  1106,  1267,  1111,  1471,  1184,\n",
            "         1108,  3989,  1103,  1908,  7609,   119,  6940,  1205,  1103,  2472,\n",
            "          117,  1234,  1127,  1919,  1105,  7406,   117,  1774,  1106,  3277,\n",
            "         2310,  1121,  1126, 19508,  4433,   119, 14989,  1125,  1145,  2141,\n",
            "          170,  1603,  3242,  1121,  5007,   107,   188,  4130,  1106,  1267,\n",
            "         1184,  1155,  1103, 27846,  1108,   119,  1109,  1234,  1205,  1103,\n",
            "         2472,  1127,  2150,  1106,  2303,  6410,  1106,  3451,  1122,  1108,\n",
            "         1115,  1108,  5937,  1158,  1103,  1411,   119,  1622,  1481,  1103,\n",
            "        14979,  4037,   117,  9476,  1115,  1691,  1112,  1690,  6719,  1127,\n",
            "        13333,  1103,  1234,  1105,  2204,  1158,  2256,  1152,  2374,  1106,\n",
            "         2519,   119,   107,   160,  1324,   119,   119,   119,   192,  1324,\n",
            "          118,  1184,   107,   188,  5664,   136,   107, 14989,  1163,  1165,\n",
            "         1131,  1486,  1155,  1103, 10676,   119, 21372,  8661, 15540,  1116,\n",
            "         1127,  1217, 26551,  1120,  1147,  7616,  1105,  1625,  1107,  1103,\n",
            "         2472,  1108,  1217,  6011,  1166,  1719,  1118,  1103, 10444,  4037,\n",
            "         1137,  1118,  1103,  6464,  7207,  1112,  1152, 14207, 15841,  1205,\n",
            "         1103,  2472,   119,  5007,  1454,  2019, 14989,  1105,  1486,  1131,\n",
            "         1108,  1253,  2903,   119,  1249,  1119,  1350,  1763,  1123,   117,\n",
            "         1119,  2347,  2552,  1104,   170,  1602, 23609, 13002,  1115,  1108,\n",
            "         1781,  1532,  1481,  1123,   119,   107,   157, 15499,  8842,  2249,\n",
            "         3663,   106,   155, 27370,   106,   107,  1119,  6031,  1114,  4517,\n",
            "          119,  1230,  4632,  5354,  2795,  1123,  1149,  1104,  1123,  1143,\n",
            "         6602,  9866,  5305,  1352,  1105,  1131,  1454,  1123,  1246,  1198,\n",
            "         1107,  1159,  1106,  1267,   170,  6464,  1217,  1781,  1532,  1105,\n",
            "         1164,  1106,  4585,   119,  1153,  6673,  1105,  1310,  1919,  2019,\n",
            "         5007,   107,   188,  4130,   119,  4114,  1112,  1131,  2085,   117,\n",
            "         1131, 23376,  1113,  1103,  1884, 15114,  6078,  2472,  1105,  2204,\n",
            "         1106,  1123,  4257,   119,  1109,  6464,  6093,  1261,  1103,  3767,\n",
            "         1105,  3867,  1106,  1601,  1103,  7275,   119,  1135, 19647,  1107,\n",
            "         1105,  4029,  1106,  4585,  1205,  1103,  1873,   119,  1130,  2050,\n",
            "         1394, 13851,  1193,   117,  3274,  2411,  1868,  1106,  1103,  1873,\n",
            "          107,   188,  4256,  1105,  3885,  1471,  1206,  1123,  1105,  1103,\n",
            "         6093,  8574,  4899,  1196,  1122,  1189,  1157, 10310,  2035,   119,\n",
            "         1153, 24863,  1105,  1454,  1283,  1112,  1103,  6093, 11406,  1105,\n",
            "         3867,   170, 26632,  1120,  1123,   119,  1332,  1131,  2788,  1720,\n",
            "         1125,  2171,  1106,  1123,   117,  1131,  2494,  1533,  1123,  1257,\n",
            "         1105,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  2617,   152,  2559, 25081, 22476,   119,   107,   138, 17458,\n",
            "         1106,  1412, 15649,  1629, 16810,   106,   107,  2617,  1163,  1517,\n",
            "          146,  1125,  1845,  3455,   119,   146,  1350,  1166,  1506,  1103,\n",
            "         1395,  1105,  2490,  1125,  1866,  1120,  1147,  7072,  2355,  1146,\n",
            "         1147,  7537,   119,   107, 19656,   106,   107,   146,  1767,  9187,\n",
            "          144, 17167, 10061,  5354,  1121,  1103,  1171,  1104,  1103,  1395,\n",
            "          119,  2617,  1125,  1117,  2525,  1107,  1103,  1586,  1112,  1218,\n",
            "          119,   107, 17723,   117,  1202,  1128,  1138,  1625,  1128,  1156,\n",
            "         1176,  1106,  1474,   136,   107,  1119,  1455,  1143,   119,   146,\n",
            "         1245, 22489,   119,   146,  1350,  1120,  1103,  1747,  1173,  1120,\n",
            "         1103, 22073,  1196,  3610,  1139,  1257,  1106,  2617,   119,   146,\n",
            "         1383,  1103, 22073,  1205,   119,   146,  1261,   170,  1996,  2184,\n",
            "         1105,   146, 11596,  2617,  1106,  2211,  1117,  1289,  2355,  1117,\n",
            "         4077,  2525,  1196,   146,  1310,  1106,  2936,   119,  1109,  1168,\n",
            "         6065,  6069, 19201,  1112,  1218,   119,   107,  2617,   117,  6243,\n",
            "         1128,  1111,  1142,   119,   146,   107,   182,  1304,  9473,  1115,\n",
            "         1128,  1328,  1106,  1143,  1106,  2866,  1103,  5096, 27727,   107,\n",
            "          188,  3060,   119,  1438,   117,   146,  1821,  3737,   119,   119,\n",
            "          119,   146,  1538,  6246,  1240, 12839,  2906,   117,   107,   146,\n",
            "         1163,  1106,  1140,   119,   107,   146,   107,   182,  2959,   119,\n",
            "          107,  6064,  1107,  1103,  1395,  2494,  1608,  1106,  1147,  3474,\n",
            "          119,  1135,  1108,  5119,  2490,  1108,   170,  1376, 10478, 19386,\n",
            "         5208,  1174,   146,  1125,  1454,  1205,  2617,   107,   188,  2906,\n",
            "          119,  1135,  1882,  1176,  1152,  1450,   146,  1108,  1280,  1106,\n",
            "         1129,  4172,  1103, 10890,  1115,  3440,   119,   107, 17723,   136,\n",
            "          107,  2617,  1163,   117,   170,  1376,  6764,   146,  1445,   107,\n",
            "          189, 10795,  1103, 10890,   119,   107,   146,  1274,   107,   189,\n",
            "         2437,   117,   146,  1354,  1128,  1458,  1106,  1862,  1112,   170,\n",
            "        16810,   119,   107,   107,  1135,   107,   188,  2276,  1115,   146,\n",
            "         1225,  1474,   146,  1108,  3888,  1107, 12137,   170,  1207,  1578,\n",
            "         1107,  1644,   119,  1438,   117,  1515,  1549,  1122,  1199,  1354,\n",
            "          146,  2788,  1122,  2010,   107,   189,  1129,  1107,  1139,  1436,\n",
            "         2199,  1106,  7568,  1103,  1648,  1104, 16810,   119,   119,   119,\n",
            "          107,   146,  1500,  1140,   117,  1373,  1114,  1103,  1832,  1104,\n",
            "         1103,  1395,   119,   107,   146,  1525,  1122,  1662,  1106, 10737,\n",
            "         1991,  1111,  1184,  2171,  1165,   146,  1108,  2534,  5096, 27727,\n",
            "         1105,   146,  1274,   107,   189,  1341,   146,  1821,  4451,  1177,\n",
            "         1263,  1112,  1115,  3430,  1106,  3118,  1107,  1139,  1713,   119,\n",
            "          107,  1109,  1395,  1108,  1304,  3589,  1112,   146,  1163,  1122,\n",
            "          119,  1220,  1450,  1184,  1125,  2171,  1106,  1143,  1105,  1147,\n",
            "         9712, 12233,  1108,  6281,   119,   107,   146,  1108,  3693,  1106,\n",
            "        15810,  1142,  1120,   170,  1224,  1553,  1133,   146,  1267,  1208,\n",
            "         1110,   170,  1618,  1159,  1190,  1251,   117,   107,   146,  1598,\n",
            "          119,   107,  3743,   119,   119,   119,   146,  1138,  3468,  1106,\n",
            "         1862,  1106,  1103,  3284,  1951,  1112,   170,  9140,   119,  1192,\n",
            "         1163,  1115,  1195,  1132,  1303,  1106,  1815,  1113,   119,   119,\n",
            "          119,  1103,  1314,  1159,   146,  1108,  2816,  1114,  1991,  1108,\n",
            "         1165,   146,  1108,  1253,  1684,  1112,   170,  9140,  1111,  1103,\n",
            "         2021,  2049,   119,   146,  4663,  1115,  1103,  1178,  1236,  1111,\n",
            "         1143,  1106,  1815,  1113,  1110,  1106,  1862,  1106,  1115,  1553,\n",
            "         1107,  1139,  1297,  1187,  3839,  1104,  1103,  5021,  8435,  1127,\n",
            "         5205,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,  1454,  1123,  1246,  1213,   119, 16709,  6769,  1506,  1123,\n",
            "         1339,  1165,  1131,  1486,  1103,  1610, 17224,  1115,  3932,   119,\n",
            "         2431,  1463,  1119,  1125,  4987,  1123,   117,  3274,  1125,  1678,\n",
            "         1103,  1554,  2049,  1104,  1103,  2035,  1105,  1108, 13699,  1194,\n",
            "         1103,  1171,  1105,  1103,  8839,   107,   188, 11222,  1127, 27032,\n",
            "         1121,  1117,  2229,  1112,  1119,  1108, 15905,  1193, 11590,  1166,\n",
            "          119,  1124,  1691,  1106,  1129,  2044,  1133,  1173,  1119, 20720,\n",
            "         1146,   170,  1415,  4528,  1104,  1892,   119, 14989,  2494,  5534,\n",
            "         1283,  1121,  1103,   176, 26930, 11743,  3617,   119,  1327,   119,\n",
            "          119,   119,  1184,  1110,  1142,  8710,   119,   119,   119,  3274,\n",
            "         1354,  1112,  1119, 25835,  1184,  1336,  1138,  1151,  1117,  1314,\n",
            "         4899,   119, 13313,   119,   119,   119,   159,  1200,  5389,  1233,\n",
            "          119,   119,   119,  9406,   119,   119,   119, 14680, 21943,  1186,\n",
            "          119,   119,   119, 23665, 18484,   119,   119,   119,  1184,  1674,\n",
            "         1155,  1142,  1928,   119,   119,   119,   138,  4020,   118,  1870,\n",
            "          118,  4509,  2296,  1310,  1106,  1321,  1654,  1104,  1117,  1404,\n",
            "          119,  7935,  1117,  1257,  1533,  1105,  1126,  2838,  1104,  5805,\n",
            "          175, 10771,  9041,  1166, 24144,  1117,  1339,   119,  1124,   185,\n",
            "        13422,  1471,  1228,  1103,  8839,   107,   188,  1263, 11222,  1173,\n",
            "         6953,  1122,  1205,  1103,  2472,  1114,   170,  1632,  2971,  1104,\n",
            "         2049,   119,  1135, 13990,  1113,  1103,  1747,  1105,  1338,  1106,\n",
            "         1832,   170,  1374,  3422,  1283,   119,  3274,  2411,  1866,  1146,\n",
            "         1105, 13285,  2045,  1154,  1117,  4130,   119, 14989,   117,  4853,\n",
            "         1118,  1155,  1142,   117,  2542,  1112,  1119,  3015,  1146,   170,\n",
            "        24561,  1174, 24181, 15634,  1121,  1141,  1104,  1117,  8609,  1173,\n",
            "         8621,  1106,  2647,  1171,  1154,  1103,  2472,   119,  1124,  2242,\n",
            "         1103,  1812,  1173,  1866,  4749,  1103,  7121,   117,  1103,  4346,\n",
            "         1120,  1117,  1334,  1105,  2407,  1106,  1129,  3795,   119,  1109,\n",
            "        24728,  1882,  1106,  1138,  7284,  1185,  2629,  1113,  1140,  1208,\n",
            "          119,   107,   150,   119,   119,   119,  1828,   119,  5007,   136,\n",
            "          107, 14989,  1455,   117,  4853,  1105,  4742,  5528,  1118,  1117,\n",
            "         4962,  1849,  1107, 24160,   119,   107, 11255,   117,   107,  1119,\n",
            "         1163,   119,   107, 11255,  1142,  1282,  1120,  1517,  1191,  1128,\n",
            "         1274,   107,   189,  3683,  1106,  2939,   119,   119,   119,   107,\n",
            "         1124,  2910,  1114,  1126,  3665,  1472,  1490,   117,  2504,  1105,\n",
            "         1443, 12775,   119,  1109,  6093,  1108,  1171,  1106,  1157,  1623,\n",
            "         1120,  1142,  1553,  1105,  8518,  9733,  1107, 27071,  1104,  3274,\n",
            "          119,  3274,   107,   188,  1319,  1339,  1915,   188,  2430,  1596,\n",
            "         1105,  8362,  8057,  5305,  1112,  1119,  3932,  1111,  1103,  6093,\n",
            "         1106,  1294,  1157,  1815,   119,  1249,  1122,  4601,  1140,   117,\n",
            "         1119,  2843,  1171,  2776,  1105,  1508,  1117,  1289,  1113,  1103,\n",
            "         4346,   107,   188,  4282,   119,  1130,  1141, 15085,  4018,   117,\n",
            "         1119,  2411,  3583,  1103,  4346,   117,  6964,  1213,  1105,  1173,\n",
            "         2434,  1106,  1141,  5656,  1481,  1103,  6093,  1114,  1117,  4346,\n",
            "         3795,  1120,  1117,  1334,   119,  1109,  6093,  2111,  1108,  7958,\n",
            "         1107,  1524,  1104, 14989,   117,  1150,  2729,  1154,  1157, 10140,\n",
            "         1894,  1257,  2613,  1106,  1267,  1184,  1156,  3333,  1397,   119,\n",
            "         3274, 13285,  1866,  1146,  1105,  2494,  1608,  1103,  4346,  1106,\n",
            "         1157, 24561,   119,  1332,  1122, 14376,  1171,  1154,  1282,   117,\n",
            "         1103,  6093,  1519,  1149,   170, 15659,  5354,  1105,  1173,  4267,\n",
            "        10606,  1566, 14867,  1906,  1268,  1196, 14989,   107,   188,  1257,\n",
            "          119,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1166,  1139,  1246,  1105,  1231, 19951,  1139,  1648,  1112,\n",
            "          170,  9140,  3093,  1106,  1129,  1103,  1436,  1236,   119,  8696,\n",
            "          119,   119,   119,   146,  1138,   170,  3407,  2255,  1106,   119,\n",
            "          119,   119,   107,   146,  1350,  1107, 18653,  1161,   107,   188,\n",
            "         1704,  2447,   119,  1153,  1608,  1103,  5410,  1114,   170,  2003,\n",
            "          119,   107,   146,  1328,  1106,  1129,  1175,  1111,  1139,  1376,\n",
            "         2104,  1165,  1131,   107,   188,  2407,  1106,  3873,  1103,  1768,\n",
            "          119,  1135,  1108,  1123,  4185,  1106,  1250,  3338,  1143, 15097,\n",
            "         2740,  1112,   170, 21330, 17805,  1105,   146,  1169,   107,   189,\n",
            "         5403,   170,  1618,  2174,  1111,  1103,  1241,  1104,  1366,   119,\n",
            "          146,   107,  1396,  1151,  1177,  6531,  1111,  1292,  1314,  1160,\n",
            "         1201,  1105,   146,  1444,  1106,  1294,  1122,  1146,  1106,  1123,\n",
            "          119,   119,   119,   146,  1169,   107,   189,  1341,  1104,   170,\n",
            "         1618,  1236,   117,   107,   146,  1845,   119,   107,  1573,   117,\n",
            "          146,  2810,  1128,  2437,  1725,   146,  1821, 11027,  1136,  1106,\n",
            "         1862,  1112,   170, 16810,   119,   107,   146,  1261,  1330,  1440,\n",
            "         1213,  1113,  1103,  1236,  1171,  1106,  1139,  1952,  1105,  3535,\n",
            "         1199,  1104,  1103,  6065,  2479,  7591,  1183,   118,  7074,   119,\n",
            "         1220,  2494,  1310,  1106, 12647, 15554,  1181,  1105,  2028,  1522,\n",
            "         1236,  1106,   170,  2288,   184, 11583,   119,   146,  1522, 18653,\n",
            "         1161,   170,  1992,  8363,  1196,  2807,  1171,  1205,   119,   107,\n",
            "         4514,  1128,   117, 17723,   117,   107,  1131,  3125,  1171,  1106,\n",
            "         1143,   119,   107,  1135,   107,   188,  1111,  1128,   117,   107,\n",
            "          146,  1500,  1123,   119,  1109,  1710,  1598,  1111,  1330,  1160,\n",
            "         2005,   119,  1284,  1155,  1767,  1103,  4398,  1105, 10572,  2705,\n",
            "         2936,   119,  1109, 10572,  2705,  1163,  1115,  1119,  1156,  1817,\n",
            "          170, 13710,  1106,  2292, 17791,  4260,  1103,  1648,  1104,  2534,\n",
            "         1104,  3284,  1106, 12434,  1143,  1112,   170,  9140,   119,  1109,\n",
            "         4398,  1896,  1119,  1180,   191,  6094,  1732,  1111,  1139, 25644,\n",
            "         1105,  1156,  7572,  1322, 18649,  1143,  1165,  1103,  1159,  1338,\n",
            "          119,   146, 16490,  1172,  1241,  1111,  1147, 18569,   119,  2577,\n",
            "         1103,  1710,  1338,  1106,   170,  1601,   117,  5876,  1105,  4387,\n",
            "         1241,  1189,   170,  1603,  4055,  1164,  1147,  2174,  1112, 23641,\n",
            "          119,  1130,  1901,   117,  6980,   119, 14919,  1460,   170,   171,\n",
            "         6094, 10457,  1104,  4637,  1105,  1828,   119,  5137,   170,  1602,\n",
            "         1981, 10198,  1114,  1117,  1711,   107,   188, 16810, 10999,  1295,\n",
            "          188,  3121, 16283,  2135,  1122,  1107,  1653,  2998,  1158,   117,\n",
            "         1241, 15897,  1104,  1103,  1331,   119,   146,  1180,  1267,   170,\n",
            "         7591,  1218,  1158,  1146,  1107,  1103,  2655,  1104,  4387,   107,\n",
            "          188,  2552,  1112,  1119,  1460,  1103,  1981, 10198,   119,  1124,\n",
            "         4144,  1103,  1981, 10198,  1113,  1105,  1163,  1119,  1156,  4330,\n",
            "         1122,  1451,  1285,  1106,  1250,  1106,  3874,  1117,  1711,   107,\n",
            "          188,  9050,   119,  1220,  1241,  1125,  1145,  4029,  9794,  1111,\n",
            "         1143,   119,  1622,  5876,   146,  1400,   170,  9901,  1383,  1104,\n",
            "         8228,  1116,  1114,  1139,  1105, 18653,  1161,   107,   188,  4351,\n",
            "         5757,  1113,  1172,   119,  4387,  1522,  1143,   170, 10577,  6307,\n",
            "         1104,  1471,  1105,  6003,  1678,  1120,   170, 17869, 10804,  1196,\n",
            "         1119,  2085,  1283,   119,  1130,  1103,  2655,  1119,  1125,  1637,\n",
            "          170,  1603,  3802,  1106,  1143,   131,  1706, 17723,   132,  6003,\n",
            "          107,   188,  4840,  2491,  1113,  1107,  1128,   119,   119,   119,\n",
            "          146,   107,  1325,  1309,  5042,  1115,   119,  4514,  1128,   119,\n",
            "          119,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,   107,   146,  1274,   107,   189,  1328,  1106,  9488,  1991,\n",
            "          119,   119,   119,  1191,  1128,  1138,  1251,  4232,  1106,  1686,\n",
            "          117,  1817,  1208,   119,   119,   119,   107,  3274,  1500, 14989,\n",
            "         1443,  1251,  2951,  1104,  7471,   119, 14989,  2494,  2471,  1173,\n",
            "         1400,  1228,  1103,  1747,  1105,  1868,  1763,  3274,   119,  3274,\n",
            "         1454,  1213,  1105,  3544,  1103,  1168,  2447,   117,  1702,  1205,\n",
            "         1103,  2472,  1120,  1184,  1108, 14715,  8508,  3384,   119,  1124,\n",
            "         1486,  1978,  6464,  9476, 14156,  1146,  1184,  1915,  1104,  1103,\n",
            "        24210,   119,  9326, 10831,  1127,  4009,  1113,  1103,  3091,  1104,\n",
            "         1103,  2472,  1114,  1251,  8771,  1640,  1515,  5742,  1103, 20723,\n",
            "          119,  3274,  2494,  2045,  2019,  1103,  4719,  9476,  1114,  1185,\n",
            "         2951,  1104,  2945,  1137,  4095,   119,  1109,  7207,  2347,  2135,\n",
            "         1117,  2915,  1976,  1105,  1155,  1104,  1172,  1454,  1106,  1339,\n",
            "         1140,   119,  1220, 11206,  1147,  4295,  3307,  1120,  1103, 11870,\n",
            "         1104,  6330,  1870,  1330,  1297,  1173, 11406,  1120,  3274,   119,\n",
            "         3274,  1915,  8362,  3702,  5790,  1118,  1147,  1107,  3121,  3080,\n",
            "        13759,  1105, 13285,  3932,  1111,  1172,  1106,  2035,   119,  1109,\n",
            "         6464,  7207,  2494,  1427,  1154,   170,  3170,  1213,  1140,  1105,\n",
            "         1793,  1106,  2884,  1140,  1107,   119,  1124,  4707,  1117,  1257,\n",
            "         1106,  1103,  1286,  1105,  1268,  1105,  2023,  1117,  1289,  1113,\n",
            "         1117,  4346,   119,  1130,  1103, 14119,  1104,  1126,  2552,   117,\n",
            "         1155,  1978,  7207,  1427,  1107,  1111,  1103,  2311,   119,  1220,\n",
            "         1155,  4874,  1120,  1140,  1105,  1793,  1106,  2035,  1121,  1807,\n",
            "          119,  3274,   188, 20543,  1906,  1283,  1160,  1114,  1103, 24561,\n",
            "         1104,  1117,  4346,  1173, 14245,  1223,  1330,   107,   188,  2035,\n",
            "          119,  1249,  1119,  3152,  1146,   117,  1119,  3583,  1117,  6275,\n",
            "         1105,  2195,  1141, 22026,   117, 15601,  1122,  4044,  1107,  1160,\n",
            "         1121,  1103,  3248,  1105,  3989,  1122,  1106,  4267, 10606,  1566,\n",
            "        14867,  1566,   119,  1124,  1976,  6964,  1213,  1105,  1173,  1338,\n",
            "         1205,  1113,  1330,  1104,  1103,  7207,  1114,   170,  1248,  7391,\n",
            "        26632,  1121,  1103,  1499,   117, 15869,  5910,  1122,  1107,  1544,\n",
            "         1105,  9769,  1122,  1196,  3610,  1117,  4346,  1106,  1157, 24561,\n",
            "          119,  1109,  2735,  1421,  7207,  1231, 16016,  1174,  1173,  1793,\n",
            "         1106,  2035,  1254,   119,  1230,  1171,  1108,  1106,  1117,  6380,\n",
            "          117,  2128,  1172,  1120,  1103,  4316,   119,  1252,  1112,  1152,\n",
            "        22853,   117,  1119,  3098,  8709,  1171,  2087, 10913,  3537,  1166,\n",
            "         1155,  1421,  1104,  1103,  9476,  1105,  3583,  1117,  4346,  1107,\n",
            "         2286,   118,  3043,   119,  1799,  1253,  1107,  1103,  1586,   117,\n",
            "         1119, 27581,  1194,  1155,  1421,  1114,  1160,  3613, 26632,  1279,\n",
            "         1104,  1117,  4346,  1196,  4636,   117, 16520,  1172,  1196,  1152,\n",
            "         1256,  1855,  1103,  1747,   119,  2857,  1147,  1473, 13881,  1125,\n",
            "        25149,  1121,  1103,  1586,   117,  1103,  2472,  1108,  1286,  1107,\n",
            "         1126, 21914,  3747,  1112,  1103,  3223,  8390,  1194,  1114,  1126,\n",
            "         3427,  1293,  1233,   119,  1124,  8399,   170,  1822,  6106,  1173,\n",
            "         1508,  1117,  4346,  1283,   119,  2611,   119,   119,   119,  3259,\n",
            "         1125,  4984,  1113,  1103,  1353,  1411,   119,  1109,  5915,  2416,\n",
            "         1118,  1103, 11339,  1108,  1253, 16126,  1103,  2472,  1187,  1152,\n",
            "         1125, 11584,  1181,  1103,  4037,   119,  1109,  2472,  1253,  1915,\n",
            "         3427,  1105,   170,  1609, 10335,  1125,  4984,  1113,  1103,  1411,\n",
            "         1112,  1103,  2472, 15248, 18978,  1181,  1194,  1103, 19691,   119,\n",
            "         1335,  5007,   107,   188,  4130,   117,   170, 12563,  1609,  1108,\n",
            "         5178,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,   119, 14247, 22834,  1200,   119,   107,  9187,   107,  4387,\n",
            "         5137,   138,  1376,  1229,  1224,   117,  1103,  1710,  1108, 14042,\n",
            "         1205,  1105,  1103,  6065,  1127,  2150,  1106,  1817,   119,  7585,\n",
            "         1125,  4984,  6153,  2570,  1904,  1196,  1105,  9738,  1105, 18653,\n",
            "         1161,  1163,  1152,  1156,  1321,  1123,  1313,  1196,  2309,  1991,\n",
            "         1105,  6343,  1111,  1199,  1170,   118,  1710, 27475,  1120,   170,\n",
            "         1469, 16516, 21216,   119,  2577,  1119,  1261,  1117,  1817,   117,\n",
            "         1103,  4398,  1522,  1143,   170,  1602,  1312,  1114,  7630,  1121,\n",
            "         2490,  1120,  1103,  1710,  1373,  1114,  2357,  2200,  7416,  1121,\n",
            "         2490,  1675,   119,  1109,  1524,  1104,  1122,  1125,  1103,  1734,\n",
            "          107,  1370, 17723,   107,  5757,  1107,   170,   188,  2340, 10550,\n",
            "         2284, 17927,   119,  1124,  1163,  1122,  1108,   170,  1933,  3366,\n",
            "         1118, 18653,  1161,  1113,  1103,  1236,  1166,  1121,  1980,   119,\n",
            "         1335,  1103,  1171, 18653,  1161,  1125,  1189,   170,  2436,  1104,\n",
            "         5324,  1131,  1105,   146,  1125,  1678,  1166,  1103,  1201,  1373,\n",
            "         1114,  1126, 10400, 18653,  1161,  1125,  1637,  1229,  1131,  1108,\n",
            "         1107,   122,  1582,  3654,  1164,  1123,  6149,  1105,  2513,  1106,\n",
            "         1250,  3338,  1143,   119,  1135,  2709,  1143,  1114,  1177,  1277,\n",
            "         2810,  1106,  2373,  1164,  1155,  1123, 21681,  1164,  2479,   170,\n",
            "        21330, 17805,  1105,  1115,  1131,  2018,   107,   189,  1549,  1146,\n",
            "         1115,  4185,  2693,  1155,  1131,  1125,  1151,  1194,  1290,  1173,\n",
            "          119,  6382,   117,  4783,  1121,  9187,   144, 17167, 10061,  1774,\n",
            "         1106,  6387,  1103,  1314,  1104,  1103,  2094,  1121,  1103,   171,\n",
            "         9435,  2105,  1952,  1196,  1122,  1108,  8733,  1146,  1105,   170,\n",
            "         1374,  2021,  3099,  1105, 24987, 25132,  1796,  1104,  1103,  2885,\n",
            "          117,  1178,  1991,  1105,  6343,  1127,  1286,   119,   107,   156,\n",
            "        25948,  1128,  1454,  1205,  1103,  2261,  1104, 16810,   117,   107,\n",
            "         6343,  1163,  1106,  1143,   119,   107,   146,   107,   173,  3097,\n",
            "         1106,  1138,  1899,  1128,  1107,  2175,   119,   119,   119,   107,\n",
            "         1124,  2910,  1114,   170,  6812, 16305,  1107,  1117,  1490,   119,\n",
            "          146,  2387,  1105,  3348,  1171,   119,   107,  4514,  1128,   117,\n",
            "         6343,   132,  1122,  1108,  1541, 17873,  1111,  1128,  1106,  1508,\n",
            "         1142,  1487,   117,   107,   146,  1500,  1140,   119,   107,  1135,\n",
            "          107,   188,  1177,  3505,  1106,  1221,   146,  1138,  2053,  1149,\n",
            "         1175,  1150,  1156,  1301,  1194,  1177,  1277,  3819,  1111,  1143,\n",
            "          119,   119,   119,   146,  3319,   146,  6278,  1115,  1170,   146,\n",
            "         1125,  1454,  1139,  1171,  1113,  2490,  1292,  1314,  1160,  1201,\n",
            "          119,   107,   107,   146,  1178,  1338,  1146,  1114,  1103,  1911,\n",
            "         1111,  1103,  1710,  1170,  2617,  1270,  1143,  1105,  1163,  1119,\n",
            "         1458,  1106,  1202,  1380,  1111,  1128,   117,  4387,  1105,  5876,\n",
            "          119,   119,   119, 12051,   117,   146,   107,   182,  3753,   146,\n",
            "         1256,  1338,  1146,  1114,  1142,   117,   107,  6343,  1163, 21815,\n",
            "          117,  1702,  1213,  1103,  1395,   119,   107,   146,   107,   182,\n",
            "         1136,  1932,  1103,  3271,  2564,  1106,  1508,  1487,  1126,  1856,\n",
            "          119,   119,   119,  9738,  2375,  1143,   117,  1463,   119,  1153,\n",
            "         1108,  1103,  1141,  1150,  1189,  1155,  1103,  2179,  3675,   117,\n",
            "         1189,  1103, 20624,  1105,  1189,  1103,  8727,  1116,   119,  1153,\n",
            "          107,   188,  2385,  6228,   117,  2108,  1165,  7585,  1116,  1110,\n",
            "         4395,  1123,   119,   107,   107, 18653,  1161,  3093,  1106,  1138,\n",
            "         1276,   170,  1910,  1107,  9738,   119,   146,  1341,  1131,   107,\n",
            "          188,  1579,  1151,  4405,  1118,  6323,  1176,  1991,  1105,  1103,\n",
            "         1168,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,   119,  1109,  5915,  2416,  1118,  1103, 11339,  1108,  1253,\n",
            "        16126,  1103,  2472,  1187,  1152,  1125, 11584,  1181,  1103,  4037,\n",
            "          119,  1109,  2472,  1253,  1915,  3427,  1105,   170,  1609, 10335,\n",
            "         1125,  4984,  1113,  1103,  1411,  1112,  1103,  2472, 15248, 18978,\n",
            "         1181,  1194,  1103, 19691,   119,  1335,  5007,   107,   188,  4130,\n",
            "          117,   170, 12563,  1609,  1108,  5178, 14088, 26434,  1103,  4604,\n",
            "          119,  7323,   117,  3274,  1108,  7410,  1117, 21924,   119,  1124,\n",
            "         1125,  1114,  1140,   170,  2221,  4920,  1105,  1108,  4004,  1103,\n",
            "         4346,  1119,  1125,  1215,  1222,  1103, 11339,   119,   138,  3111,\n",
            "         1104,  1257,  2542,  1140,  1121,  1103,  1796,   119,   107,   150,\n",
            "          118,  1828,   119,  5007,   136,   107,   170,  1873,   107,   188,\n",
            "         1490,  1270,  1106,  1140,   119,  1124,  1454,  1117,  1246,  2776,\n",
            "         2019,  1103,  1839,   119,   107,   138,   118,  1132,   119,   119,\n",
            "          119,  1132,  1128,  1280,  4476,   136,   107,  1131,  1455,   117,\n",
            "         1781,  3805,  1115,  1119,  1691,  1106,  1129,  2128,   119,  1124,\n",
            "         3037,  1103,  1490,  1112, 14989,   107,   188,   119,   107,   146,\n",
            "         1821,   119,   119,   119,   107,  1119,  1163,   117,  1253,  9209,\n",
            "         1112,   188,  2430,  1596,  1112,  1119,  1225,  2206,  1107,  1103,\n",
            "         1285,   119,  1124,  1454,  2019,  1103,  1442,  1105,  2045,  2494,\n",
            "         2019,  1123,   117,  3219,  1228,  1103,  4130,  4204,  1113,  1117,\n",
            "         1236,  1149,   119,   107,   146,   119,   119,   119,  1821,  1136,\n",
            "         2318,  1106,  1129,  1107,  1142,  1282,   119,   119,   119,   107,\n",
            "         1119,  1598,   117,  1136,  1543,  2552,  3232,  1114,  1103,  1873,\n",
            "          119,  1124,  1338,  1106,   170,  1831,  1120,  1123,  1334,   119,\n",
            "          107,  2627,  1132,  1128,   136,   107,  1131,  1455,  1140,   119,\n",
            "          107,  1327,  2171,  1106,  1128,  2206,  2052,   136,   107,   107,\n",
            "          146,   107,   182,  1136,  1176,  1128,   119,   119,   119,   146,\n",
            "          107,   182,  1136,  1176,  2256,  1303,   117,   107,  1119,  1500,\n",
            "         1123,   117,  1253, 11676,  1106,  1440,  1123,  1107,  1103,  1257,\n",
            "          119,   107,   146,  1138,   170,  2299,  3007,   119,  1370,  1103,\n",
            "         1314,  1374,  1201,   146,   107,  1396,  4205,  1150,   146,  1821,\n",
            "         1105,  1725,   146,  2207,  1146,  1303,   119,  3570,   146,  1400,\n",
            "         1115,  2590,   119,   119,   119,   146,  1138,   170,  6061,  5900,\n",
            "          132,  1141,  1115,  1143,  5253,  1116,  8568,  1105,  3612,   119,\n",
            "          119,   119,   107,   107,  2777,  1209,  1128,  1301,   117,  1828,\n",
            "          119,  5007,   136,   107,  1131,  1455,   119,   107,  1422,  1271,\n",
            "         2762,   107,   189,   107,  3274,  5007,   107,   119,   119,   119,\n",
            "         1157,   107,   159,  1200,  5389,  1233,   107,   117,   107,  1119,\n",
            "         1500,  1123,   119,   107,  1247,  1110,   170,  6569,  5170,  1909,\n",
            "          119,   119,   119,   146,  1138,   170,  4019,  1106,  1831,  1122,\n",
            "          119,   146,  1221,  1725,  1343,  8568,  3623,  1366,  1105,   146,\n",
            "         1138,  1106,  2496,   119,   119,   119,  1122,   107,   188,  1107,\n",
            "         1139,  1892,   119,   107,   107,  3100,  1195,  1518,  1267,  1128,\n",
            "         1254,   136,   107,   107,   119,   119,   119,  1302,   117,   107,\n",
            "         1119,  1163, 12172,  1193,   119,  1556,  1115,  1119,  3885,  1117,\n",
            "         4920,  1166,  1117,  2342,  1173, 13285,  2045,  1154,  1103, 10335,\n",
            "         4873,  1480,   119,   113, 15969, 12880,  2069, 14038, 12480, 24805,\n",
            "          114,   113,   138,   120,   151,   131,  1188,  6073,  2762,   107,\n",
            "          189,  1166,  1870,  1133,  1122,   107,   188,   170,  1263,  6073,\n",
            "         1177,   146,   107,  1325,  3593,  1122,  1107,  1160, 26853,  1106,\n",
            "         2549,  1122,  1146,   119,  5893,  1141,  1909,  1146,  1770,   119,\n",
            "          114,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101, 23641,  1115,  1131,  1309,  1541,  1276,  1103,  2640,  1106,\n",
            "         1221,  2256,  1123,  1425,   119,  2431,  1107,  7876,  1278,  1131,\n",
            "         1108,  1579,  5205,  1213,  1143,  1107,  1123,  1714,  1159,   119,\n",
            "          119,   119,   146,  1108,  1103,  1436,  1910,  1131,  1125,  1290,\n",
            "         1412,  2153,  2085,  1283,   117,   107,   146,  1500,  6343,   119,\n",
            "          107,  1337,  1108,  1165,  1131,  2204,  1107,  1567,  1114, 21330,\n",
            "         2598,  1133,  1122, 11519,  6841,  1123,  1121,  1103,  1168,  4067,\n",
            "          119,   107,  6343,  2471,   119,   107,  9738,  1163,  1131,  1108,\n",
            "         1176,  1115,  1315,  1114,  1123,  2104,   119,  1153,  3097,  1106,\n",
            "         7311,  1149,  1114,  1123,  2104,  1112,  1131,  1108,  2898,  1146,\n",
            "          117,  1256,  1170,  9103,  1245,   170,  4545,   117,  1290,  1152,\n",
            "         1315,  1238,   107,   189,  1138,  2153,   119,  5723,   170, 10499,\n",
            "         1115,  1123,  2104,  1452,  1103,  1236,  1131,  1225,   119,   146,\n",
            "         4819,  2156,  1181,  2061,  1177,  1277,  1111, 15352,  1343,  1160,\n",
            "          119,   119,   119,  1122,   107,   188,  1216,   170, 12343,   119,\n",
            "          119,   119,  1133,  1536,  1260, 24956,  4333,   117,  1195,  1431,\n",
            "         1243,  1280,  1191,  1195,  1328,  1106,  2283,  1146,  1114,  9738,\n",
            "         1105, 18653,  1161,   119,   107,   146,  2675,  1105,  1866,  1146,\n",
            "         1106,  1243,  1139,  4920,   119,  1249,   146,  1225,   117,   146,\n",
            "         2045,  1118,  6343,  1105,   119,   119,   119,  4005,  1140,  1113,\n",
            "         1103,  4310,   119,   107,  5749,  1254,   117,   107,   146,  1163,\n",
            "         4526,   119,  1135,  2347,  1140,  1228,  3542,  1105,   146,  1400,\n",
            "          170,  2640,  1106,  1267,  1140, 18407,   170,  1996, 10356,  1104,\n",
            "         1894,   119,   146, 16833,  1105,  3015,  1146,  1139,  4920,   119,\n",
            "         1284,  1189,  1412,  1236,  1106,  1103,  1442,  1256,  1463,  6343,\n",
            "         8892, 21120,  2495, 12165,  1481,   117,  1774,  1106,  4750,  1117,\n",
            "         5544, 21082,  1339,  1121,  1143,   119,  1249,   146,  2843,  1149,\n",
            "         1154,  1103,  1480,  1586,   117,   146,  2729,  1146,  1154,  1103,\n",
            "         2940,  1105,  1310,  2422,   119,  7025,   119,   119,   119,   146,\n",
            "         1431,  1309,  1321,  1172,  1111,  3609,   119,   119,   119,   146,\n",
            "         1354,   119,  1135,   107,   188,  1315,  1662,  1106,  1525,  1172,\n",
            "         1105,  1315,  3123,  1106,  3857,  1172,   119,   119,   119,  1105,\n",
            "          146,  1209,  1309,  3857,  1172,  1254,   119,  1109,  5135,   119,\n",
            "          119,   119,   113,   138,   120,   151,   131,  1188,  6073,  2207,\n",
            "         1146,   170,  1974,  2039,  1190,   146,  1148,  3005,  1133,   146,\n",
            "         1176,  1122,  1277,  1618,  1103,  1236,  1122,  1110,   119,   119,\n",
            "          119,   146,   107,   182,  3753,  1293, 17024,  1348,   146,  1169,\n",
            "         1129,  1517,  1103,  6601,  4090,  1143,   119,  2431,   146,  1125,\n",
            "          170,  2337,  1762,   118, 19029,  4899,  1112,   146,  1724,  1122,\n",
            "          119, 10756,   117,  1115,   107,   188,  1122,  1111,  1226,   122,\n",
            "         1104,  1139, 17723, 17685, 14927,   119,  1135,  1261,  1143, 21985,\n",
            "         1106,  1508,  1142,  1141,  3294,  1272,   146,   107,  1396,  1151,\n",
            "         5116,  1114,  1297,  1133,  4416, 21373,   111, 18491, 23601,  6126,\n",
            "         1522,  1143,  1103,  2797,  1106,  1294,  1146,  1142,  6073,   119,\n",
            "         1109,  1509,  1226,  1104,  1103, 14927,  1209,  1129,  1417,   107,\n",
            "        17723,   107,   188,  7549,   107,  1105,   146,  1328,  1106,  3146,\n",
            "         1115,  1196,  9053,  3302,  2502,  1149,  1177,  2215, 17169,   119,\n",
            "          119,   119,   119,  2048,   117,  1105,  1141,  1509,  3805,   131,\n",
            "          146,  1341,   146,  1328,  1106,  3593,  1167, 12860,  6867,  5442,\n",
            "        26778,   119,   146,   107,   182,  1177,  4035,  1582,  4412,  2433,\n",
            "         1118,  1103,  2650,  1105,  2801,  1115,   146,  1198,  1631,  4270,\n",
            "         7747,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1], [tensor([  101,  1454,  1123,  1246,  1213,   119, 16709,  6769,  1506,  1123,\n",
            "         1339,  1165,  1131,  1486,  1103,  1610, 17224,  1115,  3932,   119,\n",
            "         2431,  1463,  1119,  1125,  4987,  1123,   117,  3274,  1125,  1678,\n",
            "         1103,  1554,  2049,  1104,  1103,  2035,  1105,  1108, 13699,  1194,\n",
            "         1103,  1171,  1105,  1103,  8839,   107,   188, 11222,  1127, 27032,\n",
            "         1121,  1117,  2229,  1112,  1119,  1108, 15905,  1193, 11590,  1166,\n",
            "          119,  1124,  1691,  1106,  1129,  2044,  1133,  1173,  1119, 20720,\n",
            "         1146,   170,  1415,  4528,  1104,  1892,   119, 14989,  2494,  5534,\n",
            "         1283,  1121,  1103,   176, 26930, 11743,  3617,   119,  1327,   119,\n",
            "          119,   119,  1184,  1110,  1142,  8710,   119,   119,   119,  3274,\n",
            "         1354,  1112,  1119, 25835,  1184,  1336,  1138,  1151,  1117,  1314,\n",
            "         4899,   119, 13313,   119,   119,   119,   159,  1200,  5389,  1233,\n",
            "          119,   119,   119,  9406,   119,   119,   119, 14680, 21943,  1186,\n",
            "          119,   119,   119, 23665, 18484,   119,   119,   119,  1184,  1674,\n",
            "         1155,  1142,  1928,   119,   119,   119,   138,  4020,   118,  1870,\n",
            "          118,  4509,  2296,  1310,  1106,  1321,  1654,  1104,  1117,  1404,\n",
            "          119,  7935,  1117,  1257,  1533,  1105,  1126,  2838,  1104,  5805,\n",
            "          175, 10771,  9041,  1166, 24144,  1117,  1339,   119,  1124,   185,\n",
            "        13422,  1471,  1228,  1103,  8839,   107,   188,  1263, 11222,  1173,\n",
            "         6953,  1122,  1205,  1103,  2472,  1114,   170,  1632,  2971,  1104,\n",
            "         2049,   119,  1135, 13990,  1113,  1103,  1747,  1105,  1338,  1106,\n",
            "         1832,   170,  1374,  3422,  1283,   119,  3274,  2411,  1866,  1146,\n",
            "         1105, 13285,  2045,  1154,  1117,  4130,   119, 14989,   117,  4853,\n",
            "         1118,  1155,  1142,   117,  2542,  1112,  1119,  3015,  1146,   170,\n",
            "        24561,  1174, 24181, 15634,  1121,  1141,  1104,  1117,  8609,  1173,\n",
            "         8621,  1106,  2647,  1171,  1154,  1103,  2472,   119,  1124,  2242,\n",
            "         1103,  1812,  1173,  1866,  4749,  1103,  7121,   117,  1103,  4346,\n",
            "         1120,  1117,  1334,  1105,  2407,  1106,  1129,  3795,   119,  1109,\n",
            "        24728,  1882,  1106,  1138,  7284,  1185,  2629,  1113,  1140,  1208,\n",
            "          119,   107,   150,   119,   119,   119,  1828,   119,  5007,   136,\n",
            "          107, 14989,  1455,   117,  4853,  1105,  4742,  5528,  1118,  1117,\n",
            "         4962,  1849,  1107, 24160,   119,   107, 11255,   117,   107,  1119,\n",
            "         1163,   119,   107, 11255,  1142,  1282,  1120,  1517,  1191,  1128,\n",
            "         1274,   107,   189,  3683,  1106,  2939,   119,   119,   119,   107,\n",
            "         1124,  2910,  1114,  1126,  3665,  1472,  1490,   117,  2504,  1105,\n",
            "         1443, 12775,   119,  1109,  6093,  1108,  1171,  1106,  1157,  1623,\n",
            "         1120,  1142,  1553,  1105,  8518,  9733,  1107, 27071,  1104,  3274,\n",
            "          119,  3274,   107,   188,  1319,  1339,  1915,   188,  2430,  1596,\n",
            "         1105,  8362,  8057,  5305,  1112,  1119,  3932,  1111,  1103,  6093,\n",
            "         1106,  1294,  1157,  1815,   119,  1249,  1122,  4601,  1140,   117,\n",
            "         1119,  2843,  1171,  2776,  1105,  1508,  1117,  1289,  1113,  1103,\n",
            "         4346,   107,   188,  4282,   119,  1130,  1141, 15085,  4018,   117,\n",
            "         1119,  2411,  3583,  1103,  4346,   117,  6964,  1213,  1105,  1173,\n",
            "         2434,  1106,  1141,  5656,  1481,  1103,  6093,  1114,  1117,  4346,\n",
            "         3795,  1120,  1117,  1334,   119,  1109,  6093,  2111,  1108,  7958,\n",
            "         1107,  1524,  1104, 14989,   117,  1150,  2729,  1154,  1157, 10140,\n",
            "         1894,  1257,  2613,  1106,  1267,  1184,  1156,  3333,  1397,   119,\n",
            "         3274, 13285,  1866,  1146,  1105,  2494,  1608,  1103,  4346,  1106,\n",
            "         1157, 24561,   119,  1332,  1122, 14376,  1171,  1154,  1282,   117,\n",
            "         1103,  6093,  1519,  1149,   170, 15659,  5354,  1105,  1173,  4267,\n",
            "        10606,  1566, 14867,  1906,  1268,  1196, 14989,   107,   188,  1257,\n",
            "          119,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), tensor([  101,  1106,  7311,  1149,  1114,  1123,  2104,  1112,  1131,  1108,\n",
            "         2898,  1146,   117,  1256,  1170,  9103,  1245,   170,  4545,   117,\n",
            "         1290,  1152,  1315,  1238,   107,   189,  1138,  2153,   119,  5723,\n",
            "          170, 10499,  1115,  1123,  2104,  1452,  1103,  1236,  1131,  1225,\n",
            "          119,   146,  4819,  2156,  1181,  2061,  1177,  1277,  1111, 15352,\n",
            "         1343,  1160,   119,   119,   119,  1122,   107,   188,  1216,   170,\n",
            "        12343,   119,   119,   119,  1133,  1536,  1260, 24956,  4333,   117,\n",
            "         1195,  1431,  1243,  1280,  1191,  1195,  1328,  1106,  2283,  1146,\n",
            "         1114,  9738,  1105, 18653,  1161,   119,   107,   146,  2675,  1105,\n",
            "         1866,  1146,  1106,  1243,  1139,  4920,   119,  1249,   146,  1225,\n",
            "          117,   146,  2045,  1118,  6343,  1105,   119,   119,   119,  4005,\n",
            "         1140,  1113,  1103,  4310,   119,   107,  5749,  1254,   117,   107,\n",
            "          146,  1163,  4526,   119,  1135,  2347,  1140,  1228,  3542,  1105,\n",
            "          146,  1400,   170,  2640,  1106,  1267,  1140, 18407,   170,  1996,\n",
            "        10356,  1104,  1894,   119,   146, 16833,  1105,  3015,  1146,  1139,\n",
            "         4920,   119,  1284,  1189,  1412,  1236,  1106,  1103,  1442,  1256,\n",
            "         1463,  6343,  8892, 21120,  2495, 12165,  1481,   117,  1774,  1106,\n",
            "         4750,  1117,  5544, 21082,  1339,  1121,  1143,   119,  1249,   146,\n",
            "         2843,  1149,  1154,  1103,  1480,  1586,   117,   146,  2729,  1146,\n",
            "         1154,  1103,  2940,  1105,  1310,  2422,   119,  7025,   119,   119,\n",
            "          119,   146,  1431,  1309,  1321,  1172,  1111,  3609,   119,   119,\n",
            "          119,   146,  1354,   119,  1135,   107,   188,  1315,  1662,  1106,\n",
            "         1525,  1172,  1105,  1315,  3123,  1106,  3857,  1172,   119,   119,\n",
            "          119,  1105,   146,  1209,  1309,  3857,  1172,  1254,   119,  1109,\n",
            "         5135,   119,   119,   119,   113,   138,   120,   151,   131,  1188,\n",
            "         6073,  2207,  1146,   170,  1974,  2039,  1190,   146,  1148,  3005,\n",
            "         1133,   146,  1176,  1122,  1277,  1618,  1103,  1236,  1122,  1110,\n",
            "          119,   119,   119,   146,   107,   182,  3753,  1293, 17024,  1348,\n",
            "          146,  1169,  1129,  1517,  1103,  6601,  4090,  1143,   119,  2431,\n",
            "          146,  1125,   170,  2337,  1762,   118, 19029,  4899,  1112,   146,\n",
            "         1724,  1122,   119, 10756,   117,  1115,   107,   188,  1122,  1111,\n",
            "         1226,   122,  1104,  1139, 17723, 17685, 14927,   119,  1135,  1261,\n",
            "         1143, 21985,  1106,  1508,  1142,  1141,  3294,  1272,   146,   107,\n",
            "         1396,  1151,  5116,  1114,  1297,  1133,  4416, 21373,   111, 18491,\n",
            "        23601,  6126,  1522,  1143,  1103,  2797,  1106,  1294,  1146,  1142,\n",
            "         6073,   119,  1109,  1509,  1226,  1104,  1103, 14927,  1209,  1129,\n",
            "         1417,   107, 17723,   107,   188,  7549,   107,  1105,   146,  1328,\n",
            "         1106,  3146,  1115,  1196,  9053,  3302,  2502,  1149,  1177,  2215,\n",
            "        17169,   119,   119,   119,   119,  2048,   117,  1105,  1141,  1509,\n",
            "         3805,   131,   146,  1341,   146,  1328,  1106,  3593,  1167, 12860,\n",
            "         6867,  5442, 26778,   119,   146,   107,   182,  1177,  4035,  1582,\n",
            "         4412,  2433,  1118,  1103,  2650,  1105,  2801,  1115,   146,  1198,\n",
            "         1631,  4270,  7747,   146,  1505,  1103,  1342,  1106,  3593,   113,\n",
            "         1105,  1104,  1736,  1114, 22078,   131, 10419,  1198,  1213,  1103,\n",
            "        12078,   146,   107,  1325,  1138,  7722,  1104,  7670,   114,   119,\n",
            "          146,  1138,  1126,  1911,  1111,  1139,  1148,  1664,   118, 17723,\n",
            "        17685,  1933,  1133,   146,  1274,   107,   189,  1328,  1106,  1474,\n",
            "         1184,  1122,  1110,  1272,  1157,  3777,  2272,  1106,  1103,  6593,\n",
            "         1104,   157,   111,   157,  1105,   146,   107,   173,  1897,  1136,\n",
            "          188,  5674,  2723,  1115,  1111,  2256,  1150,   107,   188,  1870,\n",
            "         1106,  1505,  1122,   119,   119,   119,  1267,  1128,  1224,   106,\n",
            "          114,   102]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1]), 1]]\n",
            "tensor([[  101,  1230,  6566,  2411,  1608,  1117,  2209,  1106, 21722,   117,\n",
            "          1679, 24271,  1117,  4585,  1105,  7963,  1117,  1319,  4346,  1106,\n",
            "          3542,   119, 21722,   107,   188,  4346,  1899,  1117,  1105,  1103,\n",
            "          1160, 25144,  1114,   170,  4900, 17159,  1115,  8293,  1149,  1852,\n",
            "          3772,   119,   107,  3969,  1128,  1107,  2630,   106,   107, 21722,\n",
            "          6104,   117, 13769,  1103,  4282,  1104,  1117,  1319,  6275,   119,\n",
            "          1109,  1656,  1104, 21722,   107,   188,  4346,  2411,  4601,  1146,\n",
            "          1114,   170,  1783,  1183,  9556,  1115,  2569,  1103,  2049,  1104,\n",
            "          1117, 26632,  3059,  1166,   119, 21722,  3691,  1256,  5747,  1105,\n",
            "          2873,  1103, 12604,  1399,  1171,   170,  4600,  1995,  1623,  1121,\n",
            "          1147,  1560,  1700,  1114,   170,  1353,  7552,  1121,  1117,  4346,\n",
            "          6187,  7111,  3375,   119, 21722,  4860,  1113,  1117,  4257,  1173,\n",
            "          1976,  1866,  1146,  1106,  2407,  1471,  1111,  1330,  2035,   119,\n",
            "          1124,  1350,  1120,  1117,  6566,   117,  1150,  1471,  1108, 12699,\n",
            "          1158,  1117, 26715,   119,  1249,  1770,  1112,  1119,  3390,  1257,\n",
            "          1852,  1103, 25731,  9349, 25469,  4621,  1117,  3437,  1108,  2355,\n",
            "           117,  1119,  1108,  1678,   170,  4197,   119,  1731,   119,   119,\n",
            "           119,  1293,  1674,  1119,  1138,   119,   119,   119,  1115,  4346,\n",
            "           119,   119,   119,   136, 21722,  1354,   119,  1124,  1125,  2331,\n",
            "          1562,  1103,   176, 16481,  8671,  6275,  1115,  1117, 19114,  1208,\n",
            "           192, 12350,  1174,  1219,  1117,  2321,  1114,  1727, 18045,  1656,\n",
            "          1103, 17784, 17564,  1160,  1201,  1196,   119,  1135,  1108, 11178,\n",
            "          1103,  1269,  4346,  1115,  1103,  9445,  4753,  5122, 23665, 18484,\n",
            "          1471,  1125,  1215,  1106,  2321,  1103,  3420,  5266,  1104,  1103,\n",
            "          5725,  1362,  1165,  1119, 10474,  2433,  1222,  1172,   119, 21722,\n",
            "          1238,   107,   189,  1138,  1277,  1159,  1106,  1965,  1117,  3774,\n",
            "          1112,  1117,  3437,  1108,  1770,  1113,  1103,  2035,  1254,   119,\n",
            "         21722,  2925,  1117,  1268,  1981,  1105,  1122, 17531, 14963,  1229,\n",
            "           170,  7483,  1193,  1981,  2578,  2200,  1807,  1122,   119,  1230,\n",
            "          6566,  1814,  1117,  4346,  1213,  1105,  1793,  1106, 26632, 21722,\n",
            "          1121,  1103,  1268,  1334,   119, 21722,  2347,  1103,  4346,  1118,\n",
            "          1157,  6275,  1606,  1103,   188, 26426,  1874,   118,  1981,   117,\n",
            "          1454,  1213,  1105,  3885,  1117,  3437,  1481,  1140,  1114,  1117,\n",
            "          1103,  9927,  3220,  1107,  1117,  1268,  1981,   119,  1230,  6048,\n",
            "          6964,  1107,  2286,  1586,  1105,  7040,  1117,  4346,  1154,  1103,\n",
            "          1747,  1106,  3345,  1117, 11550,   119,  1135,  9626,   170,  1415,\n",
            "          3245,  1324,  1154,  1103,  4033,  1112,  1122,  2795,  1117,  2303,\n",
            "           119,   107,  2091,  1136,  8306,  1115,  1134,  1128,  2834,  2147,\n",
            "           119,   119,   119,   107,  1117,  3437,  1163,   117,  4239,  1117,\n",
            "          4346,  1149,  1104,  1103,  1747,   119,  1124,  1316,  1117,  1286,\n",
            "          1981,  1107,  1524,  1104,  1140,  1105, 12114,  1210,  4840, 12604,\n",
            "          1106, 16858,  1117,  1404,   119,  1124,  5850,  1103, 12604,  1120,\n",
            "         21722,  1173,  1310,  3179,  2019,  1140,   119,   107,  1327,  1202,\n",
            "          1128,  1928,  1115,   146,  1169,   107,   189,  2147,   119,   119,\n",
            "           119,   136,   107, 21722,  1163, 19353, 13789,  1193,   119,  1124,\n",
            "          1316,  1117, 27324,  1981,  1107,  1524,  1104,  1117,  1339,  1105,\n",
            "          8173,  1122,  1154,   170,  7374,   119, 21722,  3583,  1117,  4346,\n",
            "          1105, 26551,  1122,  1106,  1117,  1334,  1229, 24386,  1117,  7374,\n",
            "          1154,  1103,  1586,   119,  3929,  1104,  1117, 20580,  1313,   117,\n",
            "           170,  2878,  6746, 26124,  1181,  2019,  1140,   119,  1124,  1533,\n",
            "          1117,  1289,  1105,  2347,  1103, 24485,  4231,  1107,  1117,  7374,\n",
            "           119,   102]])\n"
          ]
        }
      ],
      "source": [
        "for input1, mask1, input2, mask2,target1 in test_dataloader:\n",
        "    print(input1)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ZPDRCqAy2g"
      },
      "source": [
        "##Actual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Dy89ja9t8JC"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "\n",
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data_pos,\n",
        "\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of chunks.\n",
        "        self.per_pair_dataset = data_pos\n",
        "\n",
        "\n",
        "\n",
        "        # self.per_author_dataset_masks = data_masks\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "        #self.per_author_dataset_ids,self.per_author_dataset_masks = shuffleAll(self.tmp_per_author_dataset_ids,self.tmp_per_author_dataset_masks)\n",
        "        #del self.tmp_per_author_dataset_ids, self.tmp_per_author_dataset_masks\n",
        "        # for x in self.dict_All_pairs:\n",
        "        #     print(x)\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "        return len(list(self.per_pair_dataset.keys()))#+len(list(self.per_pair_dataset2.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id = random.choice(list(self.per_pair_dataset.keys()))\n",
        "        x = self.per_pair_dataset[id]\n",
        "\n",
        "        batchText1 = []\n",
        "        batchText2 = []\n",
        "        batchMask1 = []\n",
        "        batchMask2 = []\n",
        "        labels = []\n",
        "       \n",
        "        for item in x:\n",
        "            \n",
        "            batchText1.append(item[0])\n",
        "            batchMask1.append(item[1])\n",
        "            batchText2.append(item[2])\n",
        "            batchMask2.append(item[3])\n",
        "\n",
        "            labels.append(item[4])\n",
        "\n",
        "        return torch.stack(batchText1), torch.stack(batchMask1), torch.stack(batchText2), torch.stack(batchMask2), torch.LongTensor(labels),id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1veN4-5uBs_",
        "outputId": "39934797-ce1e-481e-b6d0-60734857b43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "186\n"
          ]
        }
      ],
      "source": [
        "# dataset_test = base_path+'/PAN20/PAN20_512_Notrunc_perPairid_AllChunks_onlyNums_uncased_test'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "dataset_test = base_path+'/PAN22/dict_perPairid_pan22_test_PAN22_256_overlap_T5_memo-essays_POS_test'\n",
        "# dataset_test = base_path+'/PAN15/PAN15_128_test_uncased'\n",
        "\n",
        "dataset_test = getPickleFileInDict(dataset_test)\n",
        "dataset_test = MyDatasetTest(dataset_test,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=1,shuffle=False)\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4lyoHitr-Ap",
        "outputId": "fc98ccad-7eaf-4217-d5bb-dd0940fd48a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "201\n"
          ]
        }
      ],
      "source": [
        "dataset_test2 = base_path+'/PAN20/PAN21_512_Notrunc_perPairid_AllChunks_onlyNums_uncased_test'#dict_per_pairID_pan22_test_PAN22_256_test_Nooverlapping_balance_clean_pairid'#dict_per_pairID_pan22_test_PAN22_test_pairs_256_per_id'\n",
        "\n",
        "dataset_test2 = getPickleFileInDict(dataset_test2)\n",
        "dataset_test2 = MyDatasetTest(dataset_test2,0.5)\n",
        "test_dataloader2 = torch.utils.data.DataLoader(dataset_test2, batch_size=1,shuffle=False)\n",
        "print(len(test_dataloader2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA6BxKxSKFLE"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JLE-xlLY_IVh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def createPlot(trainLoss,valLoss,epochs):\n",
        "      # print(valLoss)\n",
        "      # print(trainLoss)\n",
        "      plt.plot(range(0,epochs),valLoss,label = 'Validation Loss')\n",
        "      plt.plot(range(0,epochs),trainLoss,label = 'Training Loss')\n",
        "      plt.legend(loc='upper right')\n",
        "      plt.xlabel('Epoch')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDO03lLRUDKv"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "def scatter(x, labels, subtitle=None,title='Batch'):\n",
        "    # Create a scatter plot of all the \n",
        "    # the embeddings of the model.\n",
        "    # We choose a color palette with seaborn.\n",
        "  \n",
        "    labels = torch.where(labels==-1,torch.tensor(0),labels)\n",
        "    labels = labels.detach().numpy()\n",
        "    \n",
        "    palette = np.array(sns.color_palette(\"hls\", 2))\n",
        "    \n",
        "    # We create a scatter plot.\n",
        "    f = plt.figure(figsize=(8, 8))\n",
        "    ax = plt.subplot(aspect='equal')\n",
        "    sc = ax.scatter(x[:,0], x[:,1], lw=0,alpha = 0.5, s=40,\n",
        "                    c=palette[labels.astype('int')] )\n",
        "    ax.title.set_text(title)                \n",
        "    plt.xlim(-25, 25)\n",
        "    plt.ylim(-25, 25)\n",
        "    ax.axis('off')\n",
        "    ax.axis('tight')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R1f-2Yfweqqy"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=3,verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 3\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "            path (str): Path for the checkpoint to be saved to.\n",
        "                            Default: 'checkpoint.pt'\n",
        "            trace_func (function): trace print function.\n",
        "                            Default: print            \n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "        self.trace_func = trace_func\n",
        "        # self.epoch = epoch\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y_Uhp5zsB_q"
      },
      "outputs": [],
      "source": [
        "def calcF1score(test_result, test_labels):\n",
        "    \n",
        "    # with torch.no_grad():\n",
        "    #     test_result = n_model(b_input_ids1, b_input_mask1)\n",
        "        # print(test_result)\n",
        "\n",
        "    values,labels = torch.max(test_result, 1)\n",
        "    # print(labels)\n",
        "    # y_pred = test_result.cpu().data.numpy()\n",
        "    # print(y_pred)\n",
        "    # print(test_labels.cpu().data.numpy().astype(int))\n",
        "    return f1_score(labels.cpu().data.numpy().astype(int), test_labels.cpu().data.numpy().astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drjlMI4m-ART"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def calcAccuracy(probs, y_true):\n",
        "    # print(probs)\n",
        "    # preds = probs[:, 1]\n",
        "    preds = probs.cpu().data.numpy()\n",
        "    y_true = y_true.cpu().data.numpy()\n",
        "    # fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "    # print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETjDj28QiYJ2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import brier_score_loss\n",
        "def calcBrier(probs,y_true):\n",
        "  # predict probabilities\n",
        "  preds = probs[:, 1]\n",
        "  # keep the predictions for class 1 only\n",
        "  \n",
        "  # calculate bier score\n",
        "  loss = brier_score_loss(y_true, preds)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpnF9g9p7OK5"
      },
      "source": [
        "# Simple Classification Task with one BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNzvj1Q9bZn_"
      },
      "outputs": [],
      "source": [
        "from joblib.logger import print_function\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "class myModel(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (False,False)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModel, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "     \n",
        "      self.bertModel.eval()\n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      self.FC1 = nn.Linear(in_features = inputFeatures,out_features = 256)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.tanh = nn.Tanh()\n",
        "      self.GAvgPooling = nn.AvgPool1d(2)\n",
        "      self.layerNorm = nn.LayerNorm(inputFeatures)\n",
        "      self.dropout = nn.Dropout(0.5)\n",
        "      self.FC2 = nn.Linear(in_features = 256,out_features = 2)\n",
        "      self.dropout2 = nn.Dropout(0.1)\n",
        "      self.FC3 = nn.Linear(in_features = 16,out_features = 2)\n",
        "      self.softmax = nn.LogSoftmax()\n",
        "      # self.FC = nn.Sequential(\n",
        "      #   nn.Linear(in_features = inputFeatures,out_features = 256),\n",
        "      #   nn.ReLU(),\n",
        "      #   nn.Dropout(0.5),\n",
        "      #   nn.Linear(in_features = 256,out_features = 128),\n",
        "      #   nn.ReLU(),\n",
        "      #   nn.Dropout(0.2),\n",
        "      #   nn.Linear(in_features = 128,out_features = 32),\n",
        "      #   nn.ReLU(),\n",
        "      #   nn.Dropout(0.1),\n",
        "      #   nn.Linear(in_features = 32,out_features = 16),\n",
        "      #   nn.Linear(in_features = 16,out_features = 2),\n",
        "      #   nn.Softmax()\n",
        "      # )\n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      del hidden_states\n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      del hidden_states\n",
        "      return concatEmbeddingLayers\n",
        "\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][1:]\n",
        "      # `hidden_states` is a Python list.\n",
        "\n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      del hidden_states\n",
        "      # sumEmbeddingLayers = torch.stack( [hidden_states[i] for i in range(self.startLayer,self.endLayer)]).sum(0)\n",
        "      # sumEmbeddingLayers = torch.sum(hidden_states[0][self.startLayer:self.endLayer], dim=0)\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask ,strategy='avg'):\n",
        "      if strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'max':\n",
        "         max_setence_embeddings = torch.max(token_embeddings,dim=1)\n",
        "         return max_setence_embeddings\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      # print(bertOutputs[0])\n",
        "      embeddings = bertOutputs[1]\n",
        "      return embeddings\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      # outputs2 =  self.bertModel(sent_id, mask)\n",
        "      # outputs =  self.bertModel.embeddings(sent_id, mask)\n",
        "      outputs = self.bertModel(sent_id, mask)\n",
        "      # print(outputs)\n",
        "      # print(outputs2)\n",
        "      # print(outputs.shape)\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings #, self.FC(embeddings)\n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        return embeddings #, self.FC(embeddings)\n",
        "      else:\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings #, self.FC(embeddings)\n",
        "\n",
        "  def forward(self, sent_id1, mask1): #,sent_id2, mask2\n",
        "\n",
        "      # forward pass of input 1\n",
        "      # print(sent_id1)\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      \n",
        "      # forward pass of input 2\n",
        "      # output2 = self.forwardOnce(sent_id2, mask2)\n",
        "      # avg1 = self.pooling(output1,mask1,'avg')\n",
        "      # avg2 = self.pooling(output2,mask2,'avg')\n",
        "      # output11 = output1[:, -1, :]\n",
        "      # output22 = output2[:, -1, :]\n",
        "      # print(output11)\n",
        "      # concatenated = torch.cat((output11,output22),dim=1)\n",
        "      # concatenated = torch.cat((avg1,avg2),dim=1)\n",
        "      # output = torch.cosine_similarity(avg1, avg2)\n",
        "      # print(concatenated.shape)\n",
        "      # out = self.layerNorm(output11)\n",
        "      # out = self.tanh(out)\n",
        "      # out = self.dropout()\n",
        "      # out = self.GAvgPooling(output11)\n",
        "      out = self.FC1(output1)\n",
        "      # out = self.tanh(out)\n",
        "      out = self.dropout(out)\n",
        "      out = self.FC2(out)\n",
        "      # out = self.FC3(out)\n",
        "      \n",
        "      # out = self.softmax(out)\n",
        "      return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZxru21G4sZa"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion,validation_dataloader):\n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    step = 0\n",
        "    # for (input1,mask1, target1),(input2,mask2, target2) in validation_dataloader:\n",
        "    for (input1,mask1, target1) in validation_dataloader:\n",
        "        step += 1\n",
        "        if step == 12:\n",
        "          break \n",
        "        b_input_ids1 = input1.to(device)\n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        target1 = target1.type(torch.LongTensor)\n",
        "        b_labels = target1.to(device)\n",
        "        # b_labels = torch.squeeze(b_labels,1)\n",
        "        # b_input_ids2 = input2.to(device)\n",
        "        # b_input_mask2 = mask2.to(device)\n",
        "\n",
        "        # print(b_input_ids1)        \n",
        "        out = model(b_input_ids1, b_input_mask1) #,b_input_ids2, b_input_mask2\n",
        "        loss = criterion(out,b_labels)\n",
        "        total_eval_loss += loss.item()\n",
        "        print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 Val Loss \"+str(loss.item()))\n",
        "    avg_val_loss = total_eval_loss/11# len(validation_dataloader)\n",
        "    model.train()\n",
        "    return  avg_val_loss #avg_val_accuracy, avg_val_f1,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVBEdiTy4_q5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def mytrainStep(model,criterion):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      if torch.cuda.is_available():\n",
        "         model.to(device)\n",
        "      # embModel.eval()\n",
        "      \n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      warmup_percent = 0.2\n",
        "      total_steps = math.ceil(3*10*1./len(train_dataloader))\n",
        "      warmup_steps = int(total_steps*warmup_percent)\n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      # optimizer = torch.optim.SGD(model.parameters(),lr = 0.0001)  \n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=2e-5,\n",
        "                                    correct_bias=True) #eps=1e-8\n",
        "      # optimizer = torch.optim.Adam(model.parameters() ,lr=0.0001)\n",
        "      scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=5*20)\n",
        "\n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain2 = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid2 = list()\n",
        "      listOfF1Valid = list()\n",
        "      model.train()\n",
        "      for epoch_i in range(0, 5):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss2 = 0\n",
        "\n",
        "          model.train()\n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          # for (input1,mask1, target1),(input2,mask2, target2) in train_dataloader:\n",
        "          for (input1,mask1, target1) in train_dataloader:  \n",
        "              step +=1\n",
        "              if step == 21:\n",
        "                  break \n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  b_input_ids1 = input1.to(device)\n",
        "                  b_input_mask1 = mask1.to(device)\n",
        "                  target1 = target1.type(torch.LongTensor)\n",
        "                  b_labels = target1.to(device)\n",
        "             \n",
        "                  # b_labels = torch.squeeze(b_labels,1)\n",
        "                  # b_input_ids2 = input2.to(device)\n",
        "                  # b_input_mask2 = mask2.to(device)\n",
        "              \n",
        "              model.zero_grad()  \n",
        "\n",
        "              # with embModel.parameters() == False:\n",
        "                       \n",
        "              out = model(b_input_ids1, b_input_mask1) #b_input_ids2, b_input_mask2 \n",
        "              # print(out)\n",
        "              # print(b_labels)\n",
        "              loss = criterion(out,b_labels)\n",
        "\n",
        "              total_train_loss2 += loss.item()\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"====  Train Loss \"+str(loss.item()))\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              optimizer.step()\n",
        "              scheduler.step()\n",
        "          avg_train_loss2 = total_train_loss2 /20# len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== AVG. Train Loss \"+str(avg_train_loss2))            \n",
        "          listOflossesTrain2.append(avg_train_loss2)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss2))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss2 = validation(model,epoch_i,criterion,validation_dataloader)\n",
        "          listOflossesValid2.append(avg_val_loss2)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss2))\n",
        "          # print(\"  Validation avg-F1: {0:.2f}\".format(avg_val_f1))\n",
        "          # print(\"  Validation avg-Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss2,\n",
        "                'Validation Loss': avg_val_loss2,\n",
        "                # 'Valid. avg F1.': avg_val_f1,\n",
        "                # 'Valid. avg Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "          )\n",
        "          # early_stopping2(avg_val_loss2, model)\n",
        "        \n",
        "          # if early_stopping2.early_stop:\n",
        "          #     print(\"Early stopping\")\n",
        "          #     break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain2,listOflossesValid2,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "L5AvqoUxx_KK",
        "outputId": "68650ed9-ea41-4cbf-e2cf-3a554087007e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "========== Epoch 0 Batch 1====  Train Loss 0.7309959530830383\n",
            "========== Epoch 0 Batch 2====  Train Loss 0.7840278744697571\n",
            "========== Epoch 0 Batch 3====  Train Loss 0.7234556674957275\n",
            "========== Epoch 0 Batch 4====  Train Loss 0.7144984006881714\n",
            "========== Epoch 0 Batch 5====  Train Loss 0.643143355846405\n",
            "========== Epoch 0 Batch 6====  Train Loss 0.6636090874671936\n",
            "========== Epoch 0 Batch 7====  Train Loss 0.9880408048629761\n",
            "========== Epoch 0 Batch 8====  Train Loss 0.9625285863876343\n",
            "========== Epoch 0 Batch 9====  Train Loss 0.8394081592559814\n",
            "========== Epoch 0 Batch 10====  Train Loss 0.6127581596374512\n",
            "========== Epoch 0 Batch 11====  Train Loss 0.9860552549362183\n",
            "========== Epoch 0 Batch 12====  Train Loss 0.6727496981620789\n",
            "========== Epoch 0 Batch 13====  Train Loss 0.5994307398796082\n",
            "========== Epoch 0 Batch 14====  Train Loss 0.7234736680984497\n",
            "========== Epoch 0 Batch 15====  Train Loss 0.6072721481323242\n",
            "========== Epoch 0 Batch 16====  Train Loss 0.692035973072052\n",
            "========== Epoch 0 Batch 17====  Train Loss 0.6437287330627441\n",
            "========== Epoch 0 Batch 18====  Train Loss 0.7690757513046265\n",
            "========== Epoch 0 Batch 19====  Train Loss 0.7198647260665894\n",
            "========== Epoch 0 Batch 20====  Train Loss 0.7210415601730347\n",
            "========== Epoch 0 ==== AVG. Train Loss 0.7398597151041031\n",
            "\n",
            "  Training epoch took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "========== Epoch 0 Batch 1==== Step 1 Val Loss 0.7143352031707764\n",
            "========== Epoch 0 Batch 2==== Step 1 Val Loss 0.7128051519393921\n",
            "========== Epoch 0 Batch 3==== Step 1 Val Loss 0.7215539216995239\n",
            "========== Epoch 0 Batch 4==== Step 1 Val Loss 0.7052209377288818\n",
            "========== Epoch 0 Batch 5==== Step 1 Val Loss 0.709438681602478\n",
            "========== Epoch 0 Batch 6==== Step 1 Val Loss 0.6935631632804871\n",
            "========== Epoch 0 Batch 7==== Step 1 Val Loss 0.6729578375816345\n",
            "========== Epoch 0 Batch 8==== Step 1 Val Loss 0.6692647933959961\n",
            "========== Epoch 0 Batch 9==== Step 1 Val Loss 0.6699690222740173\n",
            "========== Epoch 0 Batch 10==== Step 1 Val Loss 0.6935858130455017\n",
            "========== Epoch 0 Batch 11==== Step 1 Val Loss 0.6922485828399658\n",
            "  Average Validation Loss: 0.70\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "========== Epoch 1 Batch 1====  Train Loss 0.6142500042915344\n",
            "========== Epoch 1 Batch 2====  Train Loss 0.7021715641021729\n",
            "========== Epoch 1 Batch 3====  Train Loss 0.7239940762519836\n",
            "========== Epoch 1 Batch 4====  Train Loss 0.7027384042739868\n",
            "========== Epoch 1 Batch 5====  Train Loss 0.7720069289207458\n",
            "========== Epoch 1 Batch 6====  Train Loss 0.6881914734840393\n",
            "========== Epoch 1 Batch 7====  Train Loss 0.5830601453781128\n",
            "========== Epoch 1 Batch 8====  Train Loss 0.681787371635437\n",
            "========== Epoch 1 Batch 9====  Train Loss 0.6752941012382507\n",
            "========== Epoch 1 Batch 10====  Train Loss 0.6164604425430298\n",
            "========== Epoch 1 Batch 11====  Train Loss 0.7088868021965027\n",
            "========== Epoch 1 Batch 12====  Train Loss 0.8551192283630371\n",
            "========== Epoch 1 Batch 13====  Train Loss 0.7430675625801086\n",
            "========== Epoch 1 Batch 14====  Train Loss 0.7268675565719604\n",
            "========== Epoch 1 Batch 15====  Train Loss 0.6616191864013672\n",
            "========== Epoch 1 Batch 16====  Train Loss 0.5877976417541504\n",
            "========== Epoch 1 Batch 17====  Train Loss 0.6573256850242615\n",
            "========== Epoch 1 Batch 18====  Train Loss 0.4778680205345154\n",
            "========== Epoch 1 Batch 19====  Train Loss 0.638822078704834\n",
            "========== Epoch 1 Batch 20====  Train Loss 0.5690748691558838\n",
            "========== Epoch 1 ==== AVG. Train Loss 0.6693201571702957\n",
            "\n",
            "  Training epoch took: 0:00:16\n",
            "\n",
            "Running Validation...\n",
            "========== Epoch 1 Batch 1==== Step 1 Val Loss 0.7909449338912964\n",
            "========== Epoch 1 Batch 2==== Step 1 Val Loss 0.7711228132247925\n",
            "========== Epoch 1 Batch 3==== Step 1 Val Loss 0.8130735158920288\n",
            "========== Epoch 1 Batch 4==== Step 1 Val Loss 0.7102630138397217\n",
            "========== Epoch 1 Batch 5==== Step 1 Val Loss 0.7291185855865479\n",
            "========== Epoch 1 Batch 6==== Step 1 Val Loss 0.6907971501350403\n",
            "========== Epoch 1 Batch 7==== Step 1 Val Loss 0.7021052241325378\n",
            "========== Epoch 1 Batch 8==== Step 1 Val Loss 0.6895912289619446\n",
            "========== Epoch 1 Batch 9==== Step 1 Val Loss 0.6220302581787109\n",
            "========== Epoch 1 Batch 10==== Step 1 Val Loss 0.6961360573768616\n",
            "========== Epoch 1 Batch 11==== Step 1 Val Loss 0.6859233379364014\n",
            "  Average Validation Loss: 0.72\n",
            "  Validation took: 0:00:19\n",
            "\n",
            "======== Epoch 3 / 2 ========\n",
            "Training...\n",
            "========== Epoch 2 Batch 1====  Train Loss 0.5704241394996643\n",
            "========== Epoch 2 Batch 2====  Train Loss 0.5067524909973145\n",
            "========== Epoch 2 Batch 3====  Train Loss 0.477790504693985\n",
            "========== Epoch 2 Batch 4====  Train Loss 0.6207395195960999\n",
            "========== Epoch 2 Batch 5====  Train Loss 0.485698938369751\n",
            "========== Epoch 2 Batch 6====  Train Loss 0.4811757206916809\n",
            "========== Epoch 2 Batch 7====  Train Loss 0.5023292899131775\n",
            "========== Epoch 2 Batch 8====  Train Loss 0.4868965148925781\n",
            "========== Epoch 2 Batch 9====  Train Loss 0.5995723605155945\n",
            "========== Epoch 2 Batch 10====  Train Loss 0.5181020498275757\n",
            "========== Epoch 2 Batch 11====  Train Loss 0.7124093174934387\n",
            "========== Epoch 2 Batch 12====  Train Loss 0.6336371898651123\n",
            "========== Epoch 2 Batch 13====  Train Loss 0.6790526509284973\n",
            "========== Epoch 2 Batch 14====  Train Loss 0.6503691077232361\n",
            "========== Epoch 2 Batch 15====  Train Loss 0.6194857954978943\n",
            "========== Epoch 2 Batch 16====  Train Loss 0.3705114722251892\n",
            "========== Epoch 2 Batch 17====  Train Loss 0.7179090976715088\n",
            "========== Epoch 2 Batch 18====  Train Loss 0.32848790287971497\n",
            "========== Epoch 2 Batch 19====  Train Loss 0.4313611090183258\n",
            "========== Epoch 2 Batch 20====  Train Loss 0.5017192959785461\n",
            "========== Epoch 2 ==== AVG. Train Loss 0.5447212234139442\n",
            "\n",
            "  Training epoch took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "========== Epoch 2 Batch 1==== Step 1 Val Loss 1.125227689743042\n",
            "========== Epoch 2 Batch 2==== Step 1 Val Loss 1.0040000677108765\n",
            "========== Epoch 2 Batch 3==== Step 1 Val Loss 1.2913838624954224\n",
            "========== Epoch 2 Batch 4==== Step 1 Val Loss 1.0290530920028687\n",
            "========== Epoch 2 Batch 5==== Step 1 Val Loss 0.9685745239257812\n",
            "========== Epoch 2 Batch 6==== Step 1 Val Loss 0.7695379257202148\n",
            "========== Epoch 2 Batch 7==== Step 1 Val Loss 0.5160898566246033\n",
            "========== Epoch 2 Batch 8==== Step 1 Val Loss 0.5456339716911316\n",
            "========== Epoch 2 Batch 9==== Step 1 Val Loss 0.40473243594169617\n",
            "========== Epoch 2 Batch 10==== Step 1 Val Loss 0.7507397532463074\n",
            "========== Epoch 2 Batch 11==== Step 1 Val Loss 0.6331869959831238\n",
            "  Average Validation Loss: 0.82\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 4 / 2 ========\n",
            "Training...\n",
            "========== Epoch 3 Batch 1====  Train Loss 0.3525038957595825\n",
            "========== Epoch 3 Batch 2====  Train Loss 0.2951377332210541\n",
            "========== Epoch 3 Batch 3====  Train Loss 0.30987441539764404\n",
            "========== Epoch 3 Batch 4====  Train Loss 0.4545021653175354\n",
            "========== Epoch 3 Batch 5====  Train Loss 0.31351423263549805\n",
            "========== Epoch 3 Batch 6====  Train Loss 0.3385915458202362\n",
            "========== Epoch 3 Batch 7====  Train Loss 0.29700905084609985\n",
            "========== Epoch 3 Batch 8====  Train Loss 0.3096923232078552\n",
            "========== Epoch 3 Batch 9====  Train Loss 0.48049214482307434\n",
            "========== Epoch 3 Batch 10====  Train Loss 0.2800927758216858\n",
            "========== Epoch 3 Batch 11====  Train Loss 0.4937456250190735\n",
            "========== Epoch 3 Batch 12====  Train Loss 0.4014517366886139\n",
            "========== Epoch 3 Batch 13====  Train Loss 0.5778437852859497\n",
            "========== Epoch 3 Batch 14====  Train Loss 0.5037928819656372\n",
            "========== Epoch 3 Batch 15====  Train Loss 0.3703334331512451\n",
            "========== Epoch 3 Batch 16====  Train Loss 0.21866440773010254\n",
            "========== Epoch 3 Batch 17====  Train Loss 0.4724859893321991\n",
            "========== Epoch 3 Batch 18====  Train Loss 0.2271166890859604\n",
            "========== Epoch 3 Batch 19====  Train Loss 0.23650570213794708\n",
            "========== Epoch 3 Batch 20====  Train Loss 0.29611071944236755\n",
            "========== Epoch 3 ==== AVG. Train Loss 0.3614730626344681\n",
            "\n",
            "  Training epoch took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "========== Epoch 3 Batch 1==== Step 1 Val Loss 1.3476823568344116\n",
            "========== Epoch 3 Batch 2==== Step 1 Val Loss 1.1653039455413818\n",
            "========== Epoch 3 Batch 3==== Step 1 Val Loss 1.5777631998062134\n",
            "========== Epoch 3 Batch 4==== Step 1 Val Loss 1.1213921308517456\n",
            "========== Epoch 3 Batch 5==== Step 1 Val Loss 1.0918818712234497\n",
            "========== Epoch 3 Batch 6==== Step 1 Val Loss 0.800494372844696\n",
            "========== Epoch 3 Batch 7==== Step 1 Val Loss 0.5474660396575928\n",
            "========== Epoch 3 Batch 8==== Step 1 Val Loss 0.610687255859375\n",
            "========== Epoch 3 Batch 9==== Step 1 Val Loss 0.43247297406196594\n",
            "========== Epoch 3 Batch 10==== Step 1 Val Loss 0.8361412286758423\n",
            "========== Epoch 3 Batch 11==== Step 1 Val Loss 0.6141040921211243\n",
            "  Average Validation Loss: 0.92\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "======== Epoch 5 / 2 ========\n",
            "Training...\n",
            "========== Epoch 4 Batch 1====  Train Loss 0.21731328964233398\n",
            "========== Epoch 4 Batch 2====  Train Loss 0.17630885541439056\n",
            "========== Epoch 4 Batch 3====  Train Loss 0.16866165399551392\n",
            "========== Epoch 4 Batch 4====  Train Loss 0.3764573037624359\n",
            "========== Epoch 4 Batch 5====  Train Loss 0.1833655685186386\n",
            "========== Epoch 4 Batch 6====  Train Loss 0.25069424510002136\n",
            "========== Epoch 4 Batch 7====  Train Loss 0.22954608500003815\n",
            "========== Epoch 4 Batch 8====  Train Loss 0.1703660786151886\n",
            "========== Epoch 4 Batch 9====  Train Loss 0.3121241629123688\n",
            "========== Epoch 4 Batch 10====  Train Loss 0.1995132565498352\n",
            "========== Epoch 4 Batch 11====  Train Loss 0.24480073153972626\n",
            "========== Epoch 4 Batch 12====  Train Loss 0.3830582797527313\n",
            "========== Epoch 4 Batch 13====  Train Loss 0.46245497465133667\n",
            "========== Epoch 4 Batch 14====  Train Loss 0.442915141582489\n",
            "========== Epoch 4 Batch 15====  Train Loss 0.27786681056022644\n",
            "========== Epoch 4 Batch 16====  Train Loss 0.18730106949806213\n",
            "========== Epoch 4 Batch 17====  Train Loss 0.49075639247894287\n",
            "========== Epoch 4 Batch 18====  Train Loss 0.1515790820121765\n",
            "========== Epoch 4 Batch 19====  Train Loss 0.17489442229270935\n",
            "========== Epoch 4 Batch 20====  Train Loss 0.20863556861877441\n",
            "========== Epoch 4 ==== AVG. Train Loss 0.265430648624897\n",
            "\n",
            "  Training epoch took: 0:00:15\n",
            "\n",
            "Running Validation...\n",
            "========== Epoch 4 Batch 1==== Step 1 Val Loss 1.418270230293274\n",
            "========== Epoch 4 Batch 2==== Step 1 Val Loss 1.2152040004730225\n",
            "========== Epoch 4 Batch 3==== Step 1 Val Loss 1.6844780445098877\n",
            "========== Epoch 4 Batch 4==== Step 1 Val Loss 1.1481025218963623\n",
            "========== Epoch 4 Batch 5==== Step 1 Val Loss 1.1253939867019653\n",
            "========== Epoch 4 Batch 6==== Step 1 Val Loss 0.8175451755523682\n",
            "========== Epoch 4 Batch 7==== Step 1 Val Loss 0.5978924632072449\n",
            "========== Epoch 4 Batch 8==== Step 1 Val Loss 0.6598659157752991\n",
            "========== Epoch 4 Batch 9==== Step 1 Val Loss 0.4751183092594147\n",
            "========== Epoch 4 Batch 10==== Step 1 Val Loss 0.8769702315330505\n",
            "========== Epoch 4 Batch 11==== Step 1 Val Loss 0.6269095540046692\n",
            "  Average Validation Loss: 0.97\n",
            "  Validation took: 0:00:18\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:31 (h:mm:ss)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = myModel(bert_emb_layer=11,startLayer=1,endLayer=4,bertModel=bertModel)\n",
        "# criterion1 = nn.CosineEmbeddingLoss(margin = 0.5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "mytrainStep(model,criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205CYsvMMXyt"
      },
      "source": [
        "# Classification Task with Siamese BERTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At6QHoLTMXMG"
      },
      "outputs": [],
      "source": [
        "class myModel(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (True,True)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModel, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      # self.bertModel.train()  \n",
        "      # for p in self.bertModel.parameters():\n",
        "      #     p.requires_grad = False\n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      # self.dropout1 = nn.Dropout(0.1)\n",
        "      self.bilstm = nn.LSTM(input_size=768, hidden_size=768,batch_first=True,bidirectional=True)\n",
        "      self.FC1 = nn.Linear(in_features = 768*4,out_features = 256)\n",
        "      \n",
        "      self.FC2 = nn.Linear(in_features = 256,out_features = 64)\n",
        "      self.FC3 = nn.Linear(in_features = 64,out_features = 2)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.tanh = nn.Tanh()\n",
        "      self.dropout = nn.Dropout(0.2)\n",
        "      # self.FC = nn.Sequential(\n",
        "      #   nn.Linear(in_features = inputFeatures*2,out_features = 256),\n",
        "      #   nn.ReLU(),\n",
        "      #   nn.Dropout(0.1),\n",
        "      #   nn.Linear(in_features = 256,out_features = 2),\n",
        "      #   # nn.ReLU(),\n",
        "      #   # nn.Dropout(0.1),\n",
        "      #   # nn.Linear(in_features = 64,out_features = 16),\n",
        "      #   # nn.ReLU(),\n",
        "      #   # nn.Dropout(0.1),\n",
        "      #   # nn.Linear(in_features = 16,out_features = 2),\n",
        "      #   # nn.Linear(in_features = 16,out_features = 2)\n",
        "      #   nn.LogSoftmax()\n",
        "      # )\n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:]#bertOutputs[1][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      del hidden_states\n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] #bertOutputs[1][1:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      del hidden_states\n",
        "      return concatEmbeddingLayers\n",
        "  def init_hidden(self, batch_size):\n",
        "        #Initialization of the LSTM hidden and cell states\n",
        "        h0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        c0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][1:]\n",
        "      # `hidden_states` is a Python list.\n",
        "\n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[-4:]).sum(0)\n",
        "      del hidden_states\n",
        "      # sumEmbeddingLayers = torch.stack( [hidden_states[i] for i in range(self.startLayer,self.endLayer)]).sum(0)\n",
        "      # sumEmbeddingLayers = torch.sum(hidden_states[0][self.startLayer:self.endLayer], dim=0)\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask ,strategy='avg'):\n",
        "      if strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'max':\n",
        "         max_setence_embeddings = torch.max(token_embeddings,dim=1)\n",
        "         return max_setence_embeddings\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "     \n",
        "      embeddings = bertOutputs[1]\n",
        "      return embeddings\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(sent_id, attention_mask=mask)\n",
        "     \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings #, self.FC(embeddings)\n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        return embeddings #, self.FC(embeddings)\n",
        "      else:\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings #, self.FC(embeddings)\n",
        "\n",
        "  def forward(self, sent_id1, mask1,sent_id2, mask2,hidden):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      output2  = self.forwardOnce(sent_id2, mask2)\n",
        "    \n",
        "  \n",
        "     \n",
        "   \n",
        "      # output1 = output1[:, 0, :]\n",
        "      # output2 = output2[:, 0, :]\n",
        "      # output1 = self.tanh(output1)\n",
        "      # output1 = self.dropout(output1)\n",
        "      out1, (hidden1,cell1) = self.bilstm(output1,hidden)\n",
        "      out2, (hidden2,cell2) = self.bilstm(output2,hidden)\n",
        "      # out1,hidden11 = self.bilstm(output1,hidden)\n",
        "      #Extract only the hidden state from the last LSTM cell\n",
        "      # out11 = out1[:,-1,:]\n",
        "      # print(hidden11)\n",
        "      # h1 = hidden1\n",
        "      # final_repr = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1)\n",
        "\n",
        "      out_split1 = out1.view(sent_id1.shape[0], 512, 2, 768)\n",
        "      out_split2 = out2.view(sent_id2.shape[0], 512, 2, 768)\n",
        "      # out_split1 = out1.view(sent_id1.shape[0], 128, 2, 768)\n",
        "      out_forward1 = out_split1[:, :, 0, :]\n",
        "      out_backward1 = out_split1[:, :, 1, :]\n",
        "      out_forward2 = out_split2[:, :, 0, :]\n",
        "      out_backward2 = out_split2[:, :, 1, :]\n",
        "      batch_indices = torch.arange(0, sent_id1.shape[0], device=device)\n",
        "      seq_indices = 512 - 1\n",
        "      direction_full1 = torch.cat([out_split1[batch_indices, seq_indices, 0], out_split1[batch_indices, 0, 1]], dim=-1)\n",
        "      direction_full2 = torch.cat([out_split2[batch_indices, seq_indices, 0], out_split2[batch_indices, 0, 1]], dim=-1)\n",
        "      out = torch.cat((direction_full1, direction_full2), 1)\n",
        "      out = self.dropout(out)\n",
        "      out = self.FC1(out)\n",
        "      out = self.relu(out)\n",
        "      out = self.FC2(out)\n",
        "      out = self.FC3(out)\n",
        "      # return F.normalize(direction_full1), F.normalize(direction_full2),output2\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo0zxMFiMpyD"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion,validation_dataloader):\n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.bertModel.eval()\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    step = 0\n",
        "    totalF1 = 0\n",
        "    # for (input1,mask1, target1),(input2,mask2, target2) in validation_dataloader:\n",
        "    for (input1,mask1,input2,mask2,target1) in validation_dataloader:\n",
        "        step += 1\n",
        "       \n",
        "        b_input_ids1 = input1.to(device)\n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        # target1 = target1.type(torch.LongTensor)\n",
        "        b_labels = target1.to(device)\n",
        "        # b_labels = torch.unsqueeze(b_labels,1)\n",
        "        b_input_ids2 = input2.to(device)\n",
        "        b_input_mask2 = mask2.to(device)\n",
        "        f1=0\n",
        "        h = model.init_hidden(b_input_ids1.shape[0])\n",
        "        with torch.no_grad():        \n",
        "          out = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,h) #,b_input_ids2, b_input_mask2\n",
        "          loss = criterion(out,b_labels)\n",
        "          total_eval_loss += loss.item()\n",
        "          f1 = calcF1score(out,b_labels)\n",
        "          totalF1+=f1\n",
        "          # probs = F.softmax(out, dim=1).cpu().numpy()\n",
        "         \n",
        "          print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 Val. Loss \"+str(loss.item()), \" ==== \",str(f1))\n",
        "    avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "    model.bertModel.train()\n",
        "    model.train()\n",
        "    avg_val_f1 = totalF1/len(validation_dataloader)\n",
        "    return  avg_val_loss,avg_val_f1 #avg_val_accuracy, avg_val_f1,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M17NdTz6MvUV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "early_stopping1 = EarlyStopping(patience=2, verbose=True)\n",
        "def mytrainStep(model,criterion):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      if torch.cuda.is_available():\n",
        "         model.to(device)\n",
        "      warmup_percent = 0.2\n",
        "      total_steps = math.ceil(3*10*1./1600)\n",
        "      warmup_steps = 0 #int(total_steps*warmup_percent)\n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      for module in modules:\n",
        "          for param in module.parameters():\n",
        "              param.requires_grad = False\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=2e-5, #5e-5, 3e-5, 2e-5\n",
        "                                    correct_bias=False) #eps=1e-8,\n",
        "      # optimizer = torch.optim.Adam(model.parameters() ,lr=0.0001)\n",
        "      #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=5*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain2 = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid2 = list()\n",
        "      listOfF1Valid = list()\n",
        "      f1_total = 0\n",
        "      for epoch_i in range(0, 1):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss2 = 0\n",
        "\n",
        "          model.train()\n",
        "          f1_total = 0\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          # for (input1,mask1, target1),(input2,mask2, target2) in train_dataloader:\n",
        "          for (input1,mask1,input2,mask2,target1) in train_dataloader:  \n",
        "              step +=1\n",
        "              # if step == 21:\n",
        "              #     break \n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  b_input_ids1 = input1.to(device)\n",
        "                  b_input_mask1 = mask1.to(device)\n",
        "                  # target1 = target1.type(torch.LongTensor)\n",
        "                  b_labels = target1.to(device)\n",
        "                  # print(b_labels)\n",
        "                  # b_labels = torch.unsqueeze(b_labels,1)\n",
        "                  # print(b_labels)\n",
        "                  b_input_ids2 = input2.to(device)\n",
        "                  b_input_mask2 = mask2.to(device)\n",
        "                  h = model.init_hidden(b_input_ids1.shape[0])\n",
        "              model.zero_grad()  \n",
        "\n",
        "              # with embModel.parameters() == False:\n",
        "                       \n",
        "              out = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,h) #b_input_ids2, b_input_mask2 \n",
        "              \n",
        "              loss = criterion(out,b_labels)\n",
        "              f1=0\n",
        "              total_train_loss2 += loss.item()\n",
        "              f1 = calcF1score(out,b_labels)\n",
        "              f1_total+=f1\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"====  Train Loss \"+str(loss.item()), \" ==== \",str(f1))#\n",
        "              \n",
        "              loss.backward()\n",
        "              # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optimizer.step()\n",
        "              # scheduler.step()\n",
        "          avg_train_loss2 = total_train_loss2 / len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== AVG. Train Loss \"+str(avg_train_loss2),\"===== \",str(f1_total/20))            \n",
        "          listOflossesTrain2.append(avg_train_loss2)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss2))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss2,avg_val_f1 = validation(model,epoch_i,criterion,validation_dataloader)\n",
        "          listOflossesValid2.append(avg_val_loss2)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss2))\n",
        "          print(\"  Validation avg-F1: {0:.2f}\".format(avg_val_f1))\n",
        "          # print(\"  Validation avg-Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss2,\n",
        "                'Validation Loss': avg_val_loss2,\n",
        "                # 'Valid. avg F1.': avg_val_f1,\n",
        "                # 'Valid. avg Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "          )\n",
        "          # early_stopping1(avg_val_loss2, model)\n",
        "        \n",
        "          # if early_stopping1.early_stop:\n",
        "          #     print(\"Early stopping\")\n",
        "          #     break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain2,listOflossesValid2,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "18KMOp2cNeNF",
        "outputId": "3ffc2d89-9ee5-4401-aeff-ae520d297ac8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "========== Epoch 0 Batch 1====  Train Loss 0.7020729780197144  ====  0.0\n",
            "========== Epoch 0 Batch 2====  Train Loss 0.6920119524002075  ====  0.45161290322580644\n",
            "========== Epoch 0 Batch 3====  Train Loss 0.6827157139778137  ====  0.7083333333333333\n",
            "========== Epoch 0 Batch 4====  Train Loss 0.7090359926223755  ====  0.6086956521739131\n",
            "========== Epoch 0 Batch 5====  Train Loss 0.7010071873664856  ====  0.5777777777777777\n",
            "========== Epoch 0 Batch 6====  Train Loss 0.6964473128318787  ====  0.05714285714285715\n",
            "========== Epoch 0 Batch 7====  Train Loss 0.6894224286079407  ====  0.0\n",
            "========== Epoch 0 Batch 8====  Train Loss 0.6838886737823486  ====  0.0\n",
            "========== Epoch 0 Batch 9====  Train Loss 0.6787237524986267  ====  0.0\n",
            "========== Epoch 0 Batch 10====  Train Loss 0.7680231332778931  ====  0.0\n",
            "========== Epoch 0 Batch 11====  Train Loss 0.7231895327568054  ====  0.0\n",
            "========== Epoch 0 Batch 12====  Train Loss 0.6871258020401001  ====  0.0\n",
            "========== Epoch 0 Batch 13====  Train Loss 0.6911314725875854  ====  0.0\n",
            "========== Epoch 0 Batch 14====  Train Loss 0.6906747817993164  ====  0.5084745762711864\n",
            "========== Epoch 0 Batch 15====  Train Loss 0.6943545341491699  ====  0.6444444444444445\n",
            "========== Epoch 0 Batch 16====  Train Loss 0.699580729007721  ====  0.6526315789473685\n",
            "========== Epoch 0 Batch 17====  Train Loss 0.6895173788070679  ====  0.6804123711340206\n",
            "========== Epoch 0 Batch 18====  Train Loss 0.7076273560523987  ====  0.5934065934065934\n",
            "========== Epoch 0 Batch 19====  Train Loss 0.7076199054718018  ====  0.5934065934065934\n",
            "========== Epoch 0 Batch 20====  Train Loss 0.6895167827606201  ====  0.7326732673267327\n",
            "========== Epoch 0 Batch 21====  Train Loss 0.6941490173339844  ====  0.6593406593406593\n",
            "========== Epoch 0 Batch 22====  Train Loss 0.691771388053894  ====  0.6511627906976744\n",
            "========== Epoch 0 Batch 23====  Train Loss 0.6940855383872986  ====  0.43636363636363634\n",
            "========== Epoch 0 Batch 24====  Train Loss 0.6938751935958862  ====  0.30434782608695654\n",
            "========== Epoch 0 Batch 25====  Train Loss 0.6917612552642822  ====  0.0625\n",
            "========== Epoch 0 Batch 26====  Train Loss 0.6917036771774292  ====  0.0\n",
            "========== Epoch 0 Batch 27====  Train Loss 0.695862889289856  ====  0.0\n",
            "========== Epoch 0 Batch 28====  Train Loss 0.700698733329773  ====  0.0\n",
            "========== Epoch 0 Batch 29====  Train Loss 0.692205548286438  ====  0.0\n",
            "========== Epoch 0 Batch 30====  Train Loss 0.6845139861106873  ====  0.0\n",
            "========== Epoch 0 Batch 31====  Train Loss 0.6930898427963257  ====  0.0\n",
            "========== Epoch 0 Batch 32====  Train Loss 0.6986833214759827  ====  0.0\n",
            "========== Epoch 0 Batch 33====  Train Loss 0.6906538605690002  ====  0.0\n",
            "========== Epoch 0 Batch 34====  Train Loss 0.6907270550727844  ====  0.0\n",
            "========== Epoch 0 Batch 35====  Train Loss 0.6923670172691345  ====  0.0\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e537cc47ebfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# criterion=nn.BCEWithLogitsLoss()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmytrainStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-9264624248bf>\u001b[0m in \u001b[0;36mmytrainStep\u001b[0;34m(model, criterion)\u001b[0m\n\u001b[1;32m     85\u001b[0m               \u001b[0;31m# with embModel.parameters() == False:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m               \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_input_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#b_input_ids2, b_input_mask2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m               \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-54e95953121c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id1, mask1, sent_id2, mask2, hidden)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0;31m# forward pass of input 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m       \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0moutput2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-54e95953121c>\u001b[0m in \u001b[0;36mforwardOnce\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupLayersMode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[0;32m-> 1019\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1020\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    607\u001b[0m                 )\n\u001b[1;32m    608\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    610\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         )\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model = myModel(bert_emb_layer=5,startLayer=3,endLayer=7,bertModel=bertModel)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion=nn.BCEWithLogitsLoss()\n",
        "mytrainStep(model,criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iJxWrzN5etsK",
        "outputId": "192489b9-eb5a-4a1f-8ca1-d67eeed737ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Thesis/MaskAll/Separated/myModel.pt'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.save(model.state_dict(),'myModel.pt')\n",
        "import shutil\n",
        "shutil.copy(\"myModel.pt\",'/content/drive/My Drive/Thesis/MaskAll/Separated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OaRFSVAeQFH"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVfLShVA7VEo"
      },
      "source": [
        "# Contrastive Learning in 2 steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIsc579-7Zmv"
      },
      "source": [
        "Step 1: Learning Embeddings with Contrastive loss <br>\n",
        "Step 2: Freeze NN that learns Embeddings and Start training of NN that is a simple Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeis-Qqt7Top"
      },
      "outputs": [],
      "source": [
        "from joblib.logger import print_function\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "class myModelEmbeddings(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (False,False)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModelEmbeddings, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      \n",
        "      # for param in model.bertModel.parameters():\n",
        "      #     param.requires_grad = False\n",
        "      # modules = [self.bertModel.embeddings, *self.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "     \n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "     \n",
        "      # self.bilstm = nn.LSTM(768, 256, batch_first=True,bidirectional=True) \n",
        "      self.FC = nn.Linear(in_features = 768,out_features = 384)\n",
        "     \n",
        "      self.tanh = nn.Tanh()\n",
        "      self.dropout = nn.Dropout(0.5),\n",
        "     \n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      \n",
        "      return  layerOutput\n",
        "  def getAttentionHead(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[3][0:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer]\n",
        "      return  layerOutput\n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][0:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      \n",
        "      return concatEmbeddingLayers\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      embeddings = bertOutputs[1]\n",
        "      return embeddings\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      # `hidden_states` is a Python list.\n",
        "      \n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      del hidden_states\n",
        "\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask, strategy='avg'):\n",
        "      if strategy == 'max':\n",
        "        #  avg_setence_embeddings = torch.mean(token_embeddings,dim=1)\n",
        "        #  print(avg_setence_embeddings.shape)\n",
        "         input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "         max_setence_embeddings = torch.max(token_embeddings, 1)[0]\n",
        "         return max_setence_embeddings\n",
        "      elif strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'sum':\n",
        "        sum_setence_embeddings = torch.sum(token_embeddings[0:len(token_embeddings)],1)\n",
        "        return sum_setence_embeddings\n",
        "\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(sent_id, attention_mask=mask)\n",
        "     \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        return embeddings \n",
        "      else:\n",
        "        # embeddings = self.getAttentionHead(outputs)\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings \n",
        "      \n",
        " \n",
        "  def forward(self, sent_id1, mask1,sent_id2, mask2,b_labels):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      # forward pass of input 2\n",
        "      output2  = self.forwardOnce(sent_id2, mask2)\n",
        "     \n",
        "      # print(output1.shape)\n",
        "      # print(output1[:,11,:,:])\n",
        "      # print(output1[:,11,:,:].shape)\n",
        "      # avgoutput1 = self.pooling(output1,mask1,'avg')\n",
        "      # avgoutput2 = self.pooling(output2,mask2,'avg')\n",
        "      # avgoutput1 = output1.mean(1)\n",
        "      # avgoutput2 = output2.mean(1)\n",
        "      # concatenated = torch.cat((avgoutput1,avgoutput1),dim=0)\n",
        "      \n",
        "  \n",
        "      cos = nn.CosineSimilarity(dim=1)\n",
        "      # output11 = output1[:, 0, :]\n",
        "      # output22 = output2[:, 0, :]\n",
        "      # print(output11)\n",
        "      # output11 = F.normalize(output11, p=2, dim=1)\n",
        "      # output22 = F.normalize(output22, p=2, dim=1)\n",
        "      # print(output11)\n",
        "      # lstm1 = self.bilstm(output1)\n",
        "      # lstm2 = self.bilstm(output2)\n",
        "      # output1 = self.dropout(avgoutput1)\n",
        "      FC11 = self.FC(output1)\n",
        "      # FC11 = self.tanh(FC11)\n",
        "      # output2 = self.dropout(avgoutput1)\n",
        "      FC22 = self.FC(output1)\n",
        "      # FC22 = self.tanh(FC22)\n",
        "      output = cos(FC11, FC22)\n",
        "      \n",
        "      print(output)\n",
        "     \n",
        "      print(b_labels)\n",
        "\n",
        "      \n",
        "\n",
        "      plt.hist(output.cpu().data.numpy(), bins=6)\n",
        "      plt.show()\n",
        "      return FC11, FC22,output\n",
        "\n",
        "class myModelFC(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(myModelFC, self).__init__()\n",
        "      inputFeatures = 256 #32\n",
        "      self.FC = nn.Sequential(\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Dropout(0.1),  \n",
        "        nn.Linear(in_features = inputFeatures*2,out_features = 2),\n",
        "        # nn.Tanh(),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(0.1), #0.5\n",
        "        # nn.Linear(in_features = 256,out_features = 32),\n",
        "        # nn.Linear(in_features = 32,out_features = 2),\n",
        "      \n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "      concatenated = torch.cat((input1,input1),dim=1)\n",
        "      \n",
        "      out = self.FC(concatenated)\n",
        "      # print(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycQJUZROJ57v"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning.reducers import MeanReducer\n",
        "modelEmb = myModelEmbeddings(bert_emb_layer=5,startLayer=4,endLayer=12,bertModel=bertModel)\n",
        "modelCLS = myModelFC()\n",
        "# criterion1 = nn.CosineEmbeddingLoss(margin = 0.4)#margin = 0.5\n",
        "# func = distances.LpDistance\n",
        "# criterion1 = ContrastiveLossSiamese(margin1 = 0.9)\n",
        "# criterion1 = getLossFunction(device)\n",
        "# criterion1 =nn.BCELoss()\n",
        "#reducer=MeanReducer()\n",
        "criterion1 = losses.ContrastiveLoss(distance = distances.CosineSimilarity(),reducer=MeanReducer(),pos_margin=1, neg_margin=-0.1) #distance = distances.CosineSimilarity()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "early_stopping1 = EarlyStopping(patience=2, path='checkpointEmb.pt',verbose=True)\n",
        "early_stopping2 = EarlyStopping(patience=3, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dews8Y_MC53k"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion1,validation_dataloader,modelFC=None,criterion2 = None):\n",
        "    \n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.bertModel.eval()\n",
        " \n",
        "    model.eval()\n",
        "    if modelFC is not None:\n",
        "        modelFC.eval()\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # tsne = TSNE()\n",
        "    step = 0\n",
        "    concatAll = []\n",
        "    totalF1 = 0\n",
        "    # for (input1,mask1, target1),(input2,mask2, target2) in validation_dataloader:\n",
        "    for input1, mask1, input2, mask2,target1 in validation_dataloader:\n",
        "    # for batch in test_dataloader:\n",
        "        step += 1 \n",
        "        # if step == 71:\n",
        "        #   break\n",
        "\n",
        "        # b_input_ids1 = batch[0].permute(0,2,1).squeeze(2).to(device)\n",
        "        # b_input_mask1 = batch[1].permute(0,2,1).squeeze(2).to(device)\n",
        "        # label = batch[4].type(torch.LongTensor)\n",
        "        # b_input_ids2 = batch[2].permute(0,2,1).squeeze(2).to(device)\n",
        "        # b_input_mask2 = batch[3].permute(0,2,1).squeeze(2).to(device)\n",
        "        # label = label.type(torch.LongTensor)\n",
        "        # b_labels = label.to(device)\n",
        "        b_input_ids1 = input1.to(device)\n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        target1 = target1.type(torch.LongTensor)\n",
        "        # if modelFC==None:\n",
        "        #   target1 = target1.type(torch.LongTensor)\n",
        "        #   target1 = torch.where(target1==0,torch.tensor(-1),target1)\n",
        "        #   # target1 = torch.where(target1==1,torch.tensor(2),target1)\n",
        "        #   # target1 = torch.where(target1==-1,torch.tensor(1),target1)\n",
        "        #   # target1 = torch.where(target1==2,torch.tensor(0),target1)\n",
        "        # else:\n",
        "        #   target1 = target1.type(torch.LongTensor)\n",
        "        b_labels = target1.to(device)\n",
        "        # b_labels = b_labels.unsqueeze(1)\n",
        "        # b_labels = torch.squeeze(b_labels,1)\n",
        "        b_input_ids2 = input2.to(device)\n",
        "        b_input_mask2 = mask2.to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            if modelFC==None:\n",
        "\n",
        "              FC11,FC22,cos = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)             \n",
        "              \n",
        "              pos_indices = torch.where(b_labels==1)[0]\n",
        "              neg_indices = torch.where(b_labels==0)[0]\n",
        "              # print(pos_indices)\n",
        "              # print(neg_indices)\n",
        "              indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "              loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # loss1 = criterion1(FC11,FC22,b_labels)\n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "              \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2)\n",
        "              FC11,FC22,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)\n",
        "              out = modelFC(FC11,FC22)\n",
        "              loss2 = criterion2(out,b_labels)\n",
        "            if modelFC == None:\n",
        "                total_eval_loss += loss1.item()\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 AVG. val Loss \"+str(loss1.item()))\n",
        "                # label_ids = b_labels.to('cpu').numpy()\n",
        "                \n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "            \n",
        "                total_eval_loss += loss2.item()\n",
        "                f1 = calcF1score(out,b_labels)\n",
        "                totalF1 += f1\n",
        "                probs = F.softmax(out, dim=1).cpu().numpy()\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                print(probs) \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 AVG. val Loss \"+str(loss2.item()), \"==== \",str(f1))\n",
        "                modelFC.train()\n",
        "    \n",
        "    avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "    avg_f1_val = totalF1/len(validation_dataloader)\n",
        "    \n",
        "    if modelFC==None:\n",
        "       model.bertModel.train()\n",
        "       model.train()\n",
        "    else:\n",
        "       model.bertModel.eval()\n",
        "       model.eval()\n",
        "       modelFC.train()  \n",
        "    return  avg_val_loss,avg_f1_val #avg_val_accuracy, avg_val_f1,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnZuv8ViDjvt"
      },
      "outputs": [],
      "source": [
        "def mytrainStep2(model,criterion1,criterion2,embModel):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      embModel.bertModel.eval()\n",
        "      embModel.eval()\n",
        "      for param in embModel.bertModel.parameters():\n",
        "          param.requires_grad = False\n",
        "      for param2 in embModel.parameters():\n",
        "          param2.requires_grad = False    \n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "         model.to(device)\n",
        "      \n",
        "      \n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      optimizer2 = torch.optim.Adam(model.parameters(),\n",
        "                                    lr=0.0002\n",
        "                                    )\n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=3*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain2 = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid2 = list()\n",
        "      listOfF1Valid = list()\n",
        "      totalf1 = 0\n",
        "      epoch_stop = 0\n",
        "      for epoch_i in range(0, 8):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss2 = 0\n",
        "\n",
        "          model.train()\n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          totalf1 = 0\n",
        "          for (input1,mask1,input2,mask2,target1) in train_dataloader:\n",
        "              step +=1\n",
        "              # if step ==201 :\n",
        "              #   break\n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  b_input_ids1 = input1.to(device)\n",
        "                  b_input_mask1 = mask1.to(device)\n",
        "                  target1 = target1.type(torch.LongTensor)\n",
        "                  b_labels = target1.to(device)\n",
        "                  \n",
        "                  # b_labels = torch.squeeze(b_labels,1)\n",
        "                  b_input_ids2 = input2.to(device)\n",
        "                  b_input_mask2 = mask2.to(device)\n",
        "              \n",
        "              model.zero_grad()  \n",
        "\n",
        "              # with embModel.parameters() == False:\n",
        "              # for p in embModel.parameters():\n",
        "              #     p.requires_grad = False            \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              FC11,FC22,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)\n",
        "              out = model(FC11,FC22)\n",
        "             \n",
        "              loss = criterion2(out,b_labels)\n",
        "\n",
        "              total_train_loss2 += loss.item()\n",
        "              f1 = calcF1score(out,b_labels)\n",
        "              totalf1+=f1\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 2 Train Loss \"+str(loss.item()),\"====== \",str(f1))\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              optimizer2.step()\n",
        "              # scheduler.step()\n",
        "              \n",
        "          avg_train_loss2 = total_train_loss2 /len(train_dataloader)\n",
        "          avg_f1_train = totalf1/len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 2 Train Loss \"+str(avg_train_loss2),\"====== \",str(avg_f1_train))            \n",
        "          listOflossesTrain2.append(avg_train_loss2)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss2))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss2,avg_f1_val = validation(embModel,epoch_i,criterion1,validation_dataloader,model,criterion2)\n",
        "          listOflossesValid2.append(avg_val_loss2)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss2))\n",
        "          print(\"  Validation avg-F1: {0:.2f}\".format(avg_f1_val))\n",
        "          # print(\"  Validation avg-Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss2,\n",
        "                'Validation Loss': avg_val_loss2,\n",
        "                # 'Valid. avg F1.': avg_val_f1,\n",
        "                # 'Valid. avg Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "          )\n",
        "          early_stopping2(avg_val_loss2, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping2.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              \n",
        "              break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain2,listOflossesValid2,epoch_stop)\n",
        "      "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvAjrAM2CG-3"
      },
      "outputs": [],
      "source": [
        "num_training_steps = 176 #len(train_dataloader)\n",
        "from pytorch_metric_learning.utils import common_functions\n",
        "common_functions.COLLECT_STATS = True\n",
        "\n",
        "\n",
        "# loss = loss_fn(emb, labels)\n",
        "\n",
        "def mytrainStep1(model,criterion1,criterion2):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      # reducers = criterion1.reducer.reducers\n",
        "    \n",
        "      if torch.cuda.is_available():\n",
        "          model.to(device)\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      # optimizer = torch.optim.Adam(model.parameters(),\n",
        "      #                               lr=0.0001)\n",
        "      modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:7]] #Replace 8 by what you want\n",
        "      for module in modules:\n",
        "          for param in module.parameters():\n",
        "              param.requires_grad = False\n",
        "      # modules2 = [model.bertModel.embeddings, *model.bertModel.encoder.layer[8:]] #Replace 8 by what you want\n",
        "      # for module in modules2:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=1e-5, #5e-5, 3e-5, 2e-5\n",
        "                                    correct_bias=False) #eps=1e-8,\n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=4*500)\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid = list()\n",
        "      listOfF1Valid = list()\n",
        "      epoch_stop = 0\n",
        "      for epoch_i in range(0, 3):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss = 0\n",
        "          model.bertModel.train()\n",
        "          model.train()\n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          # for (input1,mask1, target1),(input2,mask2, target2) in train_dataloader:\n",
        "          accum_iter = 4\n",
        "          for batch_idx,(input1, mask1, input2, mask2,target1) in  enumerate(train_dataloader):\n",
        "              step +=1\n",
        "              # if step ==201 :\n",
        "              #   break\n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # if torch.cuda.is_available():\n",
        "              b_input_ids1 = input1.to(device)\n",
        "              b_input_mask1 = mask1.to(device)\n",
        "              target1 = target1.type(torch.LongTensor)\n",
        "              # target1 = torch.where(target1==0,torch.tensor(-1),target1)\n",
        "            \n",
        "              b_labels = target1.to(device)\n",
        "              # b_labels = b_labels.unsqueeze(1)\n",
        "              b_input_ids2 = input2.to(device)\n",
        "              b_input_mask2 = mask2.to(device)\n",
        "              # b_labels=b_labels.reshape(-1,1)\n",
        "              # b_labels = torch.squeeze(b_labels,1)\n",
        "              model.zero_grad()\n",
        "              # with torch.set_grad_enabled(True):\n",
        "                  #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              # print(a1)\n",
        "              # print(a2)\n",
        "              FC11,FC22,cos = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)\n",
        "                  # loss = criterion1(out1,out2,b_labels)\n",
        "              pos_indices = torch.where(b_labels==1)[0]\n",
        "              neg_indices = torch.where(b_labels==0)[0]\n",
        "                  # print(pos_indices)\n",
        "                  # print(neg_indices)\n",
        "              indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "                  \n",
        "                 \n",
        "              # loss1 = criterion1(FC11,FC22,b_labels)\n",
        "              \n",
        "              loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # loss1 = loss1 / accum_iter\n",
        "     \n",
        "          \n",
        "              total_train_loss += loss1.item()\n",
        "                  # print(loss.data[0])\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 1  Train Loss \"+str(loss1.item()))\n",
        "\n",
        "              loss1.backward()\n",
        "              # if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
        "                      # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optimizer.step()\n",
        "                  # optimizer.zero_grad()\n",
        "              # scheduler.step()\n",
        "                      \n",
        "          avg_train_loss = total_train_loss /len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 1 AVG. Train Loss \"+str(avg_train_loss))            \n",
        "          listOflossesTrain.append(avg_train_loss)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss,avgf1 = validation(model,epoch_i,criterion1,validation_dataloader)\n",
        "          listOflossesValid.append(avg_val_loss)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "       \n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "       \n",
        "          early_stopping1(avg_val_loss, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping1.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              # break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain,listOflossesValid,3)\n",
        "\n",
        "      # mytrainStep2(modelCLS,criterion1,criterion2,model)\n",
        "      # torch.save(model.state_dict(), 'checkPointEmb.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JtSP09WHGW5t",
        "outputId": "8aeac5e9-c427-41aa-9807-61f07cc32f43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 1==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 2==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 3==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 4==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 5==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 6==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 7==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 8==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPA0lEQVR4nO3db4xldX3H8fenyyqmaoByi1sWO0aNhNiwtNMtlsboKu2KTUWjRtIYHpCsTbDBVNuifaA2NYFEpX1gTdZC2SYWxT8EA/7pFtdQG4PO6ooLq5EiprtZ2TFKhDSlXfj2wT1bx9mZvXfm3jt3fzPvV3Iz5/zOufd8bmb3k5Mz50+qCklSe35p2gEkSatjgUtSoyxwSWqUBS5JjbLAJalRZ6zlxs4999yamZlZy01KUvP279//46rqLR5f0wKfmZlhbm5uLTcpSc1L8sOlxj2EIkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1dIEn2ZTkW0nu6uZfkOS+JA8l+WSSZ0wupiRpsZXsgV8HHFowfyNwU1W9CPgpcM04g0mSTm2oAk+yFXgt8A/dfIAdwKe7VfYAV04ioCRpacNeifm3wF8Az+nmfwV4rKqOd/OHgfOXemOSXcAugOc///mrT3qamrn+7mlHWNIjN7x22hEkTdjAPfAkfwgcq6r9q9lAVe2uqtmqmu31TrqUX5K0SsPsgV8G/FGSK4AzgecCfwecleSMbi98K3BkcjElSYsN3AOvqndX1daqmgHeAny5qv4Y2Ae8sVvtauDOiaWUJJ1klPPA/xL4syQP0T8mfvN4IkmShrGi28lW1VeAr3TTDwPbxx9JkjQMr8SUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVqmIcan5nk60m+neSBJO/vxm9N8oMkB7rXtsnHlSSdMMwTeZ4EdlTVE0k2A19N8oVu2Z9X1acnF0+StJyBBV5VBTzRzW7uXjXJUJKkwYY6Bp5kU5IDwDFgb1Xd1y36QJL7k9yU5JkTSylJOslQBV5VT1XVNmArsD3JS4F3AxcCvw2cQ/8p9SdJsivJXJK5+fn5McWWJK3oLJSqegzYB+ysqqPV9yTwjyzzhPqq2l1Vs1U12+v1Rk8sSQKGOwull+SsbvpZwOXAd5Ns6cYCXAkcnGRQSdIvGuYslC3AniSb6Bf+7VV1V5IvJ+kBAQ4AfzLBnJKkRYY5C+V+4JIlxndMJJEkaSheiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNGuaZmGcm+XqSbyd5IMn7u/EXJLkvyUNJPpnkGZOPK0k6YZg98CeBHVV1MbAN2JnkUuBG4KaqehHwU+CaycWUJC02sMCr74ludnP3KmAH8OlufA/9J9NLktbIUMfAk2xKcgA4BuwF/gN4rKqOd6scBs5f5r27kswlmZufnx9HZkkSQxZ4VT1VVduArcB24MJhN1BVu6tqtqpme73eKmNKkhZb0VkoVfUYsA94GXBWkjO6RVuBI2POJkk6hWHOQuklOaubfhZwOXCIfpG/sVvtauDOSYWUJJ3sjMGrsAXYk2QT/cK/varuSvIg8IkkfwN8C7h5gjklSYsMLPCquh+4ZInxh+kfD5ckTYFXYkpSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1Kjhnkm5gVJ9iV5MMkDSa7rxt+X5EiSA93risnHlSSdMMwzMY8D76yqbyZ5DrA/yd5u2U1V9cHJxZMkLWeYZ2IeBY52048nOQScP+lgkqRTW9Ex8CQz9B9wfF839PYk9ye5JcnZy7xnV5K5JHPz8/MjhZUk/dzQBZ7k2cBngHdU1c+AjwIvBLbR30P/0FLvq6rdVTVbVbO9Xm8MkSVJMGSBJ9lMv7w/XlWfBaiqR6vqqap6GvgYsH1yMSVJiw1zFkqAm4FDVfXhBeNbFqz2euDg+ONJkpYzzFkolwFvBb6T5EA39h7gqiTbgAIeAd42kYSSpCUNcxbKV4Essejz448jSRqWV2JKUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSo4Z5JuYFSfYleTDJA0mu68bPSbI3yfe7n2dPPq4k6YRh9sCPA++sqouAS4Frk1wEXA/cU1UvBu7p5iVJa2RggVfV0ar6Zjf9OHAIOB94HbCnW20PcOWkQkqSTraiY+BJZoBLgPuA86rqaLfoR8B5y7xnV5K5JHPz8/MjRJUkLTR0gSd5NvAZ4B1V9bOFy6qqgFrqfVW1u6pmq2q21+uNFFaS9HNDFXiSzfTL++NV9dlu+NEkW7rlW4Bjk4koSVrKMGehBLgZOFRVH16w6HPA1d301cCd448nSVrOGUOscxnwVuA7SQ50Y+8BbgBuT3IN8EPgzZOJKElaysACr6qvAllm8avGG0eSNCyvxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDfNMzFuSHEtycMHY+5IcSXKge10x2ZiSpMWG2QO/Fdi5xPhNVbWte31+vLEkSYMMLPCquhf4yRpkkSStwCjHwN+e5P7uEMvZy62UZFeSuSRz8/PzI2xOkrTQagv8o8ALgW3AUeBDy61YVburaraqZnu93io3J0labFUFXlWPVtVTVfU08DFg+3hjSZIGWVWBJ9myYPb1wMHl1pUkTcYZg1ZIchvwCuDcJIeB9wKvSLINKOAR4G0TzChJWsLAAq+qq5YYvnkCWSRJK+CVmJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNfBSeklabOb6u6cdoTmP3PDasX+me+CS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUQMLPMktSY4lObhg7Jwke5N8v/t59mRjSpIWG2YP/FZg56Kx64F7qurFwD3dvCRpDQ0s8Kq6F/jJouHXAXu66T3AlWPOJUkaYLXHwM+rqqPd9I+A85ZbMcmuJHNJ5ubn51e5OUnSYiP/EbOqCqhTLN9dVbNVNdvr9UbdnCSps9oCfzTJFoDu57HxRZIkDWO1Bf454Opu+mrgzvHEkSQNa5jTCG8Dvga8JMnhJNcANwCXJ/k+8OpuXpK0hgbeTraqrlpm0avGnEWStAJeiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUQMvpT9dzFx/97QjSNJpxT1wSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KiRTiNM8gjwOPAUcLyqZscRSpI02DjOA39lVf14DJ8jSVoBD6FIUqNGLfAC/iXJ/iS7llohya4kc0nm5ufnR9ycJOmEUQv896rqN4HXANcmefniFapqd1XNVtVsr9cbcXOSpBNGKvCqOtL9PAbcAWwfRyhJ0mCrLvAkv5zkOSemgd8HDo4rmCTp1EY5C+U84I4kJz7nn6vqi2NJJUkaaNUFXlUPAxePMYskaQU8jVCSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNVKBJ9mZ5HtJHkpy/bhCSZIGG+WhxpuAjwCvAS4Crkpy0biCSZJObZQ98O3AQ1X1cFX9D/AJ4HXjiSVJGmSUp9KfD/zngvnDwO8sXinJLmBXN/tEku+NsM2VOBf48Rpt67STGzf292eD//7x+5923z83jvT2X19qcJQCH0pV7QZ2T3o7iyWZq6rZtd7u6cLv7/f3+6//7z/KIZQjwAUL5rd2Y5KkNTBKgX8DeHGSFyR5BvAW4HPjiSVJGmTVh1Cq6niStwNfAjYBt1TVA2NLNro1P2xzmvH7b2x+/w0gVTXtDJKkVfBKTElqlAUuSY1a1wWe5E1JHkjydJJ1f0rRCRv5FgdJbklyLMnBaWdZa0kuSLIvyYPdv/vrpp1pLSU5M8nXk3y7+/7vn3amSVvXBQ4cBN4A3DvtIGvFWxxwK7Bz2iGm5Djwzqq6CLgUuHaD/e6fBHZU1cXANmBnkkunnGmi1nWBV9WhqlqrKz9PFxv6FgdVdS/wk2nnmIaqOlpV3+ymHwcO0b9iekOovie62c3da12fpbGuC3yDWuoWBxvmP7H6kswAlwD3TTfJ2kqyKckB4Biwt6rW9fef+KX0k5bkX4HnLbHor6rqzrXOI01bkmcDnwHeUVU/m3aetVRVTwHbkpwF3JHkpVW1bv8e0nyBV9Wrp53hNOMtDjawJJvpl/fHq+qz084zLVX1WJJ99P8esm4L3EMo64+3ONigkgS4GThUVR+edp61lqTX7XmT5FnA5cB3p5tqstZ1gSd5fZLDwMuAu5N8adqZJq2qjgMnbnFwCLj9NLvFwUQluQ34GvCSJIeTXDPtTGvoMuCtwI4kB7rXFdMOtYa2APuS3E9/R2ZvVd015UwT5aX0ktSodb0HLknrmQUuSY2ywCWpURa4JDXKApe0oY3zBmhJXrngDKADSf47yZVDvvfCJF9L8mSSdw31Hs9CkbSRJXk58ATwT1X10jF+7jnAQ8DWqvqvRcseqaqZRWO/Sv/p81cCP62qDw7ahnvgkja0pW6AluSFSb6YZH+Sf0ty4So++o3AFxaX9ylyHKuqbwD/O+wGLHBJOtlu4E+r6reAdwF/v4rPeAtw21hTLdL8vVAkaZy6m4H9LvCp/t0JAHhmt+wNwF8v8bYjVfUHCz5jC/Ab9K+IPjH2EfpXywL8WnfXRIBPVdUHVpPVApekX/RLwGNVtW3xgu4GYcPcJOzNwB1V9f+HQ6rq2hPT3THwkz5/NUElSZ3uFrw/SPIm6N8kLMnFK/yYq5jw4RPwLBRJG1x3A7RXAOcCjwLvBb4MfJT+DbI2A5+oqqUOnSz1eTPAvwMXVNXTy6yz1FkozwPmgOcCT9M/M+aiU93T3QKXpEZ5CEWSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEb9H1wvxqo5kl2FAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 9==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 10==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 11==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 12==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 13==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 14==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 15==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANRklEQVR4nO3dX4xc9XmH8ecLdv6opALkLXEBdyOCgqxUmHZFSakiQkLrkAsgSqJwgbhAci6gAolcWOlFkqqVQErgiqI6AuFKFEoKCBTSpJRaoqkQyZo4xOBGUOqothy8iCBAVWkNby/2uNksu57xzj//dp+PtPLMmTNz3pHh0dHZc45TVUiS2nPSpAeQJK2MAZekRhlwSWqUAZekRhlwSWrUunFubMOGDTU9PT3OTUpS83bv3v1KVU0tXj7WgE9PTzM7OzvOTUpS85L8fKnlHkKRpEYZcElqlAGXpEYZcElqlAGXpEYZcElqVM+AJ3lfkh8m+UmS55J8vVv+oSRPJ3kxyd8lec/ox5UkHdXPHvhbwKVVdT6wBdia5CLgVuD2qvow8EvgutGNKUlarGfAa96b3dP13U8BlwJ/3y3fCVw5kgklSUvq60rMJCcDu4EPA3cA/w68VlVHulUOAGcu895twDaATZs2DTrvCWd6+2OTHmFJ+2/5zKRHkDRiff0Ss6rerqotwFnAhcB5/W6gqnZU1UxVzUxNvetSfknSCh3XWShV9RqwC/gYcGqSo3vwZwEHhzybJOkY+jkLZSrJqd3j9wOXAfuYD/nnutWuBR4Z1ZCSpHfr5xj4RmBndxz8JOCBqvpOkueB+5P8BfBj4K4RzilJWqRnwKvqWeCCJZa/xPzxcEnSBHglpiQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1qmfAk5ydZFeS55M8l+TGbvnXkhxMsqf7uXz040qSjlrXxzpHgJur6pkkHwB2J3m8e+32qvrG6MaTJC2nZ8Cr6hBwqHv8RpJ9wJmjHkySdGzHdQw8yTRwAfB0t+iGJM8muTvJacu8Z1uS2SSzc3NzAw0rSfqVvgOe5BTgQeCmqnoduBM4B9jC/B76N5d6X1XtqKqZqpqZmpoawsiSJOgz4EnWMx/ve6vqIYCqermq3q6qd4BvAReObkxJ0mL9nIUS4C5gX1XdtmD5xgWrXQXsHf54kqTl9HMWysXANcBPk+zpln0FuDrJFqCA/cCXRjKhJGlJ/ZyF8gMgS7z03eGPI0nql1diSlKjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNapnwJOcnWRXkueTPJfkxm756UkeT/JC9+dpox9XknRUP3vgR4Cbq2ozcBFwfZLNwHbgiao6F3iiey5JGpOeAa+qQ1X1TPf4DWAfcCZwBbCzW20ncOWohpQkvdtxHQNPMg1cADwNnFFVh7qXfgGcscx7tiWZTTI7Nzc3wKiSpIX6DniSU4AHgZuq6vWFr1VVAbXU+6pqR1XNVNXM1NTUQMNKkn6lr4AnWc98vO+tqoe6xS8n2di9vhE4PJoRJUlL6ecslAB3Afuq6rYFLz0KXNs9vhZ4ZPjjSZKWs66PdS4GrgF+mmRPt+wrwC3AA0muA34OfGE0I0qSltIz4FX1AyDLvPzJ4Y4jSeqXV2JKUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1qp/7gUvSr5ne/tikR2jO/ls+M/TPdA9ckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUT0DnuTuJIeT7F2w7GtJDibZ0/1cPtoxJUmL9bMHfg+wdYnlt1fVlu7nu8MdS5LUS8+AV9WTwKtjmEWSdBwGOQZ+Q5Jnu0Mspy23UpJtSWaTzM7NzQ2wOUnSQisN+J3AOcAW4BDwzeVWrKodVTVTVTNTU1Mr3JwkabEVBbyqXq6qt6vqHeBbwIXDHUuS1MuKAp5k44KnVwF7l1tXkjQaPf9FniT3AZcAG5IcAL4KXJJkC1DAfuBLI5xRkrSEngGvqquXWHzXCGaRJB0Hr8SUpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqVM+AJ7k7yeEkexcsOz3J40le6P48bbRjSpIW62cP/B5g66Jl24Enqupc4InuuSRpjHoGvKqeBF5dtPgKYGf3eCdw5ZDnkiT1sNJj4GdU1aHu8S+AM5ZbMcm2JLNJZufm5la4OUnSYgP/ErOqCqhjvL6jqmaqamZqamrQzUmSOisN+MtJNgJ0fx4e3kiSpH6sNOCPAtd2j68FHhnOOJKkfvVzGuF9wFPAR5IcSHIdcAtwWZIXgE91zyVJY7Su1wpVdfUyL31yyLNIko6DV2JKUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqPWDfLmJPuBN4C3gSNVNTOMoSRJvQ0U8M4nquqVIXyOJOk4eAhFkho16B54Af+YpIC/rqodi1dIsg3YBrBp06YVb2h6+2Mrfq8krUaD7oH/UVX9HvBp4PokH1+8QlXtqKqZqpqZmpoacHOSpKMGCnhVHez+PAw8DFw4jKEkSb2tOOBJfiPJB44+Bv4Y2DuswSRJxzbIMfAzgIeTHP2cv62q7w1lKklSTysOeFW9BJw/xFkkScfB0wglqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaNVDAk2xN8rMkLybZPqyhJEm9rTjgSU4G7gA+DWwGrk6yeViDSZKObZA98AuBF6vqpar6H+B+4IrhjCVJ6mXdAO89E/jPBc8PAH+weKUk24Bt3dM3k/xsgG0ejw3AK2Pa1gknt67t788a//vH73/Cff/cOtDbf2ephYMEvC9VtQPYMertLJZktqpmxr3dE4Xf3+/v91/933+QQygHgbMXPD+rWyZJGoNBAv4j4NwkH0ryHuCLwKPDGUuS1MuKD6FU1ZEkNwDfB04G7q6q54Y22eDGftjmBOP3X9v8/mtAqmrSM0iSVsArMSWpUQZckhq1qgOe5PNJnkvyTpJVf0rRUWv5FgdJ7k5yOMneSc8ybknOTrIryfPdf/c3TnqmcUryviQ/TPKT7vt/fdIzjdqqDjiwF/gs8OSkBxkXb3HAPcDWSQ8xIUeAm6tqM3ARcP0a+7t/C7i0qs4HtgBbk1w04ZlGalUHvKr2VdW4rvw8UazpWxxU1ZPAq5OeYxKq6lBVPdM9fgPYx/wV02tCzXuze7q++1nVZ2ms6oCvUUvd4mDN/E+seUmmgQuApyc7yXglOTnJHuAw8HhVrervP/JL6UctyT8BH1zipT+rqkfGPY80aUlOAR4Ebqqq1yc9zzhV1dvAliSnAg8n+WhVrdrfhzQf8Kr61KRnOMF4i4M1LMl65uN9b1U9NOl5JqWqXkuyi/nfh6zagHsIZfXxFgdrVJIAdwH7quq2Sc8zbkmmuj1vkrwfuAz4t8lONVqrOuBJrkpyAPgY8FiS7096plGrqiPA0Vsc7AMeOMFucTBSSe4DngI+kuRAkusmPdMYXQxcA1yaZE/3c/mkhxqjjcCuJM8yvyPzeFV9Z8IzjZSX0ktSo1b1HrgkrWYGXJIaZcAlqVEGXJIaZcAlrWnDvAFakk8sOANoT5L/TnJln+89L8lTSd5K8uW+3uNZKJLWsiQfB94E/qaqPjrEzz0deBE4q6r+a9Fr+6tqetGy32L+X5+/EvhlVX2j1zbcA5e0pi11A7Qk5yT5XpLdSf4lyXkr+OjPAf+wON7HmONwVf0I+N9+N2DAJenddgB/WlW/D3wZ+KsVfMYXgfuGOtUizd8LRZKGqbsZ2B8C356/OwEA7+1e+yzw50u87WBV/cmCz9gI/C7zV0QfXXYH81fLAvx2d9dEgG9X1V+uZFYDLkm/7iTgtarasviF7gZh/dwk7AvAw1X1/4dDqur6o4+7Y+Dv+vyVDCpJ6nS34P2PJJ+H+ZuEJTn/OD/makZ8+AQ8C0XSGtfdAO0SYAPwMvBV4J+BO5m/QdZ64P6qWurQyVKfNw38K3B2Vb2zzDpLnYXyQWAW+E3gHebPjNl8rHu6G3BJapSHUCSpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUf8HtIchQsalA9cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 16==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 17==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 18==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 19==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 20==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 21==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 22==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 23==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 24==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 25==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 26==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 27==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 28==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 29==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 30==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 31==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 32==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 33==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 34==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 35==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 36==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 37==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEGCAYAAAB8Ys7jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANTklEQVR4nO3db4hl9X3H8fcn7jYJNUXtTs1WpRNEIkuKazuIqSUYE1tjHriGJMQH4gNh80CLUvNgSR+YlhYMJPootWxQ3IJVTFSUaP5s7YK1iM2s3ZrVbdDaDd1l445YUSk1Xf32wZxtJuPM3Lv3z9z9zbxfMMy955x77/eivDmcOedsqgpJUnveN+kBJEmDMeCS1CgDLkmNMuCS1CgDLkmNMuCS1KgNvTZI8gHgSeD93fbfrapbk3wEuB/4TWAvcG1V/WKl99q0aVNNT08PPbQkrSd79+59taqmFi/vGXDgbeCyqnoryUbgqSTfB/4UuKOq7k/yN8D1wJ0rvdH09DSzs7MDjC9J61eSny21vOchlJr3Vvd0Y/dTwGXAd7vlu4BtI5hTktSnvo6BJzklyT7gKLAb+Hfg9ao61m1yCDhrPCNKkpbSV8Cr6p2q2gqcDVwEnN/vByTZnmQ2yezc3NyAY0qSFjuhs1Cq6nVgD/Bx4LQkx4+hnw0cXuY1O6tqpqpmpqbecwxekjSgngFPMpXktO7xB4HLgQPMh/zz3WbXAY+Ma0hJ0nv1cxbKZmBXklOYD/4DVfW9JC8A9yf5S+BfgLvGOKckaZGeAa+q54ALl1j+MvPHwyVJE+CVmJLUKAMuSY3q5xi4VjC947FJj7Ckg7d9dtIjSBoz98AlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIa1TPgSc5JsifJC0meT3JTt/xrSQ4n2df9XDn+cSVJx23oY5tjwC1V9WySDwF7k+zu1t1RVd8Y33iSpOX0DHhVHQGOdI/fTHIAOGvcg0mSVnZCx8CTTAMXAs90i25M8lySu5OcvsxrtieZTTI7Nzc31LCSpF/qO+BJTgUeBG6uqjeAO4Fzga3M76F/c6nXVdXOqpqpqpmpqakRjCxJgj4DnmQj8/G+t6oeAqiqV6rqnap6F/g2cNH4xpQkLdbPWSgB7gIOVNXtC5ZvXrDZ1cD+0Y8nSVpOP2ehXAJcC/wkyb5u2VeBa5JsBQo4CHx5LBNKkpbUz1koTwFZYtXjox9HktQvr8SUpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEb1DHiSc5LsSfJCkueT3NQtPyPJ7iQvdr9PH/+4kqTj+tkDPwbcUlVbgIuBG5JsAXYAT1TVecAT3XNJ0irpGfCqOlJVz3aP3wQOAGcBVwG7us12AdvGNaQk6b1O6Bh4kmngQuAZ4MyqOtKt+jlw5jKv2Z5kNsns3NzcEKNKkhbqO+BJTgUeBG6uqjcWrquqAmqp11XVzqqaqaqZqampoYaVJP1SXwFPspH5eN9bVQ91i19Jsrlbvxk4Op4RJUlL6ecslAB3AQeq6vYFqx4FruseXwc8MvrxJEnL2dDHNpcA1wI/SbKvW/ZV4DbggSTXAz8DvjieESVJS+kZ8Kp6Csgyqz812nEkSf3ySkxJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RG9Qx4kruTHE2yf8GyryU5nGRf93PleMeUJC3Wzx74PcAVSyy/o6q2dj+Pj3YsSVIvPQNeVU8Cr63CLJKkEzDMMfAbkzzXHWI5fWQTSZL6MmjA7wTOBbYCR4BvLrdhku1JZpPMzs3NDfhxkqTFBgp4Vb1SVe9U1bvAt4GLVth2Z1XNVNXM1NTUoHNKkhYZKOBJNi94ejWwf7ltJUnjsaHXBknuAy4FNiU5BNwKXJpkK1DAQeDLY5xRkrSEngGvqmuWWHzXGGaRJJ0Ar8SUpEYZcElqVM9DKJK02PSOxyY9QnMO3vbZkb+ne+CS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmN6hnwJHcnOZpk/4JlZyTZneTF7vfp4x1TkrRYP3vg9wBXLFq2A3iiqs4DnuieS5JWUc+AV9WTwGuLFl8F7Ooe7wK2jXguSVIPgx4DP7OqjnSPfw6cudyGSbYnmU0yOzc3N+DHSZIWG/qPmFVVQK2wfmdVzVTVzNTU1LAfJ0nqDBrwV5JsBuh+Hx3dSJKkfgwa8EeB67rH1wGPjGYcSVK/+jmN8D7gaeCjSQ4luR64Dbg8yYvAp7vnkqRVtKHXBlV1zTKrPjXiWSRJJ8ArMSWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUT2vxDxZTO94bNIjSNJJxT1wSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWrUUP8iT5KDwJvAO8CxqpoZxVCSpN5G8U+qfbKqXh3B+0iSToCHUCSpUcMGvIAfJdmbZPtSGyTZnmQ2yezc3NyQHydJOm7YgP9hVf0e8BnghiSfWLxBVe2sqpmqmpmamhry4yRJxw0V8Ko63P0+CjwMXDSKoSRJvQ0c8CS/nuRDxx8DfwTsH9VgkqSVDXMWypnAw0mOv8/fVdUPRjKVJKmngQNeVS8DF4xwFknSCfA0QklqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYNFfAkVyT5aZKXkuwY1VCSpN4GDniSU4BvAZ8BtgDXJNkyqsEkSSsbZg/8IuClqnq5qn4B3A9cNZqxJEm9DBPws4D/XPD8ULdMkrQKNoz7A5JsB7Z3T99K8tNxf2ZnE/DqKn3WSSdfX9/fn3X+3x+//0n3/fP1oV7+O0stHCbgh4FzFjw/u1v2K6pqJ7BziM8ZSJLZqppZ7c89Wfj9/f5+/7X//Yc5hPJj4LwkH0nya8CXgEdHM5YkqZeB98Cr6liSG4EfAqcAd1fV8yObTJK0oqGOgVfV48DjI5pl1Fb9sM1Jxu+/vvn914FU1aRnkCQNwEvpJalRazrgSb6Q5Pkk7yZZ83+RPm493+Igyd1JjibZP+lZVluSc5LsSfJC9//9TZOeaTUl+UCSf07yr933//NJzzRuazrgwH7gc8CTkx5ktXiLA+4Brpj0EBNyDLilqrYAFwM3rLP/9m8Dl1XVBcBW4IokF094prFa0wGvqgNVtVoXDp0s1vUtDqrqSeC1Sc8xCVV1pKqe7R6/CRxgHV0dXfPe6p5u7H7W9B/51nTA1ylvcSCSTAMXAs9MdpLVleSUJPuAo8DuqlrT33/sl9KPW5K/Bz68xKo/q6pHVnseadKSnAo8CNxcVW9Mep7VVFXvAFuTnAY8nORjVbVm/x7SfMCr6tOTnuEk09ctDrQ2JdnIfLzvraqHJj3PpFTV60n2MP/3kDUbcA+hrD3e4mCdShLgLuBAVd0+6XlWW5Kpbs+bJB8ELgf+bbJTjdeaDniSq5McAj4OPJbkh5Oeadyq6hhw/BYHB4AH1tMtDpLcBzwNfDTJoSTXT3qmVXQJcC1wWZJ93c+Vkx5qFW0G9iR5jvkdmd1V9b0JzzRWXokpSY1a03vgkrSWGXBJapQBl6RGGXBJapQBl7SujfIGaEk+ueAMoH1J/ifJtj5fe36Sp5O8neQrfb3Gs1AkrWdJPgG8BfxtVX1shO97BvAScHZV/feidQeranrRst9i/h8v3gb8V1V9o9dnuAcuaV1b6gZoSc5N8oMke5P8Y5LzB3jrzwPfXxzvFeY4WlU/Bv633w8w4JL0XjuBP6mq3we+Avz1AO/xJeC+kU61SPP3QpGkUepuBvYHwHfm704AwPu7dZ8D/mKJlx2uqj9e8B6bgd9l/oro48u+xfzVsgC/3d01EeA7VfVXg8xqwCXpV70PeL2qti5e0d0grJ+bhH0ReLiq/v9wSFXdcPxxdwz8Pe8/yKCSpE53C97/SPIFmL9JWJILTvBtrmHMh0/As1AkrXPdDdAuBTYBrwC3Av8A3Mn8DbI2AvdX1VKHTpZ6v2ngn4BzqurdZbZZ6iyUDwOzwG8A7zJ/ZsyWle7pbsAlqVEeQpGkRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWrU/wHfijGGB41lgwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 38==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 39==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 40==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 41==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 42==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANSElEQVR4nO3db4hl9X3H8fcnumlCTVHZqdmqdIKVyJLi2g7W1BKMie3GPFBDEuID2QfC5oEWBfNgSR8kKS0oJPooFTYobsFqTVWUmCbZ2gWbIiazdmNWt0FrN3SXjTtiRKXUdt1vH8yZZjrO7L0799/+Zt4vGObec8+953tZfXM4e87ZVBWSpPa8Z9IDSJJWx4BLUqMMuCQ1yoBLUqMMuCQ16vRxbmzjxo01PT09zk1KUvP27t37alVNLV0+1oBPT08zOzs7zk1KUvOS/Hy55R5CkaRGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGjfVKzLVoescTkx5hWQdv//SkR5A0Yu6BS1KjDLgkNapnwJO8L8mPkvwkyfNJvtYt/1CSZ5K8lORvk7x39ONKkhb0swf+NnBlVV0MbAG2JrkMuAO4q6p+B/glcOPoxpQkLdUz4DXvre7phu6ngCuBv+uW7wKuHcmEkqRl9XUMPMlpSfYBR4HdwL8Br1fVsW6VQ8C5K7x3e5LZJLNzc3PDmFmSRJ8Br6p3qmoLcB5wKXBRvxuoqp1VNVNVM1NT7/oHJSRJq3RSZ6FU1evAHuCjwJlJFs4jPw84POTZJEkn0M9ZKFNJzuwevx+4CjjAfMg/2622DXhsVENKkt6tnysxNwG7kpzGfPAfqqrvJHkBeDDJXwD/AtwzwjklSUv0DHhVPQdcsszyl5k/Hi5JmgCvxJSkRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRvUMeJLzk+xJ8kKS55Pc0i3/apLDSfZ1P1ePflxJ0oLT+1jnGHBbVT2b5APA3iS7u9fuqqqvj248SdJKega8qo4AR7rHbyY5AJw76sEkSSd2UsfAk0wDlwDPdItuTvJcknuTnLXCe7YnmU0yOzc3N9CwkqRf6TvgSc4AHgZurao3gLuBC4AtzO+hf2O591XVzqqaqaqZqampIYwsSYI+A55kA/Pxvr+qHgGoqleq6p2qOg58C7h0dGNKkpbq5yyUAPcAB6rqzkXLNy1a7Tpg//DHkyStpJ+zUC4HbgB+mmRft+zLwPVJtgAFHAS+OJIJJUnL6ucslB8CWeal7w5/HElSv7wSU5IaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVE9A57k/CR7kryQ5Pkkt3TLz06yO8mL3e+zRj+uJGlBP3vgx4DbqmozcBlwU5LNwA7gyaq6EHiyey5JGpOeAa+qI1X1bPf4TeAAcC5wDbCrW20XcO2ohpQkvdtJHQNPMg1cAjwDnFNVR7qXfgGcs8J7tieZTTI7Nzc3wKiSpMX6DniSM4CHgVur6o3Fr1VVAbXc+6pqZ1XNVNXM1NTUQMNKkn6lr4An2cB8vO+vqke6xa8k2dS9vgk4OpoRJUnL6ecslAD3AAeq6s5FLz0ObOsebwMeG/54kqSVnN7HOpcDNwA/TbKvW/Zl4HbgoSQ3Aj8HPj+aESVJy+kZ8Kr6IZAVXv7EcMeRJPXLKzElqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVE9/1X6U8X0jicmPYIknVLcA5ekRhlwSWpUz4AnuTfJ0ST7Fy37apLDSfZ1P1ePdkxJ0lL97IHfB2xdZvldVbWl+/nucMeSJPXSM+BV9RTw2hhmkSSdhEGOgd+c5LnuEMtZQ5tIktSX1Qb8buACYAtwBPjGSism2Z5kNsns3NzcKjcnSVpqVQGvqleq6p2qOg58C7j0BOvurKqZqpqZmppa7ZySpCVWFfAkmxY9vQ7Yv9K6kqTR6HklZpIHgCuAjUkOAV8BrkiyBSjgIPDFEc4oSVpGz4BX1fXLLL5nBLNIkk6CV2JKUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqN6BjzJvUmOJtm/aNnZSXYnebH7fdZox5QkLdXPHvh9wNYly3YAT1bVhcCT3XNJ0hj1DHhVPQW8tmTxNcCu7vEu4NohzyVJ6mG1x8DPqaoj3eNfAOestGKS7Ulmk8zOzc2tcnOSpKUG/kvMqiqgTvD6zqqaqaqZqampQTcnSeqsNuCvJNkE0P0+OryRJEn9WG3AHwe2dY+3AY8NZxxJUr/6OY3wAeBp4MNJDiW5EbgduCrJi8Anu+eSpDE6vdcKVXX9Ci99YsizSJJOgldiSlKjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNarn7WQlaanpHU9MeoTmHLz900P/TPfAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRA12JmeQg8CbwDnCsqmaGMZQkqbdhXEr/8ap6dQifI0k6CR5CkaRGDRrwAn6QZG+S7cutkGR7ktkks3NzcwNuTpK0YNCA/1FV/R7wKeCmJB9bukJV7ayqmaqamZqaGnBzkqQFAwW8qg53v48CjwKXDmMoSVJvqw54kl9P8oGFx8AfA/uHNZgk6cQGOQvlHODRJAuf8zdV9b2hTCVJ6mnVAa+ql4GLhziLJOkkeBqhJDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDVqoIAn2ZrkZ0leSrJjWENJknpbdcCTnAZ8E/gUsBm4PsnmYQ0mSTqxQfbALwVeqqqXq+q/gQeBa4YzliSpl9MHeO+5wH8sen4I+IOlKyXZDmzvnr6V5GcDbPNkbAReHdO2Tjm5Y31/f9b5nz9+/1Pu++eOgd7+28stHCTgfamqncDOUW9nqSSzVTUz7u2eKvz+fn+//9r//oMcQjkMnL/o+XndMknSGAwS8B8DFyb5UJL3Al8AHh/OWJKkXlZ9CKWqjiW5Gfg+cBpwb1U9P7TJBjf2wzanGL//+ub3XwdSVZOeQZK0Cl6JKUmNMuCS1Kg1HfAkn0vyfJLjSdb8KUUL1vMtDpLcm+Rokv2TnmXckpyfZE+SF7r/7m+Z9EzjlOR9SX6U5Cfd9//apGcatTUdcGA/8BngqUkPMi7e4oD7gK2THmJCjgG3VdVm4DLgpnX2Z/82cGVVXQxsAbYmuWzCM43Umg54VR2oqnFd+XmqWNe3OKiqp4DXJj3HJFTVkap6tnv8JnCA+Sum14Wa91b3dEP3s6bP0ljTAV+nlrvFwbr5n1jzkkwDlwDPTHaS8UpyWpJ9wFFgd1Wt6e8/8kvpRy3JPwAfXOalP6uqx8Y9jzRpSc4AHgZurao3Jj3POFXVO8CWJGcCjyb5SFWt2b8PaT7gVfXJSc9wivEWB+tYkg3Mx/v+qnpk0vNMSlW9nmQP838fsmYD7iGUtcdbHKxTSQLcAxyoqjsnPc+4JZnq9rxJ8n7gKuBfJzvVaK3pgCe5Lskh4KPAE0m+P+mZRq2qjgELtzg4ADx0it3iYKSSPAA8DXw4yaEkN056pjG6HLgBuDLJvu7n6kkPNUabgD1JnmN+R2Z3VX1nwjONlJfSS1Kj1vQeuCStZQZckhplwCWpUQZckhplwCWta8O8AVqSjy86A2hfkv9Kcm2f770oydNJ3k7ypb7e41koktazJB8D3gL+uqo+MsTPPRt4CTivqv5zyWsHq2p6ybLfZP5fn78W+GVVfb3XNtwDl7SuLXcDtCQXJPlekr1J/inJRav46M8Cf7803ieY42hV/Rj4n343YMAl6d12An9aVb8PfAn4q1V8xheAB4Y61RLN3wtFkoapuxnYHwLfnr87AQC/1r32GeDPl3nb4ar6k0WfsQn4XeaviF5Y9k3mr5YF+K3urokA366qv1zNrAZckv6/9wCvV9WWpS90Nwjr5yZhnwcerar/OxxSVTctPO6Ogb/r81czqCSp092C99+TfA7mbxKW5OKT/JjrGfHhE/AsFEnrXHcDtCuAjcArwFeAfwTuZv4GWRuAB6tquUMny33eNPDPwPlVdXyFdZY7C+WDwCzwG8Bx5s+M2Xyie7obcElqlIdQJKlRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalR/wv5FSV5Bs2FOAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 43==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 44==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOn0lEQVR4nO3df4xldX2H8ectrJUUGyDc4pYfHYNEQmhZ2ukWa2MUpV2xqWDUyB+EpiRrG2ww0barJlWbmmCq8k+tzRooa0KhKBCIiLrFTSiNBWfpgguLlSKmkJUdowRIU9qFT/+Ys3UYZvbeuT/27nfneSU3c++5597zuQGeXM6ccyZVhSSpPa+Y9gCSpOEYcElqlAGXpEYZcElqlAGXpEYdfSg3duKJJ9bMzMyh3KQkNW/nzp0/rqre0uWHNOAzMzPMzc0dyk1KUvOS/HC55e5CkaRGGXBJapQBl6RGGXBJapQBl6RGGXBJalTfgCd5VZL7kjyQ5KEkn+yWX5fkB0l2dbcNkx9XknTAIMeBPw+cX1XPJVkH3JPkzu65P62qr0xuPEnSSvoGvBYuGP5c93Bdd/Mi4pI0ZQOdiZnkKGAn8Drg81V1b5I/Bj6V5C+Au4AtVfX8Mq/dDGwGOO2008Y2uCZnZssdU9v241e9Y2rblloz0C8xq+qFqtoAnAJsTHI28BHgTOA3gBOAP1/htVuraraqZnu9l53KL0ka0qqOQqmqp4EdwKaq2lsLngf+Htg4iQElScsb5CiUXpLjuvvHABcAjyRZ3y0LcBGwe5KDSpJeapB94OuBbd1+8FcAN1XVV5N8K0kPCLAL+KMJzilJWmKQo1AeBM5dZvn5E5lIkjQQz8SUpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqVN+AJ3lVkvuSPJDkoSSf7Ja/Nsm9SR5N8o9JXjn5cSVJBwzyDfx54PyqOgfYAGxKch7waeDqqnod8FPg8smNKUlaqm/Aa8Fz3cN13a2A84GvdMu3ARdNZEJJ0rIG2gee5Kgku4B9wHbgP4Cnq2p/t8oTwMkrvHZzkrkkc/Pz8+OYWZLEgAGvqheqagNwCrAROHPQDVTV1qqararZXq835JiSpKVWdRRKVT0N7ADeAByX5OjuqVOAJ8c8myTpIAY5CqWX5Lju/jHABcAeFkL+7m61y4DbJjWkJOnlju6/CuuBbUmOYiH4N1XVV5M8DNyY5K+AfwOumeCckqQl+ga8qh4Ezl1m+WMs7A+XJE2BZ2JKUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqMMuCQ1yoBLUqP6BjzJqUl2JHk4yUNJruyWfyLJk0l2dbcLJz+uJOmAowdYZz/woaq6P8mrgZ1JtnfPXV1Vn5nceJKklfQNeFXtBfZ2959Nsgc4edKDSZIOblX7wJPMAOcC93aLPpDkwSTXJjl+hddsTjKXZG5+fn6kYSVJPzNwwJMcC9wMfLCqngG+AJwObGDhG/pnl3tdVW2tqtmqmu31emMYWZIEAwY8yToW4n19Vd0CUFVPVdULVfUi8EVg4+TGlCQtNchRKAGuAfZU1ecWLV+/aLWLgd3jH0+StJJBjkJ5I3Ap8N0ku7plHwUuSbIBKOBx4P0TmVCStKxBjkK5B8gyT31t/ONIkgblmZiS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmN6hvwJKcm2ZHk4SQPJbmyW35Cku1Jvt/9PH7y40qSDhjkG/h+4ENVdRZwHnBFkrOALcBdVXUGcFf3WJJ0iPQNeFXtrar7u/vPAnuAk4F3Atu61bYBF01qSEnSy61qH3iSGeBc4F7gpKra2z31I+CkFV6zOclckrn5+fkRRpUkLTZwwJMcC9wMfLCqnln8XFUVUMu9rqq2VtVsVc32er2RhpUk/cxAAU+yjoV4X19Vt3SLn0qyvnt+PbBvMiNKkpYzyFEoAa4B9lTV5xY9dTtwWXf/MuC28Y8nSVrJ0QOs80bgUuC7SXZ1yz4KXAXclORy4IfAeyczoiRpOX0DXlX3AFnh6beOdxxJ0qA8E1OSGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGtU34EmuTbIvye5Fyz6R5Mkku7rbhZMdU5K01CDfwK8DNi2z/Oqq2tDdvjbesSRJ/fQNeFXdDfzkEMwiSVqFUfaBfyDJg90uluNXWinJ5iRzSebm5+dH2JwkabFhA/4F4HRgA7AX+OxKK1bV1qqararZXq835OYkSUsNFfCqeqqqXqiqF4EvAhvHO5YkqZ+hAp5k/aKHFwO7V1pXkjQZR/dbIckNwJuBE5M8AXwceHOSDUABjwPvn+CMkqRl9A14VV2yzOJrJjCLJGkV+gZcMLPljmmPIEkv46n0ktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQoAy5JjTLgktQo/yKPpENuLf6Vq8evesfY39Nv4JLUqL4BT3Jtkn1Jdi9adkKS7Um+3/08frJjSpKWGuQb+HXApiXLtgB3VdUZwF3dY0nSIdQ34FV1N/CTJYvfCWzr7m8DLhrzXJKkPobdB35SVe3t7v8IOGlM80iSBjTyLzGrqoBa6fkkm5PMJZmbn58fdXOSpM6wAX8qyXqA7ue+lVasqq1VNVtVs71eb8jNSZKWGjbgtwOXdfcvA24bzziSpEENchjhDcC3gdcneSLJ5cBVwAVJvg+8rXssSTqE+p6JWVWXrPDUW8c8iyRpFTwTU5IaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIa1fev0h9MkseBZ4EXgP1VNTuOoSRJ/Y0U8M5bqurHY3gfSdIquAtFkho1asAL+GaSnUk2L7dCks1J5pLMzc/Pj7g5SdIBowb8t6vq14C3A1ckedPSFapqa1XNVtVsr9cbcXOSpANGCnhVPdn93AfcCmwcx1CSpP6GDniSn0/y6gP3gd8Bdo9rMEnSwY1yFMpJwK1JDrzPP1TV18cylSSpr6EDXlWPAeeMcRZJ0ip4GKEkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjRvmbmIfUzJY7pj2CJB1W/AYuSY0y4JLUqJECnmRTku8leTTJlnENJUnqb+iAJzkK+DzwduAs4JIkZ41rMEnSwY3yDXwj8GhVPVZV/wPcCLxzPGNJkvoZ5SiUk4H/XPT4CeA3l66UZDOwuXv4XJLvjbDNcToR+PG0hxiTI+az5NNHzmfhCPrngp9lZPn0SC//5eUWTvwwwqraCmyd9HZWK8lcVc1Oe45x8LMcnvwsh6cj6bOMsgvlSeDURY9P6ZZJkg6BUQL+HeCMJK9N8krgfcDt4xlLktTP0LtQqmp/kg8A3wCOAq6tqofGNtnkHXa7dUbgZzk8+VkOT0fMZ0lVTXsGSdIQPBNTkhplwCWpUWs64En+OskjSR5McmuS46Y907CSvCfJQ0leTNLcIVJH0mUZklybZF+S3dOeZVRJTk2yI8nD3b9fV057pmEleVWS+5I80H2WT057plGt6YAD24Gzq+pXgX8HPjLleUaxG3gXcPe0B1mtI/CyDNcBm6Y9xJjsBz5UVWcB5wFXNPzP5nng/Ko6B9gAbEpy3pRnGsmaDnhVfbOq9ncP/5WFY9mbVFV7qupwOct1tY6oyzJU1d3AT6Y9xzhU1d6qur+7/yywh4WzsJtTC57rHq7rbk0fxbGmA77EHwJ3TnuINWq5yzI0GYkjWZIZ4Fzg3ulOMrwkRyXZBewDtldVs58FGvqLPMNK8k/Aa5Z56mNVdVu3zsdY+F/F6w/lbKs1yGeRJiHJscDNwAer6plpzzOsqnoB2ND9vuvWJGdXVbO/qzjiA15VbzvY80n+APg94K11mB8U3++zNMzLMhzGkqxjId7XV9Ut055nHKrq6SQ7WPhdRbMBX9O7UJJsAv4M+P2q+q9pz7OGeVmGw1SSANcAe6rqc9OeZxRJegeONEtyDHAB8Mh0pxrNmg448DfAq4HtSXYl+btpDzSsJBcneQJ4A3BHkm9Me6ZBdb9IPnBZhj3ATY1dluElktwAfBt4fZInklw+7ZlG8EbgUuD87r+RXUkunPZQQ1oP7EjyIAtfGrZX1VenPNNIPJVekhq11r+BS1KzDLgkNcqAS1KjDLgkNcqAS1rTxnnxsSRvWXS0zq4k/53kogFfe2aSbyd5PsmHB3qNR6FIWsuSvAl4DvhSVZ09xvc9AXgUOGXpeSZJHq+qmSXLfpGFvz5/EfDTqvpMv234DVzSmrbcxceSnJ7k60l2JvnnJGcO8dbvBu4c9CTBqtpXVd8B/nfQDRhwSXq5rcCfVNWvAx8G/naI93gfcMNYp1riiL8WiiStRnfhrt8CvrxwJQEAfq577l3AXy7zsier6ncXvcd64FdYOLv4wLLPs3BmK8AvdVdFBPhyVX1qmFkNuCS91CuAp6tqw9Inuot5DXJBr/cCt1bV/+8OqaorDtzv9oG/7P2HGVSS1Okul/uDJO+BhQt6JTlnlW9zCRPefQIehSJpjesuPvZm4ETgKeDjwLeAL7BwAax1wI1Vtdyuk+Xebwb4F+DUqnpxhXWWOwrlNcAc8AvAiywcGXPWwa6/bsAlqVHuQpGkRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRv0f7o2vxFO2gC0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 45==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 46==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 47==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 48==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 49==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 50==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 51==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPMUlEQVR4nO3df4xlZX3H8fdHFsUUW8C9riug4w9SQmxc2umW1qZB0BZpImjVyB92m9CsJtJook2p/qE2NZVGJWliSVahrIlFBTFQf3YFGmqj4GAX2GW1IGLKZmHHKgJpSwt8+8c9a8fhzt47M/fe4dl5v5KTe+5zzj3ne/bOfPbss885J1WFJKk9z1jrAiRJK2OAS1KjDHBJapQBLkmNMsAlqVEbprmzjRs31szMzDR3KUnNu+22235UVb3F7VMN8JmZGebm5qa5S0lqXpIfDmq3C0WSGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckho11SsxNT0zF39prUtoyn0f/v21LkFaNs/AJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktSooQGe5Jgktya5PcneJB/s2q9M8oMku7tpy+TLlSQdMsrdCB8DzqqqR5McDXwjyVe6ZX9aVddMrjxJ0lKGBnhVFfBo9/bobqpJFiVJGm6kPvAkRyXZDRwEdlXVLd2iDyW5I8mlSZ61xGe3J5lLMjc/Pz+msiVJIwV4VT1RVVuAk4CtSV4O/DlwKvDrwAnAny3x2R1VNVtVs71eb0xlS5KWNQqlqh4CbgLOqaoD1fcY8HfA1kkUKEkabJRRKL0kx3XzzwZeA3w3yeauLcD5wJ5JFipJ+nmjjELZDOxMchT9wP9cVX0xyY1JekCA3cDbJ1inJGmRUUah3AGcPqD9rIlUJEkaiVdiSlKjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElq1CgPNT4mya1Jbk+yN8kHu/YXJ7klyT1JPpvkmZMvV5J0yChn4I8BZ1XVK4AtwDlJzgAuAS6tqpcBPwEunFyZkqTFhgZ49T3avT26mwo4C7ima98JnD+RCiVJA43UB57kqCS7gYPALuD7wENV9Xi3yv3AiUt8dnuSuSRz8/Pz46hZksSIAV5VT1TVFuAkYCtw6qg7qKodVTVbVbO9Xm+FZUqSFlvWKJSqegi4CfhN4LgkG7pFJwH7x1ybJOkwRhmF0ktyXDf/bOA1wD76Qf7GbrVtwHWTKlKS9FQbhq/CZmBnkqPoB/7nquqLSe4CPpPkL4F/BS6fYJ2SpEWGBnhV3QGcPqD9Xvr94ZKkNeCVmJLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGjXKQ41PTnJTkruS7E3yzq79A0n2J9ndTedOvlxJ0iGjPNT4ceDdVfWdJM8Bbkuyq1t2aVV9ZHLlSZKWMspDjQ8AB7r5R5LsA06cdGGSpMNbVh94khn6T6i/pWu6KMkdSa5IcvyYa5MkHcbIAZ7kWODzwLuq6mHgMuClwBb6Z+gfXeJz25PMJZmbn58fQ8mSJBgxwJMcTT+8P11V1wJU1YNV9URVPQl8Atg66LNVtaOqZqtqttfrjatuSVr3RhmFEuByYF9VfWxB++YFq70e2DP+8iRJSxllFMorgbcCdybZ3bW9F7ggyRaggPuAt02kQknSQKOMQvkGkAGLvjz+ciRJo/JKTElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRozyV/uQkNyW5K8neJO/s2k9IsivJ3d3r8ZMvV5J0yChn4I8D766q04AzgHckOQ24GLihqk4BbujeS5KmZGiAV9WBqvpON/8IsA84ETgP2NmtthM4f1JFSpKeall94ElmgNOBW4BNVXWgW/QAsGmJz2xPMpdkbn5+fhWlSpIWGjnAkxwLfB54V1U9vHBZVRVQgz5XVTuqaraqZnu93qqKlST9v5ECPMnR9MP701V1bdf8YJLN3fLNwMHJlChJGmSUUSgBLgf2VdXHFiy6HtjWzW8Drht/eZKkpWwYYZ1XAm8F7kyyu2t7L/Bh4HNJLgR+CLx5MiVKkgYZGuBV9Q0gSyw+e7zlSJJG5ZWYktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaNcpDja9IcjDJngVtH0iyP8nubjp3smVKkhYb5Qz8SuCcAe2XVtWWbvryeMuSJA0zNMCr6mbgx1OoRZK0DKvpA78oyR1dF8vxS62UZHuSuSRz8/Pzq9idJGmhlQb4ZcBLgS3AAeCjS61YVTuqaraqZnu93gp3J0labEUBXlUPVtUTVfUk8Alg63jLkiQNs6IAT7J5wdvXA3uWWleSNBkbhq2Q5CrgTGBjkvuB9wNnJtkCFHAf8LYJ1ihJGmBogFfVBQOaL59ALZKkZfBKTElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNGnop/dPFzMVfWusSJOlpxTNwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaNTTAk1yR5GCSPQvaTkiyK8nd3evxky1TkrTYKGfgVwLnLGq7GLihqk4BbujeS5KmaGiAV9XNwI8XNZ8H7OzmdwLnj7kuSdIQK+0D31RVB7r5B4BNS62YZHuSuSRz8/PzK9ydJGmxVf8nZlUVUIdZvqOqZqtqttfrrXZ3kqTOSgP8wSSbAbrXg+MrSZI0ipUG+PXAtm5+G3DdeMqRJI1qlGGEVwHfBH45yf1JLgQ+DLwmyd3Aq7v3kqQpGno/8Kq6YIlFZ4+5FknSMnglpiQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRg19pNrhJLkPeAR4Ani8qmbHUZQkabhVBXjnVVX1ozFsR5K0DHahSFKjVhvgBfxjktuSbB9HQZKk0ay2C+W3q2p/kucBu5J8t6puXrhCF+zbAV74wheucneSpENWdQZeVfu714PAF4CtA9bZUVWzVTXb6/VWsztJ0gIrDvAkv5DkOYfmgd8F9oyrMEnS4a2mC2UT8IUkh7bz91X11bFUJUkaasUBXlX3Aq8YYy2SpGVwGKEkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUqFUFeJJzknwvyT1JLh5XUZKk4VYc4EmOAj4OvBY4DbggyWnjKkySdHirOQPfCtxTVfdW1f8AnwHOG09ZkqRhNqzisycC/77g/f3AbyxeKcl2YHv39tEk31vFPidtI/CjtS5iDa3b488l6/fYOx7/0/v4XzSocTUBPpKq2gHsmPR+xiHJXFXNrnUda2U9H/96Pnbw+Fs9/tV0oewHTl7w/qSuTZI0BasJ8G8DpyR5cZJnAm8Brh9PWZKkYVbchVJVjye5CPgacBRwRVXtHVtla6OJrp4JWs/Hv56PHTz+Jo8/VbXWNUiSVsArMSWpUQa4JDVqXQd4kjcl2ZvkySRLDiE6Um8ZkOSEJLuS3N29Hr/Eek8k2d1NTf9H9bDvMsmzkny2W35LkpnpVzk5Ixz/HyWZX/B9//Fa1DkJSa5IcjDJniWWJ8nfdH82dyT51WnXuFzrOsCBPcAbgJuXWuEIv2XAxcANVXUKcEP3fpD/qqot3fS66ZU3XiN+lxcCP6mqlwGXApdMt8rJWcbP8mcXfN+fnGqRk3UlcM5hlr8WOKWbtgOXTaGmVVnXAV5V+6pq2JWhR/ItA84DdnbzO4Hz17CWaRjlu1z4Z3INcHaSTLHGSTqSf5aHqqqbgR8fZpXzgE9V37eA45Jsnk51K7OuA3xEg24ZcOIa1TJum6rqQDf/ALBpifWOSTKX5FtJWg75Ub7Ln61TVY8DPwWeO5XqJm/Un+U/6LoQrkly8oDlR6rmftcnfin9WkvydeD5Axa9r6qum3Y903a441/4pqoqyVJjSl9UVfuTvAS4McmdVfX9cdeqp4V/AK6qqseSvI3+v0bOWuOatIQjPsCr6tWr3ETTtww43PEneTDJ5qo60P1T8eAS29jfvd6b5J+A04EWA3yU7/LQOvcn2QD8EvAf0ylv4oYef1UtPNZPAn89hbqeLpr7XbcLZbgj+ZYB1wPbuvltwFP+RZLk+CTP6uY3Aq8E7ppaheM1yne58M/kjcCNdeRc7Tb0+Bf1+b4O2DfF+tba9cAfdqNRzgB+uqCL8empqtbtBLyefj/XY8CDwNe69hcAX16w3rnAv9E/63zfWtc9xuN/Lv3RJ3cDXwdO6NpngU92878F3Anc3r1euNZ1r/KYn/JdAn8BvK6bPwa4GrgHuBV4yVrXPOXj/ytgb/d93wScutY1j/HYrwIOAP/b/d5fCLwdeHu3PPRH6Xy/+1mfXeuah01eSi9JjbILRZIaZYBLUqMMcElqlAEuSY0ywCWta8NucrXMbb1qwY3Adif571GvXk5yapJvJnksyXtG+oyjUCStZ0l+B3iU/n1QXj7G7Z5AfzjqSVX1n4uW3VdVM4vankf/6fPn07+h2keG7cMzcEnrWg24yVWSlyb5apLbkvxzklNXsOk3Al9ZHN6HqeNgVX2b/jj1kRjgkvRUO4A/qapfA94D/O0KtvEW+hcPTcwRfy8USVqOJMfSvwL56gV3Ej50O4k30L9ydbH9VfV7C7axGfgV+g99P9T2cfq3ogB4QZLd3fzVVfWhldRqgEvSz3sG8FBVbVm8oKquBa4dYRtvBr5QVT/rDqmqdxya7/rAn7L9lRQqSepU1cPAD5K8CX72qLVXLHMzFzDh7hNwFIqkdS7JVcCZwEb6N7V7P3Aj/UeqbQaOBj5TVYO6TgZtbwb4F+DkqnpyiXUGjUJ5PjAH/CLwJP2RMad1f6EM3pcBLkltsgtFkhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RG/R8u+ZNRLgBQowAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 52==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 53==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANSElEQVR4nO3db4hl9X3H8fcn7qYJNUVlp2ar0glWIkuKaztYU0swJrYb88A1JCE+EB8ImwdaFMyDJX2QpLSgkOijVNiguAWrNVVRYppkaxdsipjM2o1Z3Qat3dBdNu6IEZVS29VvH8yZZjrO7L0799/+Zt4vGObec8+953tZfXM4e87ZVBWSpPa8Z9IDSJJWx4BLUqMMuCQ1yoBLUqMMuCQ1asM4N7Zp06aanp4e5yYlqXn79u17paqmli4fa8Cnp6eZnZ0d5yYlqXlJfr7ccg+hSFKjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1Kjxnol5lo0vfPxSY+wrEO3fXrSI0gaMffAJalRBlySGtUz4Enel+RHSX6S5LkkX+uWfyjJ00leTPK3Sd47+nElSQv62QN/C7iiqi4CtgLbklwK3A7cWVW/A/wSuGF0Y0qSluoZ8Jr3Zvd0Y/dTwBXA33XLdwPbRzKhJGlZfR0DT3Jakv3AMWAP8G/Aa1V1vFvlMHDOCu/dkWQ2yezc3NwwZpYk0WfAq+rtqtoKnAtcAlzY7waqaldVzVTVzNTUu/5BCUnSKp3UWShV9RqwF/gocEaShfPIzwWODHk2SdIJ9HMWylSSM7rH7weuBA4yH/LPdqtdDzw6qiElSe/Wz5WYm4HdSU5jPvgPVtV3kjwPPJDkL4B/Ae4e4ZySpCV6BryqngUuXmb5S8wfD5ckTYBXYkpSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSo3oGPMl5SfYmeT7Jc0lu7pZ/NcmRJPu7n6tGP64kacGGPtY5DtxaVc8k+QCwL8me7rU7q+rroxtPkrSSngGvqqPA0e7xG0kOAueMejBJ0omd1DHwJNPAxcDT3aKbkjyb5J4kZ67wnh1JZpPMzs3NDTSsJOlX+g54ktOBh4Bbqup14C7gfGAr83vo31jufVW1q6pmqmpmampqCCNLkqDPgCfZyHy876uqhwGq6uWqeruq3gG+BVwyujElSUv1cxZKgLuBg1V1x6Llmxetdg1wYPjjSZJW0s9ZKJcB1wE/TbK/W/Zl4NokW4ECDgFfHMmEkqRl9XMWyg+BLPPSd4c/jiSpX16JKUmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1KieAU9yXpK9SZ5P8lySm7vlZyXZk+SF7veZox9XkrSgnz3w48CtVbUFuBS4MckWYCfwRFVdADzRPZckjUnPgFfV0ap6pnv8BnAQOAe4GtjdrbYb2D6qISVJ73ZSx8CTTAMXA08DZ1fV0e6lXwBnr/CeHUlmk8zOzc0NMKokabG+A57kdOAh4Jaqen3xa1VVQC33vqraVVUzVTUzNTU10LCSpF/pK+BJNjIf7/uq6uFu8ctJNnevbwaOjWZESdJy+jkLJcDdwMGqumPRS48B13ePrwceHf54kqSVbOhjncuA64CfJtnfLfsycBvwYJIbgJ8Dnx/NiJKk5fQMeFX9EMgKL39iuONIkvrllZiS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmN6hnwJPckOZbkwKJlX01yJMn+7ueq0Y4pSVqqnz3we4Ftyyy/s6q2dj/fHe5YkqReega8qp4EXh3DLJKkkzDIMfCbkjzbHWI5c2gTSZL6stqA3wWcD2wFjgLfWGnFJDuSzCaZnZubW+XmJElLrSrgVfVyVb1dVe8A3wIuOcG6u6pqpqpmpqamVjunJGmJVQU8yeZFT68BDqy0riRpNDb0WiHJ/cDlwKYkh4GvAJcn2QoUcAj44ghnlCQto2fAq+raZRbfPYJZJEknwSsxJalRBlySGmXAJalRBlySGmXAJalRBlySGmXAJalRBlySGtXzQh5JWmp65+OTHqE5h2779NA/0z1wSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRhlwSWqUAZekRvUMeJJ7khxLcmDRsrOS7EnyQvf7zNGOKUlaqp898HuBbUuW7QSeqKoLgCe655KkMeoZ8Kp6Enh1yeKrgd3d493A9iHPJUnqYbXHwM+uqqPd418AZ6+0YpIdSWaTzM7Nza1yc5KkpQb+S8yqKqBO8PquqpqpqpmpqalBNydJ6qw24C8n2QzQ/T42vJEkSf1YbcAfA67vHl8PPDqccSRJ/ernNML7gaeADyc5nOQG4DbgyiQvAJ/snkuSxmhDrxWq6toVXvrEkGeRJJ0Er8SUpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqVM/byZ4qpnc+PukRJOmU4h64JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDVqoCsxkxwC3gDeBo5X1cwwhpIk9TaMS+k/XlWvDOFzJEknwUMoktSoQQNewA+S7EuyY7kVkuxIMptkdm5ubsDNSZIWDBrwP6qq3wM+BdyY5GNLV6iqXVU1U1UzU1NTA25OkrRgoIBX1ZHu9zHgEeCSYQwlSept1QFP8utJPrDwGPhj4MCwBpMkndggZ6GcDTySZOFz/qaqvjeUqSRJPa064FX1EnDREGeRJJ0ETyOUpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYZcElqlAGXpEYNFPAk25L8LMmLSXYOayhJUm+rDniS04BvAp8CtgDXJtkyrMEkSSc2yB74JcCLVfVSVf038ABw9XDGkiT1smGA954D/Mei54eBP1i6UpIdwI7u6ZtJfjbANk/GJuCVMW3rlJPb1/f3Z53/+eP3P+W+f24f6O2/vdzCQQLel6raBewa9XaWSjJbVTPj3u6pwu/v9/f7r/3vP8ghlCPAeYuen9stkySNwSAB/zFwQZIPJXkv8AXgseGMJUnqZdWHUKrqeJKbgO8DpwH3VNVzQ5tscGM/bHOK8fuvb37/dSBVNekZJEmr4JWYktQoAy5JjVrTAU/yuSTPJXknyZo/pWjBer7FQZJ7khxLcmDSs4xbkvOS7E3yfPff/c2TnmmckrwvyY+S/KT7/l+b9EyjtqYDDhwAPgM8OelBxsVbHHAvsG3SQ0zIceDWqtoCXArcuM7+7N8Crqiqi4CtwLYkl054ppFa0wGvqoNVNa4rP08V6/oWB1X1JPDqpOeYhKo6WlXPdI/fAA4yf8X0ulDz3uyebux+1vRZGms64OvUcrc4WDf/E2tekmngYuDpyU4yXklOS7IfOAbsqao1/f1Hfin9qCX5B+CDy7z0Z1X16LjnkSYtyenAQ8AtVfX6pOcZp6p6G9ia5AzgkSQfqao1+/chzQe8qj456RlOMd7iYB1LspH5eN9XVQ9Pep5JqarXkuxl/u9D1mzAPYSy9niLg3UqSYC7gYNVdcek5xm3JFPdnjdJ3g9cCfzrZKcarTUd8CTXJDkMfBR4PMn3Jz3TqFXVcWDhFgcHgQdPsVscjFSS+4GngA8nOZzkhknPNEaXAdcBVyTZ3/1cNemhxmgzsDfJs8zvyOypqu9MeKaR8lJ6SWrUmt4Dl6S1zIBLUqMMuCQ1yoBLUqMMuKR1bZg3QEvy8UVnAO1P8l9Jtvf53guTPJXkrSRf6us9noUiaT1L8jHgTeCvq+ojQ/zcs4AXgXOr6j+XvHaoqqaXLPtN5v/1+e3AL6vq67224R64pHVtuRugJTk/yfeS7EvyT0kuXMVHfxb4+6XxPsEcx6rqx8D/9LsBAy5J77YL+NOq+n3gS8BfreIzvgDcP9Splmj+XiiSNEzdzcD+EPj2/N0JAPi17rXPAH++zNuOVNWfLPqMzcDvMn9F9MKybzJ/tSzAb3V3TQT4dlX95WpmNeCS9P+9B3itqrYufaG7QVg/Nwn7PPBIVf3f4ZCqunHhcXcM/F2fv5pBJUmd7ha8/57kczB/k7AkF53kx1zLiA+fgGehSFrnuhugXQ5sAl4GvgL8I3AX8zfI2gg8UFXLHTpZ7vOmgX8Gzquqd1ZYZ7mzUD4IzAK/AbzD/JkxW050T3cDLkmN8hCKJDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXqfwGGACV5sklS3QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 54==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 55==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 56==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 57==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 58==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 59==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 60==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 61==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 62==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 63==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 64==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 65==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 66==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 67==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 68==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 69==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 70==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 71==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 72==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 73==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 74==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 75==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 76==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 77==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 78==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 79==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQTElEQVR4nO3de5BkZX3G8e8jKyBYym0lCMQhoiKioG4RlIplQBAvAUrRghhdI0pZ0QRjNG4sk4qpJIKVxGiiZW0BurkhghcQrwRQTFR0Qe6rgggGBHYUkBgrKOaXP85ZGdue7Z5Lz/AO30/VVJ/znnP6/N7Z6YfD233eTlUhSWrPQ5a7AEnS/BjgktQoA1ySGmWAS1KjDHBJatSqpTzZbrvtVlNTU0t5Sklq3mWXXfb9qlo92L6kAT41NcXGjRuX8pSS1LwkNw9rdwhFkhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIataR3YkpaGabWfXK5S2jKTae8YCLP6xW4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEaNHeBJtkny9STn9+v7JLk0yQ1Jzkqy7eTKlCQNmssV+MnAphnrpwLvqqp9gbuAExezMEnS1o0V4En2Al4AnNavBzgMOKffZQNw7CQKlCQNN+4V+N8Dfwz8X7++K3B3Vd3Xr98C7DnswCQnJdmYZOP09PSCipUk3W9kgCd5IbC5qi6bzwmqan1VramqNatXr57PU0iShhhnPvBDgaOTPB/YHngE8G5gpySr+qvwvYBbJ1emJGnQyCvwqvqTqtqrqqaA44GLquplwMXAcf1ua4FzJ1alJOmXLORz4G8B3pjkBrox8dMXpyRJ0jjm9JVqVfV54PP98o3AwYtfkiRpHN6JKUmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqNGBniS7ZN8NcmVSa5N8va+fZ8klya5IclZSbadfLmSpC3GuQK/Fzisqg4EDgKOSnIIcCrwrqraF7gLOHFyZUqSBo0M8Or8qF99aP9TwGHAOX37BuDYiVQoSRpqrDHwJNskuQLYDFwAfBu4u6ru63e5BdhzlmNPSrIxycbp6enFqFmSxJgBXlU/q6qDgL2Ag4H9xj1BVa2vqjVVtWb16tXzLFOSNGhOn0KpqruBi4FnADslWdVv2gu4dZFrkyRtxTifQlmdZKd++WHAEcAmuiA/rt9tLXDupIqUJP2yVaN3YQ9gQ5Jt6AL/w1V1fpLrgA8l+Uvg68DpE6xTkjRgZIBX1VXAU4e030g3Hi5JWgbeiSlJjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjRgZ4kr2TXJzkuiTXJjm5b98lyQVJru8fd558uZKkLca5Ar8P+KOq2h84BHhdkv2BdcCFVfU44MJ+XZK0REYGeFXdVlWX98v/DWwC9gSOATb0u20Ajp1UkZKkXzanMfAkU8BTgUuB3avqtn7T7cDusxxzUpKNSTZOT08voFRJ0kxjB3iShwMfAd5QVffM3FZVBdSw46pqfVWtqao1q1evXlCxkqT7jRXgSR5KF97/WlUf7ZvvSLJHv30PYPNkSpQkDTPOp1ACnA5sqqq/m7HpPGBtv7wWOHfxy5MkzWbVGPscCrwcuDrJFX3bW4FTgA8nORG4GXjpZEqUJA0zMsCr6j+AzLL58MUtR5I0Lu/ElKRGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNWrXcBUgPBFPrPrncJUhzNvIKPMkZSTYnuWZG2y5JLkhyff+482TLlCQNGmcI5YPAUQNt64ALq+pxwIX9uiRpCY0M8Kq6BLhzoPkYYEO/vAE4dpHrkiSNMN83MXevqtv65duB3RepHknSmBb8KZSqKqBm257kpCQbk2ycnp5e6OkkSb35BvgdSfYA6B83z7ZjVa2vqjVVtWb16tXzPJ0kadB8A/w8YG2/vBY4d3HKkSSNa5yPEZ4JfBl4QpJbkpwInAIckeR64Dn9uiRpCY28kaeqTphl0+GLXIskaQ68lV6SGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNWrVchcwrql1n1zuEppy0ykvWO4SJE2YV+CS1KgFBXiSo5J8M8kNSdYtVlGSpNHmHeBJtgHeCzwP2B84Icn+i1WYJGnrFnIFfjBwQ1XdWFU/AT4EHLM4ZUmSRlnIm5h7Av81Y/0W4NcHd0pyEnBSv3pvkmsWcM7W7QZ8fylOlFOX4ixztmT9f4Cy/w/S/ufUBff9McMaJ/4plKpaD6wHSLKxqtZM+pwPVPbf/tv/B2f/J9X3hQyh3ArsPWN9r75NkrQEFhLgXwMel2SfJNsCxwPnLU5ZkqRR5j2EUlX3JXk98FlgG+CMqrp2xGHr53u+FcL+P7jZ/wevifQ9VTWJ55UkTZh3YkpSowxwSWrUogd4kickuWLGzz1J3jCwz7OT/HDGPn+22HUspyR/mOTaJNckOTPJ9gPbt0tyVj8FwaVJppan0skYo/+vTDI949//1ctV62JLcnLf72sH/+777Unynv7f/qokT1uOOidljP6vqNd+kjOSbJ55f0uSXZJckOT6/nHnWY5d2+9zfZK18yqgqib2Q/fm5u3AYwbanw2cP8lzL9cP3Q1O3wEe1q9/GHjlwD6/B7y/Xz4eOGu5617i/r8S+MflrnUCfT8AuAbYge4DAv8O7Duwz/OBTwMBDgEuXe66l7j/K+q1DzwLeBpwzYy2dwLr+uV1wKlDjtsFuLF/3Llf3nmu55/0EMrhwLer6uYJn+eBZhXwsCSr6P6Yvzew/RhgQ798DnB4kixhfZM2qv8r1RPpAvnHVXUf8AXgRQP7HAP8U3W+AuyUZI+lLnRCxun/ilJVlwB3DjTPfH1vAI4dcuhzgQuq6s6qugu4ADhqruefdIAfD5w5y7ZnJLkyyaeTPGnCdSyZqroV+Bvgu8BtwA+r6nMDu/18GoL+D/2HwK5LWeekjNl/gBf3QwjnJNl7yPYWXQP8RpJdk+xAd7U92LdhU1DsuUT1Tdo4/YcV+tqfYfequq1fvh3Yfcg+i/J3MLEA72/uORo4e8jmy+mGVQ4E/gH4+KTqWGr9eNcxwD7Ao4Edk/zO8la1dMbs/yeAqap6Ct2VxwZWgKraBJwKfA74DHAF8LNlLWoJjdn/FfvaH6a68ZKJfVZ7klfgzwMur6o7BjdU1T1V9aN++VPAQ5PsNsFaltJzgO9U1XRV/RT4KPDMgX1+Pg1BP8zwSOAHS1rl5Izsf1X9oKru7VdPA56+xDVOTFWdXlVPr6pnAXcB3xrYZUVPQTGq/yv8tb/FHVuGxfrHzUP2WZS/g0kG+AnMMnyS5Fe2jPkmObivY6UE2HeBQ5Ls0PfxcGDTwD7nAVvedT4OuKj/L/VKMLL/A2O+Rw9ub1mSR/WPv0o3/vtvA7ucB7yi/zTKIXRDTLexQozq/wp/7W8x8/W9Fjh3yD6fBY5MsnP/f61H9m1zM6F3Znek+0d55Iy21wKv7ZdfD1wLXAl8BXjmcr+bvMj9fzvwDboxwX8GtgP+Aji637493dDSDcBXgV9b7pqXuP/vmPHvfzGw33LXvIh9/yJwXd+3w/u2mX/7ofsilG8DVwNrlrvmJe7/inrt012k3gb8lG4c+0S697MuBK6n+yTOLv2+a4DTZhz7qj4DbgB+dz7n91Z6SWqUd2JKUqMMcElqlAEuSY0ywCWpUQa4pK0aNmHTAp7rNwcmu/vfJMNuNR927M5JPtbfwfvVJAfMst9hSS7vJ9Xa0N9rsdXjZ5uEK8mBSb6c5Ookn0jyiL592yQf6NuvTPLsBf1iuufcrz/XvUneNM4xBrikUT7IPObpGKaqLq6qg6rqIOAw4Md0d27+giQ3DTn8rcAV1d3B+wrg3UOOewjdnb3HV9UBwM3c/5nsocf3Qf4a4GDgQOCFSfbtjzmNbmKqJwMfA97ct7+m78+TgSOAv+3PvRB3An9ANxXFWAxwSVtVQyZsSvLYJJ9JclmSLybZbx5PfRzw6ar68Zj77w9c1Nf0DWAqyeA8I7sCP6mqLXeAXgC8eMTxW5uE6/HAJSOeazNwN93nvElyZH8lfXmSs5M8fJzOVdXmqvoa3WfKx2KAS5qP9cDvV9XTgTcB75vHc2xtsrthrqQP1v4uzsfQ3YI+0/eBVUnW9OvHcf8t67Mdv7VJuK6lm9sH4CUDz3V0klVJ9qGbDmLvflqAtwHPqaqnARuBN86hj3My7y81lvTg1F9RPhM4e8YsyNv1215Ed9ftoFur6rkznmMP4MnMuH08yXuBQ/vVRye5ol8+u6r+CjgFeHfffjXwdQYmy6qqSnI88K4k29ENz2zZZ+jxVbUpyZZJuP6HX5yE61XAe5L8Kd0t8j/p28+gu3LfSDdM86X+mEPors7/s//dbAt8ue/fO4DfGvK7+XhVvW1I+0jeiSlppHTfGnV+VR3Qv5H3zaqa9zzmSU4GnlRVJ82y/aaqmtrK8aH74pCnVNU9W9nvSODVVfXScY9P8tfALVX1voH2xwP/UlUHDznPl4BXA48FfruqTpitplGS/Dnwo6oaORbuEIqkOekD7ztJXgI//5q4A+f4NLNOdjebJDulm6YaurC8ZFh4z5hQazvgLcD7Rx0/2yRcM9ofQjc0suW5dkiyY798BHBfVV1HN7/LoVveBE2yYx/8E2GAS9qqJGfSDQM8IcktSU4EXgacmORKfnGceJznm6IbS/7CHEt5InBNkm/STVd98ozn/FSSR/erb06yCbgK+ERVXTTqeOAjSa6jm6v+dVV1d99+QpJv0U3O9j3gA337o4DL+/O8BXg5QFVN031l4JlJrqL7vY31Bm+6mRpvoRszf1v/u37EVo9xCEWS2uQVuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5Jjfp/AH9Y+cEWqTgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 80==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 81==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 82==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 83==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 84==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 85==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 86==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 87==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 88==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 89==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 90==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 91==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 92==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 93==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 94==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 95==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEFCAYAAAD69rxNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANR0lEQVR4nO3db4hl9X3H8fcnumlCTVHZqdmqdIKVyJLi2g7W1BKMie3GPHANSYgPxAfC5oEWBfNA0gdJSgsKiT5KhQ2KW7BaUxUlpkm2dsGmiMms3ZjVbdDaDd1l444YUSm1Xf32wZxppuOdvXdm7p/9zbxfMMy95557z/ey+uZy5pxzU1VIktrznkkPIElaHQMuSY0y4JLUKAMuSY0y4JLUqFPHubHNmzfX9PT0ODcpSc3bt2/fK1U1tXT5WAM+PT3N7OzsODcpSc1L8vNey92FIkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNMuCS1CgDLkmNGuuZmOvR9K2PT3qEng7d9ulJjyBpxPwELkmNMuCS1Ki+AU/yviQ/SvKTJM8l+Vq3/ENJnk7yYpK/TfLe0Y8rSVowyCfwt4DLq+pCYBuwPcklwO3AnVX1O8AvgetHN6Ykaam+Aa95b3Z3N3U/BVwO/F23fDewYyQTSpJ6GmgfeJJTkuwHjgF7gH8DXquq490qh4Gzl3nuziSzSWbn5uaGMbMkiQEDXlVvV9U24BzgYuCCQTdQVbuqaqaqZqam3vWFEpKkVVrRUShV9RqwF/gocHqShePIzwGODHk2SdIJDHIUylSS07vb7weuAA4yH/LPdqtdBzw6qiElSe82yJmYW4DdSU5hPvgPVtV3kjwPPJDkL4B/Ae4e4ZySpCX6BryqngUu6rH8Jeb3h0uSJsAzMSWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUQZckhplwCWpUX0DnuTcJHuTPJ/kuSQ3dcu/muRIkv3dz5WjH1eStODUAdY5DtxSVc8k+QCwL8me7rE7q+rroxtPkrScvgGvqqPA0e72G0kOAmePejBJ0omtaB94kmngIuDpbtGNSZ5Nck+SM5Z5zs4ks0lm5+bm1jSsJOlXBg54ktOAh4Cbq+p14C7gPGAb85/Qv9HreVW1q6pmqmpmampqCCNLkmDAgCfZxHy876uqhwGq6uWqeruq3gG+BVw8ujElSUsNchRKgLuBg1V1x6LlWxatdjVwYPjjSZKWM8hRKJcC1wI/TbK/W/Zl4Jok24ACDgFfHMmEkqSeBjkK5YdAejz03eGPI0kalGdiSlKjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNapvwJOcm2RvkueTPJfkpm75mUn2JHmh+33G6MeVJC0Y5BP4ceCWqtoKXALckGQrcCvwRFWdDzzR3ZckjUnfgFfV0ap6prv9BnAQOBu4CtjdrbYb2DGqISVJ77aifeBJpoGLgKeBs6rqaPfQL4CzlnnOziSzSWbn5ubWMKokabGBA57kNOAh4Oaqen3xY1VVQPV6XlXtqqqZqpqZmppa07CSpF8ZKOBJNjEf7/uq6uFu8ctJtnSPbwGOjWZESVIvgxyFEuBu4GBV3bHooceA67rb1wGPDn88SdJyTh1gnUuBa4GfJtnfLfsycBvwYJLrgZ8Dnx/NiJKkXvoGvKp+CGSZhz8x3HEkSYPyTExJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJapQBl6RG9Q14knuSHEtyYNGyryY5kmR/93PlaMeUJC01yCfwe4HtPZbfWVXbup/vDncsSVI/fQNeVU8Cr45hFknSCqxlH/iNSZ7tdrGcMbSJJEkDWW3A7wLOA7YBR4FvLLdikp1JZpPMzs3NrXJzkqSlVhXwqnq5qt6uqneAbwEXn2DdXVU1U1UzU1NTq51TkrTEqgKeZMuiu1cDB5ZbV5I0Gqf2WyHJ/cBlwOYkh4GvAJcl2QYUcAj44ghnlCT10DfgVXVNj8V3j2AWSdIKeCamJDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDWq77fSnyymb3180iNI0knFT+CS1CgDLkmNMuCS1Ki+AU9yT5JjSQ4sWnZmkj1JXuh+nzHaMSVJSw3yCfxeYPuSZbcCT1TV+cAT3X1J0hj1DXhVPQm8umTxVcDu7vZuYMeQ55Ik9bHafeBnVdXR7vYvgLOWWzHJziSzSWbn5uZWuTlJ0lJr/iNmVRVQJ3h8V1XNVNXM1NTUWjcnSeqsNuAvJ9kC0P0+NryRJEmDWG3AHwOu625fBzw6nHEkSYMa5DDC+4GngA8nOZzkeuA24IokLwCf7O5Lksao77VQquqaZR76xJBnkSStgGdiSlKjDLgkNcqAS1KjDLgkNcqAS1KjDLgkNcqAS1KjmvlOTEknD7+jduUO3fbpob+mn8AlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIaZcAlqVEGXJIataYvdEhyCHgDeBs4XlUzwxhKktTfML6R5+NV9coQXkeStALuQpGkRq014AX8IMm+JDt7rZBkZ5LZJLNzc3Nr3JwkacFaA/5HVfV7wKeAG5J8bOkKVbWrqmaqamZqamqNm5MkLVhTwKvqSPf7GPAIcPEwhpIk9bfqgCf59SQfWLgN/DFwYFiDSZJObC1HoZwFPJJk4XX+pqq+N5SpJEl9rTrgVfUScOEQZ5EkrYCHEUpSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSowy4JDXKgEtSo9YU8CTbk/wsyYtJbh3WUJKk/lYd8CSnAN8EPgVsBa5JsnVYg0mSTmwtn8AvBl6sqpeq6r+BB4CrhjOWJKmfU9fw3LOB/1h0/zDwB0tXSrIT2NndfTPJz9awzZXYDLwypm2ddHL7xn7/bPB/f3z/J937z+1revpv91q4loAPpKp2AbtGvZ2lksxW1cy4t3uy8P37/n3/6//9r2UXyhHg3EX3z+mWSZLGYC0B/zFwfpIPJXkv8AXgseGMJUnqZ9W7UKrqeJIbge8DpwD3VNVzQ5ts7ca+2+Yk4/vf2Hz/G0CqatIzSJJWwTMxJalRBlySGrWuA57kc0meS/JOknV/SNGCjXyJgyT3JDmW5MCkZxm3JOcm2Zvk+e6/+5smPdM4JXlfkh8l+Un3/r826ZlGbV0HHDgAfAZ4ctKDjIuXOOBeYPukh5iQ48AtVbUVuAS4YYP9278FXF5VFwLbgO1JLpnwTCO1rgNeVQeralxnfp4sNvQlDqrqSeDVSc8xCVV1tKqe6W6/ARxk/ozpDaHmvdnd3dT9rOujNNZ1wDeoXpc42DD/E2tekmngIuDpyU4yXklOSbIfOAbsqap1/f5Hfir9qCX5B+CDPR76s6p6dNzzSJOW5DTgIeDmqnp90vOMU1W9DWxLcjrwSJKPVNW6/XtI8wGvqk9OeoaTjJc42MCSbGI+3vdV1cOTnmdSquq1JHuZ/3vIug24u1DWHy9xsEElCXA3cLCq7pj0POOWZKr75E2S9wNXAP862alGa10HPMnVSQ4DHwUeT/L9Sc80alV1HFi4xMFB4MGT7BIHI5XkfuAp4MNJDie5ftIzjdGlwLXA5Un2dz9XTnqoMdoC7E3yLPMfZPZU1XcmPNNIeSq9JDVqXX8Cl6T1zIBLUqMMuCQ1yoBLUqMMuKQNbZgXQEvy8UVHAO1P8l9Jdgz43AuSPJXkrSRfGug5HoUiaSNL8jHgTeCvq+ojQ3zdM4EXgXOq6j+XPHaoqqaXLPtN5r99fgfwy6r6er9t+Alc0obW6wJoSc5L8r0k+5L8U5ILVvHSnwX+fmm8TzDHsar6MfA/g27AgEvSu+0C/rSqfh/4EvBXq3iNLwD3D3WqJZq/FookDVN3MbA/BL49f3UCAH6te+wzwJ/3eNqRqvqTRa+xBfhd5s+IXlj2TebPlgX4re6qiQDfrqq/XM2sBlyS/r/3AK9V1balD3QXCBvkImGfBx6pqv/bHVJVNyzc7vaBv+v1VzOoJKnTXYL335N8DuYvEpbkwhW+zDWMePcJeBSKpA2uuwDaZcBm4GXgK8A/Ancxf4GsTcADVdVr10mv15sG/hk4t6reWWadXkehfBCYBX4DeIf5I2O2nuia7gZckhrlLhRJapQBl6RGGXBJapQBl6RGGXBJapQBl6RGGXBJatT/Ap3DJXk1UpOfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 96==== Step 1  Train Loss 1.100000023841858\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 97==== Step 1  Train Loss 1.1000001430511475\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 98==== Step 1  Train Loss 1.0999999046325684\n",
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
            "        1.0000], device='cuda:0', grad_fn=<SumBackward1>)\n",
            "tensor([0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 99==== Step 1  Train Loss 1.100000023841858\n",
            "  Batch   100  of  3,213.    Elapsed: 0:02:24.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-0d90f1a56888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmytrainStep1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelEmb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e2225c05e6ee>\u001b[0m in \u001b[0;36mmytrainStep1\u001b[0;34m(model, criterion1, criterion2)\u001b[0m\n\u001b[1;32m     95\u001b[0m               \u001b[0;31m# print(a1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m               \u001b[0;31m# print(a2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m               \u001b[0mFC11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFC22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_input_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m                   \u001b[0;31m# loss = criterion1(out1,out2,b_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m               \u001b[0mpos_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-55571ff837ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id1, mask1, sent_id2, mask2, b_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0;31m# forward pass of input 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m       \u001b[0;31m# forward pass of input 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0moutput2\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-55571ff837ad>\u001b[0m in \u001b[0;36mforwardOnce\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupLayersMode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "mytrainStep1(modelEmb,criterion1,criterion2)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRWK1Ad2YyBS",
        "outputId": "2f03c8c3-b17e-4f3f-94d5-77f77792d2f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "myModelEmbeddings(\n",
              "  (bertModel): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (FC): Linear(in_features=768, out_features=32, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelEmb.load_state_dict(torch.load('checkPointEmb.pt'))\n",
        "\n",
        "modelEmb.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ciqda7wmasXy"
      },
      "outputs": [],
      "source": [
        "mytrainStep2(modelCLS,criterion1,criterion2,modelEmb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GEICXUkmCoT"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.copy(\"checkpointEmb.pt\",'/content/drive/My Drive/Thesis/MaskAll/Separated')\n",
        "shutil.copy(\"checkpoint.pt\",'/content/drive/My Drive/Thesis/MaskAll/Separated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF-oOiXwopPG"
      },
      "source": [
        "# BERT + Bi-LSTM CL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2sYqBGboYIm"
      },
      "outputs": [],
      "source": [
        "class myModelEmbeddings(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (False,False)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModelEmbeddings, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      \n",
        "      # for param in model.bertModel.parameters():\n",
        "      #     param.requires_grad = False\n",
        "      # modules = [self.bertModel.embeddings, *self.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "     \n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      self.bilstm = nn.LSTM(input_size=768, hidden_size=768,batch_first=True,bidirectional=True)#num_layers=3,dropout=0.2,\n",
        "      self.dropout = nn.Dropout(0.1)  \n",
        "       \n",
        "      self.dropout2 = nn.Dropout(0.1)\n",
        "      self.dropout3 = nn.Dropout(0.1)\n",
        "      # self.conv_1 = nn.Conv1d(768, 128, 3)\n",
        "      # self.maxPool = nn.MaxPool1d(2)\n",
        "      self.tanh = nn.Tanh()\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout4 = nn.Dropout(0.1)\n",
        "      self.FC1 = nn.Linear(in_features = 256*4,out_features = 2)\n",
        "      \n",
        "      self.FC2 = nn.Linear(in_features = 64,out_features = 2)\n",
        "      self.FC3 = nn.Linear(in_features = 16,out_features = 2)\n",
        "     \n",
        "      \n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      \n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][0:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      \n",
        "      return concatEmbeddingLayers\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      embeddings = bertOutputs[0] #last hidden states\n",
        "      #embeddings = bertOutputs[1] # pooler\n",
        "      return embeddings\n",
        "  def getCLSEmbeddingsFromLayers(self,bertOutputs ):\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      \n",
        "      \n",
        "      # Extract the hidden state for the [CLS] token from last four encode layers\n",
        "      last_layer_hidden_states = hidden_states[2:6]\n",
        "      cls = []\n",
        "      for layer in last_layer_hidden_states:\n",
        "          cls.append(layer[:,0,:])\n",
        "      cls_embeddings = torch.stack(cls, dim=1)\n",
        "      del cls\n",
        "    \n",
        "      #concat_cls_embeddings = torch.cat(cls_embeddings, dim=1).unsqueeze(1)\n",
        "\n",
        "    \n",
        "      \n",
        "      \n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[-2:]).sum(0) #sum only CLS Embs from last four layers\n",
        "      #cls = sumEmbeddingLayers[:,0,:]\n",
        "\n",
        "      return cls_embeddings\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      # `hidden_states` is a Python list.\n",
        "     \n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[-4:]).sum(0)\n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[-4:]).mean(dim=0)\n",
        "      del hidden_states\n",
        "\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask, strategy='avg'):\n",
        "      if strategy == 'max':\n",
        "        #  avg_setence_embeddings = torch.mean(token_embeddings,dim=1)\n",
        "        #  print(avg_setence_embeddings.shape)\n",
        "         input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "         max_setence_embeddings = torch.max(token_embeddings, 1)[0]\n",
        "         return max_setence_embeddings\n",
        "      elif strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'sum':\n",
        "        sum_setence_embeddings = torch.sum(token_embeddings[0:len(token_embeddings)],1)\n",
        "        return sum_setence_embeddings\n",
        "\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(input_ids=sent_id, attention_mask=mask,decoder_input_ids=sent_id)\n",
        "    \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        # embeddings = self.getCLSEmbeddingsFromLayers(outputs)\n",
        "        return embeddings \n",
        "      else:\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings \n",
        "      \n",
        "  def init_hidden(self, batch_size):\n",
        "        #Initialization of the LSTM hidden and cell states\n",
        "        h0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        c0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "  def forward(self, sent_id1, mask1,hidden):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      \n",
        "    \n",
        "  \n",
        "     \n",
        "   \n",
        "      # output1 = output1[:, 0, :]\n",
        "      # output2 = output2[:, 0, :]\n",
        "      # output1 = self.tanh(output1)\n",
        "      # output1 = self.dropout(output1)\n",
        "      out1, (hidden1,cell1) = self.bilstm(output1,hidden)\n",
        "      # out1,hidden11 = self.bilstm(output1,hidden)\n",
        "      #Extract only the hidden state from the last LSTM cell\n",
        "      # out11 = out1[:,-1,:]\n",
        "      # print(hidden11)\n",
        "      # h1 = hidden1\n",
        "      # final_repr = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1)\n",
        "\n",
        "      # out_split1 = out1.view(sent_id1.shape[0], 512, 2, 768)\n",
        "    \n",
        "      out_split1 = out1.view(sent_id1.shape[0], 256, 2, 768)\n",
        "      out_forward1 = out_split1[:, :, 0, :]\n",
        "      out_backward1 = out_split1[:, :, 1, :]\n",
        "      batch_indices = torch.arange(0, sent_id1.shape[0], device=device)\n",
        "      seq_indices = 256 - 1\n",
        "      direction_full1 = torch.cat([out_split1[batch_indices, seq_indices, 0], out_split1[batch_indices, 0, 1]], dim=-1)\n",
        "      # return F.normalize(direction_full1), F.normalize(direction_full2),output2\n",
        "      return direction_full1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZUfSEJos6H"
      },
      "outputs": [],
      "source": [
        "class myModelEmbeddings(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (True,True)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModelEmbeddings, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      \n",
        "      # for param in model.bertModel.parameters():\n",
        "      #     param.requires_grad = False\n",
        "      # modules = [self.bertModel.embeddings, *self.bertModel.encoder.layer[:4]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "     \n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      self.bilstm = nn.LSTM(input_size=768, hidden_size=768,batch_first=True,bidirectional=True)#num_layers=3,dropout=0.2,\n",
        "      self.dropout = nn.Dropout(0.1)  \n",
        "       \n",
        "      self.dropout2 = nn.Dropout(0.1)\n",
        "      self.dropout3 = nn.Dropout(0.1)\n",
        "      # self.conv_1 = nn.Conv1d(768, 128, 3)\n",
        "      # self.maxPool = nn.MaxPool1d(2)\n",
        "      self.tanh = nn.Tanh()\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout4 = nn.Dropout(0.1)\n",
        "      self.FC1 = nn.Linear(in_features = 256*4,out_features = 2)\n",
        "      \n",
        "      self.FC2 = nn.Linear(in_features = 64,out_features = 2)\n",
        "      self.FC3 = nn.Linear(in_features = 16,out_features = 2)\n",
        "     \n",
        "      \n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      \n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][0:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      \n",
        "      return concatEmbeddingLayers\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      embeddings = bertOutputs[0] #last hidden states\n",
        "      return embeddings\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      # `hidden_states` is a Python list.\n",
        "     \n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[-4:]).sum(0)\n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[-4:]).mean(dim=0)\n",
        "      del hidden_states\n",
        "\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask, strategy='avg'):\n",
        "      if strategy == 'max':\n",
        "        #  avg_setence_embeddings = torch.mean(token_embeddings,dim=1)\n",
        "        #  print(avg_setence_embeddings.shape)\n",
        "         input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "         max_setence_embeddings = torch.max(token_embeddings, 1)[0]\n",
        "         return max_setence_embeddings\n",
        "      elif strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'sum':\n",
        "        sum_setence_embeddings = torch.sum(token_embeddings[0:len(token_embeddings)],1)\n",
        "        return sum_setence_embeddings\n",
        "\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(sent_id, attention_mask=mask)\n",
        "    \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        return embeddings \n",
        "      else:\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings \n",
        "      \n",
        "  def init_hidden(self, batch_size):\n",
        "        #Initialization of the LSTM hidden and cell states\n",
        "        h0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        c0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "  def forward(self, sent_id1, mask1,sent_id2, mask2,b_labels,hidden):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      # forward pass of input 2\n",
        "      output2  = self.forwardOnce(sent_id2, mask2)\n",
        "    \n",
        "  \n",
        "      cos = nn.CosineSimilarity(dim=1)\n",
        "   \n",
        "      # output1 = output1[:, 0, :]\n",
        "      # output2 = output2[:, 0, :]\n",
        "      # output1 = self.tanh(output1)\n",
        "      # output1 = self.dropout(output1)\n",
        "      out1, (hidden1,cell1) = self.bilstm(output1,hidden)\n",
        "      # out1,hidden11 = self.bilstm(output1,hidden)\n",
        "      #Extract only the hidden state from the last LSTM cell\n",
        "      # out11 = out1[:,-1,:]\n",
        "      # print(hidden11)\n",
        "      # h1 = hidden1\n",
        "      # final_repr = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1)\n",
        "\n",
        "      out_split1 = out1.view(sent_id1.shape[0], 512, 2, 768)\n",
        "      out_forward1 = out_split1[:, :, 0, :]\n",
        "      out_backward1 = out_split1[:, :, 1, :]\n",
        "      batch_indices = torch.arange(0, sent_id1.shape[0], device=device)\n",
        "      seq_indices = 512 - 1\n",
        "      direction_full1 = torch.cat([out_split1[batch_indices, seq_indices, 0], out_split1[batch_indices, 0, 1]], dim=-1)\n",
        "      # direction_full1 = torch.cat([out_forward1[:, -1, :], out_backward1[:, 0, :]], dim=1)   \n",
        "      # print(h1[1])\n",
        "      # Access the last hidden state of the forward LSTM\n",
        "      # print(hidden1)\n",
        "      # forward_hidden = h1[0, :, :]\n",
        "      # print(forward_hidden)\n",
        "      # Access the last hidden state of the backward LSTM\n",
        "      # backward_hidden = h1[1, :, :]\n",
        "      # print(backward_hidden)\n",
        "      # h1 = self.dropout2(h1)\n",
        "      # print(h1)\n",
        "      # print(h1.shape)\n",
        "      # direction_1, direction_2 = forward_hidden,backward_hidden#h1[0], h1[1]\n",
        "      # direction_full1 = torch.cat((direction_1, direction_2), 1)\n",
        "      # direction_full1 = self.tanh(direction_full1)\n",
        "      # direction_full1 = self.dropout(direction_full1)\n",
        "      # direction_full1 = self.conv_1(direction_full1)\n",
        "      # direction_full1 = self.maxPool(direction_full1)\n",
        "      # output2 = self.tanh(output2)\n",
        "      # output2 = self.dropout(output2)\n",
        "      out2, (hidden2,cell2) = self.bilstm(output2,hidden)\n",
        "      # out22 = out2[:,-1,:]\n",
        "      # h2 = hidden2\n",
        "      # final_repr = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=1)\n",
        "      out_split2 = out2.view(sent_id1.shape[0], 512, 2, 768)\n",
        "      out_forward2 = out_split2[:, :, 0, :]\n",
        "      out_backward2 = out_split2[:, :, 1, :]\n",
        "      # direction_full2 = torch.cat([out_forward2[:, -1, :], out_backward2[:, 0, :]], dim=1)\n",
        "      direction_full2 = torch.cat([out_split2[batch_indices, seq_indices, 0], out_split2[batch_indices, 0, 1]], dim=-1)\n",
        "      # forward_hidden2 = h2[0, :, :]\n",
        "      # backward_hidden2 = h2[1, :, :]\n",
        "      # direction_21, direction_22 = forward_hidden2,backward_hidden2#h2[0], h2[1]\n",
        "      # direction_full2 = torch.cat((direction_21, direction_22), 1)\n",
        "      # direction_full2 = self.tanh(direction_full2)\n",
        "      # direction_full2 = self.dropout(direction_full2)\n",
        "      # direction_full2 = self.conv_1(direction_full2)\n",
        "      # direction_full2 = self.maxPool(direction_full2)\n",
        "      # FC11 = self.FC(output1)\n",
        "      # FC11 = self.tanh(FC11)\n",
        "      # FC22 = self.FC(output2)\n",
        "      # FC22 = self.tanh(FC22)\n",
        "\n",
        "      # avgoutput1 = self.pooling(output1,mask1,'avg')\n",
        "      # avgoutput2 = self.pooling(output2,mask2,'avg')\n",
        "      # out = torch.cat((direction_full1, direction_full2), 1)\n",
        "      \n",
        "      # out = self.dropout(out)\n",
        "      # direction_full1 = self.tanh(direction_full1)\n",
        "      # out = self.relu(out)\n",
        "      # out = self.FC1(out)\n",
        "      # out = self.relu(out)\n",
        "      # FC11 = self.tanh(FC11)\n",
        "      # out = self.dropout4(out)\n",
        "\n",
        "      # direction_full2 = self.tanh(direction_full2)\n",
        "      # FC22 = self.FC1(direction_full2)\n",
        "      # FC11 = self.tanh(FC11)\n",
        "      # FC22 = self.dropout4(FC22)\n",
        "      # out = self.FC2(out)\n",
        "      # out = self.dropout4(out)\n",
        "      # output = self.FC3(out)\n",
        "      output2 = cos(direction_full1, direction_full2)\n",
        "      print(output2)\n",
        "     \n",
        "      print(b_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # plt.hist(output2.cpu().data.numpy(), bins=6)\n",
        "      # plt.show()\n",
        "      # return F.normalize(direction_full1), F.normalize(direction_full2),output2\n",
        "      return direction_full1, direction_full2,output2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mhez6UFLNtN"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion1,validation_dataloader,modelFC=None,criterion2 = None):\n",
        "    \n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    # model.bertModel.eval()\n",
        " \n",
        "    model.eval()\n",
        "    if modelFC is not None:\n",
        "        modelFC.eval()\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # tsne = TSNE()\n",
        "    step = 0\n",
        "    concatAll = []\n",
        "    totalF1 = 0\n",
        "    totalAcc = 0\n",
        "    avg_val_accuracy = 0\n",
        "    # for (input1,mask1, target1),(input2,mask2, target2) in validation_dataloader:\n",
        "    # for batch in test_dataloader:\n",
        "    # for anchor,anchorMask,replicas,replicasMask in validation_dataloader:\n",
        "\n",
        "    for input1, mask1, input2, mask2 in validation_dataloader:\n",
        "        step += 1\n",
        "        # if step == 71:\n",
        "        #    break\n",
        "        # b_input_mask1 = anchorMask.to(device)#torch.ones(anchor.shape[:-1], device=device)\n",
        "        # b_input_mask2 = replicasMask.to(device)#torch.ones(replicas.shape[:-1], device=device)\n",
        "        # b_input_ids1 = anchor.to(device)\n",
        "        # b_input_ids2 = replicas.to(device)\n",
        "        # labels = torch.arange(0, b_input_mask1.shape[0], device=device)\n",
        "        \n",
        "        b_input_ids1 = input1.to(device)\n",
        "       \n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        # target1 = target1.type(torch.FloatTensor)\n",
        "        # b_labels = target1.to(device)\n",
        "        h = model.init_hidden(b_input_ids1.shape[0])\n",
        "        b_input_ids2 = input2.to(device)\n",
        "        b_input_mask2 = mask2.to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            if modelFC==None:\n",
        "\n",
        "              FC11 = model(b_input_ids1, b_input_mask1,h)             \n",
        "              FC22 = model(b_input_ids2, b_input_mask2,h)             \n",
        "              cos = nn.CosineSimilarity(dim=1)\n",
        "              output2 = cos(FC11, FC22)\n",
        "              # pos_indices = torch.where(b_labels==1)[0]\n",
        "              # neg_indices = torch.where(b_labels==0)[0]\n",
        "              # # print(pos_indices)\n",
        "              # # print(neg_indices)\n",
        "              # indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "              # loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # output = F.cosine_similarity(FC11, FC22)\n",
        "             \n",
        "              # loss1 = criterion1(output, b_labels)\n",
        "              # labels = torch.cat([b_labels, b_labels], dim=0)      \n",
        "              # loss1 = criterion1(embeddings,b_labels.repeat(2))\n",
        "            \n",
        "              loss1,acc = criterion1(FC11,FC22)\n",
        "              # loss1= criterion1(FC11, labels, ref_emb=FC22, ref_labels=labels)\n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "              \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2)\n",
        "              FC11,FC22,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,h)\n",
        "              out = modelFC(FC11,FC22)\n",
        "              loss2 = criterion2(out,b_labels)\n",
        "            if modelFC == None:\n",
        "                total_eval_loss += loss1.item()\n",
        "                # print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 AVG. val Loss \"+str(loss1.item()))\n",
        "                # f1 = calcF1score(cos,b_labels)\n",
        "                # totalF1 += f1\n",
        "                # probs = F.softmax(cos, dim=1).cpu().numpy()\n",
        "                # accuracy = calcAccuracy(probs,b_labels)\n",
        "                totalAcc+=0#acc.cpu().item()\n",
        "              \n",
        "                # print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                # print(probs) \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 AVG. val Loss \"+str(loss1.item()),\"acc =\",str(acc))\n",
        "                # label_ids = b_labels.to('cpu').numpy()\n",
        "                \n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "            \n",
        "                total_eval_loss += loss2.item()\n",
        "                f1 = calcF1score(out,b_labels)\n",
        "                totalF1 += f1\n",
        "                probs = F.softmax(out, dim=1).cpu().numpy()\n",
        "                accuracy = calcAccuracy(probs,b_labels)\n",
        "                totalAcc+=accuracy\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                print(probs) \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2  val Loss \"+str(loss2.item()), \"==== \",str(f1),\"===== Acc = \",str(accuracy))\n",
        "                modelFC.train()\n",
        "    if modelFC==None:\n",
        "      avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "      avg_f1_val = totalF1/len(validation_dataloader)\n",
        "      avg_val_accuracy = totalAcc/len(validation_dataloader)\n",
        "    else:\n",
        "       avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "       avg_f1_val = totalF1/len(validation_dataloader)\n",
        "       avg_val_accuracy = totalAcc/len(validation_dataloader)\n",
        "    \n",
        "    \n",
        "    if modelFC==None:\n",
        "       model.bertModel.train()\n",
        "       model.train()\n",
        "    else:\n",
        "       model.bertModel.eval()\n",
        "       model.eval()\n",
        "       modelFC.train()  \n",
        "    return  avg_val_loss,avg_f1_val ,avg_val_accuracy, #avg_val_f1,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6naPNmF4p8r8"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "def mytrainStep1(model,criterion1,criterion2):\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          model.to(device)\n",
        "      # for param in model.bertModel.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      modules = [model.bertModel.decoder,*model.bertModel.encoder.block[:10]]\n",
        "      for module in modules:\n",
        "          for param in module.parameters():\n",
        "              param.requires_grad = False\n",
        "      # optimizer = torch.optim.Adam(model.parameters(),\n",
        "      #                               lr=0.0001)\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=0.0003,#0.003, #5e-5, 3e-5, 2e-5 #0.001 0.00003\n",
        "                                    #weight_decay=1e-5, \n",
        "                                    correct_bias=False) #eps=1e-8,len(train_dataloader)\n",
        "      #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=6*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid = list()\n",
        "      listOfF1Valid = list()\n",
        "      epoch_stop = 0\n",
        "      model.bertModel.train()\n",
        "      model.train()\n",
        "      totalAcc = 0\n",
        "      for epoch_i in range(0, 2):\n",
        "\n",
        "          # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:7]] #Replace 8 by what you want\n",
        "          # for module in modules:\n",
        "          #     for param in module.parameters():\n",
        "          #         param.requires_grad = False\n",
        "          # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[7:]] #Replace 8 by what you want\n",
        "          # for module in modules:\n",
        "          #     for param in module.parameters():\n",
        "          #         param.requires_grad = False\n",
        "          # if epoch_i > 0:\n",
        "\n",
        "          #     for param in model.bertModel.parameters():\n",
        "          #         param.requires_grad = False\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss = 0\n",
        "          # model.bertModel.train()\n",
        "          \n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          # for (input1,mask1, target1),(input2,mask2, target2) in train_dataloader:\n",
        "          accum_iter = 4\n",
        "          # for anchor,anchorMask,replicas,replicasMask in train_dataloader:\n",
        "\n",
        "    # for input1, mask1, input2, mask2,target1 in validation_dataloader:\n",
        "              # step += 1\n",
        "\n",
        "              # b_input_mask1 = anchorMask.to(device)\n",
        "              # b_input_mask2 = replicasMask.to(device)\n",
        "              # labels = torch.arange(0, b_input_mask1.shape[0], device=device)\n",
        "              # b_input_ids1 = anchor.to(device)\n",
        "              # b_input_ids2 = replicas.to(device)\n",
        "          for batch_idx,(input1, mask1, input2, mask2) in  enumerate(train_dataloader):\n",
        "              step+=1\n",
        "              # if step==151:\n",
        "              #    break\n",
        "              # # Progress update every 40 batches.\n",
        "              # h = model.init_hidden(target1.size(0))\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # if torch.cuda.is_available():\n",
        "              b_input_ids1 = input1.to(device)\n",
        "              b_input_mask1 = mask1.to(device)\n",
        "              # target1 = target1.type(torch.FloatTensor)\n",
        "              # target1 = torch.where(target1==0,torch.tensor(-1),target1)\n",
        "            \n",
        "              # b_labels = target1.to(device)\n",
        "              h = model.init_hidden(b_input_ids1.shape[0])\n",
        "              # # b_labels = b_labels.unsqueeze(1)\n",
        "              b_input_ids2 = input2.to(device)\n",
        "              b_input_mask2 = mask2.to(device)\n",
        "              # b_labels=b_labels.reshape(-1,1)\n",
        "              # b_labels = torch.squeeze(b_labels,1)\n",
        "              model.zero_grad()\n",
        "              # optimizer.zero_grad()\n",
        "              # with torch.set_grad_enabled(True):\n",
        "                  #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              # print(a1)\n",
        "              # print(a2)\n",
        "              # FC11,FC22,cos = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels,h)\n",
        "              FC11 = model(b_input_ids1, b_input_mask1,h)             \n",
        "              FC22 = model(b_input_ids2, b_input_mask2,h)\n",
        "              # cos = nn.CosineSimilarity(dim=1)\n",
        "              # output2 = cos(FC11, FC22)\n",
        "              # print(output2)\n",
        "              # print(b_labels)       \n",
        "              # pos_indices = torch.where(b_labels==1)[0]\n",
        "              # neg_indices = torch.where(b_labels==0)[0]\n",
        "              #     # print(pos_indices)\n",
        "              #     # print(neg_indices)\n",
        "              # indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "                  \n",
        "              # loss1 = criterion1(cos,b_labels)\n",
        "              # loss1= criterion1(FC11, labels, ref_emb=FC22, ref_labels=labels)\n",
        "              # labels = torch.cat([b_labels, b_labels], dim=0)      \n",
        "              # loss1 = criterion1(embeddings,b_labels.repeat(2))\n",
        "              loss1,acc = criterion1(FC11,FC22)\n",
        "              # (loss1 / gradient_accumulations).backward()\n",
        "\n",
        "              # if ((batch_idx + 1) % gradient_accumulations == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
        "              #     optimizer.step()\n",
        "              #     model.zero_grad()\n",
        "              # print(output)\n",
        "              # loss1 = criterion1(output, b_labels)\n",
        "              # loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # loss1 = loss1 / accum_iter\n",
        "     \n",
        "              # apply weight decay regularization\n",
        "              \n",
        "              # totalAcc += acc.cpu().item()\n",
        "              # print(totalAcc) \n",
        "              total_train_loss += loss1.item()\n",
        "                  # print(loss.data[0])\n",
        "             \n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 1  Train Loss \"+str(loss1.item()),\"acc =\",str(acc))\n",
        "              # decay = 0.0001\n",
        "              # for param in model.parameters():\n",
        "              #     loss1 += decay * torch.norm(param, p='fro')\n",
        "              loss1.backward()\n",
        "              # if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
        "              # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optimizer.step()\n",
        "                  # optimizer.zero_grad()\n",
        "              #scheduler.step()\n",
        "                      \n",
        "          avg_train_loss = total_train_loss /len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 1 AVG. Train Loss \"+str(avg_train_loss))            \n",
        "          listOflossesTrain.append(avg_train_loss)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss,avgf1,avgAcc = validation(model,epoch_i,criterion1,validation_dataloader)\n",
        "          scheduler.step(avg_val_loss)\n",
        "          listOflossesValid.append(avg_val_loss)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "          print(\"  Average Validation Accuracy: {0:.2f}\".format(avgAcc))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "       \n",
        "          early_stopping1(avg_val_loss, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping1.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              # break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain,listOflossesValid,2)\n",
        "      return listOflossesValid, listOflossesTrain\n",
        "      # mytrainStep2(modelCLS,criterion1,criterion2,model)\n",
        "      # torch.save(model.state_dict(), 'checkPointEmblstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KEoxDILFUB-"
      },
      "outputs": [],
      "source": [
        "class InfoNCE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InfoNCE, self).__init__()\n",
        "        self.t = torch.nn.Parameter(torch.tensor([0.07]))\n",
        "    def sim_matrix(self,a, b, eps=1e-8):\n",
        "        \"\"\"\n",
        "        added eps for numerical stability\n",
        "        \"\"\"\n",
        "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "        return sim_mt.to(device)    \n",
        "    def forward(self, x1, x2):\n",
        "        # if labels is None:\n",
        "        batch_size = x1.shape[0]\n",
        "        similarity_matrix = self.sim_matrix(x1, x2)\n",
        "        # print(similarity_matrix)\n",
        "        diagonal = torch.diagonal(similarity_matrix).unsqueeze(1)#.to(device) #diagonal are the positives\n",
        "        gt_pos = F.pad(torch.LongTensor(), (0,len(diagonal)), \"constant\", 1).to(device)#.unsqueeze(1)#.to(device)\n",
        "        \n",
        "        # gt_pos = torch.cat((1-gt_pos,gt_pos),dim=1).to(device)\n",
        "        negatives = similarity_matrix.flatten()[1:].view(batch_size-1, batch_size+1)[:,:-1].reshape(batch_size, batch_size-1).flatten().unsqueeze(1)#.to(device)\n",
        "        # plt.hist(diagonal.cpu().data.numpy(), bins=16, alpha=0.5, label='same_author')\n",
        "        # plt.hist(negatives.cpu().data.numpy(), bins=16, alpha=0.5, label='diff_author')\n",
        "        # plt.title('Score distributions for same and different author pairs')\n",
        "        # plt.xlabel('score')\n",
        "        # plt.ylabel('count')\n",
        "        # plt.show()\n",
        "        diagonal = torch.cat((1-diagonal,diagonal),dim=1).to(device)\n",
        "        lab_neg = F.pad(torch.LongTensor(), (0,len(negatives)), \"constant\", 0).to(device)#.unsqueeze(1)\n",
        "        negatives = torch.cat((1-torch.relu(negatives),negatives),dim=1)\n",
        "        # negatives = torch.cat((torch.abs(negatives),negatives),dim=1).to(device)\n",
        "        #lab_neg = F.pad(torch.Tensor(), (0,len(negatives)), \"constant\", 0).unsqueeze(1)#.to(device)\n",
        "        # negatives = torch.cat((negatives,1-negatives),dim=1).to(device)\n",
        "        # lab_neg = torch.cat((lab_neg,1-lab_neg),dim=1).to(device)\n",
        "        loss_pos = F.cross_entropy(diagonal, gt_pos).mean()#.to(device)\n",
        "        loss_neg = F.cross_entropy(negatives, lab_neg).mean()#.to(device)\n",
        "        loss = (loss_pos + loss_neg) / 2\n",
        "        with torch.no_grad():\n",
        "            preds_pos = diagonal.argmax(1)#.to(device)\n",
        "            preds_neg = negatives.argmax(1)#.to(device)\n",
        "            accuracy_pos = (torch.sum(preds_pos == gt_pos) )/len(diagonal) #.argmax(1)\n",
        "            accuracy_neg = (torch.sum(preds_neg == lab_neg) )/len(negatives)\n",
        "            accuracy = (accuracy_pos+accuracy_neg)/2\n",
        "        return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbzF-g6eWkf6"
      },
      "outputs": [],
      "source": [
        "class InfoNCE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InfoNCE, self).__init__()\n",
        "        self.t = torch.nn.Parameter(torch.tensor([0.07])).cuda()\n",
        "    def forward(self, x1, x2):\n",
        "        batch_size = x1.shape[0]\n",
        "        logits = (x1 @ x2.T) * torch.exp(self.t).clamp(max=1)\n",
        "        labels = torch.arange(0, batch_size).cuda()\n",
        "    \n",
        "        loss = (F.cross_entropy(logits.T, labels).mean() + F.cross_entropy(logits, labels).mean()) / 2\n",
        "        with torch.no_grad():\n",
        "            preds = F.softmax(logits, dim=1).argmax(-1)\n",
        "            preds_t = F.softmax(logits.T, dim=1).argmax(-1)\n",
        "\n",
        "            accuracy = (torch.sum(preds == labels) + torch.sum(preds_t == labels)) / (batch_size * 2)\n",
        "\n",
        "        return loss, accuracy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3iacYVJCBjG"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin_pos=1.0, margin_neg=0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin_pos = margin_pos\n",
        "        self.margin_neg = margin_neg\n",
        "    def sim_matrix(self,a, b, eps=1e-8):\n",
        "        \"\"\"\n",
        "        added eps for numerical stability\n",
        "        \"\"\"\n",
        "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "        return sim_mt.to(device)  \n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        batch_size = x1.shape[0]\n",
        "        similarity_matrix = self.sim_matrix(x1, x2)\n",
        "        # print(similarity_matrix)\n",
        "        diagonal = torch.diagonal(similarity_matrix)#.to(device) #diagonal are the positives\n",
        "        gt_pos = F.pad(torch.LongTensor(), (0,len(diagonal)), \"constant\", 1).to(device)#.unsqueeze(1)#.to(device)\n",
        "        \n",
        "        # gt_pos = torch.cat((1-gt_pos,gt_pos),dim=1).to(device)\n",
        "        negatives = similarity_matrix.flatten()[1:].view(batch_size-1, batch_size+1)[:,:-1].reshape(batch_size, batch_size-1).flatten()#.to(device)\n",
        "        # print(negatives)\n",
        "        lab_neg = F.pad(torch.LongTensor(), (0,len(negatives)), \"constant\", 0).to(device)\n",
        "        loss_contrastive_pos =torch.mean(torch.pow(self.margin_pos - diagonal, 2))\n",
        "        loss_contrastive_neg =torch.mean(torch.pow(torch.relu(negatives - self.margin_neg), 2))\n",
        "        print(\"pos\",str(loss_contrastive_pos))\n",
        "        print(\"neg\",str(loss_contrastive_neg))\n",
        "        if torch.isnan(loss_contrastive_neg) == False:  \n",
        "          loss_contrastive = (loss_contrastive_pos + loss_contrastive_neg)/2\n",
        "        else:\n",
        "          loss_contrastive = (loss_contrastive_pos + 0.0)/2\n",
        "        # print(loss_contrastive_pos)\n",
        "        # print(loss_contrastive_neg)\n",
        "        return loss_contrastive,0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjIwlSi1e1xO"
      },
      "source": [
        "### start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bASZdkfkLbM1"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning.reducers import MeanReducer\n",
        "modelEmb = myModelEmbeddings(bert_emb_layer=10,startLayer=6,endLayer=10,bertModel=bertModel)\n",
        "\n",
        "# criterion1 = nn.CosineEmbeddingLoss(margin = 0.5)#margin = 0.5\n",
        "# func = distances.LpDistance\n",
        "# criterion1 = ContrastiveLossSiamese(margin1 = 0.9)\n",
        "# criterion1 = getLossFunction(device)\n",
        "# criterion1 =nn.BCELoss()\n",
        "#reducer=MeanReducer()\n",
        "criterion1 = ContrastiveLoss()\n",
        "# modelEmb.load_state_dict(torch.load('/content/drive/My Drive/Thesis/PAN20/checkpointEmbuncased_PAN20_35k_CL.pt'))#/content/drive/My Drive/Thesis/PAN20/checkpointEmbv2.pt'))\n",
        "# criterion1 = InfoNCE()\n",
        "# modelEmb.cuda()\n",
        "# criterion1 = losses.ContrastiveLoss(distance = distances.CosineSimilarity(),reducer=MeanReducer(),pos_margin=0.92, neg_margin=-0.1) #distance = distances.CosineSimilarity()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "early_stopping1 = EarlyStopping(patience=4, path='checkpointEmbT5_PAN22_mem-ess.pt',verbose=True)\n",
        "early_stopping2 = EarlyStopping(patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnBFRn40Lizh",
        "outputId": "3f4fcb74-0036-4a27-e7a3-3bedbb5e7c75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
            "========== Epoch 1 Batch 4227==== Step 1  Train Loss 0.2628752589225769 acc = 0.0\n",
            "pos tensor(0.1883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4228==== Step 1  Train Loss 0.25643670558929443 acc = 0.0\n",
            "pos tensor(0.1707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4229==== Step 1  Train Loss 0.2551765441894531 acc = 0.0\n",
            "pos tensor(0.1635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4230==== Step 1  Train Loss 0.25090286135673523 acc = 0.0\n",
            "pos tensor(0.1765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4231==== Step 1  Train Loss 0.2605959475040436 acc = 0.0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4232==== Step 1  Train Loss 0.26095718145370483 acc = 0.0\n",
            "pos tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4233==== Step 1  Train Loss 0.26206353306770325 acc = 0.0\n",
            "pos tensor(0.2001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4234==== Step 1  Train Loss 0.27423423528671265 acc = 0.0\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4235==== Step 1  Train Loss 0.26999568939208984 acc = 0.0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4236==== Step 1  Train Loss 0.26757609844207764 acc = 0.0\n",
            "pos tensor(0.2135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4237==== Step 1  Train Loss 0.2759858965873718 acc = 0.0\n",
            "pos tensor(0.2045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4238==== Step 1  Train Loss 0.2719254195690155 acc = 0.0\n",
            "pos tensor(0.1583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4239==== Step 1  Train Loss 0.25135475397109985 acc = 0.0\n",
            "pos tensor(0.1732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4240==== Step 1  Train Loss 0.2568729519844055 acc = 0.0\n",
            "pos tensor(0.1731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4241==== Step 1  Train Loss 0.25451838970184326 acc = 0.0\n",
            "pos tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4242==== Step 1  Train Loss 0.2569742202758789 acc = 0.0\n",
            "pos tensor(0.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4243==== Step 1  Train Loss 0.2635105848312378 acc = 0.0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4244==== Step 1  Train Loss 0.2602003812789917 acc = 0.0\n",
            "pos tensor(0.2172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4245==== Step 1  Train Loss 0.2689659595489502 acc = 0.0\n",
            "pos tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4246==== Step 1  Train Loss 0.2636092007160187 acc = 0.0\n",
            "pos tensor(0.1851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4247==== Step 1  Train Loss 0.2604399621486664 acc = 0.0\n",
            "pos tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4248==== Step 1  Train Loss 0.25358259677886963 acc = 0.0\n",
            "pos tensor(0.1916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4249==== Step 1  Train Loss 0.25745293498039246 acc = 0.0\n",
            "pos tensor(0.1912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4250==== Step 1  Train Loss 0.2601983845233917 acc = 0.0\n",
            "pos tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4251==== Step 1  Train Loss 0.2602759897708893 acc = 0.0\n",
            "pos tensor(0.2146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4252==== Step 1  Train Loss 0.2787732481956482 acc = 0.0\n",
            "pos tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4253==== Step 1  Train Loss 0.25416895747184753 acc = 0.0\n",
            "pos tensor(0.1904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4254==== Step 1  Train Loss 0.2719641625881195 acc = 0.0\n",
            "pos tensor(0.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4255==== Step 1  Train Loss 0.2668370008468628 acc = 0.0\n",
            "pos tensor(0.1626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4256==== Step 1  Train Loss 0.2591853141784668 acc = 0.0\n",
            "pos tensor(0.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4257==== Step 1  Train Loss 0.2464992254972458 acc = 0.0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4258==== Step 1  Train Loss 0.2708175778388977 acc = 0.0\n",
            "pos tensor(0.2141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4259==== Step 1  Train Loss 0.2696203291416168 acc = 0.0\n",
            "pos tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4260==== Step 1  Train Loss 0.26317253708839417 acc = 0.0\n",
            "pos tensor(0.2156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4261==== Step 1  Train Loss 0.2794197201728821 acc = 0.0\n",
            "pos tensor(0.1760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4262==== Step 1  Train Loss 0.263679563999176 acc = 0.0\n",
            "pos tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4263==== Step 1  Train Loss 0.24765248596668243 acc = 0.0\n",
            "pos tensor(0.1810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4264==== Step 1  Train Loss 0.2632172107696533 acc = 0.0\n",
            "pos tensor(0.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4265==== Step 1  Train Loss 0.27055323123931885 acc = 0.0\n",
            "pos tensor(0.1743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4266==== Step 1  Train Loss 0.25994816422462463 acc = 0.0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4267==== Step 1  Train Loss 0.26830050349235535 acc = 0.0\n",
            "pos tensor(0.1596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4268==== Step 1  Train Loss 0.25926369428634644 acc = 0.0\n",
            "pos tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4269==== Step 1  Train Loss 0.2618292570114136 acc = 0.0\n",
            "pos tensor(0.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4270==== Step 1  Train Loss 0.25284868478775024 acc = 0.0\n",
            "pos tensor(0.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4271==== Step 1  Train Loss 0.2646893858909607 acc = 0.0\n",
            "pos tensor(0.1533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4272==== Step 1  Train Loss 0.2548168897628784 acc = 0.0\n",
            "pos tensor(0.1600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4273==== Step 1  Train Loss 0.2548435628414154 acc = 0.0\n",
            "pos tensor(0.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4274==== Step 1  Train Loss 0.27015700936317444 acc = 0.0\n",
            "pos tensor(0.1777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4275==== Step 1  Train Loss 0.2652243375778198 acc = 0.0\n",
            "pos tensor(0.2080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4276==== Step 1  Train Loss 0.26306501030921936 acc = 0.0\n",
            "pos tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4277==== Step 1  Train Loss 0.27183932065963745 acc = 0.0\n",
            "pos tensor(0.1931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4278==== Step 1  Train Loss 0.27022019028663635 acc = 0.0\n",
            "pos tensor(0.2136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4279==== Step 1  Train Loss 0.2657649517059326 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4280==== Step 1  Train Loss 0.26278722286224365 acc = 0.0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4281==== Step 1  Train Loss 0.26080793142318726 acc = 0.0\n",
            "pos tensor(0.2022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4282==== Step 1  Train Loss 0.26808595657348633 acc = 0.0\n",
            "pos tensor(0.1744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4283==== Step 1  Train Loss 0.2636275291442871 acc = 0.0\n",
            "pos tensor(0.1626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4284==== Step 1  Train Loss 0.2642548978328705 acc = 0.0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4285==== Step 1  Train Loss 0.2545120120048523 acc = 0.0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4286==== Step 1  Train Loss 0.26199352741241455 acc = 0.0\n",
            "pos tensor(0.1842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4287==== Step 1  Train Loss 0.2606734037399292 acc = 0.0\n",
            "pos tensor(0.2218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4288==== Step 1  Train Loss 0.27025607228279114 acc = 0.0\n",
            "pos tensor(0.1653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4289==== Step 1  Train Loss 0.2588913142681122 acc = 0.0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4290==== Step 1  Train Loss 0.2675577998161316 acc = 0.0\n",
            "pos tensor(0.1817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4291==== Step 1  Train Loss 0.268835186958313 acc = 0.0\n",
            "pos tensor(0.1649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4292==== Step 1  Train Loss 0.24690786004066467 acc = 0.0\n",
            "pos tensor(0.1976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4293==== Step 1  Train Loss 0.2662084698677063 acc = 0.0\n",
            "pos tensor(0.1834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4294==== Step 1  Train Loss 0.2611602544784546 acc = 0.0\n",
            "pos tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4295==== Step 1  Train Loss 0.27035343647003174 acc = 0.0\n",
            "pos tensor(0.1829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4296==== Step 1  Train Loss 0.2675355076789856 acc = 0.0\n",
            "pos tensor(0.1905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4297==== Step 1  Train Loss 0.26555556058883667 acc = 0.0\n",
            "pos tensor(0.1817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4298==== Step 1  Train Loss 0.2571573257446289 acc = 0.0\n",
            "pos tensor(0.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4299==== Step 1  Train Loss 0.272882342338562 acc = 0.0\n",
            "  Batch 4,300  of  5,204.    Elapsed: 0:55:30.\n",
            "pos tensor(0.2150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4300==== Step 1  Train Loss 0.2678830325603485 acc = 0.0\n",
            "pos tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4301==== Step 1  Train Loss 0.2719578742980957 acc = 0.0\n",
            "pos tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4302==== Step 1  Train Loss 0.2483142763376236 acc = 0.0\n",
            "pos tensor(0.1974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4303==== Step 1  Train Loss 0.2660192847251892 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4304==== Step 1  Train Loss 0.27306562662124634 acc = 0.0\n",
            "pos tensor(0.1912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4305==== Step 1  Train Loss 0.2688877582550049 acc = 0.0\n",
            "pos tensor(0.1915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4306==== Step 1  Train Loss 0.26001113653182983 acc = 0.0\n",
            "pos tensor(0.1816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4307==== Step 1  Train Loss 0.26107338070869446 acc = 0.0\n",
            "pos tensor(0.1772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4308==== Step 1  Train Loss 0.2729991674423218 acc = 0.0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4309==== Step 1  Train Loss 0.26518863439559937 acc = 0.0\n",
            "pos tensor(0.1995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4310==== Step 1  Train Loss 0.263967901468277 acc = 0.0\n",
            "pos tensor(0.1666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4311==== Step 1  Train Loss 0.2513166069984436 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4312==== Step 1  Train Loss 0.25883930921554565 acc = 0.0\n",
            "pos tensor(0.2050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4313==== Step 1  Train Loss 0.27042144536972046 acc = 0.0\n",
            "pos tensor(0.1596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4314==== Step 1  Train Loss 0.2475111037492752 acc = 0.0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4315==== Step 1  Train Loss 0.2662724256515503 acc = 0.0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4316==== Step 1  Train Loss 0.26312288641929626 acc = 0.0\n",
            "pos tensor(0.1542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4317==== Step 1  Train Loss 0.25488191843032837 acc = 0.0\n",
            "pos tensor(0.1804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4318==== Step 1  Train Loss 0.2595101594924927 acc = 0.0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4319==== Step 1  Train Loss 0.26348087191581726 acc = 0.0\n",
            "pos tensor(0.1938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4320==== Step 1  Train Loss 0.25308462977409363 acc = 0.0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4321==== Step 1  Train Loss 0.2628403604030609 acc = 0.0\n",
            "pos tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4322==== Step 1  Train Loss 0.2534898817539215 acc = 0.0\n",
            "pos tensor(0.2063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4323==== Step 1  Train Loss 0.2687997817993164 acc = 0.0\n",
            "pos tensor(0.1846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4324==== Step 1  Train Loss 0.26019495725631714 acc = 0.0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4325==== Step 1  Train Loss 0.2538605332374573 acc = 0.0\n",
            "pos tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4326==== Step 1  Train Loss 0.2581574618816376 acc = 0.0\n",
            "pos tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4327==== Step 1  Train Loss 0.25956881046295166 acc = 0.0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4328==== Step 1  Train Loss 0.26154249906539917 acc = 0.0\n",
            "pos tensor(0.1663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4329==== Step 1  Train Loss 0.2530965209007263 acc = 0.0\n",
            "pos tensor(0.2006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4330==== Step 1  Train Loss 0.2680639922618866 acc = 0.0\n",
            "pos tensor(0.1616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4331==== Step 1  Train Loss 0.2526039779186249 acc = 0.0\n",
            "pos tensor(0.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4332==== Step 1  Train Loss 0.2402041256427765 acc = 0.0\n",
            "pos tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4333==== Step 1  Train Loss 0.2626863121986389 acc = 0.0\n",
            "pos tensor(0.1883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4334==== Step 1  Train Loss 0.2717726528644562 acc = 0.0\n",
            "pos tensor(0.1951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4335==== Step 1  Train Loss 0.2660186290740967 acc = 0.0\n",
            "pos tensor(0.1726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4336==== Step 1  Train Loss 0.2648307681083679 acc = 0.0\n",
            "pos tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4337==== Step 1  Train Loss 0.25917187333106995 acc = 0.0\n",
            "pos tensor(0.1568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4338==== Step 1  Train Loss 0.26812252402305603 acc = 0.0\n",
            "pos tensor(0.1997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4339==== Step 1  Train Loss 0.2607732117176056 acc = 0.0\n",
            "pos tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4340==== Step 1  Train Loss 0.2661876678466797 acc = 0.0\n",
            "pos tensor(0.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4341==== Step 1  Train Loss 0.2565307915210724 acc = 0.0\n",
            "pos tensor(0.1571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4342==== Step 1  Train Loss 0.253276526927948 acc = 0.0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4343==== Step 1  Train Loss 0.2690340280532837 acc = 0.0\n",
            "pos tensor(0.1623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4344==== Step 1  Train Loss 0.24905481934547424 acc = 0.0\n",
            "pos tensor(0.1812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4345==== Step 1  Train Loss 0.27194759249687195 acc = 0.0\n",
            "pos tensor(0.1738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4346==== Step 1  Train Loss 0.2621363699436188 acc = 0.0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4347==== Step 1  Train Loss 0.2622014880180359 acc = 0.0\n",
            "pos tensor(0.2321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4348==== Step 1  Train Loss 0.26998287439346313 acc = 0.0\n",
            "pos tensor(0.1835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4349==== Step 1  Train Loss 0.26401853561401367 acc = 0.0\n",
            "pos tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4350==== Step 1  Train Loss 0.26532769203186035 acc = 0.0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4351==== Step 1  Train Loss 0.27417290210723877 acc = 0.0\n",
            "pos tensor(0.1704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4352==== Step 1  Train Loss 0.25857168436050415 acc = 0.0\n",
            "pos tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4353==== Step 1  Train Loss 0.2671399712562561 acc = 0.0\n",
            "pos tensor(0.2079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4354==== Step 1  Train Loss 0.2579915523529053 acc = 0.0\n",
            "pos tensor(0.2013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4355==== Step 1  Train Loss 0.26814544200897217 acc = 0.0\n",
            "pos tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4356==== Step 1  Train Loss 0.2649203836917877 acc = 0.0\n",
            "pos tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4357==== Step 1  Train Loss 0.25636035203933716 acc = 0.0\n",
            "pos tensor(0.1830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4358==== Step 1  Train Loss 0.2653754651546478 acc = 0.0\n",
            "pos tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4359==== Step 1  Train Loss 0.262233167886734 acc = 0.0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4360==== Step 1  Train Loss 0.26953262090682983 acc = 0.0\n",
            "pos tensor(0.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4361==== Step 1  Train Loss 0.25477123260498047 acc = 0.0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4362==== Step 1  Train Loss 0.2609480023384094 acc = 0.0\n",
            "pos tensor(0.1800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4363==== Step 1  Train Loss 0.2615290582180023 acc = 0.0\n",
            "pos tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4364==== Step 1  Train Loss 0.2791547179222107 acc = 0.0\n",
            "pos tensor(0.2114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4365==== Step 1  Train Loss 0.2648380398750305 acc = 0.0\n",
            "pos tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4366==== Step 1  Train Loss 0.2625281512737274 acc = 0.0\n",
            "pos tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4367==== Step 1  Train Loss 0.26749879121780396 acc = 0.0\n",
            "pos tensor(0.1811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4368==== Step 1  Train Loss 0.2672897279262543 acc = 0.0\n",
            "pos tensor(0.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4369==== Step 1  Train Loss 0.2709634006023407 acc = 0.0\n",
            "pos tensor(0.1827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4370==== Step 1  Train Loss 0.26723888516426086 acc = 0.0\n",
            "pos tensor(0.1645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4371==== Step 1  Train Loss 0.2614983916282654 acc = 0.0\n",
            "pos tensor(0.2001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4372==== Step 1  Train Loss 0.25751030445098877 acc = 0.0\n",
            "pos tensor(0.1610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4373==== Step 1  Train Loss 0.261759877204895 acc = 0.0\n",
            "pos tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4374==== Step 1  Train Loss 0.26459282636642456 acc = 0.0\n",
            "pos tensor(0.2106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4375==== Step 1  Train Loss 0.2719478905200958 acc = 0.0\n",
            "pos tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4376==== Step 1  Train Loss 0.2575017213821411 acc = 0.0\n",
            "pos tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4377==== Step 1  Train Loss 0.2711869180202484 acc = 0.0\n",
            "pos tensor(0.1703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4378==== Step 1  Train Loss 0.265379935503006 acc = 0.0\n",
            "pos tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4379==== Step 1  Train Loss 0.2636147737503052 acc = 0.0\n",
            "pos tensor(0.1768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4380==== Step 1  Train Loss 0.25972503423690796 acc = 0.0\n",
            "pos tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4381==== Step 1  Train Loss 0.26558512449264526 acc = 0.0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4382==== Step 1  Train Loss 0.27182537317276 acc = 0.0\n",
            "pos tensor(0.1455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4383==== Step 1  Train Loss 0.27933022379875183 acc = 0.0\n",
            "pos tensor(0.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4384==== Step 1  Train Loss 0.2569974958896637 acc = 0.0\n",
            "pos tensor(0.1850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4385==== Step 1  Train Loss 0.2753109037876129 acc = 0.0\n",
            "pos tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4386==== Step 1  Train Loss 0.2707587778568268 acc = 0.0\n",
            "pos tensor(0.2098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4387==== Step 1  Train Loss 0.2672046720981598 acc = 0.0\n",
            "pos tensor(0.2104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4388==== Step 1  Train Loss 0.27402031421661377 acc = 0.0\n",
            "pos tensor(0.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4389==== Step 1  Train Loss 0.2602456510066986 acc = 0.0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4390==== Step 1  Train Loss 0.2625463604927063 acc = 0.0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4391==== Step 1  Train Loss 0.26198822259902954 acc = 0.0\n",
            "pos tensor(0.1773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4392==== Step 1  Train Loss 0.26536503434181213 acc = 0.0\n",
            "pos tensor(0.1684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4393==== Step 1  Train Loss 0.257651686668396 acc = 0.0\n",
            "pos tensor(0.2066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4394==== Step 1  Train Loss 0.2773841917514801 acc = 0.0\n",
            "pos tensor(0.1629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4395==== Step 1  Train Loss 0.2548835575580597 acc = 0.0\n",
            "pos tensor(0.1773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4396==== Step 1  Train Loss 0.2654748558998108 acc = 0.0\n",
            "pos tensor(0.2262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4397==== Step 1  Train Loss 0.2687323987483978 acc = 0.0\n",
            "pos tensor(0.2037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4398==== Step 1  Train Loss 0.2591232657432556 acc = 0.0\n",
            "pos tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4399==== Step 1  Train Loss 0.263159841299057 acc = 0.0\n",
            "  Batch 4,400  of  5,204.    Elapsed: 0:56:47.\n",
            "pos tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4400==== Step 1  Train Loss 0.25780633091926575 acc = 0.0\n",
            "pos tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4401==== Step 1  Train Loss 0.26000192761421204 acc = 0.0\n",
            "pos tensor(0.2047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4402==== Step 1  Train Loss 0.2670125365257263 acc = 0.0\n",
            "pos tensor(0.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4403==== Step 1  Train Loss 0.2630283534526825 acc = 0.0\n",
            "pos tensor(0.1580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4404==== Step 1  Train Loss 0.24868710339069366 acc = 0.0\n",
            "pos tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4405==== Step 1  Train Loss 0.25586360692977905 acc = 0.0\n",
            "pos tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4406==== Step 1  Train Loss 0.2654435336589813 acc = 0.0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4407==== Step 1  Train Loss 0.2679257094860077 acc = 0.0\n",
            "pos tensor(0.1755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4408==== Step 1  Train Loss 0.2510586380958557 acc = 0.0\n",
            "pos tensor(0.1517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4409==== Step 1  Train Loss 0.26126623153686523 acc = 0.0\n",
            "pos tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4410==== Step 1  Train Loss 0.2716503143310547 acc = 0.0\n",
            "pos tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4411==== Step 1  Train Loss 0.2689860463142395 acc = 0.0\n",
            "pos tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4412==== Step 1  Train Loss 0.25366199016571045 acc = 0.0\n",
            "pos tensor(0.2354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4413==== Step 1  Train Loss 0.27093708515167236 acc = 0.0\n",
            "pos tensor(0.2122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4414==== Step 1  Train Loss 0.2689923048019409 acc = 0.0\n",
            "pos tensor(0.1744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4415==== Step 1  Train Loss 0.24157395958900452 acc = 0.0\n",
            "pos tensor(0.1909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4416==== Step 1  Train Loss 0.2569899559020996 acc = 0.0\n",
            "pos tensor(0.2211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4417==== Step 1  Train Loss 0.26948612928390503 acc = 0.0\n",
            "pos tensor(0.1883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4418==== Step 1  Train Loss 0.26161128282546997 acc = 0.0\n",
            "pos tensor(0.2098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4419==== Step 1  Train Loss 0.2589624524116516 acc = 0.0\n",
            "pos tensor(0.1878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4420==== Step 1  Train Loss 0.25500723719596863 acc = 0.0\n",
            "pos tensor(0.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4421==== Step 1  Train Loss 0.24869704246520996 acc = 0.0\n",
            "pos tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4422==== Step 1  Train Loss 0.26952874660491943 acc = 0.0\n",
            "pos tensor(0.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4423==== Step 1  Train Loss 0.26136595010757446 acc = 0.0\n",
            "pos tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4424==== Step 1  Train Loss 0.25817152857780457 acc = 0.0\n",
            "pos tensor(0.1909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4425==== Step 1  Train Loss 0.2603793442249298 acc = 0.0\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4426==== Step 1  Train Loss 0.2587593197822571 acc = 0.0\n",
            "pos tensor(0.1431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4427==== Step 1  Train Loss 0.24674499034881592 acc = 0.0\n",
            "pos tensor(0.1808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4428==== Step 1  Train Loss 0.2573712468147278 acc = 0.0\n",
            "pos tensor(0.1882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4429==== Step 1  Train Loss 0.2737894058227539 acc = 0.0\n",
            "pos tensor(0.1757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4430==== Step 1  Train Loss 0.2716597616672516 acc = 0.0\n",
            "pos tensor(0.1830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4431==== Step 1  Train Loss 0.26977258920669556 acc = 0.0\n",
            "pos tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4432==== Step 1  Train Loss 0.2741033136844635 acc = 0.0\n",
            "pos tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4433==== Step 1  Train Loss 0.25329476594924927 acc = 0.0\n",
            "pos tensor(0.1712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4434==== Step 1  Train Loss 0.25577878952026367 acc = 0.0\n",
            "pos tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4435==== Step 1  Train Loss 0.27142333984375 acc = 0.0\n",
            "pos tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4436==== Step 1  Train Loss 0.25811609625816345 acc = 0.0\n",
            "pos tensor(0.1726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4437==== Step 1  Train Loss 0.2593221068382263 acc = 0.0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4438==== Step 1  Train Loss 0.2561672329902649 acc = 0.0\n",
            "pos tensor(0.1825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4439==== Step 1  Train Loss 0.2619348168373108 acc = 0.0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4440==== Step 1  Train Loss 0.24539132416248322 acc = 0.0\n",
            "pos tensor(0.2019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4441==== Step 1  Train Loss 0.2659560739994049 acc = 0.0\n",
            "pos tensor(0.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4442==== Step 1  Train Loss 0.26152363419532776 acc = 0.0\n",
            "pos tensor(0.1905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4443==== Step 1  Train Loss 0.26394060254096985 acc = 0.0\n",
            "pos tensor(0.1919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4444==== Step 1  Train Loss 0.24967074394226074 acc = 0.0\n",
            "pos tensor(0.1920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4445==== Step 1  Train Loss 0.2651907205581665 acc = 0.0\n",
            "pos tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4446==== Step 1  Train Loss 0.2512759268283844 acc = 0.0\n",
            "pos tensor(0.1904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4447==== Step 1  Train Loss 0.2615421414375305 acc = 0.0\n",
            "pos tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4448==== Step 1  Train Loss 0.2597562074661255 acc = 0.0\n",
            "pos tensor(0.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4449==== Step 1  Train Loss 0.26160839200019836 acc = 0.0\n",
            "pos tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4450==== Step 1  Train Loss 0.2624538540840149 acc = 0.0\n",
            "pos tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4451==== Step 1  Train Loss 0.2714856266975403 acc = 0.0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4452==== Step 1  Train Loss 0.2521223723888397 acc = 0.0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4453==== Step 1  Train Loss 0.24941785633563995 acc = 0.0\n",
            "pos tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4454==== Step 1  Train Loss 0.25673040747642517 acc = 0.0\n",
            "pos tensor(0.2451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4455==== Step 1  Train Loss 0.27333295345306396 acc = 0.0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4456==== Step 1  Train Loss 0.2612166404724121 acc = 0.0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4457==== Step 1  Train Loss 0.2600136399269104 acc = 0.0\n",
            "pos tensor(0.1864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4458==== Step 1  Train Loss 0.25835537910461426 acc = 0.0\n",
            "pos tensor(0.2124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4459==== Step 1  Train Loss 0.2625081539154053 acc = 0.0\n",
            "pos tensor(0.1813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4460==== Step 1  Train Loss 0.25259578227996826 acc = 0.0\n",
            "pos tensor(0.1749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4461==== Step 1  Train Loss 0.25202101469039917 acc = 0.0\n",
            "pos tensor(0.2189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4462==== Step 1  Train Loss 0.2694547772407532 acc = 0.0\n",
            "pos tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4463==== Step 1  Train Loss 0.25378650426864624 acc = 0.0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4464==== Step 1  Train Loss 0.26252809166908264 acc = 0.0\n",
            "pos tensor(0.1920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4465==== Step 1  Train Loss 0.26131772994995117 acc = 0.0\n",
            "pos tensor(0.1990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4466==== Step 1  Train Loss 0.26319634914398193 acc = 0.0\n",
            "pos tensor(0.2264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4467==== Step 1  Train Loss 0.2664683759212494 acc = 0.0\n",
            "pos tensor(0.1761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4468==== Step 1  Train Loss 0.25084802508354187 acc = 0.0\n",
            "pos tensor(0.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4469==== Step 1  Train Loss 0.2569623589515686 acc = 0.0\n",
            "pos tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4470==== Step 1  Train Loss 0.25063756108283997 acc = 0.0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4471==== Step 1  Train Loss 0.26800432801246643 acc = 0.0\n",
            "pos tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4472==== Step 1  Train Loss 0.24627019464969635 acc = 0.0\n",
            "pos tensor(0.2176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4473==== Step 1  Train Loss 0.2678324580192566 acc = 0.0\n",
            "pos tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4474==== Step 1  Train Loss 0.27102816104888916 acc = 0.0\n",
            "pos tensor(0.2008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4475==== Step 1  Train Loss 0.26368242502212524 acc = 0.0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4476==== Step 1  Train Loss 0.2645626962184906 acc = 0.0\n",
            "pos tensor(0.2055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4477==== Step 1  Train Loss 0.26635777950286865 acc = 0.0\n",
            "pos tensor(0.2144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4478==== Step 1  Train Loss 0.27130061388015747 acc = 0.0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4479==== Step 1  Train Loss 0.26071518659591675 acc = 0.0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4480==== Step 1  Train Loss 0.2493046224117279 acc = 0.0\n",
            "pos tensor(0.2273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4481==== Step 1  Train Loss 0.2648641765117645 acc = 0.0\n",
            "pos tensor(0.1764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4482==== Step 1  Train Loss 0.25623100996017456 acc = 0.0\n",
            "pos tensor(0.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4483==== Step 1  Train Loss 0.25621113181114197 acc = 0.0\n",
            "pos tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4484==== Step 1  Train Loss 0.2630949020385742 acc = 0.0\n",
            "pos tensor(0.2306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4485==== Step 1  Train Loss 0.26955318450927734 acc = 0.0\n",
            "pos tensor(0.1794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4486==== Step 1  Train Loss 0.259081095457077 acc = 0.0\n",
            "pos tensor(0.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4487==== Step 1  Train Loss 0.26104891300201416 acc = 0.0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4488==== Step 1  Train Loss 0.27466756105422974 acc = 0.0\n",
            "pos tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4489==== Step 1  Train Loss 0.26425105333328247 acc = 0.0\n",
            "pos tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4490==== Step 1  Train Loss 0.26164209842681885 acc = 0.0\n",
            "pos tensor(0.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4491==== Step 1  Train Loss 0.24971935153007507 acc = 0.0\n",
            "pos tensor(0.1938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4492==== Step 1  Train Loss 0.263821542263031 acc = 0.0\n",
            "pos tensor(0.1850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4493==== Step 1  Train Loss 0.27160945534706116 acc = 0.0\n",
            "pos tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4494==== Step 1  Train Loss 0.2554853558540344 acc = 0.0\n",
            "pos tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4495==== Step 1  Train Loss 0.26306164264678955 acc = 0.0\n",
            "pos tensor(0.2115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4496==== Step 1  Train Loss 0.26246702671051025 acc = 0.0\n",
            "pos tensor(0.1884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4497==== Step 1  Train Loss 0.2606121301651001 acc = 0.0\n",
            "pos tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4498==== Step 1  Train Loss 0.2658659815788269 acc = 0.0\n",
            "pos tensor(0.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4499==== Step 1  Train Loss 0.263284832239151 acc = 0.0\n",
            "  Batch 4,500  of  5,204.    Elapsed: 0:58:03.\n",
            "pos tensor(0.1734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4500==== Step 1  Train Loss 0.2657935321331024 acc = 0.0\n",
            "pos tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4501==== Step 1  Train Loss 0.2637038826942444 acc = 0.0\n",
            "pos tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4502==== Step 1  Train Loss 0.26974064111709595 acc = 0.0\n",
            "pos tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4503==== Step 1  Train Loss 0.25678446888923645 acc = 0.0\n",
            "pos tensor(0.1846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4504==== Step 1  Train Loss 0.2573954463005066 acc = 0.0\n",
            "pos tensor(0.1839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4505==== Step 1  Train Loss 0.25857022404670715 acc = 0.0\n",
            "pos tensor(0.1646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4506==== Step 1  Train Loss 0.2583830654621124 acc = 0.0\n",
            "pos tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4507==== Step 1  Train Loss 0.2582623362541199 acc = 0.0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4508==== Step 1  Train Loss 0.26567476987838745 acc = 0.0\n",
            "pos tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4509==== Step 1  Train Loss 0.25428783893585205 acc = 0.0\n",
            "pos tensor(0.2054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4510==== Step 1  Train Loss 0.2675018012523651 acc = 0.0\n",
            "pos tensor(0.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4511==== Step 1  Train Loss 0.2651144862174988 acc = 0.0\n",
            "pos tensor(0.1715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4512==== Step 1  Train Loss 0.255389004945755 acc = 0.0\n",
            "pos tensor(0.2214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4513==== Step 1  Train Loss 0.27139922976493835 acc = 0.0\n",
            "pos tensor(0.1941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4514==== Step 1  Train Loss 0.2681948244571686 acc = 0.0\n",
            "pos tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4515==== Step 1  Train Loss 0.2546168565750122 acc = 0.0\n",
            "pos tensor(0.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4516==== Step 1  Train Loss 0.2605009078979492 acc = 0.0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4517==== Step 1  Train Loss 0.27441614866256714 acc = 0.0\n",
            "pos tensor(0.1866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4518==== Step 1  Train Loss 0.2607939541339874 acc = 0.0\n",
            "pos tensor(0.1789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4519==== Step 1  Train Loss 0.26360195875167847 acc = 0.0\n",
            "pos tensor(0.2040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4520==== Step 1  Train Loss 0.26560431718826294 acc = 0.0\n",
            "pos tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4521==== Step 1  Train Loss 0.26057925820350647 acc = 0.0\n",
            "pos tensor(0.1999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4522==== Step 1  Train Loss 0.25883814692497253 acc = 0.0\n",
            "pos tensor(0.1867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4523==== Step 1  Train Loss 0.2563634216785431 acc = 0.0\n",
            "pos tensor(0.2041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4524==== Step 1  Train Loss 0.2717806100845337 acc = 0.0\n",
            "pos tensor(0.1579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4525==== Step 1  Train Loss 0.2480504810810089 acc = 0.0\n",
            "pos tensor(0.1904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4526==== Step 1  Train Loss 0.2538069784641266 acc = 0.0\n",
            "pos tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4527==== Step 1  Train Loss 0.26085755228996277 acc = 0.0\n",
            "pos tensor(0.1579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4528==== Step 1  Train Loss 0.25675344467163086 acc = 0.0\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4529==== Step 1  Train Loss 0.25860100984573364 acc = 0.0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4530==== Step 1  Train Loss 0.2539004683494568 acc = 0.0\n",
            "pos tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4531==== Step 1  Train Loss 0.25364625453948975 acc = 0.0\n",
            "pos tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4532==== Step 1  Train Loss 0.2640620768070221 acc = 0.0\n",
            "pos tensor(0.2029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4533==== Step 1  Train Loss 0.25906893610954285 acc = 0.0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4534==== Step 1  Train Loss 0.2665441632270813 acc = 0.0\n",
            "pos tensor(0.2005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4535==== Step 1  Train Loss 0.2575480341911316 acc = 0.0\n",
            "pos tensor(0.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4536==== Step 1  Train Loss 0.2689669132232666 acc = 0.0\n",
            "pos tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4537==== Step 1  Train Loss 0.2503274083137512 acc = 0.0\n",
            "pos tensor(0.2011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4538==== Step 1  Train Loss 0.26175621151924133 acc = 0.0\n",
            "pos tensor(0.2041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4539==== Step 1  Train Loss 0.2580767869949341 acc = 0.0\n",
            "pos tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4540==== Step 1  Train Loss 0.2699974477291107 acc = 0.0\n",
            "pos tensor(0.2098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4541==== Step 1  Train Loss 0.25601786375045776 acc = 0.0\n",
            "pos tensor(0.2408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4542==== Step 1  Train Loss 0.27732717990875244 acc = 0.0\n",
            "pos tensor(0.2515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4543==== Step 1  Train Loss 0.2744686007499695 acc = 0.0\n",
            "pos tensor(0.2006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4544==== Step 1  Train Loss 0.26096639037132263 acc = 0.0\n",
            "pos tensor(0.2188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4545==== Step 1  Train Loss 0.2555539011955261 acc = 0.0\n",
            "pos tensor(0.2278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4546==== Step 1  Train Loss 0.2679259777069092 acc = 0.0\n",
            "pos tensor(0.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4547==== Step 1  Train Loss 0.259834885597229 acc = 0.0\n",
            "pos tensor(0.2125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4548==== Step 1  Train Loss 0.2704111337661743 acc = 0.0\n",
            "pos tensor(0.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4549==== Step 1  Train Loss 0.24810343980789185 acc = 0.0\n",
            "pos tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4550==== Step 1  Train Loss 0.2556367516517639 acc = 0.0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4551==== Step 1  Train Loss 0.25602641701698303 acc = 0.0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4552==== Step 1  Train Loss 0.260668009519577 acc = 0.0\n",
            "pos tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4553==== Step 1  Train Loss 0.24203166365623474 acc = 0.0\n",
            "pos tensor(0.2328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4554==== Step 1  Train Loss 0.27176252007484436 acc = 0.0\n",
            "pos tensor(0.1912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4555==== Step 1  Train Loss 0.2540941536426544 acc = 0.0\n",
            "pos tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4556==== Step 1  Train Loss 0.261705219745636 acc = 0.0\n",
            "pos tensor(0.2593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4557==== Step 1  Train Loss 0.28207501769065857 acc = 0.0\n",
            "pos tensor(0.1976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4558==== Step 1  Train Loss 0.25178062915802 acc = 0.0\n",
            "pos tensor(0.2478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4559==== Step 1  Train Loss 0.26276159286499023 acc = 0.0\n",
            "pos tensor(0.2251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4560==== Step 1  Train Loss 0.2633195221424103 acc = 0.0\n",
            "pos tensor(0.1997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4561==== Step 1  Train Loss 0.2557932436466217 acc = 0.0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4562==== Step 1  Train Loss 0.2565186023712158 acc = 0.0\n",
            "pos tensor(0.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4563==== Step 1  Train Loss 0.2617546319961548 acc = 0.0\n",
            "pos tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4564==== Step 1  Train Loss 0.2563737630844116 acc = 0.0\n",
            "pos tensor(0.2727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4565==== Step 1  Train Loss 0.2760424017906189 acc = 0.0\n",
            "pos tensor(0.2192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4566==== Step 1  Train Loss 0.26633524894714355 acc = 0.0\n",
            "pos tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4567==== Step 1  Train Loss 0.26099488139152527 acc = 0.0\n",
            "pos tensor(0.1995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4568==== Step 1  Train Loss 0.25897127389907837 acc = 0.0\n",
            "pos tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4569==== Step 1  Train Loss 0.2591010630130768 acc = 0.0\n",
            "pos tensor(0.2022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4570==== Step 1  Train Loss 0.2611881494522095 acc = 0.0\n",
            "pos tensor(0.2003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4571==== Step 1  Train Loss 0.2638101875782013 acc = 0.0\n",
            "pos tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4572==== Step 1  Train Loss 0.2690643072128296 acc = 0.0\n",
            "pos tensor(0.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4573==== Step 1  Train Loss 0.2568892538547516 acc = 0.0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4574==== Step 1  Train Loss 0.2615351974964142 acc = 0.0\n",
            "pos tensor(0.1759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4575==== Step 1  Train Loss 0.25839000940322876 acc = 0.0\n",
            "pos tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4576==== Step 1  Train Loss 0.27292388677597046 acc = 0.0\n",
            "pos tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4577==== Step 1  Train Loss 0.27545642852783203 acc = 0.0\n",
            "pos tensor(0.2150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4578==== Step 1  Train Loss 0.2649588882923126 acc = 0.0\n",
            "pos tensor(0.2246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4579==== Step 1  Train Loss 0.26570188999176025 acc = 0.0\n",
            "pos tensor(0.1837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4580==== Step 1  Train Loss 0.247897207736969 acc = 0.0\n",
            "pos tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4581==== Step 1  Train Loss 0.25944364070892334 acc = 0.0\n",
            "pos tensor(0.1967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4582==== Step 1  Train Loss 0.26350533962249756 acc = 0.0\n",
            "pos tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4583==== Step 1  Train Loss 0.2601044476032257 acc = 0.0\n",
            "pos tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4584==== Step 1  Train Loss 0.25007134675979614 acc = 0.0\n",
            "pos tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4585==== Step 1  Train Loss 0.25245410203933716 acc = 0.0\n",
            "pos tensor(0.1764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4586==== Step 1  Train Loss 0.2470734864473343 acc = 0.0\n",
            "pos tensor(0.1749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4587==== Step 1  Train Loss 0.24072059988975525 acc = 0.0\n",
            "pos tensor(0.1873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4588==== Step 1  Train Loss 0.25981655716896057 acc = 0.0\n",
            "pos tensor(0.1768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4589==== Step 1  Train Loss 0.25852251052856445 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4590==== Step 1  Train Loss 0.26851165294647217 acc = 0.0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4591==== Step 1  Train Loss 0.2695719003677368 acc = 0.0\n",
            "pos tensor(0.2057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4592==== Step 1  Train Loss 0.2634204626083374 acc = 0.0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4593==== Step 1  Train Loss 0.24993664026260376 acc = 0.0\n",
            "pos tensor(0.1759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4594==== Step 1  Train Loss 0.26651936769485474 acc = 0.0\n",
            "pos tensor(0.1883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4595==== Step 1  Train Loss 0.2611272931098938 acc = 0.0\n",
            "pos tensor(0.1808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4596==== Step 1  Train Loss 0.26960471272468567 acc = 0.0\n",
            "pos tensor(0.1790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4597==== Step 1  Train Loss 0.2583749294281006 acc = 0.0\n",
            "pos tensor(0.1715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4598==== Step 1  Train Loss 0.2607358992099762 acc = 0.0\n",
            "pos tensor(0.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4599==== Step 1  Train Loss 0.27318572998046875 acc = 0.0\n",
            "  Batch 4,600  of  5,204.    Elapsed: 0:59:20.\n",
            "pos tensor(0.1648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4600==== Step 1  Train Loss 0.2594294548034668 acc = 0.0\n",
            "pos tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4601==== Step 1  Train Loss 0.26532721519470215 acc = 0.0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4602==== Step 1  Train Loss 0.26454994082450867 acc = 0.0\n",
            "pos tensor(0.1563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4603==== Step 1  Train Loss 0.2556222975254059 acc = 0.0\n",
            "pos tensor(0.1936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4604==== Step 1  Train Loss 0.25904199481010437 acc = 0.0\n",
            "pos tensor(0.1905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4605==== Step 1  Train Loss 0.2691614031791687 acc = 0.0\n",
            "pos tensor(0.1795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4606==== Step 1  Train Loss 0.2512153685092926 acc = 0.0\n",
            "pos tensor(0.1597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4607==== Step 1  Train Loss 0.2579728364944458 acc = 0.0\n",
            "pos tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4608==== Step 1  Train Loss 0.24887290596961975 acc = 0.0\n",
            "pos tensor(0.1872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4609==== Step 1  Train Loss 0.26903659105300903 acc = 0.0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4610==== Step 1  Train Loss 0.2590409517288208 acc = 0.0\n",
            "pos tensor(0.1654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4611==== Step 1  Train Loss 0.2695266008377075 acc = 0.0\n",
            "pos tensor(0.2020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4612==== Step 1  Train Loss 0.26377829909324646 acc = 0.0\n",
            "pos tensor(0.1655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4613==== Step 1  Train Loss 0.26861461997032166 acc = 0.0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4614==== Step 1  Train Loss 0.2615177035331726 acc = 0.0\n",
            "pos tensor(0.1652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4615==== Step 1  Train Loss 0.2572787404060364 acc = 0.0\n",
            "pos tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4616==== Step 1  Train Loss 0.26771867275238037 acc = 0.0\n",
            "pos tensor(0.1672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4617==== Step 1  Train Loss 0.25062480568885803 acc = 0.0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4618==== Step 1  Train Loss 0.2678997218608856 acc = 0.0\n",
            "pos tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4619==== Step 1  Train Loss 0.2723628878593445 acc = 0.0\n",
            "pos tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4620==== Step 1  Train Loss 0.26740556955337524 acc = 0.0\n",
            "pos tensor(0.2252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4621==== Step 1  Train Loss 0.27114665508270264 acc = 0.0\n",
            "pos tensor(0.1850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4622==== Step 1  Train Loss 0.2600332498550415 acc = 0.0\n",
            "pos tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4623==== Step 1  Train Loss 0.26352500915527344 acc = 0.0\n",
            "pos tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4624==== Step 1  Train Loss 0.26394200325012207 acc = 0.0\n",
            "pos tensor(0.1659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4625==== Step 1  Train Loss 0.2744598090648651 acc = 0.0\n",
            "pos tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4626==== Step 1  Train Loss 0.25909683108329773 acc = 0.0\n",
            "pos tensor(0.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4627==== Step 1  Train Loss 0.26396501064300537 acc = 0.0\n",
            "pos tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4628==== Step 1  Train Loss 0.2557627558708191 acc = 0.0\n",
            "pos tensor(0.1890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4629==== Step 1  Train Loss 0.2577962577342987 acc = 0.0\n",
            "pos tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4630==== Step 1  Train Loss 0.26443642377853394 acc = 0.0\n",
            "pos tensor(0.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4631==== Step 1  Train Loss 0.25944408774375916 acc = 0.0\n",
            "pos tensor(0.2052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4632==== Step 1  Train Loss 0.2684263586997986 acc = 0.0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4633==== Step 1  Train Loss 0.25685715675354004 acc = 0.0\n",
            "pos tensor(0.1817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4634==== Step 1  Train Loss 0.2569704055786133 acc = 0.0\n",
            "pos tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4635==== Step 1  Train Loss 0.25332438945770264 acc = 0.0\n",
            "pos tensor(0.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4636==== Step 1  Train Loss 0.25685662031173706 acc = 0.0\n",
            "pos tensor(0.1892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4637==== Step 1  Train Loss 0.2498323917388916 acc = 0.0\n",
            "pos tensor(0.2055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4638==== Step 1  Train Loss 0.2656404972076416 acc = 0.0\n",
            "pos tensor(0.2088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4639==== Step 1  Train Loss 0.27072715759277344 acc = 0.0\n",
            "pos tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4640==== Step 1  Train Loss 0.2511291205883026 acc = 0.0\n",
            "pos tensor(0.1916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4641==== Step 1  Train Loss 0.25100216269493103 acc = 0.0\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4642==== Step 1  Train Loss 0.2645725607872009 acc = 0.0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4643==== Step 1  Train Loss 0.26886165142059326 acc = 0.0\n",
            "pos tensor(0.1741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4644==== Step 1  Train Loss 0.25126680731773376 acc = 0.0\n",
            "pos tensor(0.1869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4645==== Step 1  Train Loss 0.2573695480823517 acc = 0.0\n",
            "pos tensor(0.1810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4646==== Step 1  Train Loss 0.25923964381217957 acc = 0.0\n",
            "pos tensor(0.2151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4647==== Step 1  Train Loss 0.2615656852722168 acc = 0.0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4648==== Step 1  Train Loss 0.2721053659915924 acc = 0.0\n",
            "pos tensor(0.2028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4649==== Step 1  Train Loss 0.26809370517730713 acc = 0.0\n",
            "pos tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4650==== Step 1  Train Loss 0.26796525716781616 acc = 0.0\n",
            "pos tensor(0.2418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4651==== Step 1  Train Loss 0.2693420946598053 acc = 0.0\n",
            "pos tensor(0.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4652==== Step 1  Train Loss 0.26070737838745117 acc = 0.0\n",
            "pos tensor(0.2047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4653==== Step 1  Train Loss 0.25633764266967773 acc = 0.0\n",
            "pos tensor(0.1874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4654==== Step 1  Train Loss 0.2600504457950592 acc = 0.0\n",
            "pos tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4655==== Step 1  Train Loss 0.261600136756897 acc = 0.0\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4656==== Step 1  Train Loss 0.26000428199768066 acc = 0.0\n",
            "pos tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4657==== Step 1  Train Loss 0.2667394280433655 acc = 0.0\n",
            "pos tensor(0.2351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4658==== Step 1  Train Loss 0.2815655469894409 acc = 0.0\n",
            "pos tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4659==== Step 1  Train Loss 0.24394728243350983 acc = 0.0\n",
            "pos tensor(0.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4660==== Step 1  Train Loss 0.2526990473270416 acc = 0.0\n",
            "pos tensor(0.2075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4661==== Step 1  Train Loss 0.2628355920314789 acc = 0.0\n",
            "pos tensor(0.1878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4662==== Step 1  Train Loss 0.24920132756233215 acc = 0.0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4663==== Step 1  Train Loss 0.26328977942466736 acc = 0.0\n",
            "pos tensor(0.2057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4664==== Step 1  Train Loss 0.2569737434387207 acc = 0.0\n",
            "pos tensor(0.2194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4665==== Step 1  Train Loss 0.2485751509666443 acc = 0.0\n",
            "pos tensor(0.2165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4666==== Step 1  Train Loss 0.2653467357158661 acc = 0.0\n",
            "pos tensor(0.2092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4667==== Step 1  Train Loss 0.2577478885650635 acc = 0.0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4668==== Step 1  Train Loss 0.25992637872695923 acc = 0.0\n",
            "pos tensor(0.2033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4669==== Step 1  Train Loss 0.2515297532081604 acc = 0.0\n",
            "pos tensor(0.2084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4670==== Step 1  Train Loss 0.2621017396450043 acc = 0.0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4671==== Step 1  Train Loss 0.2605184316635132 acc = 0.0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4672==== Step 1  Train Loss 0.2569391429424286 acc = 0.0\n",
            "pos tensor(0.2122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4673==== Step 1  Train Loss 0.25708234310150146 acc = 0.0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4674==== Step 1  Train Loss 0.26741617918014526 acc = 0.0\n",
            "pos tensor(0.2302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4675==== Step 1  Train Loss 0.273145854473114 acc = 0.0\n",
            "pos tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4676==== Step 1  Train Loss 0.26590049266815186 acc = 0.0\n",
            "pos tensor(0.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4677==== Step 1  Train Loss 0.26351627707481384 acc = 0.0\n",
            "pos tensor(0.1867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4678==== Step 1  Train Loss 0.24983271956443787 acc = 0.0\n",
            "pos tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4679==== Step 1  Train Loss 0.2626345455646515 acc = 0.0\n",
            "pos tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4680==== Step 1  Train Loss 0.25514844059944153 acc = 0.0\n",
            "pos tensor(0.1855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4681==== Step 1  Train Loss 0.2636135220527649 acc = 0.0\n",
            "pos tensor(0.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4682==== Step 1  Train Loss 0.251193642616272 acc = 0.0\n",
            "pos tensor(0.1784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4683==== Step 1  Train Loss 0.24634987115859985 acc = 0.0\n",
            "pos tensor(0.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4684==== Step 1  Train Loss 0.2649443447589874 acc = 0.0\n",
            "pos tensor(0.2231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4685==== Step 1  Train Loss 0.26593899726867676 acc = 0.0\n",
            "pos tensor(0.1845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4686==== Step 1  Train Loss 0.2620428502559662 acc = 0.0\n",
            "pos tensor(0.1794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4687==== Step 1  Train Loss 0.24649178981781006 acc = 0.0\n",
            "pos tensor(0.2186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4688==== Step 1  Train Loss 0.2666357457637787 acc = 0.0\n",
            "pos tensor(0.2488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4689==== Step 1  Train Loss 0.2700602412223816 acc = 0.0\n",
            "pos tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4690==== Step 1  Train Loss 0.25766730308532715 acc = 0.0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4691==== Step 1  Train Loss 0.26650163531303406 acc = 0.0\n",
            "pos tensor(0.1795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4692==== Step 1  Train Loss 0.2598579525947571 acc = 0.0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4693==== Step 1  Train Loss 0.265577107667923 acc = 0.0\n",
            "pos tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4694==== Step 1  Train Loss 0.2588517665863037 acc = 0.0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4695==== Step 1  Train Loss 0.26812177896499634 acc = 0.0\n",
            "pos tensor(0.2328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4696==== Step 1  Train Loss 0.2707461416721344 acc = 0.0\n",
            "pos tensor(0.1931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4697==== Step 1  Train Loss 0.25638294219970703 acc = 0.0\n",
            "pos tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4698==== Step 1  Train Loss 0.2557513117790222 acc = 0.0\n",
            "pos tensor(0.2286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4699==== Step 1  Train Loss 0.2612336575984955 acc = 0.0\n",
            "  Batch 4,700  of  5,204.    Elapsed: 1:00:37.\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4700==== Step 1  Train Loss 0.2671058475971222 acc = 0.0\n",
            "pos tensor(0.2009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4701==== Step 1  Train Loss 0.2735307216644287 acc = 0.0\n",
            "pos tensor(0.2290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4702==== Step 1  Train Loss 0.27062830328941345 acc = 0.0\n",
            "pos tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4703==== Step 1  Train Loss 0.2540546655654907 acc = 0.0\n",
            "pos tensor(0.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4704==== Step 1  Train Loss 0.24799497425556183 acc = 0.0\n",
            "pos tensor(0.2022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4705==== Step 1  Train Loss 0.2716757357120514 acc = 0.0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4706==== Step 1  Train Loss 0.26915913820266724 acc = 0.0\n",
            "pos tensor(0.2222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4707==== Step 1  Train Loss 0.2687549293041229 acc = 0.0\n",
            "pos tensor(0.2186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4708==== Step 1  Train Loss 0.274943470954895 acc = 0.0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4709==== Step 1  Train Loss 0.26543140411376953 acc = 0.0\n",
            "pos tensor(0.1790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4710==== Step 1  Train Loss 0.26301679015159607 acc = 0.0\n",
            "pos tensor(0.1715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4711==== Step 1  Train Loss 0.24257899820804596 acc = 0.0\n",
            "pos tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4712==== Step 1  Train Loss 0.2468213587999344 acc = 0.0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4713==== Step 1  Train Loss 0.25714921951293945 acc = 0.0\n",
            "pos tensor(0.1580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4714==== Step 1  Train Loss 0.2491200864315033 acc = 0.0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4715==== Step 1  Train Loss 0.26907363533973694 acc = 0.0\n",
            "pos tensor(0.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4716==== Step 1  Train Loss 0.2643934190273285 acc = 0.0\n",
            "pos tensor(0.1811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4717==== Step 1  Train Loss 0.2508589029312134 acc = 0.0\n",
            "pos tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4718==== Step 1  Train Loss 0.26127493381500244 acc = 0.0\n",
            "pos tensor(0.1912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4719==== Step 1  Train Loss 0.26915159821510315 acc = 0.0\n",
            "pos tensor(0.1823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4720==== Step 1  Train Loss 0.2655772864818573 acc = 0.0\n",
            "pos tensor(0.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4721==== Step 1  Train Loss 0.25318652391433716 acc = 0.0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4722==== Step 1  Train Loss 0.25615090131759644 acc = 0.0\n",
            "pos tensor(0.1709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4723==== Step 1  Train Loss 0.26555898785591125 acc = 0.0\n",
            "pos tensor(0.1869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4724==== Step 1  Train Loss 0.2638510465621948 acc = 0.0\n",
            "pos tensor(0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4725==== Step 1  Train Loss 0.25491219758987427 acc = 0.0\n",
            "pos tensor(0.1756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4726==== Step 1  Train Loss 0.2571832239627838 acc = 0.0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4727==== Step 1  Train Loss 0.2624320983886719 acc = 0.0\n",
            "pos tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4728==== Step 1  Train Loss 0.27175235748291016 acc = 0.0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4729==== Step 1  Train Loss 0.2594236731529236 acc = 0.0\n",
            "pos tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4730==== Step 1  Train Loss 0.26386934518814087 acc = 0.0\n",
            "pos tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4731==== Step 1  Train Loss 0.25514858961105347 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4732==== Step 1  Train Loss 0.25641316175460815 acc = 0.0\n",
            "pos tensor(0.2007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4733==== Step 1  Train Loss 0.26688358187675476 acc = 0.0\n",
            "pos tensor(0.1742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4734==== Step 1  Train Loss 0.2632278800010681 acc = 0.0\n",
            "pos tensor(0.1670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4735==== Step 1  Train Loss 0.2580937147140503 acc = 0.0\n",
            "pos tensor(0.2037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4736==== Step 1  Train Loss 0.25937619805336 acc = 0.0\n",
            "pos tensor(0.2074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4737==== Step 1  Train Loss 0.2748295068740845 acc = 0.0\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4738==== Step 1  Train Loss 0.25565457344055176 acc = 0.0\n",
            "pos tensor(0.2146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4739==== Step 1  Train Loss 0.27513444423675537 acc = 0.0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4740==== Step 1  Train Loss 0.26250678300857544 acc = 0.0\n",
            "pos tensor(0.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4741==== Step 1  Train Loss 0.26365625858306885 acc = 0.0\n",
            "pos tensor(0.1884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4742==== Step 1  Train Loss 0.2709740698337555 acc = 0.0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4743==== Step 1  Train Loss 0.2663947343826294 acc = 0.0\n",
            "pos tensor(0.1940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4744==== Step 1  Train Loss 0.27279067039489746 acc = 0.0\n",
            "pos tensor(0.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4745==== Step 1  Train Loss 0.2614225149154663 acc = 0.0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4746==== Step 1  Train Loss 0.2696368992328644 acc = 0.0\n",
            "pos tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4747==== Step 1  Train Loss 0.2618640661239624 acc = 0.0\n",
            "pos tensor(0.1750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4748==== Step 1  Train Loss 0.2720586657524109 acc = 0.0\n",
            "pos tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4749==== Step 1  Train Loss 0.26910579204559326 acc = 0.0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4750==== Step 1  Train Loss 0.2760871648788452 acc = 0.0\n",
            "pos tensor(0.1550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4751==== Step 1  Train Loss 0.26833710074424744 acc = 0.0\n",
            "pos tensor(0.1626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4752==== Step 1  Train Loss 0.2683578133583069 acc = 0.0\n",
            "pos tensor(0.1750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4753==== Step 1  Train Loss 0.26961269974708557 acc = 0.0\n",
            "pos tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4754==== Step 1  Train Loss 0.277630090713501 acc = 0.0\n",
            "pos tensor(0.1361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4755==== Step 1  Train Loss 0.26013416051864624 acc = 0.0\n",
            "pos tensor(0.1570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4756==== Step 1  Train Loss 0.26499325037002563 acc = 0.0\n",
            "pos tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4757==== Step 1  Train Loss 0.25505930185317993 acc = 0.0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4758==== Step 1  Train Loss 0.2785934805870056 acc = 0.0\n",
            "pos tensor(0.1651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4759==== Step 1  Train Loss 0.27295008301734924 acc = 0.0\n",
            "pos tensor(0.2007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4760==== Step 1  Train Loss 0.2769324779510498 acc = 0.0\n",
            "pos tensor(0.1482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4761==== Step 1  Train Loss 0.27435818314552307 acc = 0.0\n",
            "pos tensor(0.1620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4762==== Step 1  Train Loss 0.2513939142227173 acc = 0.0\n",
            "pos tensor(0.1574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4763==== Step 1  Train Loss 0.25552040338516235 acc = 0.0\n",
            "pos tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4764==== Step 1  Train Loss 0.26799720525741577 acc = 0.0\n",
            "pos tensor(0.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4765==== Step 1  Train Loss 0.27578601241111755 acc = 0.0\n",
            "pos tensor(0.1988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4766==== Step 1  Train Loss 0.26903849840164185 acc = 0.0\n",
            "pos tensor(0.1744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4767==== Step 1  Train Loss 0.27202460169792175 acc = 0.0\n",
            "pos tensor(0.1777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4768==== Step 1  Train Loss 0.2703187167644501 acc = 0.0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4769==== Step 1  Train Loss 0.2611093819141388 acc = 0.0\n",
            "pos tensor(0.1760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4770==== Step 1  Train Loss 0.2647640109062195 acc = 0.0\n",
            "pos tensor(0.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4771==== Step 1  Train Loss 0.2648721933364868 acc = 0.0\n",
            "pos tensor(0.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4772==== Step 1  Train Loss 0.2640262842178345 acc = 0.0\n",
            "pos tensor(0.1904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4773==== Step 1  Train Loss 0.27693209052085876 acc = 0.0\n",
            "pos tensor(0.2028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4774==== Step 1  Train Loss 0.2628519535064697 acc = 0.0\n",
            "pos tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4775==== Step 1  Train Loss 0.2533600330352783 acc = 0.0\n",
            "pos tensor(0.1832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4776==== Step 1  Train Loss 0.25702619552612305 acc = 0.0\n",
            "pos tensor(0.2020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4777==== Step 1  Train Loss 0.2753638029098511 acc = 0.0\n",
            "pos tensor(0.2031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4778==== Step 1  Train Loss 0.2646827697753906 acc = 0.0\n",
            "pos tensor(0.2131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4779==== Step 1  Train Loss 0.2576216757297516 acc = 0.0\n",
            "pos tensor(0.1869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4780==== Step 1  Train Loss 0.2725103497505188 acc = 0.0\n",
            "pos tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4781==== Step 1  Train Loss 0.25296974182128906 acc = 0.0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4782==== Step 1  Train Loss 0.25098884105682373 acc = 0.0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4783==== Step 1  Train Loss 0.25602543354034424 acc = 0.0\n",
            "pos tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4784==== Step 1  Train Loss 0.2534916400909424 acc = 0.0\n",
            "pos tensor(0.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4785==== Step 1  Train Loss 0.2617434561252594 acc = 0.0\n",
            "pos tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4786==== Step 1  Train Loss 0.2674717307090759 acc = 0.0\n",
            "pos tensor(0.1905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4787==== Step 1  Train Loss 0.266132652759552 acc = 0.0\n",
            "pos tensor(0.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4788==== Step 1  Train Loss 0.25461119413375854 acc = 0.0\n",
            "pos tensor(0.1845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4789==== Step 1  Train Loss 0.25771304965019226 acc = 0.0\n",
            "pos tensor(0.2098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4790==== Step 1  Train Loss 0.2717614769935608 acc = 0.0\n",
            "pos tensor(0.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4791==== Step 1  Train Loss 0.2656603157520294 acc = 0.0\n",
            "pos tensor(0.2120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4792==== Step 1  Train Loss 0.26506978273391724 acc = 0.0\n",
            "pos tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4793==== Step 1  Train Loss 0.26160529255867004 acc = 0.0\n",
            "pos tensor(0.2517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4794==== Step 1  Train Loss 0.27645576000213623 acc = 0.0\n",
            "pos tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4795==== Step 1  Train Loss 0.25426554679870605 acc = 0.0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4796==== Step 1  Train Loss 0.26827532052993774 acc = 0.0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4797==== Step 1  Train Loss 0.25320419669151306 acc = 0.0\n",
            "pos tensor(0.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4798==== Step 1  Train Loss 0.2733387053012848 acc = 0.0\n",
            "pos tensor(0.1931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4799==== Step 1  Train Loss 0.2633172273635864 acc = 0.0\n",
            "  Batch 4,800  of  5,204.    Elapsed: 1:01:54.\n",
            "pos tensor(0.2283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4800==== Step 1  Train Loss 0.27743688225746155 acc = 0.0\n",
            "pos tensor(0.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4801==== Step 1  Train Loss 0.27048593759536743 acc = 0.0\n",
            "pos tensor(0.1862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4802==== Step 1  Train Loss 0.25314921140670776 acc = 0.0\n",
            "pos tensor(0.2058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4803==== Step 1  Train Loss 0.26830214262008667 acc = 0.0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4804==== Step 1  Train Loss 0.2673207223415375 acc = 0.0\n",
            "pos tensor(0.2110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4805==== Step 1  Train Loss 0.2633993923664093 acc = 0.0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4806==== Step 1  Train Loss 0.26489779353141785 acc = 0.0\n",
            "pos tensor(0.2043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4807==== Step 1  Train Loss 0.27441906929016113 acc = 0.0\n",
            "pos tensor(0.2447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4808==== Step 1  Train Loss 0.278154194355011 acc = 0.0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4809==== Step 1  Train Loss 0.2687234878540039 acc = 0.0\n",
            "pos tensor(0.2222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4810==== Step 1  Train Loss 0.26877275109291077 acc = 0.0\n",
            "pos tensor(0.1513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4811==== Step 1  Train Loss 0.2609573006629944 acc = 0.0\n",
            "pos tensor(0.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4812==== Step 1  Train Loss 0.2659229338169098 acc = 0.0\n",
            "pos tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4813==== Step 1  Train Loss 0.2647128701210022 acc = 0.0\n",
            "pos tensor(0.1757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4814==== Step 1  Train Loss 0.2645877003669739 acc = 0.0\n",
            "pos tensor(0.2013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4815==== Step 1  Train Loss 0.26969945430755615 acc = 0.0\n",
            "pos tensor(0.1981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4816==== Step 1  Train Loss 0.25933969020843506 acc = 0.0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4817==== Step 1  Train Loss 0.2695789039134979 acc = 0.0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4818==== Step 1  Train Loss 0.2601085305213928 acc = 0.0\n",
            "pos tensor(0.1995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4819==== Step 1  Train Loss 0.26480934023857117 acc = 0.0\n",
            "pos tensor(0.1536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4820==== Step 1  Train Loss 0.26289868354797363 acc = 0.0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4821==== Step 1  Train Loss 0.27125853300094604 acc = 0.0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4822==== Step 1  Train Loss 0.2741289436817169 acc = 0.0\n",
            "pos tensor(0.1503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4823==== Step 1  Train Loss 0.2729526162147522 acc = 0.0\n",
            "pos tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4824==== Step 1  Train Loss 0.27377480268478394 acc = 0.0\n",
            "pos tensor(0.2080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4825==== Step 1  Train Loss 0.2763636112213135 acc = 0.0\n",
            "pos tensor(0.1728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4826==== Step 1  Train Loss 0.26916447281837463 acc = 0.0\n",
            "pos tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4827==== Step 1  Train Loss 0.27124935388565063 acc = 0.0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4828==== Step 1  Train Loss 0.25731024146080017 acc = 0.0\n",
            "pos tensor(0.1494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4829==== Step 1  Train Loss 0.2541466951370239 acc = 0.0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4830==== Step 1  Train Loss 0.26190292835235596 acc = 0.0\n",
            "pos tensor(0.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4831==== Step 1  Train Loss 0.27582061290740967 acc = 0.0\n",
            "pos tensor(0.1529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4832==== Step 1  Train Loss 0.26122409105300903 acc = 0.0\n",
            "pos tensor(0.2133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4833==== Step 1  Train Loss 0.27559345960617065 acc = 0.0\n",
            "pos tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4834==== Step 1  Train Loss 0.26405906677246094 acc = 0.0\n",
            "pos tensor(0.1762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4835==== Step 1  Train Loss 0.25760453939437866 acc = 0.0\n",
            "pos tensor(0.2035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4836==== Step 1  Train Loss 0.2698800265789032 acc = 0.0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4837==== Step 1  Train Loss 0.26247745752334595 acc = 0.0\n",
            "pos tensor(0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4838==== Step 1  Train Loss 0.2704229950904846 acc = 0.0\n",
            "pos tensor(0.1771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4839==== Step 1  Train Loss 0.26284265518188477 acc = 0.0\n",
            "pos tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4840==== Step 1  Train Loss 0.2672778367996216 acc = 0.0\n",
            "pos tensor(0.1966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4841==== Step 1  Train Loss 0.25722381472587585 acc = 0.0\n",
            "pos tensor(0.1531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4842==== Step 1  Train Loss 0.2626771628856659 acc = 0.0\n",
            "pos tensor(0.2025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4843==== Step 1  Train Loss 0.27298277616500854 acc = 0.0\n",
            "pos tensor(0.1495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4844==== Step 1  Train Loss 0.2448500394821167 acc = 0.0\n",
            "pos tensor(0.1869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4845==== Step 1  Train Loss 0.2667889893054962 acc = 0.0\n",
            "pos tensor(0.2141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4846==== Step 1  Train Loss 0.26558881998062134 acc = 0.0\n",
            "pos tensor(0.1573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4847==== Step 1  Train Loss 0.25400838255882263 acc = 0.0\n",
            "pos tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4848==== Step 1  Train Loss 0.2733161449432373 acc = 0.0\n",
            "pos tensor(0.1729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4849==== Step 1  Train Loss 0.2707393765449524 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4850==== Step 1  Train Loss 0.25644195079803467 acc = 0.0\n",
            "pos tensor(0.1923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4851==== Step 1  Train Loss 0.263014554977417 acc = 0.0\n",
            "pos tensor(0.2125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4852==== Step 1  Train Loss 0.2662341594696045 acc = 0.0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4853==== Step 1  Train Loss 0.2633240818977356 acc = 0.0\n",
            "pos tensor(0.1765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4854==== Step 1  Train Loss 0.26474353671073914 acc = 0.0\n",
            "pos tensor(0.1603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4855==== Step 1  Train Loss 0.24878430366516113 acc = 0.0\n",
            "pos tensor(0.1544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4856==== Step 1  Train Loss 0.2587973177433014 acc = 0.0\n",
            "pos tensor(0.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4857==== Step 1  Train Loss 0.2615583539009094 acc = 0.0\n",
            "pos tensor(0.1812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4858==== Step 1  Train Loss 0.2747517228126526 acc = 0.0\n",
            "pos tensor(0.1726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4859==== Step 1  Train Loss 0.260928750038147 acc = 0.0\n",
            "pos tensor(0.1567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4860==== Step 1  Train Loss 0.28207409381866455 acc = 0.0\n",
            "pos tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4861==== Step 1  Train Loss 0.2551778554916382 acc = 0.0\n",
            "pos tensor(0.1812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4862==== Step 1  Train Loss 0.2650500535964966 acc = 0.0\n",
            "pos tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4863==== Step 1  Train Loss 0.25205862522125244 acc = 0.0\n",
            "pos tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4864==== Step 1  Train Loss 0.2696215510368347 acc = 0.0\n",
            "pos tensor(0.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4865==== Step 1  Train Loss 0.2557671070098877 acc = 0.0\n",
            "pos tensor(0.1306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4866==== Step 1  Train Loss 0.27092620730400085 acc = 0.0\n",
            "pos tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4867==== Step 1  Train Loss 0.2685544788837433 acc = 0.0\n",
            "pos tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4868==== Step 1  Train Loss 0.2609218955039978 acc = 0.0\n",
            "pos tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4869==== Step 1  Train Loss 0.26415789127349854 acc = 0.0\n",
            "pos tensor(0.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4870==== Step 1  Train Loss 0.2689181864261627 acc = 0.0\n",
            "pos tensor(0.1549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4871==== Step 1  Train Loss 0.2689356803894043 acc = 0.0\n",
            "pos tensor(0.1569, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4872==== Step 1  Train Loss 0.2690269351005554 acc = 0.0\n",
            "pos tensor(0.1655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4873==== Step 1  Train Loss 0.2538117468357086 acc = 0.0\n",
            "pos tensor(0.1589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4874==== Step 1  Train Loss 0.26769745349884033 acc = 0.0\n",
            "pos tensor(0.1747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4875==== Step 1  Train Loss 0.2748182713985443 acc = 0.0\n",
            "pos tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4876==== Step 1  Train Loss 0.2580326497554779 acc = 0.0\n",
            "pos tensor(0.1368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4877==== Step 1  Train Loss 0.2641996741294861 acc = 0.0\n",
            "pos tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4878==== Step 1  Train Loss 0.26109105348587036 acc = 0.0\n",
            "pos tensor(0.1608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4879==== Step 1  Train Loss 0.27313458919525146 acc = 0.0\n",
            "pos tensor(0.1476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4880==== Step 1  Train Loss 0.26446473598480225 acc = 0.0\n",
            "pos tensor(0.1638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4881==== Step 1  Train Loss 0.2766590714454651 acc = 0.0\n",
            "pos tensor(0.1557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4882==== Step 1  Train Loss 0.2690102159976959 acc = 0.0\n",
            "pos tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4883==== Step 1  Train Loss 0.2754668593406677 acc = 0.0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4884==== Step 1  Train Loss 0.27567434310913086 acc = 0.0\n",
            "pos tensor(0.1697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4885==== Step 1  Train Loss 0.2681327164173126 acc = 0.0\n",
            "pos tensor(0.1667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4886==== Step 1  Train Loss 0.2661530375480652 acc = 0.0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4887==== Step 1  Train Loss 0.268239289522171 acc = 0.0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4888==== Step 1  Train Loss 0.2589523196220398 acc = 0.0\n",
            "pos tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4889==== Step 1  Train Loss 0.2705540955066681 acc = 0.0\n",
            "pos tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4890==== Step 1  Train Loss 0.2831686735153198 acc = 0.0\n",
            "pos tensor(0.1567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4891==== Step 1  Train Loss 0.2697973847389221 acc = 0.0\n",
            "pos tensor(0.1734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4892==== Step 1  Train Loss 0.27731913328170776 acc = 0.0\n",
            "pos tensor(0.1470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4893==== Step 1  Train Loss 0.24941009283065796 acc = 0.0\n",
            "pos tensor(0.1780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4894==== Step 1  Train Loss 0.26159194111824036 acc = 0.0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4895==== Step 1  Train Loss 0.2699321508407593 acc = 0.0\n",
            "pos tensor(0.1483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4896==== Step 1  Train Loss 0.2605223059654236 acc = 0.0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4897==== Step 1  Train Loss 0.257580429315567 acc = 0.0\n",
            "pos tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4898==== Step 1  Train Loss 0.26327937841415405 acc = 0.0\n",
            "pos tensor(0.1656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4899==== Step 1  Train Loss 0.2694551348686218 acc = 0.0\n",
            "  Batch 4,900  of  5,204.    Elapsed: 1:03:10.\n",
            "pos tensor(0.1846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4900==== Step 1  Train Loss 0.25952810049057007 acc = 0.0\n",
            "pos tensor(0.1864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4901==== Step 1  Train Loss 0.26676416397094727 acc = 0.0\n",
            "pos tensor(0.2216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4902==== Step 1  Train Loss 0.2825653851032257 acc = 0.0\n",
            "pos tensor(0.1617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4903==== Step 1  Train Loss 0.2611887454986572 acc = 0.0\n",
            "pos tensor(0.1432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4904==== Step 1  Train Loss 0.2661457061767578 acc = 0.0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4905==== Step 1  Train Loss 0.24993328750133514 acc = 0.0\n",
            "pos tensor(0.2021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4906==== Step 1  Train Loss 0.2671477198600769 acc = 0.0\n",
            "pos tensor(0.1716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4907==== Step 1  Train Loss 0.2587190866470337 acc = 0.0\n",
            "pos tensor(0.1647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4908==== Step 1  Train Loss 0.26933926343917847 acc = 0.0\n",
            "pos tensor(0.1713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4909==== Step 1  Train Loss 0.25323936343193054 acc = 0.0\n",
            "pos tensor(0.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4910==== Step 1  Train Loss 0.25413885712623596 acc = 0.0\n",
            "pos tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4911==== Step 1  Train Loss 0.2702074348926544 acc = 0.0\n",
            "pos tensor(0.1682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4912==== Step 1  Train Loss 0.25897717475891113 acc = 0.0\n",
            "pos tensor(0.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4913==== Step 1  Train Loss 0.26551276445388794 acc = 0.0\n",
            "pos tensor(0.1684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4914==== Step 1  Train Loss 0.249049574136734 acc = 0.0\n",
            "pos tensor(0.1784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4915==== Step 1  Train Loss 0.2633512318134308 acc = 0.0\n",
            "pos tensor(0.1811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4916==== Step 1  Train Loss 0.25445306301116943 acc = 0.0\n",
            "pos tensor(0.1585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4917==== Step 1  Train Loss 0.26862436532974243 acc = 0.0\n",
            "pos tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4918==== Step 1  Train Loss 0.2570013999938965 acc = 0.0\n",
            "pos tensor(0.1363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4919==== Step 1  Train Loss 0.2451019138097763 acc = 0.0\n",
            "pos tensor(0.2220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4920==== Step 1  Train Loss 0.26329299807548523 acc = 0.0\n",
            "pos tensor(0.1542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4921==== Step 1  Train Loss 0.2470484972000122 acc = 0.0\n",
            "pos tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4922==== Step 1  Train Loss 0.26353415846824646 acc = 0.0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4923==== Step 1  Train Loss 0.2570783793926239 acc = 0.0\n",
            "pos tensor(0.2307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4924==== Step 1  Train Loss 0.27024030685424805 acc = 0.0\n",
            "pos tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4925==== Step 1  Train Loss 0.264457643032074 acc = 0.0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4926==== Step 1  Train Loss 0.2647126019001007 acc = 0.0\n",
            "pos tensor(0.2082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4927==== Step 1  Train Loss 0.25632476806640625 acc = 0.0\n",
            "pos tensor(0.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4928==== Step 1  Train Loss 0.2607111930847168 acc = 0.0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4929==== Step 1  Train Loss 0.2673940062522888 acc = 0.0\n",
            "pos tensor(0.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4930==== Step 1  Train Loss 0.2761019468307495 acc = 0.0\n",
            "pos tensor(0.1606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4931==== Step 1  Train Loss 0.2545435130596161 acc = 0.0\n",
            "pos tensor(0.2206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4932==== Step 1  Train Loss 0.2796146273612976 acc = 0.0\n",
            "pos tensor(0.1941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4933==== Step 1  Train Loss 0.2715243995189667 acc = 0.0\n",
            "pos tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4934==== Step 1  Train Loss 0.27183854579925537 acc = 0.0\n",
            "pos tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4935==== Step 1  Train Loss 0.2637718915939331 acc = 0.0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4936==== Step 1  Train Loss 0.2653978168964386 acc = 0.0\n",
            "pos tensor(0.1878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4937==== Step 1  Train Loss 0.2751840651035309 acc = 0.0\n",
            "pos tensor(0.2091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4938==== Step 1  Train Loss 0.27031999826431274 acc = 0.0\n",
            "pos tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4939==== Step 1  Train Loss 0.27334508299827576 acc = 0.0\n",
            "pos tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4940==== Step 1  Train Loss 0.2570914030075073 acc = 0.0\n",
            "pos tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4941==== Step 1  Train Loss 0.2557774782180786 acc = 0.0\n",
            "pos tensor(0.1607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4942==== Step 1  Train Loss 0.2560253441333771 acc = 0.0\n",
            "pos tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4943==== Step 1  Train Loss 0.2635921537876129 acc = 0.0\n",
            "pos tensor(0.1747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4944==== Step 1  Train Loss 0.2645567059516907 acc = 0.0\n",
            "pos tensor(0.1851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4945==== Step 1  Train Loss 0.26044824719429016 acc = 0.0\n",
            "pos tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4946==== Step 1  Train Loss 0.2527559995651245 acc = 0.0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4947==== Step 1  Train Loss 0.2683451175689697 acc = 0.0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4948==== Step 1  Train Loss 0.2611754536628723 acc = 0.0\n",
            "pos tensor(0.1412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4949==== Step 1  Train Loss 0.2670433521270752 acc = 0.0\n",
            "pos tensor(0.1909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4950==== Step 1  Train Loss 0.2585671544075012 acc = 0.0\n",
            "pos tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4951==== Step 1  Train Loss 0.2675940692424774 acc = 0.0\n",
            "pos tensor(0.1878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4952==== Step 1  Train Loss 0.2700697183609009 acc = 0.0\n",
            "pos tensor(0.1765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4953==== Step 1  Train Loss 0.25488972663879395 acc = 0.0\n",
            "pos tensor(0.2107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4954==== Step 1  Train Loss 0.26480746269226074 acc = 0.0\n",
            "pos tensor(0.1528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4955==== Step 1  Train Loss 0.2628839313983917 acc = 0.0\n",
            "pos tensor(0.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4956==== Step 1  Train Loss 0.25298428535461426 acc = 0.0\n",
            "pos tensor(0.1762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4957==== Step 1  Train Loss 0.26269349455833435 acc = 0.0\n",
            "pos tensor(0.1763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4958==== Step 1  Train Loss 0.25579702854156494 acc = 0.0\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4959==== Step 1  Train Loss 0.2617887854576111 acc = 0.0\n",
            "pos tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4960==== Step 1  Train Loss 0.278340607881546 acc = 0.0\n",
            "pos tensor(0.1325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4961==== Step 1  Train Loss 0.2700415849685669 acc = 0.0\n",
            "pos tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4962==== Step 1  Train Loss 0.25864100456237793 acc = 0.0\n",
            "pos tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4963==== Step 1  Train Loss 0.27104324102401733 acc = 0.0\n",
            "pos tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4964==== Step 1  Train Loss 0.26249146461486816 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4965==== Step 1  Train Loss 0.265116810798645 acc = 0.0\n",
            "pos tensor(0.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4966==== Step 1  Train Loss 0.27159133553504944 acc = 0.0\n",
            "pos tensor(0.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4967==== Step 1  Train Loss 0.2667471170425415 acc = 0.0\n",
            "pos tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4968==== Step 1  Train Loss 0.260295569896698 acc = 0.0\n",
            "pos tensor(0.1595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4969==== Step 1  Train Loss 0.2585001289844513 acc = 0.0\n",
            "pos tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4970==== Step 1  Train Loss 0.26014646887779236 acc = 0.0\n",
            "pos tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4971==== Step 1  Train Loss 0.26275312900543213 acc = 0.0\n",
            "pos tensor(0.1649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4972==== Step 1  Train Loss 0.2570236921310425 acc = 0.0\n",
            "pos tensor(0.1714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4973==== Step 1  Train Loss 0.2619237005710602 acc = 0.0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4974==== Step 1  Train Loss 0.2733237147331238 acc = 0.0\n",
            "pos tensor(0.1460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4975==== Step 1  Train Loss 0.24827724695205688 acc = 0.0\n",
            "pos tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4976==== Step 1  Train Loss 0.2770664691925049 acc = 0.0\n",
            "pos tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4977==== Step 1  Train Loss 0.269694983959198 acc = 0.0\n",
            "pos tensor(0.2269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4978==== Step 1  Train Loss 0.2729986310005188 acc = 0.0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4979==== Step 1  Train Loss 0.252835750579834 acc = 0.0\n",
            "pos tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4980==== Step 1  Train Loss 0.26438289880752563 acc = 0.0\n",
            "pos tensor(0.2088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4981==== Step 1  Train Loss 0.26842015981674194 acc = 0.0\n",
            "pos tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4982==== Step 1  Train Loss 0.258520245552063 acc = 0.0\n",
            "pos tensor(0.1870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4983==== Step 1  Train Loss 0.26560038328170776 acc = 0.0\n",
            "pos tensor(0.2102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4984==== Step 1  Train Loss 0.26973170042037964 acc = 0.0\n",
            "pos tensor(0.1560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4985==== Step 1  Train Loss 0.25887614488601685 acc = 0.0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4986==== Step 1  Train Loss 0.26644784212112427 acc = 0.0\n",
            "pos tensor(0.1596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4987==== Step 1  Train Loss 0.25858110189437866 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4988==== Step 1  Train Loss 0.2540392279624939 acc = 0.0\n",
            "pos tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4989==== Step 1  Train Loss 0.2761786878108978 acc = 0.0\n",
            "pos tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4990==== Step 1  Train Loss 0.2550954222679138 acc = 0.0\n",
            "pos tensor(0.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4991==== Step 1  Train Loss 0.25155383348464966 acc = 0.0\n",
            "pos tensor(0.1832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4992==== Step 1  Train Loss 0.2633303105831146 acc = 0.0\n",
            "pos tensor(0.1739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4993==== Step 1  Train Loss 0.25893205404281616 acc = 0.0\n",
            "pos tensor(0.2132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4994==== Step 1  Train Loss 0.2639278173446655 acc = 0.0\n",
            "pos tensor(0.1599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4995==== Step 1  Train Loss 0.24456673860549927 acc = 0.0\n",
            "pos tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4996==== Step 1  Train Loss 0.2582738399505615 acc = 0.0\n",
            "pos tensor(0.1989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4997==== Step 1  Train Loss 0.25107303261756897 acc = 0.0\n",
            "pos tensor(0.1826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4998==== Step 1  Train Loss 0.2710055708885193 acc = 0.0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4999==== Step 1  Train Loss 0.26222294569015503 acc = 0.0\n",
            "  Batch 5,000  of  5,204.    Elapsed: 1:04:27.\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5000==== Step 1  Train Loss 0.2468872368335724 acc = 0.0\n",
            "pos tensor(0.1746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5001==== Step 1  Train Loss 0.2685002386569977 acc = 0.0\n",
            "pos tensor(0.2189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5002==== Step 1  Train Loss 0.26424139738082886 acc = 0.0\n",
            "pos tensor(0.1831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5003==== Step 1  Train Loss 0.2683195173740387 acc = 0.0\n",
            "pos tensor(0.2006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5004==== Step 1  Train Loss 0.26751041412353516 acc = 0.0\n",
            "pos tensor(0.2023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5005==== Step 1  Train Loss 0.2661832869052887 acc = 0.0\n",
            "pos tensor(0.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5006==== Step 1  Train Loss 0.2666591703891754 acc = 0.0\n",
            "pos tensor(0.1876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5007==== Step 1  Train Loss 0.260331928730011 acc = 0.0\n",
            "pos tensor(0.1617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5008==== Step 1  Train Loss 0.2543509602546692 acc = 0.0\n",
            "pos tensor(0.2133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5009==== Step 1  Train Loss 0.26458704471588135 acc = 0.0\n",
            "pos tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5010==== Step 1  Train Loss 0.2694908678531647 acc = 0.0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5011==== Step 1  Train Loss 0.2580875754356384 acc = 0.0\n",
            "pos tensor(0.1862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5012==== Step 1  Train Loss 0.26627638936042786 acc = 0.0\n",
            "pos tensor(0.2127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5013==== Step 1  Train Loss 0.25725501775741577 acc = 0.0\n",
            "pos tensor(0.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5014==== Step 1  Train Loss 0.26402759552001953 acc = 0.0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5015==== Step 1  Train Loss 0.25992968678474426 acc = 0.0\n",
            "pos tensor(0.2052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5016==== Step 1  Train Loss 0.2587727904319763 acc = 0.0\n",
            "pos tensor(0.1910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5017==== Step 1  Train Loss 0.2535918354988098 acc = 0.0\n",
            "pos tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5018==== Step 1  Train Loss 0.2670347988605499 acc = 0.0\n",
            "pos tensor(0.2120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5019==== Step 1  Train Loss 0.2642260193824768 acc = 0.0\n",
            "pos tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5020==== Step 1  Train Loss 0.25624677538871765 acc = 0.0\n",
            "pos tensor(0.1981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5021==== Step 1  Train Loss 0.255803644657135 acc = 0.0\n",
            "pos tensor(0.1971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5022==== Step 1  Train Loss 0.2568378448486328 acc = 0.0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5023==== Step 1  Train Loss 0.2443002164363861 acc = 0.0\n",
            "pos tensor(0.2181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5024==== Step 1  Train Loss 0.26899680495262146 acc = 0.0\n",
            "pos tensor(0.2298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5025==== Step 1  Train Loss 0.2676427960395813 acc = 0.0\n",
            "pos tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5026==== Step 1  Train Loss 0.2576061487197876 acc = 0.0\n",
            "pos tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5027==== Step 1  Train Loss 0.26398831605911255 acc = 0.0\n",
            "pos tensor(0.2359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5028==== Step 1  Train Loss 0.2652483582496643 acc = 0.0\n",
            "pos tensor(0.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5029==== Step 1  Train Loss 0.2696075439453125 acc = 0.0\n",
            "pos tensor(0.2130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5030==== Step 1  Train Loss 0.27130234241485596 acc = 0.0\n",
            "pos tensor(0.1679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5031==== Step 1  Train Loss 0.25965628027915955 acc = 0.0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5032==== Step 1  Train Loss 0.2576209008693695 acc = 0.0\n",
            "pos tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5033==== Step 1  Train Loss 0.26625680923461914 acc = 0.0\n",
            "pos tensor(0.2216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5034==== Step 1  Train Loss 0.2609856426715851 acc = 0.0\n",
            "pos tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5035==== Step 1  Train Loss 0.26195067167282104 acc = 0.0\n",
            "pos tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5036==== Step 1  Train Loss 0.24995030462741852 acc = 0.0\n",
            "pos tensor(0.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5037==== Step 1  Train Loss 0.2575267553329468 acc = 0.0\n",
            "pos tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5038==== Step 1  Train Loss 0.26341384649276733 acc = 0.0\n",
            "pos tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5039==== Step 1  Train Loss 0.26730629801750183 acc = 0.0\n",
            "pos tensor(0.2016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5040==== Step 1  Train Loss 0.2569255232810974 acc = 0.0\n",
            "pos tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5041==== Step 1  Train Loss 0.2614768147468567 acc = 0.0\n",
            "pos tensor(0.2291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5042==== Step 1  Train Loss 0.2631239891052246 acc = 0.0\n",
            "pos tensor(0.2306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5043==== Step 1  Train Loss 0.28030622005462646 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5044==== Step 1  Train Loss 0.2502134442329407 acc = 0.0\n",
            "pos tensor(0.2033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5045==== Step 1  Train Loss 0.26041096448898315 acc = 0.0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5046==== Step 1  Train Loss 0.2562941908836365 acc = 0.0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5047==== Step 1  Train Loss 0.26845046877861023 acc = 0.0\n",
            "pos tensor(0.1826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5048==== Step 1  Train Loss 0.24631601572036743 acc = 0.0\n",
            "pos tensor(0.1990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5049==== Step 1  Train Loss 0.25646618008613586 acc = 0.0\n",
            "pos tensor(0.2179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5050==== Step 1  Train Loss 0.2706368565559387 acc = 0.0\n",
            "pos tensor(0.2386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5051==== Step 1  Train Loss 0.2621772885322571 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5052==== Step 1  Train Loss 0.2562447190284729 acc = 0.0\n",
            "pos tensor(0.2313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5053==== Step 1  Train Loss 0.2755712568759918 acc = 0.0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5054==== Step 1  Train Loss 0.24312962591648102 acc = 0.0\n",
            "pos tensor(0.2065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5055==== Step 1  Train Loss 0.26229020953178406 acc = 0.0\n",
            "pos tensor(0.2027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5056==== Step 1  Train Loss 0.25662633776664734 acc = 0.0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5057==== Step 1  Train Loss 0.24594536423683167 acc = 0.0\n",
            "pos tensor(0.2113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5058==== Step 1  Train Loss 0.2610226273536682 acc = 0.0\n",
            "pos tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5059==== Step 1  Train Loss 0.2518758475780487 acc = 0.0\n",
            "pos tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5060==== Step 1  Train Loss 0.2681247889995575 acc = 0.0\n",
            "pos tensor(0.2265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5061==== Step 1  Train Loss 0.262065589427948 acc = 0.0\n",
            "pos tensor(0.2298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5062==== Step 1  Train Loss 0.25790634751319885 acc = 0.0\n",
            "pos tensor(0.2209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5063==== Step 1  Train Loss 0.263998419046402 acc = 0.0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5064==== Step 1  Train Loss 0.26296693086624146 acc = 0.0\n",
            "pos tensor(0.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5065==== Step 1  Train Loss 0.26253145933151245 acc = 0.0\n",
            "pos tensor(0.1831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5066==== Step 1  Train Loss 0.23358884453773499 acc = 0.0\n",
            "pos tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5067==== Step 1  Train Loss 0.23448477685451508 acc = 0.0\n",
            "pos tensor(0.2199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5068==== Step 1  Train Loss 0.26129013299942017 acc = 0.0\n",
            "pos tensor(0.2347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5069==== Step 1  Train Loss 0.25665682554244995 acc = 0.0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5070==== Step 1  Train Loss 0.25491517782211304 acc = 0.0\n",
            "pos tensor(0.2359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5071==== Step 1  Train Loss 0.261910080909729 acc = 0.0\n",
            "pos tensor(0.2352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5072==== Step 1  Train Loss 0.2669762969017029 acc = 0.0\n",
            "pos tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5073==== Step 1  Train Loss 0.2560010254383087 acc = 0.0\n",
            "pos tensor(0.2500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5074==== Step 1  Train Loss 0.2680424749851227 acc = 0.0\n",
            "pos tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5075==== Step 1  Train Loss 0.24782955646514893 acc = 0.0\n",
            "pos tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5076==== Step 1  Train Loss 0.2631414234638214 acc = 0.0\n",
            "pos tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5077==== Step 1  Train Loss 0.24850553274154663 acc = 0.0\n",
            "pos tensor(0.2313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5078==== Step 1  Train Loss 0.2648068964481354 acc = 0.0\n",
            "pos tensor(0.2304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5079==== Step 1  Train Loss 0.25676190853118896 acc = 0.0\n",
            "pos tensor(0.2084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5080==== Step 1  Train Loss 0.2566925883293152 acc = 0.0\n",
            "pos tensor(0.2256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5081==== Step 1  Train Loss 0.2630598545074463 acc = 0.0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5082==== Step 1  Train Loss 0.24466048181056976 acc = 0.0\n",
            "pos tensor(0.2132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5083==== Step 1  Train Loss 0.24775908887386322 acc = 0.0\n",
            "pos tensor(0.2203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5084==== Step 1  Train Loss 0.24909146130084991 acc = 0.0\n",
            "pos tensor(0.2276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5085==== Step 1  Train Loss 0.2663872241973877 acc = 0.0\n",
            "pos tensor(0.2485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5086==== Step 1  Train Loss 0.27376312017440796 acc = 0.0\n",
            "pos tensor(0.2014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5087==== Step 1  Train Loss 0.2670626640319824 acc = 0.0\n",
            "pos tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5088==== Step 1  Train Loss 0.26570406556129456 acc = 0.0\n",
            "pos tensor(0.1810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5089==== Step 1  Train Loss 0.24843475222587585 acc = 0.0\n",
            "pos tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5090==== Step 1  Train Loss 0.24292954802513123 acc = 0.0\n",
            "pos tensor(0.2177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5091==== Step 1  Train Loss 0.26990586519241333 acc = 0.0\n",
            "pos tensor(0.1904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5092==== Step 1  Train Loss 0.2474159151315689 acc = 0.0\n",
            "pos tensor(0.1841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5093==== Step 1  Train Loss 0.254507839679718 acc = 0.0\n",
            "pos tensor(0.2227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5094==== Step 1  Train Loss 0.26845911145210266 acc = 0.0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5095==== Step 1  Train Loss 0.2627977728843689 acc = 0.0\n",
            "pos tensor(0.2501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5096==== Step 1  Train Loss 0.27495163679122925 acc = 0.0\n",
            "pos tensor(0.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5097==== Step 1  Train Loss 0.2601402699947357 acc = 0.0\n",
            "pos tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5098==== Step 1  Train Loss 0.2595306634902954 acc = 0.0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5099==== Step 1  Train Loss 0.26014190912246704 acc = 0.0\n",
            "  Batch 5,100  of  5,204.    Elapsed: 1:05:44.\n",
            "pos tensor(0.1595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5100==== Step 1  Train Loss 0.23124751448631287 acc = 0.0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5101==== Step 1  Train Loss 0.26499396562576294 acc = 0.0\n",
            "pos tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5102==== Step 1  Train Loss 0.26057833433151245 acc = 0.0\n",
            "pos tensor(0.2363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5103==== Step 1  Train Loss 0.2633581757545471 acc = 0.0\n",
            "pos tensor(0.2717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5104==== Step 1  Train Loss 0.2825254797935486 acc = 0.0\n",
            "pos tensor(0.2027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5105==== Step 1  Train Loss 0.2689487934112549 acc = 0.0\n",
            "pos tensor(0.1609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5106==== Step 1  Train Loss 0.25516289472579956 acc = 0.0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5107==== Step 1  Train Loss 0.2655167579650879 acc = 0.0\n",
            "pos tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5108==== Step 1  Train Loss 0.28274762630462646 acc = 0.0\n",
            "pos tensor(0.2097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5109==== Step 1  Train Loss 0.25841426849365234 acc = 0.0\n",
            "pos tensor(0.1827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5110==== Step 1  Train Loss 0.24808333814144135 acc = 0.0\n",
            "pos tensor(0.1896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5111==== Step 1  Train Loss 0.27585357427597046 acc = 0.0\n",
            "pos tensor(0.2154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5112==== Step 1  Train Loss 0.26872390508651733 acc = 0.0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5113==== Step 1  Train Loss 0.27165648341178894 acc = 0.0\n",
            "pos tensor(0.2236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5114==== Step 1  Train Loss 0.27515941858291626 acc = 0.0\n",
            "pos tensor(0.2265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5115==== Step 1  Train Loss 0.2651313543319702 acc = 0.0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5116==== Step 1  Train Loss 0.24636167287826538 acc = 0.0\n",
            "pos tensor(0.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5117==== Step 1  Train Loss 0.2608943283557892 acc = 0.0\n",
            "pos tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5118==== Step 1  Train Loss 0.2785194516181946 acc = 0.0\n",
            "pos tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5119==== Step 1  Train Loss 0.26024913787841797 acc = 0.0\n",
            "pos tensor(0.2163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5120==== Step 1  Train Loss 0.253817081451416 acc = 0.0\n",
            "pos tensor(0.1919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5121==== Step 1  Train Loss 0.26297733187675476 acc = 0.0\n",
            "pos tensor(0.2271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5122==== Step 1  Train Loss 0.26503556966781616 acc = 0.0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5123==== Step 1  Train Loss 0.26004666090011597 acc = 0.0\n",
            "pos tensor(0.1736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5124==== Step 1  Train Loss 0.26479244232177734 acc = 0.0\n",
            "pos tensor(0.1989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5125==== Step 1  Train Loss 0.2680990695953369 acc = 0.0\n",
            "pos tensor(0.2192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5126==== Step 1  Train Loss 0.2732129693031311 acc = 0.0\n",
            "pos tensor(0.2728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5127==== Step 1  Train Loss 0.28722602128982544 acc = 0.0\n",
            "pos tensor(0.1433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5128==== Step 1  Train Loss 0.25364476442337036 acc = 0.0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5129==== Step 1  Train Loss 0.2723866403102875 acc = 0.0\n",
            "pos tensor(0.2158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5130==== Step 1  Train Loss 0.2705686688423157 acc = 0.0\n",
            "pos tensor(0.1804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5131==== Step 1  Train Loss 0.25744742155075073 acc = 0.0\n",
            "pos tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5132==== Step 1  Train Loss 0.2663349509239197 acc = 0.0\n",
            "pos tensor(0.1865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5133==== Step 1  Train Loss 0.2686271369457245 acc = 0.0\n",
            "pos tensor(0.2235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5134==== Step 1  Train Loss 0.288632333278656 acc = 0.0\n",
            "pos tensor(0.1787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5135==== Step 1  Train Loss 0.2797491252422333 acc = 0.0\n",
            "pos tensor(0.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5136==== Step 1  Train Loss 0.2747529149055481 acc = 0.0\n",
            "pos tensor(0.1652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5137==== Step 1  Train Loss 0.2633814215660095 acc = 0.0\n",
            "pos tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5138==== Step 1  Train Loss 0.2638525366783142 acc = 0.0\n",
            "pos tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5139==== Step 1  Train Loss 0.2608994245529175 acc = 0.0\n",
            "pos tensor(0.2257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5140==== Step 1  Train Loss 0.2783604562282562 acc = 0.0\n",
            "pos tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5141==== Step 1  Train Loss 0.26289352774620056 acc = 0.0\n",
            "pos tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5142==== Step 1  Train Loss 0.2786320149898529 acc = 0.0\n",
            "pos tensor(0.1659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5143==== Step 1  Train Loss 0.25157034397125244 acc = 0.0\n",
            "pos tensor(0.2020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5144==== Step 1  Train Loss 0.27429112792015076 acc = 0.0\n",
            "pos tensor(0.1794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5145==== Step 1  Train Loss 0.2749555706977844 acc = 0.0\n",
            "pos tensor(0.2105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5146==== Step 1  Train Loss 0.2721712291240692 acc = 0.0\n",
            "pos tensor(0.1810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5147==== Step 1  Train Loss 0.2595639228820801 acc = 0.0\n",
            "pos tensor(0.1884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5148==== Step 1  Train Loss 0.2619604170322418 acc = 0.0\n",
            "pos tensor(0.2029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5149==== Step 1  Train Loss 0.27711808681488037 acc = 0.0\n",
            "pos tensor(0.1983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5150==== Step 1  Train Loss 0.2730153799057007 acc = 0.0\n",
            "pos tensor(0.2009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5151==== Step 1  Train Loss 0.26036882400512695 acc = 0.0\n",
            "pos tensor(0.1692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5152==== Step 1  Train Loss 0.2660309970378876 acc = 0.0\n",
            "pos tensor(0.2190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5153==== Step 1  Train Loss 0.27158188819885254 acc = 0.0\n",
            "pos tensor(0.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5154==== Step 1  Train Loss 0.2727881371974945 acc = 0.0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5155==== Step 1  Train Loss 0.28229719400405884 acc = 0.0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5156==== Step 1  Train Loss 0.2694525420665741 acc = 0.0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5157==== Step 1  Train Loss 0.274411678314209 acc = 0.0\n",
            "pos tensor(0.1372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5158==== Step 1  Train Loss 0.25056955218315125 acc = 0.0\n",
            "pos tensor(0.2184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5159==== Step 1  Train Loss 0.26697397232055664 acc = 0.0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5160==== Step 1  Train Loss 0.2637922763824463 acc = 0.0\n",
            "pos tensor(0.2114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5161==== Step 1  Train Loss 0.2789943218231201 acc = 0.0\n",
            "pos tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5162==== Step 1  Train Loss 0.2655285596847534 acc = 0.0\n",
            "pos tensor(0.1679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5163==== Step 1  Train Loss 0.26206159591674805 acc = 0.0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5164==== Step 1  Train Loss 0.26207634806632996 acc = 0.0\n",
            "pos tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5165==== Step 1  Train Loss 0.2649717330932617 acc = 0.0\n",
            "pos tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5166==== Step 1  Train Loss 0.26758795976638794 acc = 0.0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5167==== Step 1  Train Loss 0.2758443057537079 acc = 0.0\n",
            "pos tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5168==== Step 1  Train Loss 0.2619505822658539 acc = 0.0\n",
            "pos tensor(0.2227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5169==== Step 1  Train Loss 0.2670169174671173 acc = 0.0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5170==== Step 1  Train Loss 0.2642504870891571 acc = 0.0\n",
            "pos tensor(0.1923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5171==== Step 1  Train Loss 0.2721223831176758 acc = 0.0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5172==== Step 1  Train Loss 0.2624271512031555 acc = 0.0\n",
            "pos tensor(0.2584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5173==== Step 1  Train Loss 0.28624409437179565 acc = 0.0\n",
            "pos tensor(0.1773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5174==== Step 1  Train Loss 0.27063992619514465 acc = 0.0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5175==== Step 1  Train Loss 0.2787873446941376 acc = 0.0\n",
            "pos tensor(0.1483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5176==== Step 1  Train Loss 0.2660747766494751 acc = 0.0\n",
            "pos tensor(0.1456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5177==== Step 1  Train Loss 0.23915953934192657 acc = 0.0\n",
            "pos tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5178==== Step 1  Train Loss 0.2719936966896057 acc = 0.0\n",
            "pos tensor(0.1774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5179==== Step 1  Train Loss 0.27738112211227417 acc = 0.0\n",
            "pos tensor(0.1895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5180==== Step 1  Train Loss 0.27202069759368896 acc = 0.0\n",
            "pos tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5181==== Step 1  Train Loss 0.2743312120437622 acc = 0.0\n",
            "pos tensor(0.1485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5182==== Step 1  Train Loss 0.2542617619037628 acc = 0.0\n",
            "pos tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5183==== Step 1  Train Loss 0.2678167223930359 acc = 0.0\n",
            "pos tensor(0.1965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5184==== Step 1  Train Loss 0.275087833404541 acc = 0.0\n",
            "pos tensor(0.2306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5185==== Step 1  Train Loss 0.28431519865989685 acc = 0.0\n",
            "pos tensor(0.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5186==== Step 1  Train Loss 0.2700996398925781 acc = 0.0\n",
            "pos tensor(0.1997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5187==== Step 1  Train Loss 0.2678173780441284 acc = 0.0\n",
            "pos tensor(0.2343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5188==== Step 1  Train Loss 0.27842265367507935 acc = 0.0\n",
            "pos tensor(0.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5189==== Step 1  Train Loss 0.26606491208076477 acc = 0.0\n",
            "pos tensor(0.1893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5190==== Step 1  Train Loss 0.27397894859313965 acc = 0.0\n",
            "pos tensor(0.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5191==== Step 1  Train Loss 0.2758220434188843 acc = 0.0\n",
            "pos tensor(0.2331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5192==== Step 1  Train Loss 0.27175554633140564 acc = 0.0\n",
            "pos tensor(0.1940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5193==== Step 1  Train Loss 0.26472896337509155 acc = 0.0\n",
            "pos tensor(0.1780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5194==== Step 1  Train Loss 0.2527494728565216 acc = 0.0\n",
            "pos tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5195==== Step 1  Train Loss 0.25680261850357056 acc = 0.0\n",
            "pos tensor(0.1493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5196==== Step 1  Train Loss 0.2742757499217987 acc = 0.0\n",
            "pos tensor(0.2254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5197==== Step 1  Train Loss 0.28111207485198975 acc = 0.0\n",
            "pos tensor(0.1866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5198==== Step 1  Train Loss 0.2561798393726349 acc = 0.0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5199==== Step 1  Train Loss 0.2660117745399475 acc = 0.0\n",
            "  Batch 5,200  of  5,204.    Elapsed: 1:07:01.\n",
            "pos tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5200==== Step 1  Train Loss 0.26051801443099976 acc = 0.0\n",
            "pos tensor(0.1993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5201==== Step 1  Train Loss 0.26202037930488586 acc = 0.0\n",
            "pos tensor(0.2256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5202==== Step 1  Train Loss 0.2635008692741394 acc = 0.0\n",
            "pos tensor(0.1866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5203==== Step 1  Train Loss 0.2672136425971985 acc = 0.0\n",
            "pos tensor(0.1866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5204==== Step 1  Train Loss 0.2512873411178589 acc = 0.0\n",
            "========== Epoch 1 ==== Step 1 AVG. Train Loss 0.26449001589353316\n",
            "\n",
            "  Training epoch took: 1:07:04\n",
            "\n",
            "Running Validation...\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7996, device='cuda:0')\n",
            "========== Epoch 1 Batch 1==== Step 1 AVG. val Loss 0.40649956464767456 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.8071, device='cuda:0')\n",
            "========== Epoch 1 Batch 2==== Step 1 AVG. val Loss 0.41097450256347656 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 3==== Step 1 AVG. val Loss 0.4051622450351715 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8127, device='cuda:0')\n",
            "========== Epoch 1 Batch 4==== Step 1 AVG. val Loss 0.41250506043434143 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8022, device='cuda:0')\n",
            "========== Epoch 1 Batch 5==== Step 1 AVG. val Loss 0.4073503911495209 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8005, device='cuda:0')\n",
            "========== Epoch 1 Batch 6==== Step 1 AVG. val Loss 0.4061948359012604 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7977, device='cuda:0')\n",
            "========== Epoch 1 Batch 7==== Step 1 AVG. val Loss 0.40555018186569214 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.8011, device='cuda:0')\n",
            "========== Epoch 1 Batch 8==== Step 1 AVG. val Loss 0.4079447388648987 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8069, device='cuda:0')\n",
            "========== Epoch 1 Batch 9==== Step 1 AVG. val Loss 0.4094204008579254 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7919, device='cuda:0')\n",
            "========== Epoch 1 Batch 10==== Step 1 AVG. val Loss 0.402997225522995 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7852, device='cuda:0')\n",
            "========== Epoch 1 Batch 11==== Step 1 AVG. val Loss 0.4004318416118622 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.7926, device='cuda:0')\n",
            "========== Epoch 1 Batch 12==== Step 1 AVG. val Loss 0.4029732644557953 acc = 0.0\n",
            "pos tensor(0.0113, device='cuda:0')\n",
            "neg tensor(0.8023, device='cuda:0')\n",
            "========== Epoch 1 Batch 13==== Step 1 AVG. val Loss 0.40682223439216614 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 14==== Step 1 AVG. val Loss 0.40729251503944397 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8155, device='cuda:0')\n",
            "========== Epoch 1 Batch 15==== Step 1 AVG. val Loss 0.41330453753471375 acc = 0.0\n",
            "pos tensor(0.0161, device='cuda:0')\n",
            "neg tensor(0.7937, device='cuda:0')\n",
            "========== Epoch 1 Batch 16==== Step 1 AVG. val Loss 0.40490230917930603 acc = 0.0\n",
            "pos tensor(0.0161, device='cuda:0')\n",
            "neg tensor(0.7814, device='cuda:0')\n",
            "========== Epoch 1 Batch 17==== Step 1 AVG. val Loss 0.39878612756729126 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8037, device='cuda:0')\n",
            "========== Epoch 1 Batch 18==== Step 1 AVG. val Loss 0.4079477787017822 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8074, device='cuda:0')\n",
            "========== Epoch 1 Batch 19==== Step 1 AVG. val Loss 0.4094284474849701 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7973, device='cuda:0')\n",
            "========== Epoch 1 Batch 20==== Step 1 AVG. val Loss 0.4045742452144623 acc = 0.0\n",
            "pos tensor(0.0186, device='cuda:0')\n",
            "neg tensor(0.7770, device='cuda:0')\n",
            "========== Epoch 1 Batch 21==== Step 1 AVG. val Loss 0.39781612157821655 acc = 0.0\n",
            "pos tensor(0.0155, device='cuda:0')\n",
            "neg tensor(0.7911, device='cuda:0')\n",
            "========== Epoch 1 Batch 22==== Step 1 AVG. val Loss 0.40327924489974976 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7914, device='cuda:0')\n",
            "========== Epoch 1 Batch 23==== Step 1 AVG. val Loss 0.4028756022453308 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7846, device='cuda:0')\n",
            "========== Epoch 1 Batch 24==== Step 1 AVG. val Loss 0.3987630009651184 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8094, device='cuda:0')\n",
            "========== Epoch 1 Batch 25==== Step 1 AVG. val Loss 0.4101243019104004 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8001, device='cuda:0')\n",
            "========== Epoch 1 Batch 26==== Step 1 AVG. val Loss 0.4061066508293152 acc = 0.0\n",
            "pos tensor(0.0165, device='cuda:0')\n",
            "neg tensor(0.7906, device='cuda:0')\n",
            "========== Epoch 1 Batch 27==== Step 1 AVG. val Loss 0.40352362394332886 acc = 0.0\n",
            "pos tensor(0.0142, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 28==== Step 1 AVG. val Loss 0.40270936489105225 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8185, device='cuda:0')\n",
            "========== Epoch 1 Batch 29==== Step 1 AVG. val Loss 0.4147396385669708 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.8096, device='cuda:0')\n",
            "========== Epoch 1 Batch 30==== Step 1 AVG. val Loss 0.4109843373298645 acc = 0.0\n",
            "pos tensor(0.0102, device='cuda:0')\n",
            "neg tensor(0.8066, device='cuda:0')\n",
            "========== Epoch 1 Batch 31==== Step 1 AVG. val Loss 0.40841326117515564 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.7984, device='cuda:0')\n",
            "========== Epoch 1 Batch 32==== Step 1 AVG. val Loss 0.4047342538833618 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.7891, device='cuda:0')\n",
            "========== Epoch 1 Batch 33==== Step 1 AVG. val Loss 0.4004902243614197 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7916, device='cuda:0')\n",
            "========== Epoch 1 Batch 34==== Step 1 AVG. val Loss 0.40251103043556213 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7895, device='cuda:0')\n",
            "========== Epoch 1 Batch 35==== Step 1 AVG. val Loss 0.40178847312927246 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8271, device='cuda:0')\n",
            "========== Epoch 1 Batch 36==== Step 1 AVG. val Loss 0.4189073443412781 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.7959, device='cuda:0')\n",
            "========== Epoch 1 Batch 37==== Step 1 AVG. val Loss 0.4031917154788971 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8146, device='cuda:0')\n",
            "========== Epoch 1 Batch 38==== Step 1 AVG. val Loss 0.413169264793396 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7938, device='cuda:0')\n",
            "========== Epoch 1 Batch 39==== Step 1 AVG. val Loss 0.40378132462501526 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7999, device='cuda:0')\n",
            "========== Epoch 1 Batch 40==== Step 1 AVG. val Loss 0.40628138184547424 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7936, device='cuda:0')\n",
            "========== Epoch 1 Batch 41==== Step 1 AVG. val Loss 0.4041842818260193 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7980, device='cuda:0')\n",
            "========== Epoch 1 Batch 42==== Step 1 AVG. val Loss 0.40598928928375244 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.7981, device='cuda:0')\n",
            "========== Epoch 1 Batch 43==== Step 1 AVG. val Loss 0.4051169753074646 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7904, device='cuda:0')\n",
            "========== Epoch 1 Batch 44==== Step 1 AVG. val Loss 0.40291547775268555 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7836, device='cuda:0')\n",
            "========== Epoch 1 Batch 45==== Step 1 AVG. val Loss 0.3996885418891907 acc = 0.0\n",
            "pos tensor(0.0165, device='cuda:0')\n",
            "neg tensor(0.7747, device='cuda:0')\n",
            "========== Epoch 1 Batch 46==== Step 1 AVG. val Loss 0.3956061601638794 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7848, device='cuda:0')\n",
            "========== Epoch 1 Batch 47==== Step 1 AVG. val Loss 0.3997304141521454 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8075, device='cuda:0')\n",
            "========== Epoch 1 Batch 48==== Step 1 AVG. val Loss 0.4095943570137024 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.7901, device='cuda:0')\n",
            "========== Epoch 1 Batch 49==== Step 1 AVG. val Loss 0.40144166350364685 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8010, device='cuda:0')\n",
            "========== Epoch 1 Batch 50==== Step 1 AVG. val Loss 0.407467246055603 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8024, device='cuda:0')\n",
            "========== Epoch 1 Batch 51==== Step 1 AVG. val Loss 0.4065450429916382 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8091, device='cuda:0')\n",
            "========== Epoch 1 Batch 52==== Step 1 AVG. val Loss 0.41015538573265076 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8018, device='cuda:0')\n",
            "========== Epoch 1 Batch 53==== Step 1 AVG. val Loss 0.40652161836624146 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8154, device='cuda:0')\n",
            "========== Epoch 1 Batch 54==== Step 1 AVG. val Loss 0.4130227565765381 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7890, device='cuda:0')\n",
            "========== Epoch 1 Batch 55==== Step 1 AVG. val Loss 0.4019920825958252 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7924, device='cuda:0')\n",
            "========== Epoch 1 Batch 56==== Step 1 AVG. val Loss 0.40313225984573364 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7856, device='cuda:0')\n",
            "========== Epoch 1 Batch 57==== Step 1 AVG. val Loss 0.4001942276954651 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8122, device='cuda:0')\n",
            "========== Epoch 1 Batch 58==== Step 1 AVG. val Loss 0.4121339023113251 acc = 0.0\n",
            "pos tensor(0.0227, device='cuda:0')\n",
            "neg tensor(0.7530, device='cuda:0')\n",
            "========== Epoch 1 Batch 59==== Step 1 AVG. val Loss 0.38783717155456543 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8222, device='cuda:0')\n",
            "========== Epoch 1 Batch 60==== Step 1 AVG. val Loss 0.41710129380226135 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.8014, device='cuda:0')\n",
            "========== Epoch 1 Batch 61==== Step 1 AVG. val Loss 0.40771156549453735 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.8035, device='cuda:0')\n",
            "========== Epoch 1 Batch 62==== Step 1 AVG. val Loss 0.40842053294181824 acc = 0.0\n",
            "pos tensor(0.0090, device='cuda:0')\n",
            "neg tensor(0.8222, device='cuda:0')\n",
            "========== Epoch 1 Batch 63==== Step 1 AVG. val Loss 0.41557976603507996 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7884, device='cuda:0')\n",
            "========== Epoch 1 Batch 64==== Step 1 AVG. val Loss 0.400927871465683 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7989, device='cuda:0')\n",
            "========== Epoch 1 Batch 65==== Step 1 AVG. val Loss 0.4057190716266632 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 66==== Step 1 AVG. val Loss 0.40248435735702515 acc = 0.0\n",
            "pos tensor(0.0095, device='cuda:0')\n",
            "neg tensor(0.8189, device='cuda:0')\n",
            "========== Epoch 1 Batch 67==== Step 1 AVG. val Loss 0.41421568393707275 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8107, device='cuda:0')\n",
            "========== Epoch 1 Batch 68==== Step 1 AVG. val Loss 0.4107707440853119 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8012, device='cuda:0')\n",
            "========== Epoch 1 Batch 69==== Step 1 AVG. val Loss 0.4063847064971924 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.8001, device='cuda:0')\n",
            "========== Epoch 1 Batch 70==== Step 1 AVG. val Loss 0.40745657682418823 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8064, device='cuda:0')\n",
            "========== Epoch 1 Batch 71==== Step 1 AVG. val Loss 0.4095878601074219 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7929, device='cuda:0')\n",
            "========== Epoch 1 Batch 72==== Step 1 AVG. val Loss 0.4037698209285736 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8031, device='cuda:0')\n",
            "========== Epoch 1 Batch 73==== Step 1 AVG. val Loss 0.40781930088996887 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7988, device='cuda:0')\n",
            "========== Epoch 1 Batch 74==== Step 1 AVG. val Loss 0.40612101554870605 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.7993, device='cuda:0')\n",
            "========== Epoch 1 Batch 75==== Step 1 AVG. val Loss 0.40596961975097656 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7829, device='cuda:0')\n",
            "========== Epoch 1 Batch 76==== Step 1 AVG. val Loss 0.39828696846961975 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7956, device='cuda:0')\n",
            "========== Epoch 1 Batch 77==== Step 1 AVG. val Loss 0.40484359860420227 acc = 0.0\n",
            "pos tensor(0.0101, device='cuda:0')\n",
            "neg tensor(0.8020, device='cuda:0')\n",
            "========== Epoch 1 Batch 78==== Step 1 AVG. val Loss 0.4060458540916443 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7917, device='cuda:0')\n",
            "========== Epoch 1 Batch 79==== Step 1 AVG. val Loss 0.4028780162334442 acc = 0.0\n",
            "pos tensor(0.0103, device='cuda:0')\n",
            "neg tensor(0.8236, device='cuda:0')\n",
            "========== Epoch 1 Batch 80==== Step 1 AVG. val Loss 0.4169393479824066 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.7946, device='cuda:0')\n",
            "========== Epoch 1 Batch 81==== Step 1 AVG. val Loss 0.40261635184288025 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7998, device='cuda:0')\n",
            "========== Epoch 1 Batch 82==== Step 1 AVG. val Loss 0.4053455889225006 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7917, device='cuda:0')\n",
            "========== Epoch 1 Batch 83==== Step 1 AVG. val Loss 0.40212658047676086 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.7906, device='cuda:0')\n",
            "========== Epoch 1 Batch 84==== Step 1 AVG. val Loss 0.4008207619190216 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7868, device='cuda:0')\n",
            "========== Epoch 1 Batch 85==== Step 1 AVG. val Loss 0.40015408396720886 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7813, device='cuda:0')\n",
            "========== Epoch 1 Batch 86==== Step 1 AVG. val Loss 0.3974861204624176 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8070, device='cuda:0')\n",
            "========== Epoch 1 Batch 87==== Step 1 AVG. val Loss 0.4096207320690155 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8040, device='cuda:0')\n",
            "========== Epoch 1 Batch 88==== Step 1 AVG. val Loss 0.40795716643333435 acc = 0.0\n",
            "pos tensor(0.0189, device='cuda:0')\n",
            "neg tensor(0.7742, device='cuda:0')\n",
            "========== Epoch 1 Batch 89==== Step 1 AVG. val Loss 0.3965891897678375 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7892, device='cuda:0')\n",
            "========== Epoch 1 Batch 90==== Step 1 AVG. val Loss 0.4019983410835266 acc = 0.0\n",
            "pos tensor(0.0160, device='cuda:0')\n",
            "neg tensor(0.7945, device='cuda:0')\n",
            "========== Epoch 1 Batch 91==== Step 1 AVG. val Loss 0.4052721858024597 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.7925, device='cuda:0')\n",
            "========== Epoch 1 Batch 92==== Step 1 AVG. val Loss 0.4014880657196045 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8121, device='cuda:0')\n",
            "========== Epoch 1 Batch 93==== Step 1 AVG. val Loss 0.41188040375709534 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7927, device='cuda:0')\n",
            "========== Epoch 1 Batch 94==== Step 1 AVG. val Loss 0.4031427204608917 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8142, device='cuda:0')\n",
            "========== Epoch 1 Batch 95==== Step 1 AVG. val Loss 0.4123496413230896 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7905, device='cuda:0')\n",
            "========== Epoch 1 Batch 96==== Step 1 AVG. val Loss 0.4021115303039551 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8159, device='cuda:0')\n",
            "========== Epoch 1 Batch 97==== Step 1 AVG. val Loss 0.4135233461856842 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8039, device='cuda:0')\n",
            "========== Epoch 1 Batch 98==== Step 1 AVG. val Loss 0.4077126085758209 acc = 0.0\n",
            "pos tensor(0.0169, device='cuda:0')\n",
            "neg tensor(0.7865, device='cuda:0')\n",
            "========== Epoch 1 Batch 99==== Step 1 AVG. val Loss 0.4017016291618347 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8007, device='cuda:0')\n",
            "========== Epoch 1 Batch 100==== Step 1 AVG. val Loss 0.4063684642314911 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7831, device='cuda:0')\n",
            "========== Epoch 1 Batch 101==== Step 1 AVG. val Loss 0.3989099860191345 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8004, device='cuda:0')\n",
            "========== Epoch 1 Batch 102==== Step 1 AVG. val Loss 0.406831294298172 acc = 0.0\n",
            "pos tensor(0.0182, device='cuda:0')\n",
            "neg tensor(0.7765, device='cuda:0')\n",
            "========== Epoch 1 Batch 103==== Step 1 AVG. val Loss 0.3973700702190399 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7981, device='cuda:0')\n",
            "========== Epoch 1 Batch 104==== Step 1 AVG. val Loss 0.40603071451187134 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7980, device='cuda:0')\n",
            "========== Epoch 1 Batch 105==== Step 1 AVG. val Loss 0.40569868683815 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.7924, device='cuda:0')\n",
            "========== Epoch 1 Batch 106==== Step 1 AVG. val Loss 0.4022332727909088 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.7928, device='cuda:0')\n",
            "========== Epoch 1 Batch 107==== Step 1 AVG. val Loss 0.40247422456741333 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7964, device='cuda:0')\n",
            "========== Epoch 1 Batch 108==== Step 1 AVG. val Loss 0.40492159128189087 acc = 0.0\n",
            "pos tensor(0.0098, device='cuda:0')\n",
            "neg tensor(0.8096, device='cuda:0')\n",
            "========== Epoch 1 Batch 109==== Step 1 AVG. val Loss 0.409667432308197 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7849, device='cuda:0')\n",
            "========== Epoch 1 Batch 110==== Step 1 AVG. val Loss 0.39946386218070984 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8058, device='cuda:0')\n",
            "========== Epoch 1 Batch 111==== Step 1 AVG. val Loss 0.40844476222991943 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7909, device='cuda:0')\n",
            "========== Epoch 1 Batch 112==== Step 1 AVG. val Loss 0.402705579996109 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8192, device='cuda:0')\n",
            "========== Epoch 1 Batch 113==== Step 1 AVG. val Loss 0.4142872989177704 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8075, device='cuda:0')\n",
            "========== Epoch 1 Batch 114==== Step 1 AVG. val Loss 0.4096750319004059 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.7968, device='cuda:0')\n",
            "========== Epoch 1 Batch 115==== Step 1 AVG. val Loss 0.40460658073425293 acc = 0.0\n",
            "pos tensor(0.0166, device='cuda:0')\n",
            "neg tensor(0.8005, device='cuda:0')\n",
            "========== Epoch 1 Batch 116==== Step 1 AVG. val Loss 0.40852877497673035 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7830, device='cuda:0')\n",
            "========== Epoch 1 Batch 117==== Step 1 AVG. val Loss 0.3976796269416809 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.7858, device='cuda:0')\n",
            "========== Epoch 1 Batch 118==== Step 1 AVG. val Loss 0.3988136053085327 acc = 0.0\n",
            "pos tensor(0.0091, device='cuda:0')\n",
            "neg tensor(0.8047, device='cuda:0')\n",
            "========== Epoch 1 Batch 119==== Step 1 AVG. val Loss 0.4068591892719269 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 120==== Step 1 AVG. val Loss 0.40264374017715454 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.7944, device='cuda:0')\n",
            "========== Epoch 1 Batch 121==== Step 1 AVG. val Loss 0.40298014879226685 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.8081, device='cuda:0')\n",
            "========== Epoch 1 Batch 122==== Step 1 AVG. val Loss 0.410602331161499 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8081, device='cuda:0')\n",
            "========== Epoch 1 Batch 123==== Step 1 AVG. val Loss 0.4095239043235779 acc = 0.0\n",
            "pos tensor(0.0171, device='cuda:0')\n",
            "neg tensor(0.7960, device='cuda:0')\n",
            "========== Epoch 1 Batch 124==== Step 1 AVG. val Loss 0.4065479040145874 acc = 0.0\n",
            "pos tensor(0.0084, device='cuda:0')\n",
            "neg tensor(0.8069, device='cuda:0')\n",
            "========== Epoch 1 Batch 125==== Step 1 AVG. val Loss 0.4076055586338043 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8057, device='cuda:0')\n",
            "========== Epoch 1 Batch 126==== Step 1 AVG. val Loss 0.4089522957801819 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.7947, device='cuda:0')\n",
            "========== Epoch 1 Batch 127==== Step 1 AVG. val Loss 0.4049362540245056 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8143, device='cuda:0')\n",
            "========== Epoch 1 Batch 128==== Step 1 AVG. val Loss 0.412381112575531 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 129==== Step 1 AVG. val Loss 0.40243059396743774 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7984, device='cuda:0')\n",
            "========== Epoch 1 Batch 130==== Step 1 AVG. val Loss 0.406446635723114 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7926, device='cuda:0')\n",
            "========== Epoch 1 Batch 131==== Step 1 AVG. val Loss 0.40412282943725586 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8051, device='cuda:0')\n",
            "========== Epoch 1 Batch 132==== Step 1 AVG. val Loss 0.40865981578826904 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.7977, device='cuda:0')\n",
            "========== Epoch 1 Batch 133==== Step 1 AVG. val Loss 0.4041804075241089 acc = 0.0\n",
            "pos tensor(0.0089, device='cuda:0')\n",
            "neg tensor(0.8202, device='cuda:0')\n",
            "========== Epoch 1 Batch 134==== Step 1 AVG. val Loss 0.4145525097846985 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8091, device='cuda:0')\n",
            "========== Epoch 1 Batch 135==== Step 1 AVG. val Loss 0.41036415100097656 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.7998, device='cuda:0')\n",
            "========== Epoch 1 Batch 136==== Step 1 AVG. val Loss 0.40721970796585083 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.8038, device='cuda:0')\n",
            "========== Epoch 1 Batch 137==== Step 1 AVG. val Loss 0.4081798493862152 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8027, device='cuda:0')\n",
            "========== Epoch 1 Batch 138==== Step 1 AVG. val Loss 0.4075435698032379 acc = 0.0\n",
            "pos tensor(0.0176, device='cuda:0')\n",
            "neg tensor(0.7883, device='cuda:0')\n",
            "========== Epoch 1 Batch 139==== Step 1 AVG. val Loss 0.4029388427734375 acc = 0.0\n",
            "pos tensor(0.0113, device='cuda:0')\n",
            "neg tensor(0.8043, device='cuda:0')\n",
            "========== Epoch 1 Batch 140==== Step 1 AVG. val Loss 0.40781641006469727 acc = 0.0\n",
            "pos tensor(0.0098, device='cuda:0')\n",
            "neg tensor(0.8013, device='cuda:0')\n",
            "========== Epoch 1 Batch 141==== Step 1 AVG. val Loss 0.40556079149246216 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8002, device='cuda:0')\n",
            "========== Epoch 1 Batch 142==== Step 1 AVG. val Loss 0.40705955028533936 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.7948, device='cuda:0')\n",
            "========== Epoch 1 Batch 143==== Step 1 AVG. val Loss 0.4026896059513092 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7976, device='cuda:0')\n",
            "========== Epoch 1 Batch 144==== Step 1 AVG. val Loss 0.40528106689453125 acc = 0.0\n",
            "pos tensor(0.0167, device='cuda:0')\n",
            "neg tensor(0.7833, device='cuda:0')\n",
            "========== Epoch 1 Batch 145==== Step 1 AVG. val Loss 0.4000411033630371 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7778, device='cuda:0')\n",
            "========== Epoch 1 Batch 146==== Step 1 AVG. val Loss 0.3964223563671112 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8098, device='cuda:0')\n",
            "========== Epoch 1 Batch 147==== Step 1 AVG. val Loss 0.41126710176467896 acc = 0.0\n",
            "pos tensor(0.0101, device='cuda:0')\n",
            "neg tensor(0.8012, device='cuda:0')\n",
            "========== Epoch 1 Batch 148==== Step 1 AVG. val Loss 0.4056237041950226 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7860, device='cuda:0')\n",
            "========== Epoch 1 Batch 149==== Step 1 AVG. val Loss 0.40090152621269226 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8055, device='cuda:0')\n",
            "========== Epoch 1 Batch 150==== Step 1 AVG. val Loss 0.4084331691265106 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.7921, device='cuda:0')\n",
            "========== Epoch 1 Batch 151==== Step 1 AVG. val Loss 0.4021620750427246 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7801, device='cuda:0')\n",
            "========== Epoch 1 Batch 152==== Step 1 AVG. val Loss 0.397108793258667 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7999, device='cuda:0')\n",
            "========== Epoch 1 Batch 153==== Step 1 AVG. val Loss 0.4058647155761719 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 154==== Step 1 AVG. val Loss 0.4024578928947449 acc = 0.0\n",
            "pos tensor(0.0151, device='cuda:0')\n",
            "neg tensor(0.7920, device='cuda:0')\n",
            "========== Epoch 1 Batch 155==== Step 1 AVG. val Loss 0.4035164713859558 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7918, device='cuda:0')\n",
            "========== Epoch 1 Batch 156==== Step 1 AVG. val Loss 0.40290766954421997 acc = 0.0\n",
            "pos tensor(0.0162, device='cuda:0')\n",
            "neg tensor(0.7836, device='cuda:0')\n",
            "========== Epoch 1 Batch 157==== Step 1 AVG. val Loss 0.39992019534111023 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8092, device='cuda:0')\n",
            "========== Epoch 1 Batch 158==== Step 1 AVG. val Loss 0.4099791944026947 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8173, device='cuda:0')\n",
            "========== Epoch 1 Batch 159==== Step 1 AVG. val Loss 0.41410377621650696 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7979, device='cuda:0')\n",
            "========== Epoch 1 Batch 160==== Step 1 AVG. val Loss 0.40594905614852905 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.8052, device='cuda:0')\n",
            "========== Epoch 1 Batch 161==== Step 1 AVG. val Loss 0.4080715477466583 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7916, device='cuda:0')\n",
            "========== Epoch 1 Batch 162==== Step 1 AVG. val Loss 0.4033392369747162 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 163==== Step 1 AVG. val Loss 0.4061007499694824 acc = 0.0\n",
            "pos tensor(0.0170, device='cuda:0')\n",
            "neg tensor(0.7929, device='cuda:0')\n",
            "========== Epoch 1 Batch 164==== Step 1 AVG. val Loss 0.40495479106903076 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7900, device='cuda:0')\n",
            "========== Epoch 1 Batch 165==== Step 1 AVG. val Loss 0.40253615379333496 acc = 0.0\n",
            "pos tensor(0.0168, device='cuda:0')\n",
            "neg tensor(0.7954, device='cuda:0')\n",
            "========== Epoch 1 Batch 166==== Step 1 AVG. val Loss 0.40611615777015686 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 167==== Step 1 AVG. val Loss 0.40389811992645264 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7919, device='cuda:0')\n",
            "========== Epoch 1 Batch 168==== Step 1 AVG. val Loss 0.4033937156200409 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.8045, device='cuda:0')\n",
            "========== Epoch 1 Batch 169==== Step 1 AVG. val Loss 0.40899184346199036 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7975, device='cuda:0')\n",
            "========== Epoch 1 Batch 170==== Step 1 AVG. val Loss 0.4046335816383362 acc = 0.0\n",
            "pos tensor(0.0144, device='cuda:0')\n",
            "neg tensor(0.8001, device='cuda:0')\n",
            "========== Epoch 1 Batch 171==== Step 1 AVG. val Loss 0.4072778522968292 acc = 0.0\n",
            "pos tensor(0.0153, device='cuda:0')\n",
            "neg tensor(0.7897, device='cuda:0')\n",
            "========== Epoch 1 Batch 172==== Step 1 AVG. val Loss 0.40253356099128723 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8098, device='cuda:0')\n",
            "========== Epoch 1 Batch 173==== Step 1 AVG. val Loss 0.4107411503791809 acc = 0.0\n",
            "pos tensor(0.0092, device='cuda:0')\n",
            "neg tensor(0.8276, device='cuda:0')\n",
            "========== Epoch 1 Batch 174==== Step 1 AVG. val Loss 0.4184221029281616 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 175==== Step 1 AVG. val Loss 0.4039594829082489 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.7910, device='cuda:0')\n",
            "========== Epoch 1 Batch 176==== Step 1 AVG. val Loss 0.4011111855506897 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8066, device='cuda:0')\n",
            "========== Epoch 1 Batch 177==== Step 1 AVG. val Loss 0.4102601110935211 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8137, device='cuda:0')\n",
            "========== Epoch 1 Batch 178==== Step 1 AVG. val Loss 0.41217049956321716 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8144, device='cuda:0')\n",
            "========== Epoch 1 Batch 179==== Step 1 AVG. val Loss 0.41378459334373474 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8071, device='cuda:0')\n",
            "========== Epoch 1 Batch 180==== Step 1 AVG. val Loss 0.4099871516227722 acc = 0.0\n",
            "pos tensor(0.0095, device='cuda:0')\n",
            "neg tensor(0.8132, device='cuda:0')\n",
            "========== Epoch 1 Batch 181==== Step 1 AVG. val Loss 0.41139325499534607 acc = 0.0\n",
            "pos tensor(0.0168, device='cuda:0')\n",
            "neg tensor(0.7753, device='cuda:0')\n",
            "========== Epoch 1 Batch 182==== Step 1 AVG. val Loss 0.3960154056549072 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.7963, device='cuda:0')\n",
            "========== Epoch 1 Batch 183==== Step 1 AVG. val Loss 0.404708594083786 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.7937, device='cuda:0')\n",
            "========== Epoch 1 Batch 184==== Step 1 AVG. val Loss 0.4024548828601837 acc = 0.0\n",
            "pos tensor(0.0102, device='cuda:0')\n",
            "neg tensor(0.8080, device='cuda:0')\n",
            "========== Epoch 1 Batch 185==== Step 1 AVG. val Loss 0.40912920236587524 acc = 0.0\n",
            "pos tensor(0.0102, device='cuda:0')\n",
            "neg tensor(0.8224, device='cuda:0')\n",
            "========== Epoch 1 Batch 186==== Step 1 AVG. val Loss 0.4162842035293579 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7884, device='cuda:0')\n",
            "========== Epoch 1 Batch 187==== Step 1 AVG. val Loss 0.40014272928237915 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8109, device='cuda:0')\n",
            "========== Epoch 1 Batch 188==== Step 1 AVG. val Loss 0.4112543761730194 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.7988, device='cuda:0')\n",
            "========== Epoch 1 Batch 189==== Step 1 AVG. val Loss 0.40547674894332886 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7966, device='cuda:0')\n",
            "========== Epoch 1 Batch 190==== Step 1 AVG. val Loss 0.40517303347587585 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8022, device='cuda:0')\n",
            "========== Epoch 1 Batch 191==== Step 1 AVG. val Loss 0.4068530201911926 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.8019, device='cuda:0')\n",
            "========== Epoch 1 Batch 192==== Step 1 AVG. val Loss 0.4077737033367157 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8078, device='cuda:0')\n",
            "========== Epoch 1 Batch 193==== Step 1 AVG. val Loss 0.41086283326148987 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7875, device='cuda:0')\n",
            "========== Epoch 1 Batch 194==== Step 1 AVG. val Loss 0.4009077250957489 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8166, device='cuda:0')\n",
            "========== Epoch 1 Batch 195==== Step 1 AVG. val Loss 0.41384637355804443 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7816, device='cuda:0')\n",
            "========== Epoch 1 Batch 196==== Step 1 AVG. val Loss 0.39870485663414 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7966, device='cuda:0')\n",
            "========== Epoch 1 Batch 197==== Step 1 AVG. val Loss 0.405474454164505 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8018, device='cuda:0')\n",
            "========== Epoch 1 Batch 198==== Step 1 AVG. val Loss 0.4061543941497803 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8182, device='cuda:0')\n",
            "========== Epoch 1 Batch 199==== Step 1 AVG. val Loss 0.4144166111946106 acc = 0.0\n",
            "pos tensor(0.0100, device='cuda:0')\n",
            "neg tensor(0.8189, device='cuda:0')\n",
            "========== Epoch 1 Batch 200==== Step 1 AVG. val Loss 0.41447165608406067 acc = 0.0\n",
            "pos tensor(0.0159, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 201==== Step 1 AVG. val Loss 0.40569648146629333 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8221, device='cuda:0')\n",
            "========== Epoch 1 Batch 202==== Step 1 AVG. val Loss 0.4167426824569702 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8094, device='cuda:0')\n",
            "========== Epoch 1 Batch 203==== Step 1 AVG. val Loss 0.41077762842178345 acc = 0.0\n",
            "pos tensor(0.0153, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 204==== Step 1 AVG. val Loss 0.40747544169425964 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7816, device='cuda:0')\n",
            "========== Epoch 1 Batch 205==== Step 1 AVG. val Loss 0.3976348340511322 acc = 0.0\n",
            "pos tensor(0.0167, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 206==== Step 1 AVG. val Loss 0.40499627590179443 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7939, device='cuda:0')\n",
            "========== Epoch 1 Batch 207==== Step 1 AVG. val Loss 0.4047347605228424 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8106, device='cuda:0')\n",
            "========== Epoch 1 Batch 208==== Step 1 AVG. val Loss 0.4106791913509369 acc = 0.0\n",
            "pos tensor(0.0159, device='cuda:0')\n",
            "neg tensor(0.7968, device='cuda:0')\n",
            "========== Epoch 1 Batch 209==== Step 1 AVG. val Loss 0.4063582718372345 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7944, device='cuda:0')\n",
            "========== Epoch 1 Batch 210==== Step 1 AVG. val Loss 0.4039924144744873 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.7937, device='cuda:0')\n",
            "========== Epoch 1 Batch 211==== Step 1 AVG. val Loss 0.4033696949481964 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7877, device='cuda:0')\n",
            "========== Epoch 1 Batch 212==== Step 1 AVG. val Loss 0.4009765684604645 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8125, device='cuda:0')\n",
            "========== Epoch 1 Batch 213==== Step 1 AVG. val Loss 0.412305623292923 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8061, device='cuda:0')\n",
            "========== Epoch 1 Batch 214==== Step 1 AVG. val Loss 0.40896832942962646 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.7972, device='cuda:0')\n",
            "========== Epoch 1 Batch 215==== Step 1 AVG. val Loss 0.4045742154121399 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7859, device='cuda:0')\n",
            "========== Epoch 1 Batch 216==== Step 1 AVG. val Loss 0.40032151341438293 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.7904, device='cuda:0')\n",
            "========== Epoch 1 Batch 217==== Step 1 AVG. val Loss 0.40278005599975586 acc = 0.0\n",
            "pos tensor(0.0092, device='cuda:0')\n",
            "neg tensor(0.8093, device='cuda:0')\n",
            "========== Epoch 1 Batch 218==== Step 1 AVG. val Loss 0.4092262387275696 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8116, device='cuda:0')\n",
            "========== Epoch 1 Batch 219==== Step 1 AVG. val Loss 0.4116990864276886 acc = 0.0\n",
            "pos tensor(0.0100, device='cuda:0')\n",
            "neg tensor(0.7916, device='cuda:0')\n",
            "========== Epoch 1 Batch 220==== Step 1 AVG. val Loss 0.40084484219551086 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7834, device='cuda:0')\n",
            "========== Epoch 1 Batch 221==== Step 1 AVG. val Loss 0.39939576387405396 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8095, device='cuda:0')\n",
            "========== Epoch 1 Batch 222==== Step 1 AVG. val Loss 0.41068321466445923 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8036, device='cuda:0')\n",
            "========== Epoch 1 Batch 223==== Step 1 AVG. val Loss 0.40760883688926697 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7978, device='cuda:0')\n",
            "========== Epoch 1 Batch 224==== Step 1 AVG. val Loss 0.40579259395599365 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8193, device='cuda:0')\n",
            "========== Epoch 1 Batch 225==== Step 1 AVG. val Loss 0.4149925410747528 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7943, device='cuda:0')\n",
            "========== Epoch 1 Batch 226==== Step 1 AVG. val Loss 0.40457621216773987 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7983, device='cuda:0')\n",
            "========== Epoch 1 Batch 227==== Step 1 AVG. val Loss 0.40502649545669556 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8161, device='cuda:0')\n",
            "========== Epoch 1 Batch 228==== Step 1 AVG. val Loss 0.41362398862838745 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.8071, device='cuda:0')\n",
            "========== Epoch 1 Batch 229==== Step 1 AVG. val Loss 0.4103642702102661 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8030, device='cuda:0')\n",
            "========== Epoch 1 Batch 230==== Step 1 AVG. val Loss 0.40699219703674316 acc = 0.0\n",
            "pos tensor(0.0104, device='cuda:0')\n",
            "neg tensor(0.8068, device='cuda:0')\n",
            "========== Epoch 1 Batch 231==== Step 1 AVG. val Loss 0.4085773527622223 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7891, device='cuda:0')\n",
            "========== Epoch 1 Batch 232==== Step 1 AVG. val Loss 0.4018794298171997 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7826, device='cuda:0')\n",
            "========== Epoch 1 Batch 233==== Step 1 AVG. val Loss 0.3988379240036011 acc = 0.0\n",
            "pos tensor(0.0161, device='cuda:0')\n",
            "neg tensor(0.7962, device='cuda:0')\n",
            "========== Epoch 1 Batch 234==== Step 1 AVG. val Loss 0.4061395525932312 acc = 0.0\n",
            "pos tensor(0.0098, device='cuda:0')\n",
            "neg tensor(0.7937, device='cuda:0')\n",
            "========== Epoch 1 Batch 235==== Step 1 AVG. val Loss 0.40173619985580444 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7937, device='cuda:0')\n",
            "========== Epoch 1 Batch 236==== Step 1 AVG. val Loss 0.40360763669013977 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8146, device='cuda:0')\n",
            "========== Epoch 1 Batch 237==== Step 1 AVG. val Loss 0.41257575154304504 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7978, device='cuda:0')\n",
            "========== Epoch 1 Batch 238==== Step 1 AVG. val Loss 0.4050343334674835 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7945, device='cuda:0')\n",
            "========== Epoch 1 Batch 239==== Step 1 AVG. val Loss 0.40308699011802673 acc = 0.0\n",
            "pos tensor(0.0101, device='cuda:0')\n",
            "neg tensor(0.8076, device='cuda:0')\n",
            "========== Epoch 1 Batch 240==== Step 1 AVG. val Loss 0.4088174104690552 acc = 0.0\n",
            "pos tensor(0.0151, device='cuda:0')\n",
            "neg tensor(0.7936, device='cuda:0')\n",
            "========== Epoch 1 Batch 241==== Step 1 AVG. val Loss 0.40434277057647705 acc = 0.0\n",
            "pos tensor(0.0113, device='cuda:0')\n",
            "neg tensor(0.8042, device='cuda:0')\n",
            "========== Epoch 1 Batch 242==== Step 1 AVG. val Loss 0.4077589511871338 acc = 0.0\n",
            "pos tensor(0.0082, device='cuda:0')\n",
            "neg tensor(0.8245, device='cuda:0')\n",
            "========== Epoch 1 Batch 243==== Step 1 AVG. val Loss 0.41634270548820496 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7918, device='cuda:0')\n",
            "========== Epoch 1 Batch 244==== Step 1 AVG. val Loss 0.4023493528366089 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7977, device='cuda:0')\n",
            "========== Epoch 1 Batch 245==== Step 1 AVG. val Loss 0.4058016538619995 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8011, device='cuda:0')\n",
            "========== Epoch 1 Batch 246==== Step 1 AVG. val Loss 0.4066532850265503 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.7918, device='cuda:0')\n",
            "========== Epoch 1 Batch 247==== Step 1 AVG. val Loss 0.4031848609447479 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7805, device='cuda:0')\n",
            "========== Epoch 1 Batch 248==== Step 1 AVG. val Loss 0.3975087106227875 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7985, device='cuda:0')\n",
            "========== Epoch 1 Batch 249==== Step 1 AVG. val Loss 0.4060118496417999 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.8058, device='cuda:0')\n",
            "========== Epoch 1 Batch 250==== Step 1 AVG. val Loss 0.4102702736854553 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.8096, device='cuda:0')\n",
            "========== Epoch 1 Batch 251==== Step 1 AVG. val Loss 0.4117962718009949 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.8018, device='cuda:0')\n",
            "========== Epoch 1 Batch 252==== Step 1 AVG. val Loss 0.408519983291626 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8004, device='cuda:0')\n",
            "========== Epoch 1 Batch 253==== Step 1 AVG. val Loss 0.40543830394744873 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8124, device='cuda:0')\n",
            "========== Epoch 1 Batch 254==== Step 1 AVG. val Loss 0.41187766194343567 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8029, device='cuda:0')\n",
            "========== Epoch 1 Batch 255==== Step 1 AVG. val Loss 0.4075171947479248 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.7941, device='cuda:0')\n",
            "========== Epoch 1 Batch 256==== Step 1 AVG. val Loss 0.4035763144493103 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8078, device='cuda:0')\n",
            "========== Epoch 1 Batch 257==== Step 1 AVG. val Loss 0.4097510576248169 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.8001, device='cuda:0')\n",
            "========== Epoch 1 Batch 258==== Step 1 AVG. val Loss 0.4068174362182617 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8051, device='cuda:0')\n",
            "========== Epoch 1 Batch 259==== Step 1 AVG. val Loss 0.40831902623176575 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7765, device='cuda:0')\n",
            "========== Epoch 1 Batch 260==== Step 1 AVG. val Loss 0.3961944282054901 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7865, device='cuda:0')\n",
            "========== Epoch 1 Batch 261==== Step 1 AVG. val Loss 0.4000754654407501 acc = 0.0\n",
            "pos tensor(0.0172, device='cuda:0')\n",
            "neg tensor(0.7783, device='cuda:0')\n",
            "========== Epoch 1 Batch 262==== Step 1 AVG. val Loss 0.39772215485572815 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7930, device='cuda:0')\n",
            "========== Epoch 1 Batch 263==== Step 1 AVG. val Loss 0.4032490849494934 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7834, device='cuda:0')\n",
            "========== Epoch 1 Batch 264==== Step 1 AVG. val Loss 0.39847487211227417 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8052, device='cuda:0')\n",
            "========== Epoch 1 Batch 265==== Step 1 AVG. val Loss 0.4083470404148102 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7996, device='cuda:0')\n",
            "========== Epoch 1 Batch 266==== Step 1 AVG. val Loss 0.4061080813407898 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7851, device='cuda:0')\n",
            "========== Epoch 1 Batch 267==== Step 1 AVG. val Loss 0.3994348645210266 acc = 0.0\n",
            "pos tensor(0.0086, device='cuda:0')\n",
            "neg tensor(0.8231, device='cuda:0')\n",
            "========== Epoch 1 Batch 268==== Step 1 AVG. val Loss 0.41585785150527954 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7957, device='cuda:0')\n",
            "========== Epoch 1 Batch 269==== Step 1 AVG. val Loss 0.40377122163772583 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8045, device='cuda:0')\n",
            "========== Epoch 1 Batch 270==== Step 1 AVG. val Loss 0.4082663953304291 acc = 0.0\n",
            "pos tensor(0.0183, device='cuda:0')\n",
            "neg tensor(0.7904, device='cuda:0')\n",
            "========== Epoch 1 Batch 271==== Step 1 AVG. val Loss 0.40438610315322876 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8168, device='cuda:0')\n",
            "========== Epoch 1 Batch 272==== Step 1 AVG. val Loss 0.41431817412376404 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.8021, device='cuda:0')\n",
            "========== Epoch 1 Batch 273==== Step 1 AVG. val Loss 0.4073646068572998 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8155, device='cuda:0')\n",
            "========== Epoch 1 Batch 274==== Step 1 AVG. val Loss 0.41347336769104004 acc = 0.0\n",
            "pos tensor(0.0097, device='cuda:0')\n",
            "neg tensor(0.8113, device='cuda:0')\n",
            "========== Epoch 1 Batch 275==== Step 1 AVG. val Loss 0.41049548983573914 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7949, device='cuda:0')\n",
            "========== Epoch 1 Batch 276==== Step 1 AVG. val Loss 0.40484029054641724 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7928, device='cuda:0')\n",
            "========== Epoch 1 Batch 277==== Step 1 AVG. val Loss 0.4022749960422516 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8006, device='cuda:0')\n",
            "========== Epoch 1 Batch 278==== Step 1 AVG. val Loss 0.40667253732681274 acc = 0.0\n",
            "pos tensor(0.0103, device='cuda:0')\n",
            "neg tensor(0.8144, device='cuda:0')\n",
            "========== Epoch 1 Batch 279==== Step 1 AVG. val Loss 0.41238877177238464 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8137, device='cuda:0')\n",
            "========== Epoch 1 Batch 280==== Step 1 AVG. val Loss 0.4125511348247528 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8138, device='cuda:0')\n",
            "========== Epoch 1 Batch 281==== Step 1 AVG. val Loss 0.41219234466552734 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7787, device='cuda:0')\n",
            "========== Epoch 1 Batch 282==== Step 1 AVG. val Loss 0.3961239457130432 acc = 0.0\n",
            "pos tensor(0.0165, device='cuda:0')\n",
            "neg tensor(0.7986, device='cuda:0')\n",
            "========== Epoch 1 Batch 283==== Step 1 AVG. val Loss 0.4075547456741333 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 284==== Step 1 AVG. val Loss 0.4039806127548218 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7820, device='cuda:0')\n",
            "========== Epoch 1 Batch 285==== Step 1 AVG. val Loss 0.39870256185531616 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7762, device='cuda:0')\n",
            "========== Epoch 1 Batch 286==== Step 1 AVG. val Loss 0.39512673020362854 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8087, device='cuda:0')\n",
            "========== Epoch 1 Batch 287==== Step 1 AVG. val Loss 0.40906792879104614 acc = 0.0\n",
            "pos tensor(0.0174, device='cuda:0')\n",
            "neg tensor(0.7719, device='cuda:0')\n",
            "========== Epoch 1 Batch 288==== Step 1 AVG. val Loss 0.3946673274040222 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7931, device='cuda:0')\n",
            "========== Epoch 1 Batch 289==== Step 1 AVG. val Loss 0.4029005169868469 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.7962, device='cuda:0')\n",
            "========== Epoch 1 Batch 290==== Step 1 AVG. val Loss 0.4056907594203949 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8171, device='cuda:0')\n",
            "========== Epoch 1 Batch 291==== Step 1 AVG. val Loss 0.4132779538631439 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7954, device='cuda:0')\n",
            "========== Epoch 1 Batch 292==== Step 1 AVG. val Loss 0.4050944745540619 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7910, device='cuda:0')\n",
            "========== Epoch 1 Batch 293==== Step 1 AVG. val Loss 0.4027540683746338 acc = 0.0\n",
            "pos tensor(0.0192, device='cuda:0')\n",
            "neg tensor(0.7869, device='cuda:0')\n",
            "========== Epoch 1 Batch 294==== Step 1 AVG. val Loss 0.40302374958992004 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8048, device='cuda:0')\n",
            "========== Epoch 1 Batch 295==== Step 1 AVG. val Loss 0.40828830003738403 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8020, device='cuda:0')\n",
            "========== Epoch 1 Batch 296==== Step 1 AVG. val Loss 0.4072403609752655 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7859, device='cuda:0')\n",
            "========== Epoch 1 Batch 297==== Step 1 AVG. val Loss 0.40018603205680847 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7796, device='cuda:0')\n",
            "========== Epoch 1 Batch 298==== Step 1 AVG. val Loss 0.3975068926811218 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.8082, device='cuda:0')\n",
            "========== Epoch 1 Batch 299==== Step 1 AVG. val Loss 0.41064006090164185 acc = 0.0\n",
            "pos tensor(0.0102, device='cuda:0')\n",
            "neg tensor(0.8128, device='cuda:0')\n",
            "========== Epoch 1 Batch 300==== Step 1 AVG. val Loss 0.41146719455718994 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7973, device='cuda:0')\n",
            "========== Epoch 1 Batch 301==== Step 1 AVG. val Loss 0.4056272506713867 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.7961, device='cuda:0')\n",
            "========== Epoch 1 Batch 302==== Step 1 AVG. val Loss 0.4041004776954651 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8076, device='cuda:0')\n",
            "========== Epoch 1 Batch 303==== Step 1 AVG. val Loss 0.4093461036682129 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8071, device='cuda:0')\n",
            "========== Epoch 1 Batch 304==== Step 1 AVG. val Loss 0.4097876250743866 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.7897, device='cuda:0')\n",
            "========== Epoch 1 Batch 305==== Step 1 AVG. val Loss 0.4012972414493561 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.7885, device='cuda:0')\n",
            "========== Epoch 1 Batch 306==== Step 1 AVG. val Loss 0.4005638360977173 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.7961, device='cuda:0')\n",
            "========== Epoch 1 Batch 307==== Step 1 AVG. val Loss 0.40362975001335144 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7930, device='cuda:0')\n",
            "========== Epoch 1 Batch 308==== Step 1 AVG. val Loss 0.40310460329055786 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.7920, device='cuda:0')\n",
            "========== Epoch 1 Batch 309==== Step 1 AVG. val Loss 0.40181487798690796 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7967, device='cuda:0')\n",
            "========== Epoch 1 Batch 310==== Step 1 AVG. val Loss 0.4057132303714752 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8139, device='cuda:0')\n",
            "========== Epoch 1 Batch 311==== Step 1 AVG. val Loss 0.41242724657058716 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.7991, device='cuda:0')\n",
            "========== Epoch 1 Batch 312==== Step 1 AVG. val Loss 0.40573182702064514 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7928, device='cuda:0')\n",
            "========== Epoch 1 Batch 313==== Step 1 AVG. val Loss 0.40388259291648865 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7857, device='cuda:0')\n",
            "========== Epoch 1 Batch 314==== Step 1 AVG. val Loss 0.40076738595962524 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8061, device='cuda:0')\n",
            "========== Epoch 1 Batch 315==== Step 1 AVG. val Loss 0.4093010723590851 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7931, device='cuda:0')\n",
            "========== Epoch 1 Batch 316==== Step 1 AVG. val Loss 0.4027382731437683 acc = 0.0\n",
            "pos tensor(0.0173, device='cuda:0')\n",
            "neg tensor(0.7773, device='cuda:0')\n",
            "========== Epoch 1 Batch 317==== Step 1 AVG. val Loss 0.3973221182823181 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7924, device='cuda:0')\n",
            "========== Epoch 1 Batch 318==== Step 1 AVG. val Loss 0.40265437960624695 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7928, device='cuda:0')\n",
            "========== Epoch 1 Batch 319==== Step 1 AVG. val Loss 0.40331149101257324 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.7809, device='cuda:0')\n",
            "========== Epoch 1 Batch 320==== Step 1 AVG. val Loss 0.3977818787097931 acc = 0.0\n",
            "pos tensor(0.0178, device='cuda:0')\n",
            "neg tensor(0.7839, device='cuda:0')\n",
            "========== Epoch 1 Batch 321==== Step 1 AVG. val Loss 0.4008420705795288 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7834, device='cuda:0')\n",
            "========== Epoch 1 Batch 322==== Step 1 AVG. val Loss 0.3975699841976166 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8025, device='cuda:0')\n",
            "========== Epoch 1 Batch 323==== Step 1 AVG. val Loss 0.4076368510723114 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 324==== Step 1 AVG. val Loss 0.40407729148864746 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.7989, device='cuda:0')\n",
            "========== Epoch 1 Batch 325==== Step 1 AVG. val Loss 0.40554410219192505 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8039, device='cuda:0')\n",
            "========== Epoch 1 Batch 326==== Step 1 AVG. val Loss 0.4085613489151001 acc = 0.0\n",
            "pos tensor(0.0168, device='cuda:0')\n",
            "neg tensor(0.7843, device='cuda:0')\n",
            "========== Epoch 1 Batch 327==== Step 1 AVG. val Loss 0.4005242586135864 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7923, device='cuda:0')\n",
            "========== Epoch 1 Batch 328==== Step 1 AVG. val Loss 0.4028986096382141 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8074, device='cuda:0')\n",
            "========== Epoch 1 Batch 329==== Step 1 AVG. val Loss 0.4101678729057312 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.8055, device='cuda:0')\n",
            "========== Epoch 1 Batch 330==== Step 1 AVG. val Loss 0.4081571102142334 acc = 0.0\n",
            "pos tensor(0.0161, device='cuda:0')\n",
            "neg tensor(0.7897, device='cuda:0')\n",
            "========== Epoch 1 Batch 331==== Step 1 AVG. val Loss 0.4028812646865845 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 332==== Step 1 AVG. val Loss 0.4066120684146881 acc = 0.0\n",
            "pos tensor(0.0142, device='cuda:0')\n",
            "neg tensor(0.7984, device='cuda:0')\n",
            "========== Epoch 1 Batch 333==== Step 1 AVG. val Loss 0.4063005745410919 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.8095, device='cuda:0')\n",
            "========== Epoch 1 Batch 334==== Step 1 AVG. val Loss 0.41154730319976807 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7914, device='cuda:0')\n",
            "========== Epoch 1 Batch 335==== Step 1 AVG. val Loss 0.40212133526802063 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7855, device='cuda:0')\n",
            "========== Epoch 1 Batch 336==== Step 1 AVG. val Loss 0.3981742262840271 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7900, device='cuda:0')\n",
            "========== Epoch 1 Batch 337==== Step 1 AVG. val Loss 0.4024445116519928 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.8073, device='cuda:0')\n",
            "========== Epoch 1 Batch 338==== Step 1 AVG. val Loss 0.4106593430042267 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8041, device='cuda:0')\n",
            "========== Epoch 1 Batch 339==== Step 1 AVG. val Loss 0.40807676315307617 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7834, device='cuda:0')\n",
            "========== Epoch 1 Batch 340==== Step 1 AVG. val Loss 0.3981693685054779 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 341==== Step 1 AVG. val Loss 0.4017449915409088 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.8019, device='cuda:0')\n",
            "========== Epoch 1 Batch 342==== Step 1 AVG. val Loss 0.4075721800327301 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 343==== Step 1 AVG. val Loss 0.4020563066005707 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7920, device='cuda:0')\n",
            "========== Epoch 1 Batch 344==== Step 1 AVG. val Loss 0.40314415097236633 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.8033, device='cuda:0')\n",
            "========== Epoch 1 Batch 345==== Step 1 AVG. val Loss 0.4078643023967743 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.7977, device='cuda:0')\n",
            "========== Epoch 1 Batch 346==== Step 1 AVG. val Loss 0.4045484960079193 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8094, device='cuda:0')\n",
            "========== Epoch 1 Batch 347==== Step 1 AVG. val Loss 0.41054606437683105 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.7979, device='cuda:0')\n",
            "========== Epoch 1 Batch 348==== Step 1 AVG. val Loss 0.4049781262874603 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7904, device='cuda:0')\n",
            "========== Epoch 1 Batch 349==== Step 1 AVG. val Loss 0.401682049036026 acc = 0.0\n",
            "pos tensor(0.0142, device='cuda:0')\n",
            "neg tensor(0.7964, device='cuda:0')\n",
            "========== Epoch 1 Batch 350==== Step 1 AVG. val Loss 0.40530839562416077 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.7910, device='cuda:0')\n",
            "========== Epoch 1 Batch 351==== Step 1 AVG. val Loss 0.401187539100647 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8100, device='cuda:0')\n",
            "========== Epoch 1 Batch 352==== Step 1 AVG. val Loss 0.4108273386955261 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8036, device='cuda:0')\n",
            "========== Epoch 1 Batch 353==== Step 1 AVG. val Loss 0.40762487053871155 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.7988, device='cuda:0')\n",
            "========== Epoch 1 Batch 354==== Step 1 AVG. val Loss 0.40592581033706665 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.8005, device='cuda:0')\n",
            "========== Epoch 1 Batch 355==== Step 1 AVG. val Loss 0.40692415833473206 acc = 0.0\n",
            "pos tensor(0.0162, device='cuda:0')\n",
            "neg tensor(0.7890, device='cuda:0')\n",
            "========== Epoch 1 Batch 356==== Step 1 AVG. val Loss 0.40260642766952515 acc = 0.0\n",
            "pos tensor(0.0106, device='cuda:0')\n",
            "neg tensor(0.8124, device='cuda:0')\n",
            "========== Epoch 1 Batch 357==== Step 1 AVG. val Loss 0.41150838136672974 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8071, device='cuda:0')\n",
            "========== Epoch 1 Batch 358==== Step 1 AVG. val Loss 0.40916353464126587 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7845, device='cuda:0')\n",
            "========== Epoch 1 Batch 359==== Step 1 AVG. val Loss 0.3989720940589905 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7887, device='cuda:0')\n",
            "========== Epoch 1 Batch 360==== Step 1 AVG. val Loss 0.40021929144859314 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8084, device='cuda:0')\n",
            "========== Epoch 1 Batch 361==== Step 1 AVG. val Loss 0.41019538044929504 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8183, device='cuda:0')\n",
            "========== Epoch 1 Batch 362==== Step 1 AVG. val Loss 0.41518640518188477 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.7983, device='cuda:0')\n",
            "========== Epoch 1 Batch 363==== Step 1 AVG. val Loss 0.4054405987262726 acc = 0.0\n",
            "pos tensor(0.0090, device='cuda:0')\n",
            "neg tensor(0.8136, device='cuda:0')\n",
            "========== Epoch 1 Batch 364==== Step 1 AVG. val Loss 0.4112887978553772 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.8106, device='cuda:0')\n",
            "========== Epoch 1 Batch 365==== Step 1 AVG. val Loss 0.4114874005317688 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7897, device='cuda:0')\n",
            "========== Epoch 1 Batch 366==== Step 1 AVG. val Loss 0.4016195833683014 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.8017, device='cuda:0')\n",
            "========== Epoch 1 Batch 367==== Step 1 AVG. val Loss 0.4077167809009552 acc = 0.0\n",
            "pos tensor(0.0093, device='cuda:0')\n",
            "neg tensor(0.8113, device='cuda:0')\n",
            "========== Epoch 1 Batch 368==== Step 1 AVG. val Loss 0.4102725386619568 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8201, device='cuda:0')\n",
            "========== Epoch 1 Batch 369==== Step 1 AVG. val Loss 0.41475909948349 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.7938, device='cuda:0')\n",
            "========== Epoch 1 Batch 370==== Step 1 AVG. val Loss 0.4035891592502594 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.7947, device='cuda:0')\n",
            "========== Epoch 1 Batch 371==== Step 1 AVG. val Loss 0.4028668999671936 acc = 0.0\n",
            "pos tensor(0.0113, device='cuda:0')\n",
            "neg tensor(0.8137, device='cuda:0')\n",
            "========== Epoch 1 Batch 372==== Step 1 AVG. val Loss 0.4125082194805145 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7800, device='cuda:0')\n",
            "========== Epoch 1 Batch 373==== Step 1 AVG. val Loss 0.39695191383361816 acc = 0.0\n",
            "pos tensor(0.0088, device='cuda:0')\n",
            "neg tensor(0.8180, device='cuda:0')\n",
            "========== Epoch 1 Batch 374==== Step 1 AVG. val Loss 0.41339749097824097 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.8038, device='cuda:0')\n",
            "========== Epoch 1 Batch 375==== Step 1 AVG. val Loss 0.408215194940567 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 376==== Step 1 AVG. val Loss 0.40684497356414795 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 377==== Step 1 AVG. val Loss 0.4015723764896393 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7903, device='cuda:0')\n",
            "========== Epoch 1 Batch 378==== Step 1 AVG. val Loss 0.40240100026130676 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7896, device='cuda:0')\n",
            "========== Epoch 1 Batch 379==== Step 1 AVG. val Loss 0.4019802510738373 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7903, device='cuda:0')\n",
            "========== Epoch 1 Batch 380==== Step 1 AVG. val Loss 0.4024122953414917 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8114, device='cuda:0')\n",
            "========== Epoch 1 Batch 381==== Step 1 AVG. val Loss 0.4119330644607544 acc = 0.0\n",
            "pos tensor(0.0170, device='cuda:0')\n",
            "neg tensor(0.7768, device='cuda:0')\n",
            "========== Epoch 1 Batch 382==== Step 1 AVG. val Loss 0.39687293767929077 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8079, device='cuda:0')\n",
            "========== Epoch 1 Batch 383==== Step 1 AVG. val Loss 0.41012054681777954 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7884, device='cuda:0')\n",
            "========== Epoch 1 Batch 384==== Step 1 AVG. val Loss 0.4015202820301056 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8269, device='cuda:0')\n",
            "========== Epoch 1 Batch 385==== Step 1 AVG. val Loss 0.4190708100795746 acc = 0.0\n",
            "pos tensor(0.0092, device='cuda:0')\n",
            "neg tensor(0.8114, device='cuda:0')\n",
            "========== Epoch 1 Batch 386==== Step 1 AVG. val Loss 0.4102918207645416 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.8021, device='cuda:0')\n",
            "========== Epoch 1 Batch 387==== Step 1 AVG. val Loss 0.40888404846191406 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7923, device='cuda:0')\n",
            "========== Epoch 1 Batch 388==== Step 1 AVG. val Loss 0.4035247564315796 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7940, device='cuda:0')\n",
            "========== Epoch 1 Batch 389==== Step 1 AVG. val Loss 0.4033640921115875 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.8006, device='cuda:0')\n",
            "========== Epoch 1 Batch 390==== Step 1 AVG. val Loss 0.40677571296691895 acc = 0.0\n",
            "pos tensor(0.0153, device='cuda:0')\n",
            "neg tensor(0.7911, device='cuda:0')\n",
            "========== Epoch 1 Batch 391==== Step 1 AVG. val Loss 0.40320006012916565 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8018, device='cuda:0')\n",
            "========== Epoch 1 Batch 392==== Step 1 AVG. val Loss 0.40697142481803894 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.7970, device='cuda:0')\n",
            "========== Epoch 1 Batch 393==== Step 1 AVG. val Loss 0.4050016701221466 acc = 0.0\n",
            "pos tensor(0.0142, device='cuda:0')\n",
            "neg tensor(0.7952, device='cuda:0')\n",
            "========== Epoch 1 Batch 394==== Step 1 AVG. val Loss 0.4046986401081085 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8033, device='cuda:0')\n",
            "========== Epoch 1 Batch 395==== Step 1 AVG. val Loss 0.40811026096343994 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7939, device='cuda:0')\n",
            "========== Epoch 1 Batch 396==== Step 1 AVG. val Loss 0.4035634696483612 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8026, device='cuda:0')\n",
            "========== Epoch 1 Batch 397==== Step 1 AVG. val Loss 0.40671396255493164 acc = 0.0\n",
            "pos tensor(0.0144, device='cuda:0')\n",
            "neg tensor(0.7872, device='cuda:0')\n",
            "========== Epoch 1 Batch 398==== Step 1 AVG. val Loss 0.4007909595966339 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8016, device='cuda:0')\n",
            "========== Epoch 1 Batch 399==== Step 1 AVG. val Loss 0.4072655439376831 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.7980, device='cuda:0')\n",
            "========== Epoch 1 Batch 400==== Step 1 AVG. val Loss 0.40466707944869995 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7909, device='cuda:0')\n",
            "========== Epoch 1 Batch 401==== Step 1 AVG. val Loss 0.40331462025642395 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.8043, device='cuda:0')\n",
            "========== Epoch 1 Batch 402==== Step 1 AVG. val Loss 0.40888211131095886 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.8005, device='cuda:0')\n",
            "========== Epoch 1 Batch 403==== Step 1 AVG. val Loss 0.4074116051197052 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7992, device='cuda:0')\n",
            "========== Epoch 1 Batch 404==== Step 1 AVG. val Loss 0.4073973298072815 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7800, device='cuda:0')\n",
            "========== Epoch 1 Batch 405==== Step 1 AVG. val Loss 0.3972355127334595 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8199, device='cuda:0')\n",
            "========== Epoch 1 Batch 406==== Step 1 AVG. val Loss 0.41554245352745056 acc = 0.0\n",
            "pos tensor(0.0170, device='cuda:0')\n",
            "neg tensor(0.7757, device='cuda:0')\n",
            "========== Epoch 1 Batch 407==== Step 1 AVG. val Loss 0.3963639438152313 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8037, device='cuda:0')\n",
            "========== Epoch 1 Batch 408==== Step 1 AVG. val Loss 0.4076038599014282 acc = 0.0\n",
            "pos tensor(0.0103, device='cuda:0')\n",
            "neg tensor(0.8102, device='cuda:0')\n",
            "========== Epoch 1 Batch 409==== Step 1 AVG. val Loss 0.41021376848220825 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7770, device='cuda:0')\n",
            "========== Epoch 1 Batch 410==== Step 1 AVG. val Loss 0.39571473002433777 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8085, device='cuda:0')\n",
            "========== Epoch 1 Batch 411==== Step 1 AVG. val Loss 0.4102749526500702 acc = 0.0\n",
            "pos tensor(0.0162, device='cuda:0')\n",
            "neg tensor(0.7911, device='cuda:0')\n",
            "========== Epoch 1 Batch 412==== Step 1 AVG. val Loss 0.4036520719528198 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8133, device='cuda:0')\n",
            "========== Epoch 1 Batch 413==== Step 1 AVG. val Loss 0.41273197531700134 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8080, device='cuda:0')\n",
            "========== Epoch 1 Batch 414==== Step 1 AVG. val Loss 0.40972980856895447 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 415==== Step 1 AVG. val Loss 0.40291091799736023 acc = 0.0\n",
            "pos tensor(0.0099, device='cuda:0')\n",
            "neg tensor(0.7962, device='cuda:0')\n",
            "========== Epoch 1 Batch 416==== Step 1 AVG. val Loss 0.40308958292007446 acc = 0.0\n",
            "pos tensor(0.0167, device='cuda:0')\n",
            "neg tensor(0.7852, device='cuda:0')\n",
            "========== Epoch 1 Batch 417==== Step 1 AVG. val Loss 0.40099141001701355 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8153, device='cuda:0')\n",
            "========== Epoch 1 Batch 418==== Step 1 AVG. val Loss 0.4129136800765991 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.8059, device='cuda:0')\n",
            "========== Epoch 1 Batch 419==== Step 1 AVG. val Loss 0.4096762239933014 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.8058, device='cuda:0')\n",
            "========== Epoch 1 Batch 420==== Step 1 AVG. val Loss 0.40914374589920044 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.7931, device='cuda:0')\n",
            "========== Epoch 1 Batch 421==== Step 1 AVG. val Loss 0.403917133808136 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7844, device='cuda:0')\n",
            "========== Epoch 1 Batch 422==== Step 1 AVG. val Loss 0.3984658122062683 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7963, device='cuda:0')\n",
            "========== Epoch 1 Batch 423==== Step 1 AVG. val Loss 0.4051830470561981 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8074, device='cuda:0')\n",
            "========== Epoch 1 Batch 424==== Step 1 AVG. val Loss 0.40919145941734314 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7940, device='cuda:0')\n",
            "========== Epoch 1 Batch 425==== Step 1 AVG. val Loss 0.40287211537361145 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7786, device='cuda:0')\n",
            "========== Epoch 1 Batch 426==== Step 1 AVG. val Loss 0.3970147669315338 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.8093, device='cuda:0')\n",
            "========== Epoch 1 Batch 427==== Step 1 AVG. val Loss 0.4111444056034088 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7901, device='cuda:0')\n",
            "========== Epoch 1 Batch 428==== Step 1 AVG. val Loss 0.40251487493515015 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7904, device='cuda:0')\n",
            "========== Epoch 1 Batch 429==== Step 1 AVG. val Loss 0.40303510427474976 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8056, device='cuda:0')\n",
            "========== Epoch 1 Batch 430==== Step 1 AVG. val Loss 0.40880700945854187 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.7989, device='cuda:0')\n",
            "========== Epoch 1 Batch 431==== Step 1 AVG. val Loss 0.4049961566925049 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7932, device='cuda:0')\n",
            "========== Epoch 1 Batch 432==== Step 1 AVG. val Loss 0.40404409170150757 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8019, device='cuda:0')\n",
            "========== Epoch 1 Batch 433==== Step 1 AVG. val Loss 0.4068635106086731 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8126, device='cuda:0')\n",
            "========== Epoch 1 Batch 434==== Step 1 AVG. val Loss 0.4120139479637146 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.7892, device='cuda:0')\n",
            "========== Epoch 1 Batch 435==== Step 1 AVG. val Loss 0.4022175371646881 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8010, device='cuda:0')\n",
            "========== Epoch 1 Batch 436==== Step 1 AVG. val Loss 0.4068867266178131 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7890, device='cuda:0')\n",
            "========== Epoch 1 Batch 437==== Step 1 AVG. val Loss 0.4017653167247772 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.7918, device='cuda:0')\n",
            "========== Epoch 1 Batch 438==== Step 1 AVG. val Loss 0.40187299251556396 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.7997, device='cuda:0')\n",
            "========== Epoch 1 Batch 439==== Step 1 AVG. val Loss 0.4058951437473297 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7814, device='cuda:0')\n",
            "========== Epoch 1 Batch 440==== Step 1 AVG. val Loss 0.39783498644828796 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7774, device='cuda:0')\n",
            "========== Epoch 1 Batch 441==== Step 1 AVG. val Loss 0.3961710035800934 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7852, device='cuda:0')\n",
            "========== Epoch 1 Batch 442==== Step 1 AVG. val Loss 0.39963632822036743 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.8169, device='cuda:0')\n",
            "========== Epoch 1 Batch 443==== Step 1 AVG. val Loss 0.41499271988868713 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8022, device='cuda:0')\n",
            "========== Epoch 1 Batch 444==== Step 1 AVG. val Loss 0.4080395996570587 acc = 0.0\n",
            "pos tensor(0.0176, device='cuda:0')\n",
            "neg tensor(0.7817, device='cuda:0')\n",
            "========== Epoch 1 Batch 445==== Step 1 AVG. val Loss 0.39962485432624817 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8003, device='cuda:0')\n",
            "========== Epoch 1 Batch 446==== Step 1 AVG. val Loss 0.4067516028881073 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.7944, device='cuda:0')\n",
            "========== Epoch 1 Batch 447==== Step 1 AVG. val Loss 0.40284112095832825 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 448==== Step 1 AVG. val Loss 0.40276846289634705 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8002, device='cuda:0')\n",
            "========== Epoch 1 Batch 449==== Step 1 AVG. val Loss 0.4058983027935028 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8086, device='cuda:0')\n",
            "========== Epoch 1 Batch 450==== Step 1 AVG. val Loss 0.4103865325450897 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.8080, device='cuda:0')\n",
            "========== Epoch 1 Batch 451==== Step 1 AVG. val Loss 0.4112488627433777 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.7816, device='cuda:0')\n",
            "========== Epoch 1 Batch 452==== Step 1 AVG. val Loss 0.398084819316864 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7963, device='cuda:0')\n",
            "========== Epoch 1 Batch 453==== Step 1 AVG. val Loss 0.4050113558769226 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7986, device='cuda:0')\n",
            "========== Epoch 1 Batch 454==== Step 1 AVG. val Loss 0.40609997510910034 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8008, device='cuda:0')\n",
            "========== Epoch 1 Batch 455==== Step 1 AVG. val Loss 0.4062443673610687 acc = 0.0\n",
            "pos tensor(0.0145, device='cuda:0')\n",
            "neg tensor(0.7916, device='cuda:0')\n",
            "========== Epoch 1 Batch 456==== Step 1 AVG. val Loss 0.4030618667602539 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8119, device='cuda:0')\n",
            "========== Epoch 1 Batch 457==== Step 1 AVG. val Loss 0.4118836522102356 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8135, device='cuda:0')\n",
            "========== Epoch 1 Batch 458==== Step 1 AVG. val Loss 0.41251668334007263 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7805, device='cuda:0')\n",
            "========== Epoch 1 Batch 459==== Step 1 AVG. val Loss 0.39686888456344604 acc = 0.0\n",
            "pos tensor(0.0098, device='cuda:0')\n",
            "neg tensor(0.8233, device='cuda:0')\n",
            "========== Epoch 1 Batch 460==== Step 1 AVG. val Loss 0.41656458377838135 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8083, device='cuda:0')\n",
            "========== Epoch 1 Batch 461==== Step 1 AVG. val Loss 0.4095328748226166 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7928, device='cuda:0')\n",
            "========== Epoch 1 Batch 462==== Step 1 AVG. val Loss 0.402988076210022 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7944, device='cuda:0')\n",
            "========== Epoch 1 Batch 463==== Step 1 AVG. val Loss 0.40414533019065857 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8001, device='cuda:0')\n",
            "========== Epoch 1 Batch 464==== Step 1 AVG. val Loss 0.4061504900455475 acc = 0.0\n",
            "pos tensor(0.0101, device='cuda:0')\n",
            "neg tensor(0.8072, device='cuda:0')\n",
            "========== Epoch 1 Batch 465==== Step 1 AVG. val Loss 0.4086729884147644 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7976, device='cuda:0')\n",
            "========== Epoch 1 Batch 466==== Step 1 AVG. val Loss 0.4050668179988861 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 467==== Step 1 AVG. val Loss 0.40377894043922424 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7853, device='cuda:0')\n",
            "========== Epoch 1 Batch 468==== Step 1 AVG. val Loss 0.39970913529396057 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7991, device='cuda:0')\n",
            "========== Epoch 1 Batch 469==== Step 1 AVG. val Loss 0.4062747657299042 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.8023, device='cuda:0')\n",
            "========== Epoch 1 Batch 470==== Step 1 AVG. val Loss 0.40770116448402405 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8081, device='cuda:0')\n",
            "========== Epoch 1 Batch 471==== Step 1 AVG. val Loss 0.40999120473861694 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7784, device='cuda:0')\n",
            "========== Epoch 1 Batch 472==== Step 1 AVG. val Loss 0.39658597111701965 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.8067, device='cuda:0')\n",
            "========== Epoch 1 Batch 473==== Step 1 AVG. val Loss 0.40970858931541443 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7833, device='cuda:0')\n",
            "========== Epoch 1 Batch 474==== Step 1 AVG. val Loss 0.39916345477104187 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8031, device='cuda:0')\n",
            "========== Epoch 1 Batch 475==== Step 1 AVG. val Loss 0.40748441219329834 acc = 0.0\n",
            "pos tensor(0.0104, device='cuda:0')\n",
            "neg tensor(0.8163, device='cuda:0')\n",
            "========== Epoch 1 Batch 476==== Step 1 AVG. val Loss 0.4133684039115906 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8098, device='cuda:0')\n",
            "========== Epoch 1 Batch 477==== Step 1 AVG. val Loss 0.41150060296058655 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8039, device='cuda:0')\n",
            "========== Epoch 1 Batch 478==== Step 1 AVG. val Loss 0.4080839455127716 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 479==== Step 1 AVG. val Loss 0.4017331898212433 acc = 0.0\n",
            "pos tensor(0.0085, device='cuda:0')\n",
            "neg tensor(0.8154, device='cuda:0')\n",
            "========== Epoch 1 Batch 480==== Step 1 AVG. val Loss 0.41192057728767395 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.7884, device='cuda:0')\n",
            "========== Epoch 1 Batch 481==== Step 1 AVG. val Loss 0.40080833435058594 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8084, device='cuda:0')\n",
            "========== Epoch 1 Batch 482==== Step 1 AVG. val Loss 0.4095606803894043 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8133, device='cuda:0')\n",
            "========== Epoch 1 Batch 483==== Step 1 AVG. val Loss 0.41197794675827026 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8139, device='cuda:0')\n",
            "========== Epoch 1 Batch 484==== Step 1 AVG. val Loss 0.4134055972099304 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.8053, device='cuda:0')\n",
            "========== Epoch 1 Batch 485==== Step 1 AVG. val Loss 0.4089779257774353 acc = 0.0\n",
            "pos tensor(0.0161, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 486==== Step 1 AVG. val Loss 0.40296295285224915 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7878, device='cuda:0')\n",
            "========== Epoch 1 Batch 487==== Step 1 AVG. val Loss 0.40085455775260925 acc = 0.0\n",
            "pos tensor(0.0142, device='cuda:0')\n",
            "neg tensor(0.7811, device='cuda:0')\n",
            "========== Epoch 1 Batch 488==== Step 1 AVG. val Loss 0.3976631164550781 acc = 0.0\n",
            "pos tensor(0.0147, device='cuda:0')\n",
            "neg tensor(0.8022, device='cuda:0')\n",
            "========== Epoch 1 Batch 489==== Step 1 AVG. val Loss 0.40848493576049805 acc = 0.0\n",
            "pos tensor(0.0093, device='cuda:0')\n",
            "neg tensor(0.8125, device='cuda:0')\n",
            "========== Epoch 1 Batch 490==== Step 1 AVG. val Loss 0.4109150469303131 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.7912, device='cuda:0')\n",
            "========== Epoch 1 Batch 491==== Step 1 AVG. val Loss 0.401615172624588 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7975, device='cuda:0')\n",
            "========== Epoch 1 Batch 492==== Step 1 AVG. val Loss 0.40543973445892334 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7876, device='cuda:0')\n",
            "========== Epoch 1 Batch 493==== Step 1 AVG. val Loss 0.40088704228401184 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7982, device='cuda:0')\n",
            "========== Epoch 1 Batch 494==== Step 1 AVG. val Loss 0.4069080948829651 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7988, device='cuda:0')\n",
            "========== Epoch 1 Batch 495==== Step 1 AVG. val Loss 0.4061582684516907 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8047, device='cuda:0')\n",
            "========== Epoch 1 Batch 496==== Step 1 AVG. val Loss 0.4086105525493622 acc = 0.0\n",
            "pos tensor(0.0165, device='cuda:0')\n",
            "neg tensor(0.7906, device='cuda:0')\n",
            "========== Epoch 1 Batch 497==== Step 1 AVG. val Loss 0.4035569131374359 acc = 0.0\n",
            "pos tensor(0.0141, device='cuda:0')\n",
            "neg tensor(0.7927, device='cuda:0')\n",
            "========== Epoch 1 Batch 498==== Step 1 AVG. val Loss 0.40339577198028564 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8053, device='cuda:0')\n",
            "========== Epoch 1 Batch 499==== Step 1 AVG. val Loss 0.4073440432548523 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8187, device='cuda:0')\n",
            "========== Epoch 1 Batch 500==== Step 1 AVG. val Loss 0.41488349437713623 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7973, device='cuda:0')\n",
            "========== Epoch 1 Batch 501==== Step 1 AVG. val Loss 0.4054662883281708 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.8063, device='cuda:0')\n",
            "========== Epoch 1 Batch 502==== Step 1 AVG. val Loss 0.40985408425331116 acc = 0.0\n",
            "pos tensor(0.0144, device='cuda:0')\n",
            "neg tensor(0.7877, device='cuda:0')\n",
            "========== Epoch 1 Batch 503==== Step 1 AVG. val Loss 0.4010474681854248 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7854, device='cuda:0')\n",
            "========== Epoch 1 Batch 504==== Step 1 AVG. val Loss 0.39989379048347473 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8144, device='cuda:0')\n",
            "========== Epoch 1 Batch 505==== Step 1 AVG. val Loss 0.4130612015724182 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.8119, device='cuda:0')\n",
            "========== Epoch 1 Batch 506==== Step 1 AVG. val Loss 0.4115472733974457 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.7886, device='cuda:0')\n",
            "========== Epoch 1 Batch 507==== Step 1 AVG. val Loss 0.40083301067352295 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.7976, device='cuda:0')\n",
            "========== Epoch 1 Batch 508==== Step 1 AVG. val Loss 0.4045596718788147 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.8040, device='cuda:0')\n",
            "========== Epoch 1 Batch 509==== Step 1 AVG. val Loss 0.40832066535949707 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8175, device='cuda:0')\n",
            "========== Epoch 1 Batch 510==== Step 1 AVG. val Loss 0.4140275716781616 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.8009, device='cuda:0')\n",
            "========== Epoch 1 Batch 511==== Step 1 AVG. val Loss 0.4070029556751251 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.8069, device='cuda:0')\n",
            "========== Epoch 1 Batch 512==== Step 1 AVG. val Loss 0.4089179039001465 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8078, device='cuda:0')\n",
            "========== Epoch 1 Batch 513==== Step 1 AVG. val Loss 0.4096669554710388 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.8031, device='cuda:0')\n",
            "========== Epoch 1 Batch 514==== Step 1 AVG. val Loss 0.40784651041030884 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7898, device='cuda:0')\n",
            "========== Epoch 1 Batch 515==== Step 1 AVG. val Loss 0.4022810459136963 acc = 0.0\n",
            "pos tensor(0.0151, device='cuda:0')\n",
            "neg tensor(0.7791, device='cuda:0')\n",
            "========== Epoch 1 Batch 516==== Step 1 AVG. val Loss 0.39711296558380127 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8103, device='cuda:0')\n",
            "========== Epoch 1 Batch 517==== Step 1 AVG. val Loss 0.41098552942276 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7891, device='cuda:0')\n",
            "========== Epoch 1 Batch 518==== Step 1 AVG. val Loss 0.40201273560523987 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7978, device='cuda:0')\n",
            "========== Epoch 1 Batch 519==== Step 1 AVG. val Loss 0.4048130214214325 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8007, device='cuda:0')\n",
            "========== Epoch 1 Batch 520==== Step 1 AVG. val Loss 0.4062299430370331 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.7969, device='cuda:0')\n",
            "========== Epoch 1 Batch 521==== Step 1 AVG. val Loss 0.4050796627998352 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7686, device='cuda:0')\n",
            "========== Epoch 1 Batch 522==== Step 1 AVG. val Loss 0.3920891284942627 acc = 0.0\n",
            "pos tensor(0.0111, device='cuda:0')\n",
            "neg tensor(0.8068, device='cuda:0')\n",
            "========== Epoch 1 Batch 523==== Step 1 AVG. val Loss 0.4089512228965759 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7941, device='cuda:0')\n",
            "========== Epoch 1 Batch 524==== Step 1 AVG. val Loss 0.4024942219257355 acc = 0.0\n",
            "pos tensor(0.0093, device='cuda:0')\n",
            "neg tensor(0.8208, device='cuda:0')\n",
            "========== Epoch 1 Batch 525==== Step 1 AVG. val Loss 0.4150594472885132 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7866, device='cuda:0')\n",
            "========== Epoch 1 Batch 526==== Step 1 AVG. val Loss 0.4006845951080322 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7983, device='cuda:0')\n",
            "========== Epoch 1 Batch 527==== Step 1 AVG. val Loss 0.40588563680648804 acc = 0.0\n",
            "pos tensor(0.0105, device='cuda:0')\n",
            "neg tensor(0.8136, device='cuda:0')\n",
            "========== Epoch 1 Batch 528==== Step 1 AVG. val Loss 0.4120528995990753 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7839, device='cuda:0')\n",
            "========== Epoch 1 Batch 529==== Step 1 AVG. val Loss 0.3987208306789398 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7976, device='cuda:0')\n",
            "========== Epoch 1 Batch 530==== Step 1 AVG. val Loss 0.4050583243370056 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.8061, device='cuda:0')\n",
            "========== Epoch 1 Batch 531==== Step 1 AVG. val Loss 0.4095839262008667 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8008, device='cuda:0')\n",
            "========== Epoch 1 Batch 532==== Step 1 AVG. val Loss 0.4062718152999878 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.7962, device='cuda:0')\n",
            "========== Epoch 1 Batch 533==== Step 1 AVG. val Loss 0.4039992392063141 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8038, device='cuda:0')\n",
            "========== Epoch 1 Batch 534==== Step 1 AVG. val Loss 0.40801894664764404 acc = 0.0\n",
            "pos tensor(0.0172, device='cuda:0')\n",
            "neg tensor(0.7801, device='cuda:0')\n",
            "========== Epoch 1 Batch 535==== Step 1 AVG. val Loss 0.3986472189426422 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.8084, device='cuda:0')\n",
            "========== Epoch 1 Batch 536==== Step 1 AVG. val Loss 0.41031414270401 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7951, device='cuda:0')\n",
            "========== Epoch 1 Batch 537==== Step 1 AVG. val Loss 0.4030185341835022 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7919, device='cuda:0')\n",
            "========== Epoch 1 Batch 538==== Step 1 AVG. val Loss 0.4036271572113037 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8142, device='cuda:0')\n",
            "========== Epoch 1 Batch 539==== Step 1 AVG. val Loss 0.4132789671421051 acc = 0.0\n",
            "pos tensor(0.0112, device='cuda:0')\n",
            "neg tensor(0.7948, device='cuda:0')\n",
            "========== Epoch 1 Batch 540==== Step 1 AVG. val Loss 0.40302425622940063 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8045, device='cuda:0')\n",
            "========== Epoch 1 Batch 541==== Step 1 AVG. val Loss 0.4084978997707367 acc = 0.0\n",
            "pos tensor(0.0157, device='cuda:0')\n",
            "neg tensor(0.7750, device='cuda:0')\n",
            "========== Epoch 1 Batch 542==== Step 1 AVG. val Loss 0.3953438699245453 acc = 0.0\n",
            "pos tensor(0.0130, device='cuda:0')\n",
            "neg tensor(0.7939, device='cuda:0')\n",
            "========== Epoch 1 Batch 543==== Step 1 AVG. val Loss 0.4034369885921478 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7883, device='cuda:0')\n",
            "========== Epoch 1 Batch 544==== Step 1 AVG. val Loss 0.40040600299835205 acc = 0.0\n",
            "pos tensor(0.0168, device='cuda:0')\n",
            "neg tensor(0.7735, device='cuda:0')\n",
            "========== Epoch 1 Batch 545==== Step 1 AVG. val Loss 0.39511436223983765 acc = 0.0\n",
            "pos tensor(0.0144, device='cuda:0')\n",
            "neg tensor(0.8031, device='cuda:0')\n",
            "========== Epoch 1 Batch 546==== Step 1 AVG. val Loss 0.4087451696395874 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.8038, device='cuda:0')\n",
            "========== Epoch 1 Batch 547==== Step 1 AVG. val Loss 0.4085584282875061 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.8041, device='cuda:0')\n",
            "========== Epoch 1 Batch 548==== Step 1 AVG. val Loss 0.4083474576473236 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7897, device='cuda:0')\n",
            "========== Epoch 1 Batch 549==== Step 1 AVG. val Loss 0.4016350507736206 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7833, device='cuda:0')\n",
            "========== Epoch 1 Batch 550==== Step 1 AVG. val Loss 0.3984634280204773 acc = 0.0\n",
            "pos tensor(0.0092, device='cuda:0')\n",
            "neg tensor(0.8105, device='cuda:0')\n",
            "========== Epoch 1 Batch 551==== Step 1 AVG. val Loss 0.40983477234840393 acc = 0.0\n",
            "pos tensor(0.0101, device='cuda:0')\n",
            "neg tensor(0.8132, device='cuda:0')\n",
            "========== Epoch 1 Batch 552==== Step 1 AVG. val Loss 0.41163790225982666 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7910, device='cuda:0')\n",
            "========== Epoch 1 Batch 553==== Step 1 AVG. val Loss 0.4020877480506897 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8159, device='cuda:0')\n",
            "========== Epoch 1 Batch 554==== Step 1 AVG. val Loss 0.413886159658432 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.8124, device='cuda:0')\n",
            "========== Epoch 1 Batch 555==== Step 1 AVG. val Loss 0.4129549562931061 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8070, device='cuda:0')\n",
            "========== Epoch 1 Batch 556==== Step 1 AVG. val Loss 0.40953388810157776 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7954, device='cuda:0')\n",
            "========== Epoch 1 Batch 557==== Step 1 AVG. val Loss 0.40447282791137695 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8043, device='cuda:0')\n",
            "========== Epoch 1 Batch 558==== Step 1 AVG. val Loss 0.4081498086452484 acc = 0.0\n",
            "pos tensor(0.0163, device='cuda:0')\n",
            "neg tensor(0.7880, device='cuda:0')\n",
            "========== Epoch 1 Batch 559==== Step 1 AVG. val Loss 0.4021316468715668 acc = 0.0\n",
            "pos tensor(0.0152, device='cuda:0')\n",
            "neg tensor(0.8012, device='cuda:0')\n",
            "========== Epoch 1 Batch 560==== Step 1 AVG. val Loss 0.4082174599170685 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7892, device='cuda:0')\n",
            "========== Epoch 1 Batch 561==== Step 1 AVG. val Loss 0.4000249207019806 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.7923, device='cuda:0')\n",
            "========== Epoch 1 Batch 562==== Step 1 AVG. val Loss 0.4023059010505676 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7955, device='cuda:0')\n",
            "========== Epoch 1 Batch 563==== Step 1 AVG. val Loss 0.4055595099925995 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7923, device='cuda:0')\n",
            "========== Epoch 1 Batch 564==== Step 1 AVG. val Loss 0.40261611342430115 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.8136, device='cuda:0')\n",
            "========== Epoch 1 Batch 565==== Step 1 AVG. val Loss 0.4136173129081726 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.8106, device='cuda:0')\n",
            "========== Epoch 1 Batch 566==== Step 1 AVG. val Loss 0.41101253032684326 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.7859, device='cuda:0')\n",
            "========== Epoch 1 Batch 567==== Step 1 AVG. val Loss 0.3994133472442627 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8089, device='cuda:0')\n",
            "========== Epoch 1 Batch 568==== Step 1 AVG. val Loss 0.4105702042579651 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 569==== Step 1 AVG. val Loss 0.40266677737236023 acc = 0.0\n",
            "pos tensor(0.0155, device='cuda:0')\n",
            "neg tensor(0.7738, device='cuda:0')\n",
            "========== Epoch 1 Batch 570==== Step 1 AVG. val Loss 0.39462143182754517 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.8026, device='cuda:0')\n",
            "========== Epoch 1 Batch 571==== Step 1 AVG. val Loss 0.4076848030090332 acc = 0.0\n",
            "pos tensor(0.0099, device='cuda:0')\n",
            "neg tensor(0.8149, device='cuda:0')\n",
            "========== Epoch 1 Batch 572==== Step 1 AVG. val Loss 0.4124354422092438 acc = 0.0\n",
            "pos tensor(0.0100, device='cuda:0')\n",
            "neg tensor(0.8011, device='cuda:0')\n",
            "========== Epoch 1 Batch 573==== Step 1 AVG. val Loss 0.40553370118141174 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7895, device='cuda:0')\n",
            "========== Epoch 1 Batch 574==== Step 1 AVG. val Loss 0.401083379983902 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8040, device='cuda:0')\n",
            "========== Epoch 1 Batch 575==== Step 1 AVG. val Loss 0.4079219400882721 acc = 0.0\n",
            "pos tensor(0.0113, device='cuda:0')\n",
            "neg tensor(0.8044, device='cuda:0')\n",
            "========== Epoch 1 Batch 576==== Step 1 AVG. val Loss 0.40786221623420715 acc = 0.0\n",
            "pos tensor(0.0182, device='cuda:0')\n",
            "neg tensor(0.7664, device='cuda:0')\n",
            "========== Epoch 1 Batch 577==== Step 1 AVG. val Loss 0.39232137799263 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8101, device='cuda:0')\n",
            "========== Epoch 1 Batch 578==== Step 1 AVG. val Loss 0.41053149104118347 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8010, device='cuda:0')\n",
            "========== Epoch 1 Batch 579==== Step 1 AVG. val Loss 0.4065651297569275 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8129, device='cuda:0')\n",
            "========== Epoch 1 Batch 580==== Step 1 AVG. val Loss 0.41229259967803955 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8024, device='cuda:0')\n",
            "========== Epoch 1 Batch 581==== Step 1 AVG. val Loss 0.4067380130290985 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7980, device='cuda:0')\n",
            "========== Epoch 1 Batch 582==== Step 1 AVG. val Loss 0.4055671989917755 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.7953, device='cuda:0')\n",
            "========== Epoch 1 Batch 583==== Step 1 AVG. val Loss 0.40387850999832153 acc = 0.0\n",
            "pos tensor(0.0164, device='cuda:0')\n",
            "neg tensor(0.7784, device='cuda:0')\n",
            "========== Epoch 1 Batch 584==== Step 1 AVG. val Loss 0.39738473296165466 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.8021, device='cuda:0')\n",
            "========== Epoch 1 Batch 585==== Step 1 AVG. val Loss 0.40821513533592224 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8004, device='cuda:0')\n",
            "========== Epoch 1 Batch 586==== Step 1 AVG. val Loss 0.40647241473197937 acc = 0.0\n",
            "pos tensor(0.0173, device='cuda:0')\n",
            "neg tensor(0.7812, device='cuda:0')\n",
            "========== Epoch 1 Batch 587==== Step 1 AVG. val Loss 0.3992866277694702 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8043, device='cuda:0')\n",
            "========== Epoch 1 Batch 588==== Step 1 AVG. val Loss 0.40794897079467773 acc = 0.0\n",
            "pos tensor(0.0094, device='cuda:0')\n",
            "neg tensor(0.8188, device='cuda:0')\n",
            "========== Epoch 1 Batch 589==== Step 1 AVG. val Loss 0.4140789806842804 acc = 0.0\n",
            "pos tensor(0.0137, device='cuda:0')\n",
            "neg tensor(0.8031, device='cuda:0')\n",
            "========== Epoch 1 Batch 590==== Step 1 AVG. val Loss 0.40838536620140076 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7850, device='cuda:0')\n",
            "========== Epoch 1 Batch 591==== Step 1 AVG. val Loss 0.3999011218547821 acc = 0.0\n",
            "pos tensor(0.0163, device='cuda:0')\n",
            "neg tensor(0.7791, device='cuda:0')\n",
            "========== Epoch 1 Batch 592==== Step 1 AVG. val Loss 0.39772072434425354 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7971, device='cuda:0')\n",
            "========== Epoch 1 Batch 593==== Step 1 AVG. val Loss 0.40550902485847473 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8098, device='cuda:0')\n",
            "========== Epoch 1 Batch 594==== Step 1 AVG. val Loss 0.4108084738254547 acc = 0.0\n",
            "pos tensor(0.0144, device='cuda:0')\n",
            "neg tensor(0.7974, device='cuda:0')\n",
            "========== Epoch 1 Batch 595==== Step 1 AVG. val Loss 0.40587925910949707 acc = 0.0\n",
            "pos tensor(0.0133, device='cuda:0')\n",
            "neg tensor(0.7917, device='cuda:0')\n",
            "========== Epoch 1 Batch 596==== Step 1 AVG. val Loss 0.4024756848812103 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.8052, device='cuda:0')\n",
            "========== Epoch 1 Batch 597==== Step 1 AVG. val Loss 0.40837541222572327 acc = 0.0\n",
            "pos tensor(0.0110, device='cuda:0')\n",
            "neg tensor(0.8132, device='cuda:0')\n",
            "========== Epoch 1 Batch 598==== Step 1 AVG. val Loss 0.4121391773223877 acc = 0.0\n",
            "pos tensor(0.0148, device='cuda:0')\n",
            "neg tensor(0.7965, device='cuda:0')\n",
            "========== Epoch 1 Batch 599==== Step 1 AVG. val Loss 0.40564852952957153 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7866, device='cuda:0')\n",
            "========== Epoch 1 Batch 600==== Step 1 AVG. val Loss 0.3998548984527588 acc = 0.0\n",
            "pos tensor(0.0103, device='cuda:0')\n",
            "neg tensor(0.8065, device='cuda:0')\n",
            "========== Epoch 1 Batch 601==== Step 1 AVG. val Loss 0.40840572118759155 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7980, device='cuda:0')\n",
            "========== Epoch 1 Batch 602==== Step 1 AVG. val Loss 0.4056119918823242 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8044, device='cuda:0')\n",
            "========== Epoch 1 Batch 603==== Step 1 AVG. val Loss 0.4080585539340973 acc = 0.0\n",
            "pos tensor(0.0163, device='cuda:0')\n",
            "neg tensor(0.7865, device='cuda:0')\n",
            "========== Epoch 1 Batch 604==== Step 1 AVG. val Loss 0.40144065022468567 acc = 0.0\n",
            "pos tensor(0.0170, device='cuda:0')\n",
            "neg tensor(0.7949, device='cuda:0')\n",
            "========== Epoch 1 Batch 605==== Step 1 AVG. val Loss 0.405944287776947 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8024, device='cuda:0')\n",
            "========== Epoch 1 Batch 606==== Step 1 AVG. val Loss 0.40737760066986084 acc = 0.0\n",
            "pos tensor(0.0114, device='cuda:0')\n",
            "neg tensor(0.7800, device='cuda:0')\n",
            "========== Epoch 1 Batch 607==== Step 1 AVG. val Loss 0.3957112729549408 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.8075, device='cuda:0')\n",
            "========== Epoch 1 Batch 608==== Step 1 AVG. val Loss 0.410353422164917 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7907, device='cuda:0')\n",
            "========== Epoch 1 Batch 609==== Step 1 AVG. val Loss 0.4028600752353668 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.8002, device='cuda:0')\n",
            "========== Epoch 1 Batch 610==== Step 1 AVG. val Loss 0.4060971438884735 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7882, device='cuda:0')\n",
            "========== Epoch 1 Batch 611==== Step 1 AVG. val Loss 0.40190911293029785 acc = 0.0\n",
            "pos tensor(0.0177, device='cuda:0')\n",
            "neg tensor(0.7802, device='cuda:0')\n",
            "========== Epoch 1 Batch 612==== Step 1 AVG. val Loss 0.39893874526023865 acc = 0.0\n",
            "pos tensor(0.0160, device='cuda:0')\n",
            "neg tensor(0.7888, device='cuda:0')\n",
            "========== Epoch 1 Batch 613==== Step 1 AVG. val Loss 0.4023958444595337 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.7885, device='cuda:0')\n",
            "========== Epoch 1 Batch 614==== Step 1 AVG. val Loss 0.4012664556503296 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7945, device='cuda:0')\n",
            "========== Epoch 1 Batch 615==== Step 1 AVG. val Loss 0.40360453724861145 acc = 0.0\n",
            "pos tensor(0.0156, device='cuda:0')\n",
            "neg tensor(0.7889, device='cuda:0')\n",
            "========== Epoch 1 Batch 616==== Step 1 AVG. val Loss 0.4022519886493683 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8124, device='cuda:0')\n",
            "========== Epoch 1 Batch 617==== Step 1 AVG. val Loss 0.41222500801086426 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.8090, device='cuda:0')\n",
            "========== Epoch 1 Batch 618==== Step 1 AVG. val Loss 0.4107095003128052 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7947, device='cuda:0')\n",
            "========== Epoch 1 Batch 619==== Step 1 AVG. val Loss 0.4050661623477936 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.7998, device='cuda:0')\n",
            "========== Epoch 1 Batch 620==== Step 1 AVG. val Loss 0.4053246080875397 acc = 0.0\n",
            "pos tensor(0.0123, device='cuda:0')\n",
            "neg tensor(0.8091, device='cuda:0')\n",
            "========== Epoch 1 Batch 621==== Step 1 AVG. val Loss 0.4106793701648712 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.8000, device='cuda:0')\n",
            "========== Epoch 1 Batch 622==== Step 1 AVG. val Loss 0.40673142671585083 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8027, device='cuda:0')\n",
            "========== Epoch 1 Batch 623==== Step 1 AVG. val Loss 0.4071182310581207 acc = 0.0\n",
            "pos tensor(0.0116, device='cuda:0')\n",
            "neg tensor(0.7872, device='cuda:0')\n",
            "========== Epoch 1 Batch 624==== Step 1 AVG. val Loss 0.3994387984275818 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7877, device='cuda:0')\n",
            "========== Epoch 1 Batch 625==== Step 1 AVG. val Loss 0.40063291788101196 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.8087, device='cuda:0')\n",
            "========== Epoch 1 Batch 626==== Step 1 AVG. val Loss 0.41068199276924133 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7899, device='cuda:0')\n",
            "========== Epoch 1 Batch 627==== Step 1 AVG. val Loss 0.40168577432632446 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7954, device='cuda:0')\n",
            "========== Epoch 1 Batch 628==== Step 1 AVG. val Loss 0.403548926115036 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8032, device='cuda:0')\n",
            "========== Epoch 1 Batch 629==== Step 1 AVG. val Loss 0.4075431525707245 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.7936, device='cuda:0')\n",
            "========== Epoch 1 Batch 630==== Step 1 AVG. val Loss 0.40265190601348877 acc = 0.0\n",
            "pos tensor(0.0126, device='cuda:0')\n",
            "neg tensor(0.7963, device='cuda:0')\n",
            "========== Epoch 1 Batch 631==== Step 1 AVG. val Loss 0.40445685386657715 acc = 0.0\n",
            "pos tensor(0.0146, device='cuda:0')\n",
            "neg tensor(0.8000, device='cuda:0')\n",
            "========== Epoch 1 Batch 632==== Step 1 AVG. val Loss 0.40730124711990356 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7992, device='cuda:0')\n",
            "========== Epoch 1 Batch 633==== Step 1 AVG. val Loss 0.4059350788593292 acc = 0.0\n",
            "pos tensor(0.0120, device='cuda:0')\n",
            "neg tensor(0.7936, device='cuda:0')\n",
            "========== Epoch 1 Batch 634==== Step 1 AVG. val Loss 0.4027770161628723 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7759, device='cuda:0')\n",
            "========== Epoch 1 Batch 635==== Step 1 AVG. val Loss 0.3954344391822815 acc = 0.0\n",
            "pos tensor(0.0104, device='cuda:0')\n",
            "neg tensor(0.8038, device='cuda:0')\n",
            "========== Epoch 1 Batch 636==== Step 1 AVG. val Loss 0.40709149837493896 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8113, device='cuda:0')\n",
            "========== Epoch 1 Batch 637==== Step 1 AVG. val Loss 0.4110194146633148 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.7833, device='cuda:0')\n",
            "========== Epoch 1 Batch 638==== Step 1 AVG. val Loss 0.39736196398735046 acc = 0.0\n",
            "pos tensor(0.0099, device='cuda:0')\n",
            "neg tensor(0.8188, device='cuda:0')\n",
            "========== Epoch 1 Batch 639==== Step 1 AVG. val Loss 0.4143266975879669 acc = 0.0\n",
            "pos tensor(0.0084, device='cuda:0')\n",
            "neg tensor(0.8130, device='cuda:0')\n",
            "========== Epoch 1 Batch 640==== Step 1 AVG. val Loss 0.41068458557128906 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.7990, device='cuda:0')\n",
            "========== Epoch 1 Batch 641==== Step 1 AVG. val Loss 0.40645670890808105 acc = 0.0\n",
            "pos tensor(0.0132, device='cuda:0')\n",
            "neg tensor(0.7951, device='cuda:0')\n",
            "========== Epoch 1 Batch 642==== Step 1 AVG. val Loss 0.40415817499160767 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8022, device='cuda:0')\n",
            "========== Epoch 1 Batch 643==== Step 1 AVG. val Loss 0.4064495861530304 acc = 0.0\n",
            "pos tensor(0.0134, device='cuda:0')\n",
            "neg tensor(0.7874, device='cuda:0')\n",
            "========== Epoch 1 Batch 644==== Step 1 AVG. val Loss 0.4003789722919464 acc = 0.0\n",
            "pos tensor(0.0108, device='cuda:0')\n",
            "neg tensor(0.8019, device='cuda:0')\n",
            "========== Epoch 1 Batch 645==== Step 1 AVG. val Loss 0.40637508034706116 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.8023, device='cuda:0')\n",
            "========== Epoch 1 Batch 646==== Step 1 AVG. val Loss 0.40794745087623596 acc = 0.0\n",
            "pos tensor(0.0214, device='cuda:0')\n",
            "neg tensor(0.7797, device='cuda:0')\n",
            "========== Epoch 1 Batch 647==== Step 1 AVG. val Loss 0.40056267380714417 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.7895, device='cuda:0')\n",
            "========== Epoch 1 Batch 648==== Step 1 AVG. val Loss 0.4009871184825897 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7944, device='cuda:0')\n",
            "========== Epoch 1 Batch 649==== Step 1 AVG. val Loss 0.40437430143356323 acc = 0.0\n",
            "pos tensor(0.0115, device='cuda:0')\n",
            "neg tensor(0.8111, device='cuda:0')\n",
            "========== Epoch 1 Batch 650==== Step 1 AVG. val Loss 0.41130685806274414 acc = 0.0\n",
            "pos tensor(0.0119, device='cuda:0')\n",
            "neg tensor(0.8011, device='cuda:0')\n",
            "========== Epoch 1 Batch 651==== Step 1 AVG. val Loss 0.40648871660232544 acc = 0.0\n",
            "pos tensor(0.0155, device='cuda:0')\n",
            "neg tensor(0.7782, device='cuda:0')\n",
            "========== Epoch 1 Batch 652==== Step 1 AVG. val Loss 0.39688414335250854 acc = 0.0\n",
            "pos tensor(0.0150, device='cuda:0')\n",
            "neg tensor(0.7820, device='cuda:0')\n",
            "========== Epoch 1 Batch 653==== Step 1 AVG. val Loss 0.3984775245189667 acc = 0.0\n",
            "pos tensor(0.0131, device='cuda:0')\n",
            "neg tensor(0.7871, device='cuda:0')\n",
            "========== Epoch 1 Batch 654==== Step 1 AVG. val Loss 0.4001123607158661 acc = 0.0\n",
            "pos tensor(0.0143, device='cuda:0')\n",
            "neg tensor(0.7932, device='cuda:0')\n",
            "========== Epoch 1 Batch 655==== Step 1 AVG. val Loss 0.40372568368911743 acc = 0.0\n",
            "pos tensor(0.0127, device='cuda:0')\n",
            "neg tensor(0.7856, device='cuda:0')\n",
            "========== Epoch 1 Batch 656==== Step 1 AVG. val Loss 0.39914318919181824 acc = 0.0\n",
            "pos tensor(0.0139, device='cuda:0')\n",
            "neg tensor(0.8005, device='cuda:0')\n",
            "========== Epoch 1 Batch 657==== Step 1 AVG. val Loss 0.4072045385837555 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7976, device='cuda:0')\n",
            "========== Epoch 1 Batch 658==== Step 1 AVG. val Loss 0.40556633472442627 acc = 0.0\n",
            "pos tensor(0.0129, device='cuda:0')\n",
            "neg tensor(0.8026, device='cuda:0')\n",
            "========== Epoch 1 Batch 659==== Step 1 AVG. val Loss 0.4077739417552948 acc = 0.0\n",
            "pos tensor(0.0121, device='cuda:0')\n",
            "neg tensor(0.8054, device='cuda:0')\n",
            "========== Epoch 1 Batch 660==== Step 1 AVG. val Loss 0.4087529182434082 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.7893, device='cuda:0')\n",
            "========== Epoch 1 Batch 661==== Step 1 AVG. val Loss 0.40144777297973633 acc = 0.0\n",
            "pos tensor(0.0122, device='cuda:0')\n",
            "neg tensor(0.7963, device='cuda:0')\n",
            "========== Epoch 1 Batch 662==== Step 1 AVG. val Loss 0.40425193309783936 acc = 0.0\n",
            "pos tensor(0.0107, device='cuda:0')\n",
            "neg tensor(0.8144, device='cuda:0')\n",
            "========== Epoch 1 Batch 663==== Step 1 AVG. val Loss 0.4125381410121918 acc = 0.0\n",
            "pos tensor(0.0135, device='cuda:0')\n",
            "neg tensor(0.7894, device='cuda:0')\n",
            "========== Epoch 1 Batch 664==== Step 1 AVG. val Loss 0.40144971013069153 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7949, device='cuda:0')\n",
            "========== Epoch 1 Batch 665==== Step 1 AVG. val Loss 0.40494397282600403 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7927, device='cuda:0')\n",
            "========== Epoch 1 Batch 666==== Step 1 AVG. val Loss 0.40325063467025757 acc = 0.0\n",
            "pos tensor(0.0154, device='cuda:0')\n",
            "neg tensor(0.7999, device='cuda:0')\n",
            "========== Epoch 1 Batch 667==== Step 1 AVG. val Loss 0.40764501690864563 acc = 0.0\n",
            "pos tensor(0.0100, device='cuda:0')\n",
            "neg tensor(0.8196, device='cuda:0')\n",
            "========== Epoch 1 Batch 668==== Step 1 AVG. val Loss 0.4148033857345581 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7932, device='cuda:0')\n",
            "========== Epoch 1 Batch 669==== Step 1 AVG. val Loss 0.4035041928291321 acc = 0.0\n",
            "pos tensor(0.0109, device='cuda:0')\n",
            "neg tensor(0.8045, device='cuda:0')\n",
            "========== Epoch 1 Batch 670==== Step 1 AVG. val Loss 0.40766382217407227 acc = 0.0\n",
            "pos tensor(0.0124, device='cuda:0')\n",
            "neg tensor(0.7957, device='cuda:0')\n",
            "========== Epoch 1 Batch 671==== Step 1 AVG. val Loss 0.40401729941368103 acc = 0.0\n",
            "pos tensor(0.0128, device='cuda:0')\n",
            "neg tensor(0.8093, device='cuda:0')\n",
            "========== Epoch 1 Batch 672==== Step 1 AVG. val Loss 0.4110502600669861 acc = 0.0\n",
            "pos tensor(0.0158, device='cuda:0')\n",
            "neg tensor(0.7788, device='cuda:0')\n",
            "========== Epoch 1 Batch 673==== Step 1 AVG. val Loss 0.39729681611061096 acc = 0.0\n",
            "pos tensor(0.0125, device='cuda:0')\n",
            "neg tensor(0.8096, device='cuda:0')\n",
            "========== Epoch 1 Batch 674==== Step 1 AVG. val Loss 0.41105446219444275 acc = 0.0\n",
            "pos tensor(0.0149, device='cuda:0')\n",
            "neg tensor(0.7829, device='cuda:0')\n",
            "========== Epoch 1 Batch 675==== Step 1 AVG. val Loss 0.398886114358902 acc = 0.0\n",
            "pos tensor(0.0155, device='cuda:0')\n",
            "neg tensor(0.7933, device='cuda:0')\n",
            "========== Epoch 1 Batch 676==== Step 1 AVG. val Loss 0.4043641686439514 acc = 0.0\n",
            "pos tensor(0.0117, device='cuda:0')\n",
            "neg tensor(0.8047, device='cuda:0')\n",
            "========== Epoch 1 Batch 677==== Step 1 AVG. val Loss 0.40819868445396423 acc = 0.0\n",
            "pos tensor(0.0118, device='cuda:0')\n",
            "neg tensor(0.8026, device='cuda:0')\n",
            "========== Epoch 1 Batch 678==== Step 1 AVG. val Loss 0.4072132110595703 acc = 0.0\n",
            "pos tensor(0.0136, device='cuda:0')\n",
            "neg tensor(0.8003, device='cuda:0')\n",
            "========== Epoch 1 Batch 679==== Step 1 AVG. val Loss 0.40694358944892883 acc = 0.0\n",
            "pos tensor(0.0140, device='cuda:0')\n",
            "neg tensor(0.8015, device='cuda:0')\n",
            "========== Epoch 1 Batch 680==== Step 1 AVG. val Loss 0.40775662660598755 acc = 0.0\n",
            "pos tensor(0.0138, device='cuda:0')\n",
            "neg tensor(0.7885, device='cuda:0')\n",
            "========== Epoch 1 Batch 681==== Step 1 AVG. val Loss 0.4011673927307129 acc = 0.0\n",
            "pos tensor(0.0095, device='cuda:0')\n",
            "neg tensor(0.8148, device='cuda:0')\n",
            "========== Epoch 1 Batch 682==== Step 1 AVG. val Loss 0.4121597409248352 acc = 0.0\n",
            "  Average Validation Loss: 0.41\n",
            "  Average Validation Accuracy: 0.00\n",
            "  Validation took: 1:11:25\n",
            "EarlyStopping counter: 1 out of 4\n",
            "\n",
            "Training complete!\n",
            "Total training took 2:23:01 (h:mm:ss)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.3497739829275965, 0.4056644971070052]\n",
            "[0.2739248645536997, 0.26449001589353316]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "valLosses,trainLosses = mytrainStep1(modelEmb,criterion1,criterion2)\n",
        "print(valLosses)\n",
        "print(trainLosses)\n",
        "# myDict = {}\n",
        "# myDict['Train'] = []\n",
        "# myDict['Val'] = []\n",
        "# myDict['Train'] = trainLosses\n",
        "# myDict['Val'] = valLosses\n",
        "# with open( \"PAN20_512_cased_train_val_losses\", 'wb') as f:\n",
        "#       pickle.dump(myDict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igJbh3X4go95"
      },
      "source": [
        "1-7 unfrezze all bert No <br>\n",
        "1-7 freeze all bert lr=0.003 3 epochs ok<br>\n",
        "2-6 or 5 freeze all bert lr=0.003 1 epochs <br>\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6lEfvRHZJnw"
      },
      "outputs": [],
      "source": [
        "torch.save(modelEmb.state_dict(), 'finalSiamBERT-Bi-LSTM2.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HXFI6R6jHdvY",
        "outputId": "51d92ab5-ca72-4d02-9f51-b0c49d95e45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running Validation...\n",
            "11\n",
            "tensor([[0.7624]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.7624, 0.2376]], device='cuda:0')\n",
            "========== Epoch 0 Batch 1==== Step 1 AVG. val Loss 0.9895896315574646 acc = 0\n",
            "11\n",
            "tensor([[0.7626]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.2374, 0.7626]], device='cuda:0')\n",
            "========== Epoch 0 Batch 2==== Step 1 AVG. val Loss 0.4646657705307007 acc = 0\n",
            "11\n",
            "tensor([[0.3478]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.6522, 0.3478]], device='cuda:0')\n",
            "========== Epoch 0 Batch 3==== Step 1 AVG. val Loss 0.8569173812866211 acc = 0\n",
            "11\n",
            "tensor([[0.1175]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.8825, 0.1175]], device='cuda:0')\n",
            "========== Epoch 0 Batch 4==== Step 1 AVG. val Loss 1.1471413373947144 acc = 0\n",
            "11\n",
            "tensor([[-0.0930]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.0930,  1.0930]], device='cuda:0')\n",
            "========== Epoch 0 Batch 5==== Step 1 AVG. val Loss 0.2665294408798218 acc = 0\n",
            "11\n",
            "tensor([[0.8443]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.8443, 0.1557]], device='cuda:0')\n",
            "========== Epoch 0 Batch 6==== Step 1 AVG. val Loss 1.0955182313919067 acc = 0\n",
            "11\n",
            "tensor([[0.9054]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0946, 0.9054]], device='cuda:0')\n",
            "========== Epoch 0 Batch 7==== Step 1 AVG. val Loss 0.36778369545936584 acc = 0\n",
            "11\n",
            "tensor([[-0.1789]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.1789,  1.1789]], device='cuda:0')\n",
            "========== Epoch 0 Batch 8==== Step 1 AVG. val Loss 0.2288966029882431 acc = 0\n",
            "11\n",
            "tensor([[-0.0858]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[ 1.0858, -0.0858]], device='cuda:0')\n",
            "========== Epoch 0 Batch 9==== Step 1 AVG. val Loss 1.4414652585983276 acc = 0\n",
            "11\n",
            "tensor([[0.8333]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.1667, 0.8333]], device='cuda:0')\n",
            "========== Epoch 0 Batch 10==== Step 1 AVG. val Loss 0.41439175605773926 acc = 0\n",
            "12\n",
            "tensor([[-0.2249]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.2249,  1.2249]], device='cuda:0')\n",
            "========== Epoch 0 Batch 11==== Step 1 AVG. val Loss 0.21074911952018738 acc = 0\n",
            "11\n",
            "tensor([[-0.4803]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.4803,  1.4803]], device='cuda:0')\n",
            "========== Epoch 0 Batch 12==== Step 1 AVG. val Loss 0.13170093297958374 acc = 0\n",
            "11\n",
            "tensor([[0.9266]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0734, 0.9266]], device='cuda:0')\n",
            "========== Epoch 0 Batch 13==== Step 1 AVG. val Loss 0.3549013137817383 acc = 0\n",
            "11\n",
            "tensor([[0.1981]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.1981, 0.8019]], device='cuda:0')\n",
            "========== Epoch 0 Batch 14==== Step 1 AVG. val Loss 0.4361630976200104 acc = 0\n",
            "11\n",
            "tensor([[-0.2310]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[ 1.2310, -0.2310]], device='cuda:0')\n",
            "========== Epoch 0 Batch 15==== Step 1 AVG. val Loss 1.6704435348510742 acc = 0\n",
            "11\n",
            "tensor([[0.9823]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0177, 0.9823]], device='cuda:0')\n",
            "========== Epoch 0 Batch 16==== Step 1 AVG. val Loss 0.3228852450847626 acc = 0\n",
            "12\n",
            "tensor([[0.0436]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0436, 0.9564]], device='cuda:0')\n",
            "========== Epoch 0 Batch 17==== Step 1 AVG. val Loss 0.3374860882759094 acc = 0\n",
            "12\n",
            "tensor([[0.9683]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0317, 0.9683]], device='cuda:0')\n",
            "========== Epoch 0 Batch 18==== Step 1 AVG. val Loss 0.3306978642940521 acc = 0\n",
            "11\n",
            "tensor([[-0.0858]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[ 1.0858, -0.0858]], device='cuda:0')\n",
            "========== Epoch 0 Batch 19==== Step 1 AVG. val Loss 1.4414652585983276 acc = 0\n",
            "11\n",
            "tensor([[-0.4861]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.4861,  1.4861]], device='cuda:0')\n",
            "========== Epoch 0 Batch 20==== Step 1 AVG. val Loss 0.13028667867183685 acc = 0\n",
            "11\n",
            "tensor([[-0.5305]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.5305,  1.5305]], device='cuda:0')\n",
            "========== Epoch 0 Batch 21==== Step 1 AVG. val Loss 0.1198585107922554 acc = 0\n",
            "11\n",
            "tensor([[-0.2310]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[ 1.2310, -0.2310]], device='cuda:0')\n",
            "========== Epoch 0 Batch 22==== Step 1 AVG. val Loss 1.6704435348510742 acc = 0\n",
            "11\n",
            "tensor([[0.2881]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.2881, 0.7119]], device='cuda:0')\n",
            "========== Epoch 0 Batch 23==== Step 1 AVG. val Loss 0.5035372376441956 acc = 0\n",
            "12\n",
            "tensor([[0.3481]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.6519, 0.3481]], device='cuda:0')\n",
            "========== Epoch 0 Batch 24==== Step 1 AVG. val Loss 0.8565799593925476 acc = 0\n",
            "12\n",
            "tensor([[0.0436]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0436, 0.9564]], device='cuda:0')\n",
            "========== Epoch 0 Batch 25==== Step 1 AVG. val Loss 0.3374860882759094 acc = 0\n",
            "11\n",
            "tensor([[-0.5305]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.5305,  1.5305]], device='cuda:0')\n",
            "========== Epoch 0 Batch 26==== Step 1 AVG. val Loss 0.1198585107922554 acc = 0\n",
            "11\n",
            "tensor([[0.3832]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.3832, 0.6168]], device='cuda:0')\n",
            "========== Epoch 0 Batch 27==== Step 1 AVG. val Loss 0.5831802487373352 acc = 0\n",
            "11\n",
            "tensor([[-0.5581]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.5581,  1.5581]], device='cuda:0')\n",
            "========== Epoch 0 Batch 28==== Step 1 AVG. val Loss 0.11375541239976883 acc = 0\n",
            "11\n",
            "tensor([[0.8554]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.1446, 0.8554]], device='cuda:0')\n",
            "========== Epoch 0 Batch 29==== Step 1 AVG. val Loss 0.3995825946331024 acc = 0\n",
            "11\n",
            "tensor([[0.4065]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.4065, 0.5935]], device='cuda:0')\n",
            "========== Epoch 0 Batch 30==== Step 1 AVG. val Loss 0.6040526032447815 acc = 0\n",
            "11\n",
            "tensor([[-0.2551]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[ 1.2551, -0.2551]], device='cuda:0')\n",
            "========== Epoch 0 Batch 31==== Step 1 AVG. val Loss 1.709800362586975 acc = 0\n",
            "11\n",
            "tensor([[-0.1893]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.1893,  1.1893]], device='cuda:0')\n",
            "========== Epoch 0 Batch 32==== Step 1 AVG. val Loss 0.22468864917755127 acc = 0\n",
            "11\n",
            "tensor([[0.3478]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.6522, 0.3478]], device='cuda:0')\n",
            "========== Epoch 0 Batch 33==== Step 1 AVG. val Loss 0.8569173812866211 acc = 0\n",
            "12\n",
            "tensor([[-0.0350]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.0350,  1.0350]], device='cuda:0')\n",
            "========== Epoch 0 Batch 34==== Step 1 AVG. val Loss 0.29490381479263306 acc = 0\n",
            "13\n",
            "tensor([[0.5554]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.4446, 0.5554]], device='cuda:0')\n",
            "========== Epoch 0 Batch 35==== Step 1 AVG. val Loss 0.6392885446548462 acc = 0\n",
            "11\n",
            "tensor([[0.1124]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.1124, 0.8876]], device='cuda:0')\n",
            "========== Epoch 0 Batch 36==== Step 1 AVG. val Loss 0.37883302569389343 acc = 0\n",
            "13\n",
            "tensor([[0.2501]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.7499, 0.2501]], device='cuda:0')\n",
            "========== Epoch 0 Batch 37==== Step 1 AVG. val Loss 0.9739705920219421 acc = 0\n",
            "11\n",
            "tensor([[0.5908]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.4092, 0.5908]], device='cuda:0')\n",
            "========== Epoch 0 Batch 38==== Step 1 AVG. val Loss 0.6064430475234985 acc = 0\n",
            "12\n",
            "tensor([[0.4896]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.4896, 0.5104]], device='cuda:0')\n",
            "========== Epoch 0 Batch 39==== Step 1 AVG. val Loss 0.6827681660652161 acc = 0\n",
            "11\n",
            "tensor([[0.8554]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.1446, 0.8554]], device='cuda:0')\n",
            "========== Epoch 0 Batch 40==== Step 1 AVG. val Loss 0.3995825946331024 acc = 0\n",
            "12\n",
            "tensor([[0.9856]], device='cuda:0')\n",
            "tensor([[1.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.0144, 0.9856]], device='cuda:0')\n",
            "========== Epoch 0 Batch 41==== Step 1 AVG. val Loss 0.32108354568481445 acc = 0\n",
            "12\n",
            "tensor([[-0.1948]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.1948,  1.1948]], device='cuda:0')\n",
            "========== Epoch 0 Batch 42==== Step 1 AVG. val Loss 0.222489595413208 acc = 0\n",
            "12\n",
            "tensor([[-0.5484]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[-0.5484,  1.5484]], device='cuda:0')\n",
            "========== Epoch 0 Batch 43==== Step 1 AVG. val Loss 0.11587248742580414 acc = 0\n",
            "12\n",
            "tensor([[0.2691]], device='cuda:0')\n",
            "tensor([[0.]], device='cuda:0')\n",
            "tensor([[0., 1.]], device='cuda:0')\n",
            "tensor([[0.2691, 0.7309]], device='cuda:0')\n",
            "========== Epoch 0 Batch 44==== Step 1 AVG. val Loss 0.48864150047302246 acc = 0\n",
            "11\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-76fc0d683674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInfoNCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# modelEmb.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mavg_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgf1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelEmb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-83-f066438ee237>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(model, epoch, criterion1, validation_dataloader, modelFC, criterion2)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodelFC\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m               \u001b[0mFC11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m               \u001b[0mFC22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m               \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCosineSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5bef6ef888b1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id1, mask1, hidden)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0;31m# forward pass of input 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m       \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-5bef6ef888b1>\u001b[0m in \u001b[0;36mforwardOnce\u001b[0;34m(self, sent_id, mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforwardOnce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupLayersMode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1021\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1022\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 426\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    427\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# modelEmb.load_state_dict(torch.load('finalSiamBERT-Bi-LSTM.pt'))\n",
        "criterion1 = InfoNCE()\n",
        "# modelEmb.cuda()\n",
        "avg_val_loss,avgf1,avgAcc = validation(modelEmb,0,criterion1,test_dataloader)\n",
        "print(avg_val_loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLpM7N1HMVFU",
        "outputId": "616ff8cb-88b8-4dce-b29b-fd24b1354c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.2233308533283129\n"
          ]
        }
      ],
      "source": [
        "print(avg_val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF7AnlsDNSct"
      },
      "source": [
        "with trunc val loss = 0.5597->0.5430\n",
        "all chunks val loss = 0.230852->0.220453->0.22825->0.224591"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GdU6DvXBfH79",
        "outputId": "1fdf2892-ad50-4952-8207-ef35e86de4cb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Thesis/PAN22/checkpointEmbuncased_PAN22_16bz.pt'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "# torch.save(early_stopping1, 'earlyStopping/pt')\n",
        "shutil.copy('checkpointEmbuncased_PAN22_16bz.pt','/content/drive/My Drive/Thesis/PAN22')\n",
        "# shutil.copy('PAN20_512_uncased_train_val_losses','/content/drive/My Drive/Thesis/PAN20')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sX71T7-uoL90"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMWKZZ8LsXd1"
      },
      "outputs": [],
      "source": [
        "class myModelFC(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(myModelFC, self).__init__()\n",
        "      inputFeatures = 512 #32\n",
        "      self.FC = nn.Sequential(\n",
        "        # nn.ReLU(),\n",
        "        nn.Tanh(),\n",
        "        nn.Dropout(0.1),  \n",
        "        nn.Linear(in_features = inputFeatures*4,out_features = 256), #256\n",
        "        # nn.Tanh(),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(0.1), #0.5\n",
        "        nn.Linear(in_features = 256,out_features = 2), #64\n",
        "        # nn.ReLU(),\n",
        "        # nn.Tanh(),\n",
        "        # nn.Dropout(0.1),\n",
        "        # nn.Linear(in_features = 64,out_features = 2), #2\n",
        "        # nn.Linear(in_features = 64,out_features = 2),\n",
        "      \n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "      concatenated = torch.cat((input1,input1),dim=1)\n",
        "      \n",
        "      out = self.FC(concatenated)\n",
        "      # print(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYXz6lif2Seb"
      },
      "outputs": [],
      "source": [
        "def mytrainStep2(model,criterion1,criterion2,embModel):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      embModel.bertModel.eval()\n",
        "      embModel.eval()\n",
        "      for param in embModel.bertModel.parameters():\n",
        "          param.requires_grad = False\n",
        "      for param2 in embModel.parameters():\n",
        "          param2.requires_grad = False    \n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "         model.to(device)\n",
        "      \n",
        "      \n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      optimizer2 = torch.optim.Adam(model.parameters(),\n",
        "                                    lr=0.00001\n",
        "                                    )\n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=3*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain2 = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid2 = list()\n",
        "      listOfF1Valid = list()\n",
        "      totalf1 = 0\n",
        "      epoch_stop = 0\n",
        "      for epoch_i in range(0, 5):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss2 = 0\n",
        "\n",
        "          model.train()\n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          totalf1 = 0\n",
        "          for (input1,mask1,input2,mask2,target1) in train_dataloader:\n",
        "              step +=1\n",
        "              if step ==151 :\n",
        "                break\n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  b_input_ids1 = input1.to(device)\n",
        "                  b_input_mask1 = mask1.to(device)\n",
        "                  target1 = target1.type(torch.LongTensor)\n",
        "                  b_labels = target1.to(device)\n",
        "                  h = embModel.init_hidden(b_labels.size(0))\n",
        "                  # b_labels = torch.squeeze(b_labels,1)\n",
        "                  b_input_ids2 = input2.to(device)\n",
        "                  b_input_mask2 = mask2.to(device)\n",
        "              \n",
        "              model.zero_grad()  \n",
        "\n",
        "              # with embModel.parameters() == False:\n",
        "              # for p in embModel.parameters():\n",
        "              #     p.requires_grad = False            \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              FC11,FC22,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels,h)\n",
        "              out = model(FC11,FC22)\n",
        "             \n",
        "              loss = criterion2(out,b_labels)\n",
        "\n",
        "              total_train_loss2 += loss.item()\n",
        "              f1 = calcF1score(out,b_labels)\n",
        "              totalf1+=f1\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 2 Train Loss \"+str(loss.item()),\"====== \",str(f1))\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              optimizer2.step()\n",
        "              # scheduler.step()\n",
        "              \n",
        "          avg_train_loss2 = total_train_loss2 /150#len(train_dataloader)\n",
        "          avg_f1_train = totalf1/150#len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 2 Train Loss \"+str(avg_train_loss2),\"====== \",str(avg_f1_train))            \n",
        "          listOflossesTrain2.append(avg_train_loss2)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss2))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss2,avg_f1_val,avgAcc = validation(embModel,epoch_i,criterion1,validation_dataloader,model,criterion2)\n",
        "          listOflossesValid2.append(avg_val_loss2)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss2))\n",
        "          print(\"  Validation avg-F1: {0:.2f}\".format(avg_f1_val))\n",
        "          print(\"  Validation avg-Accuracy: {0:.2f}\".format(avgAcc))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss2,\n",
        "                'Validation Loss': avg_val_loss2,\n",
        "                # 'Valid. avg F1.': avg_val_f1,\n",
        "                # 'Valid. avg Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "          )\n",
        "          early_stopping2(avg_val_loss2, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping2.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              \n",
        "              break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain2,listOflossesValid2,epoch_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU4vQ4J5DJL5",
        "outputId": "51afb2b5-fb9e-4439-c765-bc318b915b36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "myModelEmbeddings(\n",
              "  (bertModel): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bilstm): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  (dropout3): Dropout(p=0.1, inplace=False)\n",
              "  (tanh): Tanh()\n",
              "  (relu): ReLU()\n",
              "  (dropout4): Dropout(p=0.1, inplace=False)\n",
              "  (FC1): Linear(in_features=1024, out_features=2, bias=True)\n",
              "  (FC2): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (FC3): Linear(in_features=16, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import gc\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "#modelEmb = myModelEmbeddings(bert_emb_layer=6,startLayer=9,endLayer=12,bertModel=bertModel)\n",
        "modelEmb.load_state_dict(torch.load('finalSiamBERT-Bi-LSTM2.pt'))\n",
        "\n",
        "modelEmb.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6Y1Rz5N2Y0c",
        "outputId": "16248de0-005c-4bcf-8b0f-b8ad9eb73d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "tensor([ 0.8446,  0.0375, -0.0191, -0.5123,  0.7045,  0.9427,  0.9581,  0.1240,\n",
            "         0.3576, -0.0707,  0.9492, -0.0733, -0.0232,  0.8786,  0.7015,  0.8562,\n",
            "         0.9809, -0.2167,  0.7521,  0.9364, -0.1994,  0.8521,  0.9983,  0.9656,\n",
            "         0.8600, -0.1278,  0.8732, -0.1446,  0.0778,  0.9452, -0.1244,  0.7048,\n",
            "         0.9433, -0.0730, -0.0209,  0.3029,  0.9567,  0.4802, -0.0246,  0.2960,\n",
            "         0.9945,  0.8088,  0.9669,  0.0238,  0.8318, -0.1446,  0.7633, -0.1259,\n",
            "         0.5297,  0.3119,  0.4602,  0.1466,  0.6371, -0.2206, -0.2441, -0.0418,\n",
            "         0.9941,  0.9910,  0.7803,  0.0174,  0.9503,  0.9938,  0.5052,  0.9526],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 1==== Step 2 Train Loss 0.6927030086517334 ======  0.7311827956989247\n",
            "tensor([ 0.9602,  0.0213,  0.6737, -0.0542,  0.9519, -0.0792,  0.7061,  0.8569,\n",
            "         0.9207,  0.3982, -0.1525, -0.6815, -0.0884,  0.9550,  0.8030,  0.8930,\n",
            "        -0.2730, -0.3346,  0.8951,  0.9932,  0.9728,  0.8848,  0.9422, -0.1146,\n",
            "         0.5125,  0.5992,  0.9940,  0.5490, -0.0490,  0.0248, -0.2141,  0.7655,\n",
            "        -0.0403,  0.9495,  0.6675,  0.7501,  0.9531,  0.3293,  0.8936, -0.0333,\n",
            "        -0.1396,  0.1221,  0.9877,  0.6572,  0.9973,  0.9853,  0.7880,  0.8690,\n",
            "         0.3130,  0.4466, -0.0439, -0.0268, -0.2732,  0.9566, -0.1333,  0.3717,\n",
            "        -0.1886, -0.0860,  0.8816,  0.8262,  0.8789,  0.8361, -0.0951, -0.1105],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 2==== Step 2 Train Loss 0.7111454606056213 ======  0.6516853932584269\n",
            "tensor([ 0.9179, -0.0284,  0.9536,  0.8796,  0.9114,  0.9696, -0.1489,  0.9176,\n",
            "        -0.0286, -0.1305,  0.8365, -0.1075, -0.0234,  0.9739,  0.9756,  0.9308,\n",
            "         0.8362,  0.7632, -0.0212, -0.2968,  0.9515,  0.8962,  0.7048,  0.9048,\n",
            "         0.9770,  0.9715,  0.4963,  0.7247, -0.4370,  0.8587,  0.8551,  0.4877,\n",
            "         0.8950,  0.2411, -0.0290, -0.0297,  0.8588,  0.7175,  0.8921,  0.8813,\n",
            "         0.8325, -0.0340, -0.0503,  0.9678, -0.0427,  0.4297, -0.0410,  0.9758,\n",
            "         0.8809,  0.9904, -0.0175,  0.9255,  0.7755,  0.7867,  0.1350,  0.1494,\n",
            "         0.9845,  0.9762, -0.2332, -0.0603, -0.0892,  0.8900,  0.8375, -0.1263],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 3==== Step 2 Train Loss 0.6986814141273499 ======  0.6744186046511628\n",
            "tensor([ 0.3700, -0.0333,  0.9883,  0.9918, -0.0944, -0.3407, -0.4463,  0.1615,\n",
            "         0.2241,  0.1775,  0.9960, -0.2056, -0.4478,  0.9449,  0.9239, -0.1475,\n",
            "         0.9381,  0.9319, -0.0474, -0.1204, -0.0139, -0.0151, -0.0432,  0.9561,\n",
            "         0.9962,  0.7644,  0.6363, -0.1335,  0.4516, -0.1818,  0.9599,  0.5313,\n",
            "         0.9108,  0.9971, -0.0505,  0.9524, -0.0586, -0.2124, -0.0297,  0.5575,\n",
            "        -0.0263,  0.9908,  0.9774, -0.0918,  0.9780, -0.0821, -0.0418,  0.9849,\n",
            "         0.3806, -0.0786,  0.2535,  0.9809,  0.9195, -0.0170,  0.8596, -0.2444,\n",
            "        -0.0290, -0.0286,  0.8898,  0.7299,  0.0743,  0.9621,  0.9540, -0.0232],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 4==== Step 2 Train Loss 0.7184305191040039 ======  0.617283950617284\n",
            "tensor([-0.0152, -0.0218, -0.0224,  0.9751, -0.2225, -0.0720,  0.8351, -0.0372,\n",
            "        -0.2258, -0.0236, -0.3215, -0.2007,  0.9942,  0.9357,  0.7166,  0.0172,\n",
            "        -0.2450, -0.0804, -0.1660, -0.3904,  0.0537, -0.0511, -0.5829, -0.0556,\n",
            "        -0.1886,  0.9649,  0.9738,  0.9866, -0.0658,  0.9409,  0.9842,  0.1916,\n",
            "         0.9419, -0.0356,  0.9511,  0.8483, -0.0503,  0.9654,  0.8164, -0.2222,\n",
            "         0.3542,  0.9311, -0.1213,  0.8161,  0.4589, -0.0415,  0.3005,  0.0376,\n",
            "         0.7434, -0.0451, -0.0430,  0.8942, -0.0222,  0.7329,  0.9505,  0.9266,\n",
            "         0.9858, -0.1471, -0.2763,  0.8721, -0.0947,  0.5147,  0.9543,  0.9768],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 5==== Step 2 Train Loss 0.6958853602409363 ======  0.5714285714285714\n",
            "tensor([-0.4519, -0.1339, -0.0378,  0.4632, -0.0496, -0.0352, -0.1560, -0.0682,\n",
            "         0.9541,  0.9718,  0.8822, -0.0252,  0.6702,  0.2521,  0.9963,  0.9274,\n",
            "        -0.0330, -0.4299,  0.9205,  0.9816,  0.8068,  0.9572,  0.7785,  0.8469,\n",
            "         0.9979, -0.0789, -0.0292,  0.9222,  0.8629,  0.1781, -0.1765,  0.1599,\n",
            "        -0.0235,  0.7182,  0.9758, -0.1872,  0.9944, -0.0608,  0.9939, -0.1837,\n",
            "         0.9256, -0.0612,  0.8369, -0.0161, -0.1711,  0.9773, -0.0935, -0.2692,\n",
            "         0.6295,  0.7431,  0.9870,  0.9922,  0.8851,  0.1080,  0.9337,  0.9901,\n",
            "         0.5349,  0.9652, -0.5324,  0.9370,  0.8887, -0.0217, -0.0476, -0.2160],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 6==== Step 2 Train Loss 0.670735239982605 ======  0.7200000000000001\n",
            "tensor([ 0.2513, -0.2421,  0.9965,  0.9902,  0.0045, -0.0252, -0.0354,  0.9189,\n",
            "         0.8640, -0.0226,  0.9501,  0.1540, -0.1090,  0.9019,  0.4668, -0.1842,\n",
            "         0.9653, -0.0599, -0.2732, -0.0311,  0.9539,  0.2595,  0.9874,  0.9582,\n",
            "        -0.1000,  0.9499,  0.9080, -0.0282, -0.2355, -0.1534, -0.0269,  0.8687,\n",
            "         0.9837,  0.8153, -0.1576, -0.2436,  0.0131, -0.0617,  0.9548,  0.2847,\n",
            "         0.9957, -0.0737,  0.6487, -0.1533,  0.9931,  0.9225,  0.6585, -0.2496,\n",
            "         0.1951,  0.8476,  0.7625, -0.0543,  0.9836, -0.0068,  0.0048,  0.5720,\n",
            "         0.9649, -0.0886,  0.8291,  0.8117,  0.0861, -0.0393,  0.0902,  0.8251],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 7==== Step 2 Train Loss 0.6828815340995789 ======  0.5352112676056338\n",
            "tensor([ 0.6401,  0.9122,  0.0522,  0.4355,  0.9791,  0.9058,  0.5138,  0.1252,\n",
            "         0.9385,  0.4923,  0.6408,  0.9993,  0.9451, -0.0314, -0.3486, -0.0714,\n",
            "        -0.0214,  0.8570,  0.0192, -0.1502,  0.3440, -0.0371,  0.8565, -0.0678,\n",
            "         0.9590, -0.0672, -0.0127, -0.0843,  0.8680,  0.5132,  0.4897, -0.0488,\n",
            "        -0.0883, -0.0393,  0.9473, -0.2896, -0.1487, -0.0135,  0.9700,  0.8715,\n",
            "         0.4278, -0.1789, -0.0147,  0.3467,  0.3233,  0.7734,  0.9969,  0.5006,\n",
            "        -0.1110,  0.8873,  0.1380,  0.1891,  0.3910,  0.5838,  0.9461, -0.1029,\n",
            "        -0.1026,  0.8543, -0.0598, -0.1760, -0.0206,  0.3681,  0.9609,  0.9347],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 8==== Step 2 Train Loss 0.7042545080184937 ======  0.41666666666666663\n",
            "tensor([-0.0390, -0.0961,  0.8884,  0.9487,  0.9803,  0.2332, -0.0314,  0.9974,\n",
            "        -0.2214,  0.9707, -0.2462,  0.5273, -0.1212,  0.9950,  0.8421,  0.9181,\n",
            "         0.9511,  0.5447, -0.0195,  0.2362, -0.0175,  0.3670, -0.1854, -0.1012,\n",
            "        -0.3351,  0.8963,  0.9959,  0.9511,  0.9377,  0.4594,  0.8613,  0.0804,\n",
            "        -0.0472,  0.9389,  0.9344, -0.1091, -0.1491,  0.4600, -0.0270,  0.0353,\n",
            "         0.5236, -0.1212,  0.0455,  0.8986,  0.9781,  0.8617, -0.0395, -0.1620,\n",
            "        -0.1946,  0.7012,  0.9505,  0.9956,  0.1704,  0.0232,  0.9929,  0.9937,\n",
            "         0.5034,  0.9360, -0.4723, -0.2609,  0.9768, -0.0324, -0.0299, -0.0784],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 9==== Step 2 Train Loss 0.6979802250862122 ======  0.4307692307692308\n",
            "tensor([ 0.9560, -0.0741,  0.9701,  0.9951, -0.0943,  0.0669,  0.9577,  0.9607,\n",
            "         0.0255,  0.8899,  0.5677,  0.9888,  0.8530,  0.7823,  0.7114,  0.1838,\n",
            "         0.9924,  0.9892,  0.5184,  0.8024, -0.3334,  0.9510,  0.9275,  0.9721,\n",
            "         0.8116,  0.8491,  0.9940,  0.9973,  0.9897,  0.9522,  0.9872,  0.9978,\n",
            "        -0.0334,  0.4688,  0.4894, -0.4904,  0.5907,  0.3952, -0.0344,  0.9633,\n",
            "         0.8614,  0.9924,  0.2142,  0.1082,  0.9961,  0.9846,  0.0920,  0.8632,\n",
            "        -0.0915,  0.3581,  0.7621,  0.0077, -0.0145,  0.2769, -0.0665,  0.3061,\n",
            "         0.5989,  0.3070,  0.8754,  0.9777, -0.0245,  0.8694,  0.9064,  0.8626],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 10==== Step 2 Train Loss 0.7073004245758057 ======  0.5454545454545454\n",
            "tensor([ 0.7422,  0.9960,  0.9356,  0.8693,  0.7788,  0.6756,  0.1125,  0.1672,\n",
            "         0.2129,  0.3017,  0.8447,  0.7343, -0.0402, -0.2776,  0.9700,  0.9417,\n",
            "         0.7825,  0.9152, -0.0296, -0.1492,  0.9613, -0.0646, -0.1369,  0.8136,\n",
            "         0.9011,  0.9977,  0.9960,  0.8152,  0.9941,  0.9740,  0.9865,  0.0131,\n",
            "         0.9811, -0.2805,  0.9846, -0.0586,  0.8798,  0.9102, -0.0446, -0.0267,\n",
            "        -0.0458,  0.9299,  0.9712, -0.0609, -0.2182,  0.5921, -0.0304,  0.8564,\n",
            "         0.0037,  0.8835,  0.9687,  0.9079, -0.0923,  0.8848,  0.9837,  0.0115,\n",
            "         0.9360, -0.0391,  0.7292,  0.9705, -0.6032,  0.8088,  0.9916,  0.8454],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 11==== Step 2 Train Loss 0.6959701180458069 ======  0.47619047619047616\n",
            "tensor([ 0.8664, -0.1236,  0.9406, -0.0429,  0.9422,  0.5655, -0.0446,  0.6036,\n",
            "         0.4796,  0.9984,  0.9925, -0.0587, -0.0722, -0.2110,  0.9020, -0.1137,\n",
            "        -0.1598,  0.8975,  0.9783, -0.0924, -0.0408,  0.1891,  0.9944, -0.0405,\n",
            "         0.9750,  0.9306,  0.9873, -0.2862,  0.2037, -0.0321,  0.8255,  0.5112,\n",
            "         0.6277, -0.3731,  0.0330,  0.9979,  0.9179,  0.9527, -0.1278, -0.3435,\n",
            "         0.9775,  0.7978,  0.9285, -0.0601, -0.2728,  0.1432, -0.1621, -0.0354,\n",
            "        -0.2141,  0.7683,  0.5963, -0.1329, -0.3802, -0.0984,  0.1999, -0.3725,\n",
            "         0.9181,  0.7959,  0.9580, -0.2110,  0.8757, -0.2894, -0.0773, -0.0113],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 12==== Step 2 Train Loss 0.7372512817382812 ======  0.2692307692307692\n",
            "tensor([ 0.9809,  0.4885,  0.9691, -0.1308,  0.8977,  0.7972, -0.0797,  0.8394,\n",
            "         0.8965,  0.2240, -0.0402, -0.0131, -0.1905, -0.0183,  0.9059,  0.9212,\n",
            "         0.9725, -0.0159, -0.0234,  0.9772,  0.9887, -0.0707,  0.7607, -0.0579,\n",
            "        -0.1008, -0.0291,  0.7981, -0.1957,  0.6383, -0.1171, -0.2301, -0.0994,\n",
            "         0.2166,  0.8015, -0.0193, -0.0546,  0.9060, -0.2851, -0.0268,  0.5003,\n",
            "         0.9965,  0.7943, -0.3720, -0.3189,  0.9946,  0.0699, -0.3936, -0.5043,\n",
            "         0.2371,  0.9748,  0.9048,  0.7644, -0.2269,  0.8962,  0.9840,  0.6999,\n",
            "        -0.1338, -0.1216,  0.8467,  0.9654,  0.3342,  0.8719,  0.9147,  0.9716],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 13==== Step 2 Train Loss 0.6711848378181458 ======  0.375\n",
            "tensor([ 0.6467,  0.9460,  0.8192,  0.1506, -0.0336,  0.8466,  0.3688, -0.0247,\n",
            "         0.9671, -0.0225,  0.9187,  0.1318,  0.9222,  0.9565,  0.9273,  0.6151,\n",
            "        -0.0381, -0.0710,  0.9729, -0.2533,  0.9993, -0.0245,  0.9862,  0.8897,\n",
            "         0.4592,  0.7204,  0.5685,  0.8877, -0.2758,  0.5632,  0.7704,  0.8884,\n",
            "         0.9937, -0.0644,  0.9864,  0.9469, -0.0371, -0.6835,  0.7780,  0.7618,\n",
            "         0.8155,  0.7741,  0.9817,  0.9566,  0.6597, -0.0607,  0.9141,  0.9962,\n",
            "         0.9478, -0.0802,  0.9623,  0.9668, -0.2385,  0.9458, -0.0222,  0.9921,\n",
            "         0.9721,  0.9060,  0.6605,  0.0213, -0.0661,  0.9779,  0.0246,  0.7719],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 14==== Step 2 Train Loss 0.7102800607681274 ======  0.4666666666666667\n",
            "tensor([ 0.9681, -0.2372,  0.9310,  0.8170,  0.9704,  0.8997,  0.9917, -0.2463,\n",
            "         0.9586,  0.0092,  0.0753,  0.9911,  0.8787,  0.9992, -0.0369,  0.9951,\n",
            "        -0.1028,  0.9129, -0.0099,  0.9453,  0.9995, -0.3030, -0.3236,  0.9331,\n",
            "        -0.2304,  0.8459, -0.2442,  0.2913, -0.0543,  0.6785,  0.9992, -0.1099,\n",
            "        -0.3564,  0.9175,  0.9170,  0.2760, -0.3883,  0.1581,  0.9869,  0.9179,\n",
            "         0.5361, -0.3067,  0.8897, -0.0224,  0.9437,  0.0203,  0.5257, -0.0457,\n",
            "         0.9946,  0.9798,  0.4873,  0.7026, -0.1498, -0.0624, -0.0218, -0.2198,\n",
            "         0.9936,  0.8026,  0.8482,  0.8895,  0.6352,  0.9314,  0.9234, -0.0160],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 15==== Step 2 Train Loss 0.7253081202507019 ======  0.44827586206896547\n",
            "tensor([ 0.9986,  0.2776,  0.8767,  0.9772,  0.8930,  0.3612,  0.3337,  0.9718,\n",
            "         0.0846,  0.9908,  0.2969, -0.1081, -0.0130,  0.9838,  0.9370,  0.9782,\n",
            "        -0.1203,  0.7696,  0.9856, -0.0361,  0.8948, -0.2470,  0.9603,  0.8533,\n",
            "         0.2373,  0.9911,  0.5064,  0.5815,  0.6640, -0.0183,  0.3362, -0.0385,\n",
            "         0.9943, -0.0318,  0.9708,  0.7819,  0.7810,  0.9200,  0.0431,  0.9828,\n",
            "         0.7312, -0.0544, -0.0822, -0.4837, -0.3472, -0.0322,  0.0997, -0.1631,\n",
            "         0.9472,  0.8364,  0.9795,  0.9554,  0.0095,  0.0796,  0.0834, -0.1764,\n",
            "         0.7070,  0.1841, -0.2904,  0.9577,  0.0490,  0.8955, -0.1396,  0.0407],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 16==== Step 2 Train Loss 0.699165403842926 ======  0.4230769230769231\n",
            "tensor([ 0.5720,  0.9086, -0.0215,  0.5622,  0.9058,  0.2722,  0.9527, -0.2417,\n",
            "         0.9405,  0.8764,  0.9722,  0.0068,  0.8735, -0.0336,  0.8943,  0.8360,\n",
            "        -0.0101, -0.2553,  0.9318, -0.3112,  0.0149, -0.0216,  0.9577,  0.8443,\n",
            "        -0.0595,  0.9643, -0.1247,  0.1369, -0.0503, -0.0249,  0.7997,  0.9673,\n",
            "        -0.1372,  0.5925,  0.9286, -0.1225, -0.1208, -0.0150,  0.3936,  0.5947,\n",
            "         0.9799, -0.0823,  0.1944,  0.9795,  0.7004,  0.4331,  0.9471,  0.9770,\n",
            "         0.9783,  0.8263,  0.8576,  0.9583,  0.9949,  0.1384,  0.9950, -0.3174,\n",
            "         0.1101,  0.9825,  0.9300, -0.0763, -0.0644,  0.9967, -0.0272,  0.9563],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 17==== Step 2 Train Loss 0.704174280166626 ======  0.40740740740740733\n",
            "tensor([-0.0728,  0.8114, -0.2231, -0.0882,  0.8475, -0.0164,  0.6212,  0.9499,\n",
            "         0.9672,  0.9430, -0.0543,  0.8539,  0.8946,  0.3584, -0.0263, -0.1506,\n",
            "        -0.0522,  0.9964,  0.9870, -0.0759,  0.2062,  0.9862, -0.0097,  0.1660,\n",
            "         0.7769, -0.3101, -0.0230, -0.2788,  0.9903,  0.7528, -0.0088,  0.5922,\n",
            "        -0.0358, -0.2116,  0.1596,  0.0865, -0.1143,  0.9949,  0.5098,  0.9720,\n",
            "         0.9596,  0.8600, -0.2216,  0.9282,  0.8330,  0.8922,  0.4629,  0.9492,\n",
            "         0.6285,  0.9203,  0.5653, -0.2827, -0.0763,  0.9543, -0.2926, -0.0982,\n",
            "         0.9987,  0.6299, -0.1081, -0.3621,  0.7204, -0.0080, -0.2361,  0.4087],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 18==== Step 2 Train Loss 0.6915350556373596 ======  0.4074074074074074\n",
            "tensor([-0.1596, -0.0224, -0.0796, -0.0206,  0.9417, -0.2391,  0.3427,  0.9373,\n",
            "        -0.0282,  0.9983,  0.9020, -0.0018,  0.7065, -0.0228, -0.0430, -0.1957,\n",
            "         0.9504,  0.6731, -0.0337, -0.0126,  0.0147,  0.7969, -0.2445,  0.6635,\n",
            "        -0.0482,  0.3261, -0.4871, -0.1832,  0.1166,  0.9956, -0.2433,  0.6106,\n",
            "         0.8664, -0.0379, -0.0259,  0.9587,  0.9333, -0.5855,  0.9924, -0.0928,\n",
            "         0.9856,  0.3990,  0.9760, -0.0342,  0.2253, -0.0644,  0.7158,  0.9677,\n",
            "        -0.0282,  0.9715,  0.9958,  0.9570,  0.9577,  0.5711, -0.2434,  0.9782,\n",
            "        -0.0936,  0.2705,  0.9210,  0.9969,  0.9614, -0.3588, -0.0411,  0.9693],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 19==== Step 2 Train Loss 0.7215558886528015 ======  0.34615384615384615\n",
            "tensor([ 0.8119, -0.0161, -0.0588,  0.9672, -0.0990,  0.9780,  0.9938, -0.2210,\n",
            "         0.8729,  0.9614,  0.7292,  0.9504, -0.0210,  0.8140, -0.1885,  0.7497,\n",
            "         0.5538,  0.9962, -0.2101,  0.9418, -0.0151,  0.9785, -0.0149,  0.8323,\n",
            "         0.2517, -0.0381,  0.0094, -0.3692, -0.0243,  0.0844,  0.8590,  0.9859,\n",
            "        -0.2632, -0.1100,  0.0702,  0.9323,  0.9623,  0.9945,  0.9149,  0.9766,\n",
            "         0.6332,  0.6826, -0.0806,  0.2797,  0.9693, -0.1510,  0.9750,  0.9800,\n",
            "         0.9176,  0.9568,  0.6133,  0.9512, -0.0439,  0.9789,  0.7752,  0.1347,\n",
            "         0.7938,  0.8933,  0.8015,  0.9627,  0.9589, -0.0384,  0.9547,  0.8201],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 20==== Step 2 Train Loss 0.6986566185951233 ======  0.4516129032258065\n",
            "tensor([-0.0173,  0.4125, -0.1398,  0.8705,  0.9202, -0.2146,  0.8679,  0.9636,\n",
            "         0.9192,  0.7446,  0.8579,  0.9854,  0.9227, -0.0589, -0.4097,  0.9520,\n",
            "         0.3843,  0.9275,  0.5227, -0.0299,  0.9953, -0.3095,  0.9622, -0.0214,\n",
            "         0.5377, -0.0280, -0.2993, -0.0203,  0.9675, -0.1458, -0.0683,  0.0089,\n",
            "         0.6288, -0.0276,  0.9733,  0.6987, -0.3940, -0.1918,  0.9323,  0.9687,\n",
            "         0.8285,  0.9485, -0.0148,  0.6380, -0.0218,  0.9140,  0.7853,  0.4808,\n",
            "        -0.0243,  0.9693, -0.0552, -0.1825,  0.9946,  0.0601, -0.6140, -0.0172,\n",
            "         0.2055, -0.0880,  0.8599,  0.9517,  0.5953,  0.8460,  0.1374, -0.1368],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 21==== Step 2 Train Loss 0.6942952871322632 ======  0.3529411764705882\n",
            "tensor([ 0.0840,  0.9821,  0.9876,  0.9473, -0.0346,  0.9211, -0.0409, -0.1396,\n",
            "         0.2884,  0.8820, -0.0591, -0.0513, -0.0725,  0.8759, -0.0737, -0.1681,\n",
            "        -0.0422,  0.9582, -0.0235,  0.8329,  0.8310,  0.9926,  0.4326, -0.0248,\n",
            "         0.3079,  0.9601, -0.0239,  0.9064, -0.2214,  0.1125,  0.8988,  0.0885,\n",
            "        -0.0122, -0.1240,  0.8531, -0.1569,  0.6501,  0.9532, -0.2204,  0.8350,\n",
            "        -0.0494,  0.9781,  0.2808,  0.2642, -0.0484,  0.9546,  0.8950,  0.9800,\n",
            "         0.9226, -0.6026,  0.9966,  0.0591,  0.9932,  0.9509, -0.1310,  0.8831,\n",
            "         0.9404,  0.9793,  0.9467, -0.4370, -0.0217,  0.9696, -0.1381, -0.0258],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 22==== Step 2 Train Loss 0.6952528357505798 ======  0.4827586206896552\n",
            "tensor([-0.0329, -0.0206,  0.7252,  0.9690,  0.6758,  0.8947,  0.3098, -0.0779,\n",
            "         0.9409,  0.9661,  0.1476,  0.0642,  0.8077, -0.0983,  0.4848,  0.8745,\n",
            "        -0.1956, -0.0214,  0.8361, -0.5197, -0.1052,  0.3334,  0.9345,  0.6192,\n",
            "         0.9973, -0.0696,  0.3944,  0.7513,  0.0876,  0.5808,  0.9539, -0.0436,\n",
            "         0.0811,  0.8221, -0.0215,  0.3475, -0.0445,  0.8651, -0.3663, -0.0821,\n",
            "        -0.5994,  0.2921,  0.8351, -0.0455, -0.0592,  0.2389,  0.9051, -0.0074,\n",
            "        -0.0568,  0.5059,  0.9117,  0.0226,  0.7891, -0.0375,  0.8724, -0.0200,\n",
            "        -0.0454,  0.6980, -0.0294,  0.9922,  0.4544, -0.0615, -0.0348,  0.5510],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 23==== Step 2 Train Loss 0.7009690999984741 ======  0.35294117647058826\n",
            "tensor([ 0.1053,  0.7780,  0.5494, -0.0603, -0.0416, -0.0183, -0.0213,  0.9365,\n",
            "        -0.1088,  0.9837, -0.0747,  0.9903,  0.4785,  0.4865,  0.9912, -0.0220,\n",
            "         0.9780, -0.0102,  0.9974,  0.4292,  0.9793,  0.2302,  0.6612,  0.2112,\n",
            "        -0.0443,  0.9903,  0.4127, -0.0179, -0.0209,  0.9661,  0.9359,  0.9180,\n",
            "         0.4866,  0.9601,  0.2597, -0.0522,  0.3215, -0.0157, -0.2430, -0.0551,\n",
            "         0.6903,  0.9017,  0.9277,  0.9477, -0.0282,  0.8682,  0.7927, -0.0211,\n",
            "         0.9289, -0.2126,  0.9436,  0.9741,  0.9468,  0.6783,  0.9942,  0.8788,\n",
            "        -0.4981,  0.2869,  0.9253, -0.0523, -0.0756,  0.7046, -0.0926,  0.8057],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 24==== Step 2 Train Loss 0.6848968267440796 ======  0.5396825396825397\n",
            "tensor([ 0.9241, -0.0365,  0.9703,  0.0298, -0.0357,  0.9938,  0.3791,  0.8923,\n",
            "        -0.0644,  0.5338, -0.1206,  0.6171,  0.9956, -0.0157,  0.0276, -0.0194,\n",
            "         0.7416, -0.0250,  0.8914,  0.8032,  0.4887,  0.9838, -0.0305,  0.7594,\n",
            "         0.9301,  0.8860,  0.5476,  0.9962,  0.0089,  0.2796, -0.0889,  0.8538,\n",
            "         0.0257,  0.9501, -0.0863,  0.9182,  0.1787,  0.6023,  0.9776,  0.9853,\n",
            "         0.6585, -0.3918, -0.1290, -0.0641,  0.9385,  0.9697, -0.0225,  0.8157,\n",
            "        -0.0164,  0.9530, -0.0161,  0.9930,  0.8919,  0.3814, -0.0232, -0.2622,\n",
            "         0.9938,  0.9286,  0.3206, -0.1437,  0.7493,  0.7656,  0.9174, -0.0904],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 25==== Step 2 Train Loss 0.7002776861190796 ======  0.2857142857142857\n",
            "tensor([ 0.8115,  0.9820,  0.8072,  0.8651,  0.9112, -0.0580,  0.9204, -0.0278,\n",
            "         0.9746, -0.0171, -0.3816,  0.3381, -0.0866,  0.8755,  0.9898, -0.1207,\n",
            "        -0.1351,  0.0720,  0.2293, -0.0218,  0.9149,  0.9525, -0.0367,  0.1563,\n",
            "         0.9904,  0.6094,  0.9399, -0.1946,  0.0685,  0.7788, -0.0290,  0.9971,\n",
            "        -0.0189, -0.0071, -0.0811,  0.9935,  0.9967,  0.9909, -0.1242, -0.5285,\n",
            "        -0.0292, -0.0134,  0.0535,  0.0092,  0.9618, -0.1854,  0.8461,  0.0512,\n",
            "        -0.0325, -0.0442,  0.9501,  0.9752,  0.9786,  0.9976,  0.8902, -0.3183,\n",
            "        -0.1293,  0.7874,  0.7602,  0.8970,  0.4487, -0.0292, -0.2560,  0.9220],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 26==== Step 2 Train Loss 0.6962698698043823 ======  0.4482758620689655\n",
            "tensor([-0.0308,  0.2776,  0.9215,  0.4459,  0.9085, -0.0438,  0.9975, -0.0129,\n",
            "        -0.0299,  0.3291, -0.0903,  0.9269,  0.9828,  0.9718,  0.9394, -0.6041,\n",
            "         0.9607, -0.3327,  0.2511, -0.5390,  0.9536, -0.2952, -0.0425,  0.8526,\n",
            "        -0.1206, -0.1109,  0.8788,  0.9049,  0.9833,  0.9140,  0.8270,  0.9937,\n",
            "        -0.0154,  0.6892,  0.0135,  0.5508, -0.1685,  0.9763,  0.9000, -0.2768,\n",
            "         0.6641,  0.7859,  0.5409,  0.9877,  0.0798,  0.0203,  0.9855,  0.9645,\n",
            "         0.1541,  0.9396,  0.4354,  0.2253,  0.9870, -0.2131, -0.2607,  0.8846,\n",
            "         0.8646,  0.7205,  0.8064,  0.8643, -0.2062,  0.9768, -0.2385,  0.9891],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 27==== Step 2 Train Loss 0.6946117877960205 ======  0.37931034482758624\n",
            "tensor([ 0.2957, -0.0161,  0.9265, -0.1659,  0.0882,  0.8369,  0.4998, -0.0494,\n",
            "         0.9583,  0.9127,  0.8953,  0.7042, -0.0435, -0.0013, -0.1675,  0.9942,\n",
            "         0.8777,  0.8714,  0.9201,  0.9995,  0.0842,  0.8836,  0.4373,  0.8020,\n",
            "         0.9736,  0.9516, -0.0768, -0.0217, -0.0272,  0.9733,  0.9277,  0.3377,\n",
            "        -0.0223,  0.5810,  0.9165,  0.8844,  0.7487,  0.9420, -0.0309,  0.9683,\n",
            "         0.8159,  0.9341,  0.7907, -0.0426, -0.0282, -0.0710,  0.0332,  0.9505,\n",
            "         0.0680,  0.8345,  0.7513, -0.0241,  0.9902, -0.0355,  0.8485, -0.1567,\n",
            "        -0.0032, -0.0388,  0.2208, -0.1894,  0.7469, -0.1345,  0.9031, -0.0488],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 28==== Step 2 Train Loss 0.707177460193634 ======  0.3636363636363637\n",
            "tensor([ 0.0090, -0.0831,  0.9747,  0.8917, -0.0148,  0.4615,  0.9856,  0.9703,\n",
            "        -0.0928, -0.1280,  0.8673,  0.9922,  0.9539, -0.1259, -0.0177,  0.9607,\n",
            "         0.9341,  0.5846,  0.6607, -0.0267,  0.5987,  0.9977,  0.9629, -0.0302,\n",
            "         0.9445, -0.1376, -0.1490,  0.1164,  0.5868,  0.7532,  0.4099,  0.8500,\n",
            "        -0.0562, -0.0944,  0.7553,  0.7594,  0.1708,  0.8645, -0.0286,  0.1471,\n",
            "        -0.0236,  0.0773, -0.0174, -0.2123,  0.9714,  0.9405,  0.8044,  0.9446,\n",
            "         0.4982,  0.9132,  0.9383,  0.2663, -0.1182,  0.9915,  0.9004,  0.9804,\n",
            "        -0.1461,  0.8449,  0.9928,  0.0075, -0.0531, -0.2077, -0.0680,  0.9711],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 29==== Step 2 Train Loss 0.7089633941650391 ======  0.4615384615384615\n",
            "tensor([-0.4563,  0.9489, -0.1580, -0.0268, -0.0085,  0.3535,  0.9954,  0.3343,\n",
            "        -0.3232, -0.0419,  0.9151,  0.7925,  0.2274, -0.4998,  0.6630,  0.8106,\n",
            "        -0.0345, -0.0643,  0.6087, -0.0821, -0.1205,  0.7203,  0.9449,  0.1994,\n",
            "        -0.2500, -0.0211, -0.2200, -0.1534,  0.8179,  0.9692,  0.7160,  0.8847,\n",
            "        -0.0341, -0.1534,  0.5650,  0.8448, -0.2033,  0.9877, -0.0721,  0.7155,\n",
            "         0.7662,  0.9194,  0.7779, -0.0307, -0.2868,  0.8378, -0.0962, -0.0899,\n",
            "        -0.5782,  0.9804, -0.3555,  0.9828,  0.9714,  0.9307,  0.9035, -0.0251,\n",
            "         0.4439,  0.9395,  0.9774, -0.0576,  0.0748, -0.0301,  0.8625,  0.8857],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 30==== Step 2 Train Loss 0.7027514576911926 ======  0.4126984126984127\n",
            "tensor([ 0.5419,  0.8552,  0.0325,  0.8475, -0.0376,  0.9340, -0.0321,  0.9946,\n",
            "         0.8866,  0.9608, -0.1502, -0.2563,  0.1386, -0.3192, -0.1698, -0.1626,\n",
            "         0.7088,  0.1293,  0.6260,  0.9640,  0.9946,  0.8980, -0.4456, -0.1197,\n",
            "        -0.0668,  0.6480,  0.9720,  0.7529,  0.7080,  0.8297,  0.8108,  0.0477,\n",
            "        -0.2268, -0.1340,  0.5840, -0.1199, -0.0335,  0.9722, -0.0295, -0.0319,\n",
            "         0.9486,  0.9558, -0.4320,  0.9771, -0.1570,  0.9256,  0.9986, -0.0658,\n",
            "         0.8985,  0.6495, -0.0174,  0.0677,  0.9487,  0.2726,  0.9077, -0.0093,\n",
            "         0.9039,  0.9984, -0.3686, -0.0194,  0.9835, -0.0458,  0.6722,  0.9788],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 31==== Step 2 Train Loss 0.6847763061523438 ======  0.5161290322580646\n",
            "tensor([ 0.9573,  0.9140, -0.0805,  0.7230,  0.9811, -0.0171,  0.4332,  0.9560,\n",
            "        -0.2148,  0.9032, -0.2088,  0.1382,  0.8119,  0.9972,  0.8976,  0.9475,\n",
            "         0.7587,  0.8156, -0.0075,  0.7934,  0.9449, -0.0215,  0.9971, -0.0417,\n",
            "         0.8809,  0.9970,  0.5507,  0.6192, -0.0454,  0.9612, -0.0486,  0.9778,\n",
            "         0.9795, -0.1367,  0.8657, -0.0243,  0.8173, -0.1016, -0.5612,  0.8473,\n",
            "        -0.0185,  0.5837, -0.1817, -0.0263,  0.6289,  0.0198,  0.9220,  0.7637,\n",
            "         0.9263,  0.1578,  0.4651,  0.9977,  0.7401,  0.9564,  0.8991,  0.6971,\n",
            "         0.6972, -0.1157, -0.5822,  0.9958, -0.0314, -0.1210,  0.9260, -0.2253],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 32==== Step 2 Train Loss 0.7061988115310669 ======  0.45161290322580644\n",
            "tensor([-0.0932,  0.8064,  0.8077,  0.8978,  0.9702,  0.0714, -0.0792, -0.1866,\n",
            "        -0.0373,  0.8827, -0.5246,  0.4255,  0.9890, -0.0075, -0.0150,  0.9686,\n",
            "         0.9668,  0.3684,  0.8252,  0.9978,  0.9966,  0.2951, -0.0220,  0.5793,\n",
            "         0.9871,  0.1831, -0.1397, -0.0546,  0.5461,  0.9204, -0.1380,  0.1734,\n",
            "         0.4698,  0.9673,  0.8768, -0.1644,  0.1624,  0.9706,  0.9834, -0.1756,\n",
            "         0.5743,  0.9888, -0.1765, -0.4222,  0.9986, -0.1326,  0.6355, -0.0842,\n",
            "        -0.1102,  0.6107, -0.2085,  0.8956, -0.0260, -0.0742,  0.6875, -0.0020,\n",
            "        -0.4764,  0.9931,  0.3904,  0.1985,  0.8100, -0.4138,  0.1228,  0.9460],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 33==== Step 2 Train Loss 0.6844621300697327 ======  0.5833333333333334\n",
            "tensor([ 0.5193, -0.1934, -0.1211,  0.9435,  0.9394, -0.0021,  0.2800, -0.0261,\n",
            "         0.5947, -0.0329,  0.3595,  0.6970, -0.0299,  0.2210, -0.0297,  0.7617,\n",
            "         0.8798,  0.3867,  0.0350,  0.8819, -0.0884, -0.1720,  0.0448, -0.0790,\n",
            "         0.9114, -0.0243,  0.7392, -0.4224,  0.7402, -0.2749, -0.0272,  0.3714,\n",
            "         0.9698, -0.3236,  0.6474,  0.9511,  0.9884,  0.4368,  0.9806,  0.9314,\n",
            "        -0.2875, -0.0240,  0.9166,  0.6316,  0.9911,  0.8456,  0.7578, -0.1895,\n",
            "         0.7649, -0.2018,  0.9383,  0.7074,  0.4315,  0.2003, -0.0287,  0.9671,\n",
            "         0.0036,  0.9690, -0.0309,  0.9801,  0.9899, -0.1567,  0.9324, -0.0011],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 34==== Step 2 Train Loss 0.7032294273376465 ======  0.47761194029850745\n",
            "tensor([ 0.9982,  0.7922, -0.0596, -0.1182,  0.8938,  0.8185,  0.9333,  0.9200,\n",
            "        -0.0405,  0.4743, -0.1041, -0.0080, -0.0175,  0.7906,  0.2594,  0.1576,\n",
            "        -0.0147,  0.3042,  0.9902,  0.9970,  0.9761, -0.1709, -0.1180,  0.9833,\n",
            "         0.0277,  0.9618,  0.8209,  0.9942, -0.4882,  0.1458,  0.9447, -0.2447,\n",
            "         0.2063, -0.2581,  0.3431,  0.7071,  0.8716,  0.1193, -0.0038,  0.9633,\n",
            "        -0.2987,  0.6934, -0.0165,  0.9772, -0.5273, -0.1444,  0.8850,  0.9905,\n",
            "         0.9945,  0.9875,  0.9858,  0.0836,  0.6073,  0.9425,  0.3463,  0.9852,\n",
            "        -0.2116, -0.6814, -0.2907, -0.0562,  0.5349, -0.1600,  0.9712,  0.9954],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 35==== Step 2 Train Loss 0.6918381452560425 ======  0.523076923076923\n",
            "tensor([ 0.6284,  0.9736, -0.0193,  0.4057,  0.1703,  0.6363,  0.7836, -0.0196,\n",
            "        -0.1387,  0.8140,  0.9893, -0.0832,  0.1401, -0.1151, -0.0857, -0.0250,\n",
            "         0.0818,  0.8704,  0.8398,  0.9958, -0.0192,  0.3216, -0.2191,  0.5939,\n",
            "        -0.0464,  0.0795, -0.0981, -0.7752,  0.9774, -0.2044,  0.9734,  0.9520,\n",
            "         0.9605,  0.2554, -0.0628,  0.7473, -0.0251,  0.0542, -0.0553,  0.8710,\n",
            "         0.9596,  0.8558,  0.9968,  0.6746,  0.8197,  0.9831, -0.0293,  0.9536,\n",
            "         0.9751,  0.9828,  0.0143,  0.9648,  0.9603,  0.1843,  0.9366,  0.9394,\n",
            "         0.9990,  0.8059,  0.7818,  0.9358, -0.1342, -0.0283,  0.9485,  0.7745],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 36==== Step 2 Train Loss 0.7036823034286499 ======  0.5079365079365079\n",
            "tensor([-0.2299,  0.9696,  0.5744, -0.1315,  0.4900,  0.9979, -0.0099,  0.8899,\n",
            "         0.8223, -0.0488, -0.2803,  0.9093,  0.9738,  0.9714,  0.9585,  0.9862,\n",
            "         0.9601, -0.1798, -0.1162,  0.8702,  0.2517, -0.0687,  0.0264,  0.7747,\n",
            "         0.2153,  0.9881,  0.8265,  0.8504, -0.0914, -0.0046,  0.9955,  0.7125,\n",
            "         0.7458,  0.4632,  0.5510,  0.9941,  0.5478,  0.9919,  0.9872,  0.9578,\n",
            "         0.3658,  0.0402, -0.1249,  0.9516, -0.0194,  0.2649, -0.1769, -0.0439,\n",
            "        -0.0556, -0.1049, -0.0208,  0.9988,  0.6104,  0.9272,  0.9474,  0.5269,\n",
            "         0.7904,  0.8913,  0.7151, -0.1012, -0.0990,  0.9147,  0.9976,  0.0318],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 37==== Step 2 Train Loss 0.6936046481132507 ======  0.5833333333333333\n",
            "tensor([ 9.2538e-01, -2.8789e-02, -8.6193e-02,  1.0671e-01, -5.9249e-02,\n",
            "         9.4909e-01,  2.6433e-01,  8.2962e-01,  9.9242e-01, -2.2706e-01,\n",
            "         9.3464e-01,  3.6854e-01, -2.9583e-01, -2.1543e-02,  3.7935e-02,\n",
            "         9.8394e-01,  6.8935e-01, -4.5498e-02,  9.4669e-01, -2.4827e-02,\n",
            "        -9.7216e-04, -8.2944e-02,  9.5622e-01,  8.4173e-01,  9.9219e-01,\n",
            "         9.8026e-01,  7.5162e-01, -5.0166e-02,  8.5075e-01,  9.5341e-01,\n",
            "        -5.7676e-02, -3.0933e-01,  8.8269e-01,  8.7305e-01,  9.7683e-01,\n",
            "         9.0657e-01, -1.2702e-01,  6.9167e-01,  9.5360e-01,  9.5568e-01,\n",
            "        -4.7694e-02,  9.6104e-01, -1.2105e-01,  9.7581e-01,  7.5772e-01,\n",
            "         9.7511e-01,  5.7211e-01, -2.3094e-02,  4.0513e-01,  9.6706e-01,\n",
            "         9.8475e-01,  9.6714e-01,  7.4832e-01,  9.9664e-01, -2.1056e-02,\n",
            "         5.0207e-01,  9.5602e-01,  3.0688e-01,  9.6473e-01,  4.6264e-01,\n",
            "         9.3969e-01, -3.4225e-02, -2.4058e-02, -3.0065e-01], device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 38==== Step 2 Train Loss 0.6806723475456238 ======  0.5671641791044776\n",
            "tensor([ 0.7668,  0.9199, -0.4215,  0.9894,  0.8315,  0.9715, -0.0700,  0.9975,\n",
            "        -0.1602, -0.4679, -0.0980, -0.3168,  0.8437,  0.8903,  0.7714,  0.0597,\n",
            "        -0.0061, -0.0345,  0.9206,  0.0404,  0.0785,  0.9368, -0.0758,  0.1495,\n",
            "         0.9953, -0.1777,  0.9620,  0.5287,  0.1008,  0.1384,  0.3231, -0.1258,\n",
            "        -0.2284,  0.6853,  0.9666,  0.9938,  0.8463, -0.2206,  0.8852, -0.0528,\n",
            "        -0.0265, -0.3001, -0.1167,  0.5031,  0.9740,  0.9291,  0.8802, -0.0310,\n",
            "         0.9352,  0.3839, -0.1425,  0.9655,  0.6415, -0.0388,  0.2237,  0.6047,\n",
            "         0.0482,  0.4495,  0.9688,  0.8906,  0.5322,  0.8561, -0.0457,  0.9791],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 39==== Step 2 Train Loss 0.7039222717285156 ======  0.380952380952381\n",
            "tensor([-0.0291, -0.0240,  0.5085,  0.9606, -0.2934, -0.0843,  0.2607, -0.4540,\n",
            "         0.9050,  0.8759, -0.2525,  0.7160, -0.2739, -0.3456,  0.9106,  0.3400,\n",
            "         0.9322, -0.5418, -0.0746, -0.1946,  0.5668,  0.9797, -0.0339,  0.0625,\n",
            "        -0.0535, -0.0210,  0.9328,  0.9798,  0.3657,  0.9764,  0.9803, -0.1022,\n",
            "         0.9344,  0.7982, -0.0356, -0.0222,  0.9669,  0.7918, -0.0397, -0.0917,\n",
            "         0.4427,  0.5540, -0.6210, -0.0635, -0.0472, -0.0430, -0.0426,  0.9447,\n",
            "         0.9048,  0.6282,  0.9875,  0.9304,  0.8248,  0.9928,  0.9277,  0.6605,\n",
            "         0.0075,  0.0245,  0.8650, -0.0363,  0.0399,  0.9340,  0.4528, -0.0492],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 40==== Step 2 Train Loss 0.6932048797607422 ======  0.39285714285714285\n",
            "tensor([-0.0397, -0.1758,  0.7397,  0.9873,  0.9341,  0.9250,  0.8979,  0.8551,\n",
            "        -0.0472, -0.0280,  0.9922, -0.0123,  0.5954,  0.9975,  0.5803, -0.3972,\n",
            "        -0.3101,  0.2173,  0.9473, -0.0433,  0.9287,  0.0501, -0.0155,  0.7946,\n",
            "         0.1033,  0.1977,  0.9909, -0.1242,  0.9516, -0.3907,  0.5358, -0.0412,\n",
            "        -0.2847, -0.0263,  0.9161,  0.4539,  0.8917,  0.2530,  0.9571,  0.7213,\n",
            "         0.2415,  0.9142, -0.0414,  0.8855,  0.9807,  0.2439,  0.9249,  0.9044,\n",
            "        -0.0768,  0.9299, -0.2155,  0.7516,  0.8997, -0.0891,  0.9021,  0.9334,\n",
            "         0.8815,  0.6573, -0.0599,  0.9329, -0.0605, -0.0644,  0.1049,  0.1364],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 41==== Step 2 Train Loss 0.6763304471969604 ======  0.5\n",
            "tensor([ 0.9091,  0.4004,  0.9736, -0.2143,  0.9394, -0.2035,  0.3026,  0.5014,\n",
            "         0.9273,  0.9456, -0.1309, -0.0506, -0.1943, -0.0766, -0.0727,  0.4778,\n",
            "         0.9274,  0.9912,  0.0201, -0.1756,  0.9825,  0.9540,  0.7479, -0.3773,\n",
            "         0.9935,  0.6939,  0.9055,  0.8873,  0.9792,  0.9765, -0.0270,  0.5656,\n",
            "        -0.0443, -0.1493,  0.7212, -0.0216,  0.5149,  0.2471,  0.9848,  0.4081,\n",
            "         0.4824,  0.9023,  0.9895, -0.0279,  0.8914,  0.9344, -0.2805,  0.9979,\n",
            "        -0.0280,  0.8990, -0.3570,  0.9937,  0.9976,  0.9726,  0.9365, -0.2700,\n",
            "         0.9728,  0.0489,  0.9992,  0.9230,  0.9183,  0.4178,  0.9434,  0.9893],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 42==== Step 2 Train Loss 0.6853649616241455 ======  0.6285714285714287\n",
            "tensor([ 0.9856,  0.9824,  0.0452, -0.2188,  0.9131,  0.9802, -0.3491, -0.1325,\n",
            "         0.0116,  0.7881,  0.4625, -0.1631,  0.1326, -0.1712,  0.9106,  0.8466,\n",
            "         0.5227,  0.0475,  0.9628,  0.9899,  0.9978,  0.6510,  0.5187, -0.1727,\n",
            "         0.9859,  0.0962,  0.8028,  0.9172,  0.7668,  0.0430, -0.0814,  0.8449,\n",
            "         0.2155,  0.1945,  0.3234,  0.9962,  0.9841,  0.9739,  0.7895,  0.9816,\n",
            "         0.9181,  0.9637,  0.3381,  0.9744,  0.7510, -0.0288, -0.0393, -0.2427,\n",
            "        -0.0935, -0.0639, -0.0253,  0.3879,  0.9982,  0.8972,  0.9698,  0.9841,\n",
            "        -0.0554,  0.2212, -0.2070,  0.8383, -0.1505, -0.1947,  0.9997, -0.1098],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 43==== Step 2 Train Loss 0.6868726015090942 ======  0.5396825396825397\n",
            "tensor([ 0.9759,  0.9491, -0.0793,  0.9986,  0.9516, -0.3907,  0.4167,  0.9645,\n",
            "         0.9836,  0.8589,  0.8785,  0.4071,  0.6896, -0.2695,  0.2720,  0.9348,\n",
            "        -0.1478, -0.1813,  0.7084,  0.8916, -0.0556,  0.8910,  0.1426,  0.9874,\n",
            "         0.9634,  0.8119,  0.9613,  0.9699,  0.9579, -0.0120, -0.0406, -0.0550,\n",
            "        -0.0288,  0.8803, -0.0540,  0.9412, -0.0414,  0.1226,  0.1907, -0.3288,\n",
            "        -0.0054,  0.9836, -0.0224, -0.0240,  0.9924, -0.0364,  0.2120,  0.9892,\n",
            "        -0.0607, -0.0215,  0.9809,  0.7714,  0.2979, -0.0923, -0.0387,  0.9116,\n",
            "         0.4761, -0.0169,  0.9941,  0.6203, -0.0237,  0.7625,  0.4060, -0.2010],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 44==== Step 2 Train Loss 0.6968908309936523 ======  0.42105263157894735\n",
            "tensor([ 0.5679, -0.0234,  0.9451,  0.9927, -0.2320,  0.9928, -0.4936,  0.9856,\n",
            "         0.9874,  0.7869, -0.0332,  0.8246,  0.9787,  0.9402,  0.9632, -0.0683,\n",
            "         0.8689, -0.0055,  0.9723,  0.2028, -0.0148,  0.9880, -0.0160,  0.7921,\n",
            "         0.9760, -0.0279, -0.1093, -0.0138,  0.0500,  0.9373,  0.5525,  0.7724,\n",
            "        -0.3248,  0.9587,  0.4324,  0.8725,  0.9976, -0.0322,  0.8311,  0.3596,\n",
            "         0.9593,  0.0390,  0.6642,  0.8898, -0.1943,  0.9945, -0.0215, -0.0337,\n",
            "         0.9272, -0.1357, -0.1190,  0.3508,  0.9282,  0.9111,  0.1874,  0.7138,\n",
            "        -0.0948,  0.9813,  0.1466,  0.9710,  0.9030,  0.8701, -0.0716,  0.3595],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 45==== Step 2 Train Loss 0.7007216215133667 ======  0.5294117647058824\n",
            "tensor([ 0.8738, -0.3179,  0.9495,  0.9652,  0.9758, -0.0674,  0.8460,  0.9895,\n",
            "        -0.2473, -0.0787,  0.9948, -0.0963,  0.6454, -0.0547,  0.9089, -0.0840,\n",
            "         0.5114,  0.7394,  0.7500,  0.9652,  0.8010, -0.3384,  0.9556,  0.9606,\n",
            "         0.0554,  0.9923, -0.0721,  0.6510,  0.5339,  0.8749,  0.9973,  0.9851,\n",
            "        -0.0311, -0.1009,  0.7751, -0.1320, -0.0660, -0.0433,  0.8301,  0.9908,\n",
            "         0.9069, -0.0131,  0.3670,  0.8706,  0.9919,  0.7902,  0.9702,  0.6828,\n",
            "        -0.4357,  0.9211, -0.0111, -0.3796,  0.9523, -0.5065,  0.3418,  0.9483,\n",
            "        -0.5275, -0.0750,  0.4094,  0.6552,  0.9328,  0.6475,  0.9369, -0.1956],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 46==== Step 2 Train Loss 0.7052638530731201 ======  0.47058823529411764\n",
            "tensor([-0.0068,  0.3869, -0.0572, -0.0321,  0.2066,  0.4033, -0.2289,  0.8095,\n",
            "         0.9210,  0.9819, -0.1809,  0.2882,  0.0452,  0.0306,  0.9980,  0.5841,\n",
            "         0.6852,  0.9988,  0.8569, -0.0583,  0.4297,  0.8215, -0.1430, -0.0783,\n",
            "         0.5742,  0.9503,  0.2547,  0.9728, -0.1645, -0.0460,  0.9589,  0.8708,\n",
            "         0.0339, -0.0682, -0.0045, -0.1649,  0.9862,  0.9010, -0.2437, -0.0228,\n",
            "        -0.1542,  0.4813,  0.8763, -0.0985, -0.0458,  0.3528,  0.6825,  0.8771,\n",
            "        -0.2423,  0.9194,  0.9902, -0.0095,  0.8755,  0.8549,  0.8739,  0.8307,\n",
            "        -0.0393,  0.0957,  0.7503, -0.1698, -0.0765, -0.0428,  0.1122,  0.7776],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 47==== Step 2 Train Loss 0.6890164017677307 ======  0.5423728813559322\n",
            "tensor([ 0.4064,  0.0218,  0.7509,  0.8076,  0.9788,  0.9649, -0.0132,  0.9479,\n",
            "        -0.2633,  0.8877,  0.9085, -0.0246,  0.9559,  0.8652, -0.0539,  0.9984,\n",
            "         0.9782,  0.9289, -0.0722,  0.0056, -0.1426,  0.5627,  0.7822,  0.9875,\n",
            "        -0.2490,  0.9962,  0.9923,  0.9358,  0.9972, -0.0131,  0.8050,  0.3875,\n",
            "         0.9171, -0.0167,  0.8699,  0.7913, -0.3046, -0.0576,  0.9314,  0.9890,\n",
            "         0.4207,  0.8940,  0.9657,  0.7624,  0.9250,  0.6827,  0.0332, -0.1053,\n",
            "        -0.1541,  0.9814,  0.9843, -0.3268,  0.9947,  0.6012,  0.2748, -0.0204,\n",
            "         0.3761,  0.8910,  0.9707, -0.1349,  0.0656,  0.3240, -0.3818,  0.2641],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 48==== Step 2 Train Loss 0.7071826457977295 ======  0.5507246376811593\n",
            "tensor([ 6.2245e-01,  9.8386e-01, -2.7919e-02, -1.2146e-02, -3.2066e-02,\n",
            "         6.7788e-01,  9.9305e-01, -2.4927e-02, -5.0320e-01, -9.2210e-02,\n",
            "         5.6801e-02, -5.0979e-02, -5.4688e-02, -7.8242e-02,  8.3815e-01,\n",
            "         7.0404e-01, -4.4234e-02,  9.7526e-01,  6.9861e-01, -3.0968e-01,\n",
            "         9.6482e-01,  2.6588e-01,  9.5092e-01, -4.0376e-02, -2.1761e-02,\n",
            "         5.8582e-01,  9.7097e-01,  9.7566e-01,  9.9189e-01, -1.2266e-01,\n",
            "         2.9163e-01,  8.8038e-01,  9.2476e-01,  9.3845e-01,  4.5229e-04,\n",
            "         8.7544e-01, -6.0255e-02, -1.7014e-01,  2.8685e-01,  9.3395e-01,\n",
            "        -5.5567e-02,  9.9352e-01,  7.8415e-01, -1.6716e-01,  1.6284e-01,\n",
            "         9.2970e-01,  9.7875e-01,  8.2010e-01,  9.7412e-01,  9.5647e-01,\n",
            "         2.9806e-01,  9.4927e-01, -2.6344e-01, -6.1923e-01, -3.0524e-02,\n",
            "         9.8364e-01, -7.7015e-02, -1.4114e-01, -1.6224e-01,  3.9708e-01,\n",
            "         1.4014e-02, -4.0665e-02,  7.8868e-01, -1.0376e-01], device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 49==== Step 2 Train Loss 0.6921884417533875 ======  0.5454545454545455\n",
            "tensor([-0.0336,  0.1977, -0.0782,  0.7358,  0.9275,  0.5339, -0.0275, -0.0026,\n",
            "         0.9767,  0.9181, -0.0642, -0.1195, -0.4208,  0.9207, -0.2803,  0.9902,\n",
            "         0.4894, -0.0566,  0.7933, -0.0229, -0.0318,  0.9583, -0.1135,  0.9167,\n",
            "         0.6808,  0.9136,  0.9088, -0.0193,  0.9770,  0.9925, -0.1298,  0.9227,\n",
            "         0.0115,  0.9344,  0.9399,  0.9958,  0.8947,  0.9798,  0.9687,  0.8927,\n",
            "         0.0675, -0.0569, -0.0241,  0.3195,  0.9557,  0.8886,  0.9553,  0.8761,\n",
            "         0.0759,  0.5507,  0.9326,  0.9816, -0.1998, -0.0178,  0.9441, -0.2719,\n",
            "        -0.6537,  0.9403,  0.8373, -0.0573,  0.0590, -0.0135,  0.9812,  0.9704],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 50==== Step 2 Train Loss 0.7080336809158325 ======  0.4126984126984127\n",
            "tensor([ 0.1261,  0.2272,  0.9321, -0.0187, -0.0555,  0.9582, -0.0383, -0.0979,\n",
            "         0.9363,  0.3719,  0.9027,  0.1935,  0.8414,  0.2433,  0.9953,  0.9825,\n",
            "         0.4906,  0.9778, -0.1651,  0.1620, -0.1164,  0.9377,  0.9945,  0.9907,\n",
            "         0.2785, -0.0687,  0.8010,  0.9260,  0.9143,  0.9848, -0.6131, -0.5969,\n",
            "         0.9440,  0.9298,  0.9611,  0.9673,  0.9906,  0.1887, -0.0748, -0.2203,\n",
            "         0.9009,  0.8152, -0.0122,  0.1799, -0.0482,  0.9570,  0.1526,  0.9748,\n",
            "         0.2441,  0.2862,  0.7146,  0.7436, -0.0789,  0.5298,  0.0966,  0.8573,\n",
            "         0.2214,  0.7911,  0.8199, -0.1021,  0.9868, -0.0703,  0.6862,  0.8109],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 51==== Step 2 Train Loss 0.6916801333427429 ======  0.53125\n",
            "tensor([ 0.5637,  0.9793,  0.9515, -0.2113, -0.0854, -0.2625,  0.9658,  0.8861,\n",
            "         0.6351, -0.0516, -0.0881, -0.0433,  0.0440,  0.8982,  0.9188, -0.0400,\n",
            "         0.9855,  0.9909, -0.2695, -0.0377,  0.9687,  0.9820,  0.9919,  0.3689,\n",
            "         0.4759,  0.9557,  0.8577, -0.2633,  0.8359, -0.3056, -0.0425,  0.9498,\n",
            "        -0.0379,  0.7510,  0.9505, -0.1693,  0.5993, -0.1488, -0.0464,  0.9919,\n",
            "         0.9768,  0.9647,  0.8936,  0.9809,  0.7811,  0.9861,  0.0996, -0.2069,\n",
            "        -0.0182,  0.9792,  0.9024, -0.2022,  0.6900,  0.9761, -0.3415,  0.9939,\n",
            "         0.0524,  0.9746, -0.3011,  0.7100,  0.7729, -0.0426,  0.0258,  0.7981],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 52==== Step 2 Train Loss 0.6826422810554504 ======  0.5614035087719299\n",
            "tensor([ 0.9128,  0.1295, -0.0320, -0.0909,  0.6376,  0.9591, -0.1658, -0.1136,\n",
            "        -0.3807, -0.0147, -0.0432, -0.0338, -0.0654, -0.0659, -0.0548,  0.9371,\n",
            "         0.8902, -0.2292,  0.9044,  0.8983,  0.9975,  0.8464,  0.9832,  0.9895,\n",
            "         0.7233,  0.1295,  0.5221,  0.4878, -0.0201,  0.9506,  0.3050, -0.1153,\n",
            "        -0.1446,  0.9349, -0.0886,  0.9838, -0.1272, -0.1509,  0.9652,  0.9385,\n",
            "         0.9940, -0.3579,  0.9246, -0.1751,  0.8195, -0.0987, -0.0425,  0.3137,\n",
            "         0.0721,  0.9970,  0.6456, -0.3440,  0.8053,  0.5830,  0.7478,  0.8922,\n",
            "         0.9731,  0.9874,  0.9651, -0.0966,  0.9963, -0.1682,  0.7188, -0.3547],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 53==== Step 2 Train Loss 0.7036667466163635 ======  0.39344262295081966\n",
            "tensor([ 0.1700,  0.9748,  0.5324, -0.0346, -0.2690, -0.0455,  0.9460, -0.3641,\n",
            "         0.8777,  0.8246, -0.0388,  0.9925, -0.2144, -0.4911, -0.0124, -0.1227,\n",
            "         0.8937,  0.9768,  0.9907,  0.9803, -0.0431,  0.9509, -0.3946, -0.0373,\n",
            "         0.9893, -0.0295,  0.9528,  0.9882,  0.4758, -0.0309, -0.0498,  0.8315,\n",
            "         0.8361,  0.0816,  0.1521, -0.1026,  0.4373, -0.3680,  0.0870,  0.4242,\n",
            "        -0.0662,  0.9440, -0.2275,  0.7393,  0.9085, -0.0234,  0.4202, -0.5646,\n",
            "         0.9400,  0.0174,  0.8190, -0.0655, -0.1547, -0.0557, -0.0258,  0.3814,\n",
            "        -0.0936,  0.8137,  0.9558, -0.1004,  0.4865,  0.9982, -0.0653,  0.9842],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 54==== Step 2 Train Loss 0.7096537947654724 ======  0.3703703703703704\n",
            "tensor([-0.4228, -0.0509, -0.2985, -0.0644,  0.9646,  0.8921, -0.0618,  0.3121,\n",
            "         0.9609, -0.0144,  0.9411,  0.1937,  0.4123,  0.9407,  0.9230, -0.2858,\n",
            "         0.5171,  0.0479, -0.0573,  0.0635, -0.1739, -0.3154,  0.9011,  0.9009,\n",
            "         0.5945,  0.4387,  0.9262,  0.0412,  0.5279, -0.0156,  0.9566,  0.9891,\n",
            "         0.9976,  0.9254,  0.9838, -0.0333,  0.2246,  0.3168,  0.0185,  0.9893,\n",
            "         0.0017, -0.0117,  0.8590, -0.1253,  0.9297,  0.2721,  0.3332,  0.9554,\n",
            "         0.6352, -0.0686,  0.4146,  0.7502,  0.1797,  0.9536,  0.9849,  0.9601,\n",
            "         0.2636, -0.0499, -0.3988, -0.0479,  0.7245,  0.9878,  0.9960,  0.9715],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 55==== Step 2 Train Loss 0.7020912170410156 ======  0.45901639344262296\n",
            "tensor([ 0.9759,  0.6484,  0.8719,  0.8240,  0.4640,  0.9540, -0.0542, -0.1445,\n",
            "        -0.4279,  0.9894,  0.9183, -0.0668,  0.5278,  0.9960,  0.9665,  0.8854,\n",
            "         0.9928,  0.9900, -0.0379,  0.8099,  0.9406, -0.0422,  0.9114,  0.9546,\n",
            "        -0.1451,  0.7270,  0.9500,  0.6722,  0.8978, -0.3136,  0.9927,  0.9920,\n",
            "         0.1360, -0.1000,  0.9513,  0.4835,  0.9686,  0.9076,  0.8905,  0.6749,\n",
            "         0.9715, -0.0271,  0.8727,  0.9956,  0.9806,  0.2549, -0.0432, -0.0372,\n",
            "        -0.2303, -0.0188,  0.8095, -0.2487,  0.0255, -0.0223, -0.0186,  0.8471,\n",
            "         0.9614,  0.6870, -0.2094,  0.9379,  0.5564, -0.0775,  0.2192,  0.2936],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 56==== Step 2 Train Loss 0.6944653391838074 ======  0.5079365079365079\n",
            "tensor([ 0.7649,  0.9835,  0.7796, -0.1324,  0.9716,  0.9955, -0.1058,  0.8581,\n",
            "        -0.0221,  0.0911,  0.9756,  0.7491,  0.9533, -0.1375,  0.9757, -0.1159,\n",
            "         0.9811, -0.0876,  0.9705,  0.7695,  0.9401,  0.9519, -0.1989,  0.9387,\n",
            "        -0.0308,  0.1349,  0.7276,  0.9725, -0.1858,  0.1314,  0.8732,  0.9637,\n",
            "        -0.1175, -0.3166, -0.0213,  0.9738, -0.0538,  0.8070,  0.9966, -0.2643,\n",
            "         0.9892,  0.9541,  0.6919,  0.3471, -0.0327, -0.0337, -0.0185,  0.8993,\n",
            "         0.6747,  0.6984,  0.6827,  0.9133, -0.1252,  0.9552,  0.6624,  0.9782,\n",
            "         0.8798, -0.1402,  0.9875,  0.9488,  0.2796,  0.9653,  0.9437,  0.5072],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 57==== Step 2 Train Loss 0.7071730494499207 ======  0.5\n",
            "tensor([ 0.2372, -0.0250, -0.0631,  0.8923, -0.1692,  0.8611,  0.8064,  0.5818,\n",
            "         0.9918, -0.3441,  0.5261,  0.8079,  0.3944, -0.0778,  0.8878,  0.8282,\n",
            "        -0.2465,  0.9277,  0.4939,  0.8951,  0.9709,  0.8515,  0.9092,  0.3471,\n",
            "         0.4055,  0.8231, -0.0172,  0.9380,  0.8332,  0.9318, -0.0034,  0.6954,\n",
            "        -0.6587, -0.2542,  0.5760,  0.7401, -0.0705, -0.1006, -0.0160,  0.5836,\n",
            "         0.7793,  0.9405,  0.1789, -0.1434, -0.0499,  0.6693,  0.7958,  0.9832,\n",
            "         0.5917, -0.0596, -0.1006,  0.9519, -0.1373, -0.0851,  0.8634, -0.0644,\n",
            "        -0.1395,  0.9471,  0.5444,  0.9952,  0.7058, -0.3393,  0.9915,  0.0646],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 58==== Step 2 Train Loss 0.7057980298995972 ======  0.3928571428571428\n",
            "tensor([ 0.5921, -0.1810, -0.5010,  0.9497,  0.9924,  0.7698,  0.7403,  0.7366,\n",
            "         0.9937,  0.3858, -0.0573,  0.9345,  0.9834, -0.0737,  0.9386,  0.6929,\n",
            "         0.8917, -0.3405,  0.9815,  0.0328,  0.9090, -0.4733,  0.8706,  0.7958,\n",
            "         0.9788, -0.0262, -0.1001, -0.0592, -0.0246, -0.0308, -0.0979,  0.9227,\n",
            "         0.9986, -0.0490, -0.2229,  0.7779,  0.9738,  0.4867,  0.0503, -0.0646,\n",
            "         0.6454, -0.0685,  0.7685,  0.9079,  0.9659,  0.9974,  0.9737,  0.4484,\n",
            "         0.9888,  0.1072, -0.0217,  0.9890,  0.8881,  0.9482, -0.0524,  0.9381,\n",
            "         0.9479,  0.9072, -0.0219,  0.8647,  0.8105, -0.0694, -0.0607, -0.3170],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 59==== Step 2 Train Loss 0.7082836031913757 ======  0.43749999999999994\n",
            "tensor([-3.9172e-01, -3.3021e-02, -3.7505e-02,  5.5637e-01,  3.9987e-01,\n",
            "         9.6318e-01, -6.0573e-04, -2.4146e-01,  1.5034e-01,  8.6738e-01,\n",
            "        -3.7808e-01,  9.5445e-01,  9.5758e-01, -5.9234e-02,  9.6277e-01,\n",
            "         5.5676e-01, -6.0964e-02,  6.0429e-01,  5.6758e-01,  1.4513e-02,\n",
            "        -2.7995e-02,  7.7426e-01,  6.9298e-01, -3.7596e-02,  9.8977e-01,\n",
            "         1.5961e-01, -3.7099e-02, -1.2340e-01,  9.8270e-01,  1.0933e-01,\n",
            "         9.4884e-01,  9.5885e-01,  9.7955e-01,  9.4597e-01,  1.4429e-01,\n",
            "         8.4726e-01,  1.1421e-01, -3.1128e-02,  8.8513e-01,  1.2178e-02,\n",
            "         9.6230e-01,  2.1937e-01, -7.0478e-02,  8.8553e-01,  8.6630e-01,\n",
            "         9.8577e-01, -1.4197e-01,  7.7016e-01,  9.6239e-01,  9.8368e-01,\n",
            "         9.9191e-01,  7.3893e-01, -2.5216e-02,  9.0235e-01,  9.4468e-01,\n",
            "        -3.3371e-02,  1.2479e-01,  9.9899e-01,  7.3681e-01,  9.9085e-01,\n",
            "        -2.4927e-01,  9.9727e-01,  1.5565e-01, -4.5707e-01], device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 60==== Step 2 Train Loss 0.7046037316322327 ======  0.42105263157894735\n",
            "tensor([ 0.5857,  0.7788,  0.9014,  0.9723,  0.7393, -0.2090,  0.2481,  0.0347,\n",
            "        -0.2558,  0.8818,  0.9192,  0.9884,  0.5590, -0.0765, -0.0644,  0.9163,\n",
            "         0.9980,  0.0023,  0.5795, -0.1673, -0.0522,  0.0273,  0.7415,  0.9956,\n",
            "         0.6136,  0.8087,  0.8321, -0.1559,  0.0219,  0.8850,  0.5186,  0.9760,\n",
            "         0.9004, -0.2774,  0.9759,  0.6905,  0.9947, -0.0469,  0.0499,  0.4130,\n",
            "         0.7577, -0.0832, -0.2503,  0.9714, -0.1705, -0.0437, -0.0588,  0.0758,\n",
            "         0.1475,  0.7067,  0.6038, -0.1016, -0.0253, -0.0489, -0.0196, -0.0229,\n",
            "         0.6457,  0.0532, -0.2114, -0.3036,  0.8530,  0.9040,  0.0319,  0.4274],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 61==== Step 2 Train Loss 0.6997391581535339 ======  0.4444444444444445\n",
            "tensor([-0.0421,  0.9814, -0.0166,  0.9248, -0.0811,  0.9832,  0.9744,  0.8829,\n",
            "         0.7911,  0.4502,  0.8171,  0.7209, -0.0083,  0.8967, -0.0133, -0.0086,\n",
            "         0.5679,  0.9758, -0.0328,  0.7802, -0.0453,  0.9131,  0.8458, -0.4704,\n",
            "        -0.0239,  0.8575, -0.0342,  0.9004, -0.2035,  0.7134,  0.4979, -0.0530,\n",
            "         0.9553,  0.9943, -0.0223,  0.9578,  0.3589,  0.5155,  0.7181,  0.8869,\n",
            "         0.8813,  0.9689, -0.3123, -0.1247,  0.5233,  0.7624,  0.9679, -0.2835,\n",
            "        -0.0622, -0.0277,  0.5450, -0.0357, -0.2502,  0.3901,  0.8718,  0.9636,\n",
            "        -0.1338,  0.8666,  0.7821, -0.2866,  0.8111, -0.0408,  0.8352,  0.8301],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 62==== Step 2 Train Loss 0.6801446676254272 ======  0.6111111111111112\n",
            "tensor([-0.0131,  0.9172, -0.0965, -0.0297,  0.7409, -0.0286, -0.1063,  0.0307,\n",
            "         0.9685,  0.8554,  0.9549,  0.8230, -0.2281, -0.0991,  0.9800,  0.9921,\n",
            "        -0.0251, -0.0181,  0.9826,  0.4758,  0.9051, -0.0209,  0.9900,  0.7834,\n",
            "        -0.1510,  0.9607,  0.8939,  0.8553,  0.7440,  0.1850, -0.0383,  0.9949,\n",
            "        -0.0499, -0.0698, -0.2041,  0.9581,  0.4849,  0.8903,  0.1309,  0.9792,\n",
            "         0.1131,  0.8425,  0.9721,  0.9863, -0.0018,  0.9675, -0.0415,  0.9396,\n",
            "         0.4895, -0.0328,  0.9706,  0.6742,  0.7598,  0.9105,  0.3993, -0.1479,\n",
            "        -0.0410, -0.5154,  0.9416,  0.8997, -0.0134,  0.7340, -0.0442,  0.9897],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 63==== Step 2 Train Loss 0.684425413608551 ======  0.6578947368421052\n",
            "tensor([ 0.8691,  0.7373,  0.3671, -0.4118,  0.2185, -0.1892, -0.1956,  0.9720,\n",
            "         0.1243,  0.9166,  0.8171,  0.0463,  0.9914,  0.8236,  0.8278, -0.2380,\n",
            "        -0.1472,  0.9638, -0.0449, -0.0569, -0.0567, -0.0317, -0.0171,  0.8078,\n",
            "         0.0166,  0.8149, -0.0323, -0.0469,  0.8958,  0.9687, -0.0418,  0.9935,\n",
            "        -0.5241,  0.9159,  0.9548,  0.5697, -0.0127, -0.0373,  0.9863, -0.0942,\n",
            "         0.8893, -0.0523,  0.4410,  0.2126,  0.9940,  0.3976, -0.1171,  0.6151,\n",
            "        -0.0218,  0.9763, -0.0881,  0.2675, -0.0568,  0.9899,  0.9803, -0.0154,\n",
            "        -0.2271,  0.4142, -0.0765,  0.9550,  0.9446, -0.2895,  0.9408,  0.9928],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 64==== Step 2 Train Loss 0.6961851119995117 ======  0.5\n",
            "tensor([ 0.9837,  0.9990,  0.9942,  0.8932,  0.1711,  0.9919, -0.6018,  0.9985,\n",
            "         0.9865,  0.7642, -0.0223,  0.8934, -0.1399, -0.0695,  0.3991, -0.0283,\n",
            "         0.8917,  0.9501, -0.0308,  0.9346,  0.3369,  0.9281,  0.9208,  0.6413,\n",
            "         0.9481, -0.3344, -0.0227, -0.0145, -0.0210,  0.5814,  0.2971, -0.0277,\n",
            "         0.2486,  0.9330,  0.9666,  0.8628, -0.0698, -0.0726,  0.3616,  0.8515,\n",
            "         0.0485,  0.5956,  0.1984, -0.0243,  0.9603,  0.7610,  0.9622, -0.2701,\n",
            "        -0.2671,  0.3862,  0.9496,  0.9824, -0.0386, -0.0194,  0.9944,  0.8812,\n",
            "        -0.0153,  0.9789,  0.7060,  0.9633,  0.9669, -0.0216, -0.1099, -0.2320],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 65==== Step 2 Train Loss 0.7061926126480103 ======  0.4722222222222222\n",
            "tensor([ 0.9049, -0.0022, -0.1704,  0.8299,  0.0163,  0.9962,  0.9881,  0.9170,\n",
            "         0.9846,  0.8178,  0.5953, -0.0091,  0.9836,  0.2064, -0.0147,  0.0209,\n",
            "         0.3674,  0.0378,  0.3346,  0.9987,  0.9265, -0.2345, -0.0524, -0.0459,\n",
            "         0.9536, -0.3256,  0.9841, -0.0362, -0.1592, -0.1023, -0.0433, -0.0544,\n",
            "         0.9729,  0.5693, -0.0374,  0.9986,  0.7948,  0.9802,  0.9334, -0.0139,\n",
            "         0.1054,  0.9764,  0.9451, -0.0258,  0.8969,  0.9803, -0.0357, -0.0923,\n",
            "         0.9566,  0.8343,  0.8794, -0.1555, -0.1616,  0.8025,  0.7498,  0.0027,\n",
            "         0.9955,  0.9889,  0.2243,  0.0322,  0.9249,  0.4444,  0.2308, -0.1170],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 66==== Step 2 Train Loss 0.687345564365387 ======  0.6233766233766235\n",
            "tensor([ 7.4801e-01, -4.7873e-01,  8.9677e-01, -9.0237e-02,  9.8672e-01,\n",
            "         9.8260e-01, -3.2929e-02, -5.6028e-02,  9.6710e-01,  4.6557e-01,\n",
            "         9.8866e-01,  9.6368e-01,  9.7839e-01,  3.3365e-01,  6.9387e-01,\n",
            "         8.9491e-01,  4.6107e-01,  9.2574e-01, -5.7764e-02,  8.6775e-01,\n",
            "         5.7908e-01, -2.2677e-01,  9.1959e-01,  7.9139e-01, -1.6919e-01,\n",
            "         4.3597e-01,  7.2528e-01,  9.9510e-01,  9.6558e-01,  9.1687e-01,\n",
            "         9.8668e-01, -7.1971e-02,  1.0540e-04,  7.8406e-01,  9.6312e-01,\n",
            "         9.9855e-01,  9.7532e-01, -3.0562e-02,  3.6718e-01,  7.9478e-01,\n",
            "        -3.5007e-02,  9.5864e-01,  1.1786e-01,  9.5947e-01, -4.2200e-02,\n",
            "         4.6071e-01,  3.1968e-01, -4.5932e-01,  8.7024e-01,  9.4419e-01,\n",
            "         1.7767e-02,  7.1446e-01,  8.7150e-01,  7.4575e-01,  8.9844e-01,\n",
            "         7.2599e-01,  9.9867e-01, -3.6854e-01,  7.7874e-01,  9.6586e-01,\n",
            "        -2.7066e-01, -3.2348e-02, -3.9974e-02, -9.5060e-02], device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 67==== Step 2 Train Loss 0.6921641230583191 ======  0.5507246376811593\n",
            "tensor([ 0.8021, -0.0359,  0.0650,  0.9757, -0.0209, -0.1310,  0.8522,  0.7002,\n",
            "         0.8660,  0.7160,  0.3542,  0.3889,  0.9836, -0.2375, -0.0833, -0.0203,\n",
            "        -0.0606,  0.9578,  0.9572, -0.1948, -0.0360,  0.1670,  0.3836, -0.0236,\n",
            "         0.9863,  0.1896, -0.1490,  0.9769, -0.1030,  0.9928,  0.3154, -0.1465,\n",
            "        -0.3003, -0.0101,  0.5086,  0.9983, -0.0636,  0.3558,  0.9655, -0.2343,\n",
            "         0.0247,  0.9819,  0.0521,  0.7946,  0.8619, -0.1277,  0.7765,  0.4159,\n",
            "         0.1041, -0.1626,  0.0628,  0.9763,  0.8808,  0.8870, -0.0265,  0.6725,\n",
            "         0.1373,  0.2745, -0.0619, -0.0215,  0.1714, -0.0880, -0.3027, -0.1107],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 68==== Step 2 Train Loss 0.7180744409561157 ======  0.41269841269841273\n",
            "tensor([ 0.7377,  0.7341, -0.0219, -0.1197,  0.9777,  0.5842,  0.0235,  0.9307,\n",
            "         0.6895,  0.0786, -0.0481, -0.1408,  0.9755, -0.1400,  0.9390,  0.6564,\n",
            "         0.9442,  0.5078,  0.8369,  0.4651,  0.9685,  0.7036, -0.1820,  0.9211,\n",
            "         0.9932,  0.8598,  0.9487, -0.0983,  0.8816,  0.9710, -0.0398, -0.1822,\n",
            "        -0.0240,  0.7310, -0.1565,  0.8578,  0.7818,  0.9671,  0.4327,  0.2540,\n",
            "        -0.1627,  0.9086, -0.0397, -0.0197,  0.9150,  0.9703,  0.9281,  0.8914,\n",
            "         0.7203, -0.0649,  0.7776,  0.9648, -0.2291,  0.0640, -0.1571,  0.9605,\n",
            "         0.7841, -0.0762,  0.8338,  0.9988,  0.1736,  0.2541,  0.8077,  0.9910],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 69==== Step 2 Train Loss 0.6747261881828308 ======  0.6582278481012659\n",
            "tensor([ 0.9928,  0.7190,  0.9965,  0.9775,  0.9385,  0.6502, -0.0664, -0.2116,\n",
            "         0.9774,  0.4346,  0.6717,  0.8186,  0.8342,  0.0022, -0.1224, -0.1283,\n",
            "         0.9008,  0.3023,  0.9469,  0.9281,  0.9000, -0.0158,  0.9066,  0.8565,\n",
            "        -0.3451, -0.1776,  0.7707,  0.9578,  0.5453,  0.5813,  0.9209, -0.0674,\n",
            "         0.7425,  0.0432,  0.0689,  0.1950,  0.0413, -0.4899,  0.0375,  0.9981,\n",
            "        -0.0634,  0.9440,  0.9197,  0.2735,  0.8979,  0.0292, -0.0524,  0.9608,\n",
            "         0.0934,  0.2114, -0.4601,  0.9147, -0.0389, -0.0209, -0.0331,  0.6334,\n",
            "         0.8933,  0.9951,  0.0792,  0.9174,  0.2278,  0.7972, -0.0510, -0.0362],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 70==== Step 2 Train Loss 0.7070581316947937 ======  0.5974025974025974\n",
            "tensor([ 0.5594,  0.9853, -0.0230,  0.3164,  0.9533, -0.0391,  0.9524,  0.9832,\n",
            "         0.2046, -0.1151,  0.9908, -0.0315, -0.0480, -0.0375,  0.8834,  0.0564,\n",
            "         0.8391,  0.8474,  0.8905,  0.9869, -0.2664,  0.5231,  0.8362,  0.8903,\n",
            "         0.9080,  0.8478, -0.0830,  0.8250, -0.0722, -0.1305,  0.9942,  0.9705,\n",
            "         0.9649,  0.8229, -0.0183,  0.8798, -0.2108, -0.1696,  0.9558,  0.6572,\n",
            "         0.0765, -0.4712, -0.4931, -0.2631,  0.4738,  0.9981, -0.5296, -0.0960,\n",
            "         0.2560,  0.9523,  0.9965,  0.9904,  0.9743,  0.8294,  0.6368,  0.1152,\n",
            "         0.3449, -0.0464,  0.7703,  0.9053,  0.8753,  0.4872,  0.7955,  0.9784],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 71==== Step 2 Train Loss 0.6864584684371948 ======  0.6578947368421053\n",
            "tensor([ 0.9138, -0.0269,  0.9312, -0.0177, -0.0313, -0.0477, -0.0336,  0.9799,\n",
            "         0.9908,  0.7946,  0.8836,  0.6278, -0.1143,  0.0584, -0.0899, -0.2298,\n",
            "         0.1738,  0.9486,  0.0156,  0.2932,  0.9355,  0.4203,  0.8316,  0.9816,\n",
            "         0.9909, -0.0499, -0.0116, -0.3439,  0.9859,  0.7591, -0.0095,  0.9833,\n",
            "         0.8893,  0.8794, -0.1608,  0.9096,  0.6357,  0.8663,  0.1261, -0.0834,\n",
            "        -0.0694,  0.0360,  0.9823,  0.9407, -0.0274, -0.2145,  0.0016,  0.9970,\n",
            "        -0.0232,  0.1774,  0.4251,  0.0369, -0.0312,  0.9305,  0.8695,  0.8981,\n",
            "         0.9985, -0.0293,  0.1987,  0.9225, -0.1717, -0.0037,  0.8261,  0.9607],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 72==== Step 2 Train Loss 0.707168459892273 ======  0.417910447761194\n",
            "tensor([-0.2227,  0.9235,  0.9506,  0.9591,  0.9526,  0.8703,  0.9664,  0.9375,\n",
            "         0.4358,  0.1436,  0.2400, -0.0354, -0.0228,  0.5842,  0.9910,  0.8505,\n",
            "         0.1780, -0.1914,  0.9626,  0.6097,  0.4206, -0.1074,  0.9984,  0.8210,\n",
            "        -0.0117,  0.5733,  0.8081, -0.0112,  0.9847, -0.2651, -0.0840,  0.4418,\n",
            "         0.3917,  0.9423,  0.1348,  0.7645, -0.1976, -0.4538, -0.0390,  0.9882,\n",
            "        -0.0605,  0.8794,  0.9990, -0.2555,  0.8857, -0.0419,  0.9795,  0.8747,\n",
            "         0.3183, -0.1475,  0.6621, -0.1260, -0.0254,  0.0943,  0.8240, -0.0430,\n",
            "         0.0100,  0.3600,  0.9928,  0.7818, -0.0385, -0.0175,  0.9278, -0.0949],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 73==== Step 2 Train Loss 0.6904845833778381 ======  0.5079365079365079\n",
            "tensor([-0.0355,  0.8748,  0.9885, -0.0924,  0.9106, -0.1604, -0.0182, -0.2231,\n",
            "         0.7954,  0.8945, -0.2524,  0.9306, -0.2183,  0.9892,  0.5122,  0.8765,\n",
            "        -0.0715,  0.0621,  0.7628,  0.9126,  0.4952, -0.0313,  0.8949,  0.1613,\n",
            "        -0.2855, -0.6316,  0.9772, -0.0287,  0.9699,  0.9935, -0.2290,  0.7387,\n",
            "         0.8656,  0.9323, -0.1858,  0.9937, -0.3148, -0.0825,  0.9807,  0.9941,\n",
            "         0.8789, -0.1700,  0.4439,  0.9948, -0.3196, -0.0324,  0.9577, -0.0269,\n",
            "         0.9618,  0.9936,  0.8964,  0.9616,  0.9695,  0.9077,  0.3581, -0.0622,\n",
            "        -0.0309,  0.8219,  0.7077,  0.9825, -0.0240, -0.0258,  0.8377,  0.9912],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 74==== Step 2 Train Loss 0.6984876990318298 ======  0.4444444444444445\n",
            "tensor([ 0.1453,  0.8645,  0.9832, -0.0332,  0.0105,  0.9867,  0.9924, -0.1242,\n",
            "         0.9742,  0.7462,  0.9849,  0.8925,  0.7919, -0.0176,  0.9913,  0.9502,\n",
            "        -0.0677,  0.3675, -0.2197, -0.2144,  0.9554,  0.0581,  0.9810, -0.0921,\n",
            "         0.5517,  0.4123,  0.2491,  0.9912, -0.0505,  0.3550, -0.1560,  0.9937,\n",
            "        -0.1131,  0.8955,  0.0860,  0.9119,  0.8316, -0.0124,  0.8544,  0.4394,\n",
            "         0.8982,  0.8815,  0.9158,  0.9243,  0.9400,  0.9060, -0.1901,  0.7902,\n",
            "         0.9801,  0.9831,  0.0564, -0.1374,  0.5124,  0.9713,  0.8118, -0.0329,\n",
            "         0.9895,  0.9526,  0.9788,  0.8635,  0.9329, -0.0410,  0.9865, -0.5343],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 75==== Step 2 Train Loss 0.6876622438430786 ======  0.6329113924050633\n",
            "tensor([ 0.4750,  0.9890, -0.2600,  0.9896, -0.0576, -0.2097,  0.7348, -0.2372,\n",
            "         0.9621, -0.2415,  0.8706,  0.3978,  0.8403,  0.0317,  0.3185,  0.1341,\n",
            "        -0.2220, -0.2659, -0.1219,  0.7780,  0.9443,  0.9743,  0.9437,  0.9496,\n",
            "        -0.0188,  0.0316,  0.6154,  0.7646,  0.7851, -0.0231,  0.4182,  0.2261,\n",
            "        -0.0372,  0.9461, -0.0368,  0.7181,  0.9686,  0.8021,  0.9435,  0.8260,\n",
            "        -0.0308, -0.1907,  0.9355,  0.9690,  0.9714,  0.4990,  0.2888,  0.9265,\n",
            "        -0.0534,  0.8890,  0.0194,  0.9445, -0.0471,  0.8030,  0.0188,  0.4703,\n",
            "         0.7018, -0.0335, -0.4629, -0.0499, -0.1242, -0.0602,  0.9814,  0.9799],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 76==== Step 2 Train Loss 0.7049575448036194 ======  0.49230769230769234\n",
            "tensor([ 0.9599,  0.7834,  0.3626,  0.9527,  0.0452,  0.0055,  0.9479,  0.6881,\n",
            "         0.9088,  0.9826,  0.7522,  0.8977,  0.4804, -0.5146,  0.7762, -0.0056,\n",
            "         0.8319,  0.9967,  0.0694,  0.8968,  0.9924, -0.0704, -0.0176, -0.0241,\n",
            "        -0.3473, -0.3244,  0.9964,  0.9750, -0.4454,  0.8928,  0.9470,  0.2907,\n",
            "         0.7743,  0.9918,  0.7970, -0.0852,  0.9995,  0.9903,  0.9398,  0.8517,\n",
            "        -0.1205,  0.0522,  0.0955,  0.2073,  0.7645,  0.8778, -0.0477, -0.0998,\n",
            "         0.6115,  0.9914,  0.9653, -0.3652,  0.9843,  0.9727, -0.0829,  0.7564,\n",
            "         0.0426, -0.2372, -0.0350, -0.0325,  0.8412,  0.9878,  0.5008,  0.3141],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 77==== Step 2 Train Loss 0.6688442230224609 ======  0.6933333333333334\n",
            "tensor([-0.0352, -0.0706,  0.9076,  0.9666, -0.0015,  0.6618,  0.3766,  0.9443,\n",
            "        -0.0235, -0.0333, -0.0246, -0.0163,  0.9935,  0.8472, -0.1123,  0.9917,\n",
            "         0.7532,  0.0031,  0.3455,  0.1434, -0.0272, -0.0522,  0.9112,  0.9720,\n",
            "         0.9957, -0.1996,  0.9756, -0.0532,  0.4318, -0.0324,  0.6436, -0.3886,\n",
            "        -0.1357,  0.9409,  0.9848, -0.0991,  0.8924,  0.9607, -0.1363,  0.0064,\n",
            "        -0.2702,  0.7609,  0.8064, -0.0732,  0.5882, -0.0757,  0.9349,  0.9820,\n",
            "         0.9752,  0.6970,  0.9971,  0.9501,  0.4480,  0.8833, -0.5725,  0.2694,\n",
            "        -0.0969,  0.9735, -0.6096, -0.0415,  0.2180, -0.0816, -0.2227, -0.2369],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 78==== Step 2 Train Loss 0.7200738787651062 ======  0.4117647058823529\n",
            "tensor([ 0.9743, -0.2414,  0.9914, -0.2543,  0.9077,  0.5407,  0.6476,  0.0295,\n",
            "         0.9969,  0.6540, -0.0497,  0.9836, -0.0684,  0.9951, -0.0954,  0.6955,\n",
            "         0.7273,  0.9787,  0.9152, -0.0500, -0.0253, -0.0695,  0.9861,  0.9214,\n",
            "         0.9275,  0.9957, -0.0515, -0.4375,  0.1351,  0.7958, -0.2767,  0.3085,\n",
            "        -0.1402, -0.0706,  0.7046,  0.8563,  0.4076,  0.4478,  0.9869,  0.0485,\n",
            "        -0.1862, -0.2349,  0.8860,  0.9876, -0.0165, -0.0327, -0.0746, -0.0145,\n",
            "         0.3262, -0.2609,  0.9921, -0.0232, -0.0372, -0.2288,  0.9432, -0.3052,\n",
            "         0.2298, -0.0049,  0.5918,  0.5444,  0.8373,  0.9421,  0.9826,  0.8961],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 79==== Step 2 Train Loss 0.6905803680419922 ======  0.5588235294117647\n",
            "tensor([ 0.9851,  0.6046, -0.0724,  0.9946, -0.0956, -0.0110, -0.0208,  0.8254,\n",
            "         0.5750,  0.9406,  0.8855,  0.9980, -0.2453,  0.9652, -0.1944,  0.7056,\n",
            "         0.7771,  0.9626,  0.1972, -0.0693, -0.2854,  0.9982,  0.9892,  0.9243,\n",
            "         0.9377,  0.8444, -0.0213,  0.7161,  0.9578, -0.0213,  0.9835,  0.9423,\n",
            "        -0.0732,  0.9358,  0.9934,  0.8528, -0.2763,  0.8964,  0.9552,  0.8312,\n",
            "         0.4542,  0.7859,  0.9790,  0.9866,  0.9875, -0.2824, -0.0332,  0.8892,\n",
            "         0.9937,  0.3295,  0.8886,  0.9985,  0.0219,  0.9354,  0.1396, -0.3372,\n",
            "        -0.1310, -0.5923,  0.9959, -0.2531,  0.9908,  0.9796, -0.1801,  0.9805],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 80==== Step 2 Train Loss 0.7143195271492004 ======  0.5405405405405406\n",
            "tensor([ 0.8825, -0.0288,  0.8926,  0.9457,  0.1548, -0.1017, -0.5999,  0.2565,\n",
            "         0.9052, -0.0360,  0.6423, -0.2446,  0.9946,  0.2547, -0.0653,  0.9649,\n",
            "        -0.0906,  0.6285, -0.1665, -0.0672, -0.0658,  0.9298, -0.2016, -0.1346,\n",
            "         0.8297, -0.0898,  0.9535, -0.0520,  0.3092,  0.9152,  0.8190,  0.7429,\n",
            "         0.3243, -0.0175, -0.3358,  0.9817,  0.8595, -0.4585,  0.8009,  0.9723,\n",
            "        -0.0234,  0.4342,  0.6807,  0.8476,  0.9471, -0.0829,  0.7643, -0.0242,\n",
            "         0.9451,  0.7229,  0.8286,  0.1808,  0.9903,  0.9901,  0.9748, -0.0328,\n",
            "        -0.1193,  0.9265,  0.0646, -0.0124,  0.4129, -0.0435,  0.5338, -0.2931],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 81==== Step 2 Train Loss 0.7245762348175049 ======  0.375\n",
            "tensor([-0.0297, -0.0281, -0.1385,  0.9871, -0.0596,  0.9454,  0.9585,  0.5248,\n",
            "        -0.0486, -0.0336,  0.9586, -0.1314, -0.4722,  0.9673,  0.8440, -0.1475,\n",
            "        -0.0506, -0.3629,  0.4356, -0.0692,  0.1692,  0.7949,  0.3431,  0.7882,\n",
            "        -0.0597,  0.8796,  0.4352,  0.9853,  0.9259,  0.3164,  0.8271, -0.1513,\n",
            "         0.9844, -0.0369,  0.9559,  0.9222,  0.9852,  0.6705,  0.8942,  0.9972,\n",
            "         0.7867,  0.8863, -0.0392, -0.2446,  0.8709,  0.6224, -0.1391,  0.8590,\n",
            "        -0.0280,  0.9754,  0.1063,  0.7079, -0.2214,  0.6138,  0.9556,  0.0621,\n",
            "         0.5374,  0.9198,  0.9020,  0.2377, -0.0345,  0.8476, -0.0623,  0.1749],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 82==== Step 2 Train Loss 0.7015424966812134 ======  0.4615384615384615\n",
            "tensor([ 0.9711,  0.9953,  0.9023, -0.0482, -0.3763,  0.9577, -0.0293,  0.9351,\n",
            "         0.8942, -0.3374,  0.9964, -0.1897,  0.8585, -0.0313, -0.0162,  0.6456,\n",
            "         0.1918,  0.5344,  0.9555,  0.9476, -0.1506,  0.2128,  0.5893,  0.9332,\n",
            "         0.4310,  0.7831,  0.9838,  0.9820,  0.9280,  0.1002, -0.0529,  0.5341,\n",
            "         0.3457,  0.9831,  0.9439, -0.0399,  0.9980, -0.1753, -0.0025,  0.8592,\n",
            "         0.9117,  0.9115,  0.9846,  0.9021,  0.7230,  0.9345, -0.0250,  0.8937,\n",
            "         0.2358, -0.1561,  0.9719,  0.9394, -0.1651,  0.9566, -0.1038, -0.1073,\n",
            "         0.9430, -0.4469,  0.9319,  0.8708, -0.1520,  0.2803,  0.8516, -0.1212],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 83==== Step 2 Train Loss 0.6865835189819336 ======  0.5945945945945946\n",
            "tensor([-0.0363,  0.0259,  0.0784,  0.8225,  0.7684, -0.1636,  0.9040, -0.2891,\n",
            "         0.9981,  0.7261,  0.8814,  0.1226,  0.5699, -0.0337,  0.9375, -0.3370,\n",
            "         0.9403, -0.1430,  0.9769,  0.9016,  0.9920,  0.9690,  0.1776, -0.2041,\n",
            "         0.9679, -0.5130, -0.1530,  0.9055, -0.1740,  0.3472,  0.9373,  0.2906,\n",
            "        -0.2340,  0.6946,  0.9063,  0.6940,  0.6267,  0.4324, -0.2314,  0.5533,\n",
            "         0.9216, -0.0740, -0.2914,  0.8869,  0.9940, -0.0183, -0.0290,  0.8559,\n",
            "         0.0559,  0.9369, -0.2184,  0.4267,  0.0945,  0.3079, -0.0349,  0.9319,\n",
            "        -0.0368,  0.4343,  0.6745,  0.4331,  0.4064,  0.0460,  0.9089, -0.0166],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 84==== Step 2 Train Loss 0.6931101679801941 ======  0.6111111111111112\n",
            "tensor([ 0.5394,  0.9148,  0.8116,  0.8899,  0.9651,  0.3071,  0.5582,  0.9990,\n",
            "        -0.0242, -0.0110,  0.9963,  0.0161, -0.2624, -0.2110,  0.3194, -0.0209,\n",
            "         0.8837,  0.1310,  0.9219,  0.8388,  0.9158,  0.9807,  0.9176,  0.2672,\n",
            "         0.9929,  0.7946,  0.7201,  0.9923,  0.9935,  0.0287,  0.4287,  0.9038,\n",
            "         0.9218, -0.0264, -0.1188,  0.9901, -0.0366,  0.3582,  0.8503,  0.8539,\n",
            "         0.9594,  0.9377,  0.7782,  0.8476,  0.9276,  0.8762, -0.0270,  0.9704,\n",
            "         0.9939, -0.0304, -0.0419,  0.9321, -0.0344, -0.3275,  0.1445,  0.7843,\n",
            "         0.6538,  0.7952, -0.0807, -0.1431,  0.8771, -0.3386,  0.9209, -0.0177],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 85==== Step 2 Train Loss 0.6854380369186401 ======  0.5671641791044776\n",
            "tensor([ 0.9676,  0.9770,  0.8559, -0.2942,  0.8689,  0.9033,  0.8480, -0.2635,\n",
            "        -0.2403,  0.9856,  0.7482,  0.9263,  0.9866,  0.9945,  0.9875,  0.9928,\n",
            "         0.9921,  0.9665,  0.6677, -0.0150, -0.1845,  0.8921,  0.8001,  0.7765,\n",
            "         0.9776, -0.0379, -0.2545,  0.8943, -0.0867,  0.6167, -0.0774,  0.4321,\n",
            "         0.2568,  0.3078,  0.6946,  0.9961,  0.5277, -0.0359, -0.0346,  0.9752,\n",
            "        -0.0311,  0.9251,  0.9166, -0.0125, -0.0090,  0.9797, -0.0132, -0.1771,\n",
            "        -0.0348, -0.0497,  0.7137, -0.0292, -0.0501,  0.8799,  0.9806,  0.9052,\n",
            "         0.9367, -0.0664, -0.0739, -0.1055, -0.0599,  0.9658,  0.9264,  0.9732],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 86==== Step 2 Train Loss 0.6884286403656006 ======  0.6153846153846154\n",
            "tensor([ 0.9952,  0.6454,  0.1782,  0.9493,  0.7173,  0.9543,  0.9533,  0.1506,\n",
            "        -0.1608,  0.1114,  0.8652, -0.0937, -0.0172, -0.0057,  0.9659, -0.2861,\n",
            "        -0.0356, -0.0795, -0.0138,  0.9799, -0.2877,  0.8841, -0.2483,  0.8949,\n",
            "         0.7795,  0.9195, -0.3440,  0.9342, -0.0412,  0.8897,  0.8903,  0.9322,\n",
            "         0.8299, -0.1437, -0.3534,  0.9220,  0.8543, -0.0349, -0.0350, -0.1526,\n",
            "        -0.0451,  0.8224,  0.3462,  0.8828,  0.9598,  0.9953,  0.9776,  0.9366,\n",
            "         0.3707,  0.4861, -0.0727, -0.2202,  0.9967, -0.4622,  0.6546, -0.0561,\n",
            "         0.8494, -0.0809,  0.9897,  0.0891, -0.3206,  0.6319,  0.8319, -0.4935],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 87==== Step 2 Train Loss 0.6927780508995056 ======  0.47887323943661975\n",
            "tensor([ 0.5343,  0.1942, -0.0279,  0.9835,  0.2576, -0.3513, -0.5234,  0.9048,\n",
            "        -0.3573,  0.9640,  0.1672,  0.9833,  0.9122,  0.7972, -0.0246, -0.0287,\n",
            "         0.9318,  0.9055,  0.9531,  0.9884, -0.1368,  0.4617,  0.9844,  0.8695,\n",
            "         0.2446, -0.0234,  0.9636,  0.5465,  0.5788,  0.3694, -0.6256, -0.0220,\n",
            "         0.9596,  0.9475,  0.9730, -0.1851,  0.9680, -0.0393, -0.0224,  0.3607,\n",
            "         0.9914,  0.8508,  0.9651,  0.6912,  0.5632,  0.7430,  0.8635, -0.4274,\n",
            "        -0.0264,  0.9931, -0.1381,  0.9718,  0.9456,  0.0412,  0.2550, -0.0021,\n",
            "         0.9715,  0.8936,  0.9768,  0.9627, -0.1076,  0.8382, -0.0603, -0.1338],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 88==== Step 2 Train Loss 0.6798036098480225 ======  0.5714285714285715\n",
            "tensor([ 0.8834,  0.8916,  0.9852,  0.9256,  0.0268,  0.5316,  0.9147,  0.9901,\n",
            "         0.6785, -0.0666,  0.9164,  0.9748, -0.0899,  0.9332,  0.8991, -0.1914,\n",
            "        -0.1546, -0.0742, -0.1154, -0.0654,  0.6504,  0.9629,  0.9190,  0.9787,\n",
            "        -0.3554, -0.1370, -0.0316, -0.5480,  0.7454,  0.9147,  0.2196,  0.9069,\n",
            "        -0.0397, -0.0491,  0.6636,  0.6138,  0.9860, -0.0203, -0.0363,  0.8489,\n",
            "         0.1095,  0.9855, -0.0094,  0.8176, -0.1701,  0.9899, -0.0210,  0.8225,\n",
            "         0.9726,  0.6703, -0.0035,  0.9794,  0.9990, -0.0234,  0.7508, -0.0277,\n",
            "         0.8917, -0.4063,  0.7762,  0.9341,  0.9439, -0.1556,  0.0590,  0.6072],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 89==== Step 2 Train Loss 0.7059131264686584 ======  0.39393939393939387\n",
            "tensor([ 7.8309e-01,  7.3424e-01,  4.5281e-01,  9.7617e-01, -1.7541e-01,\n",
            "         7.6908e-01, -2.7778e-02,  9.9361e-01, -2.5793e-02,  9.9723e-01,\n",
            "         9.9767e-02,  4.6182e-04, -1.8179e-01, -4.4099e-02,  9.6253e-01,\n",
            "        -2.5947e-01,  8.8323e-01,  3.0474e-02,  5.0212e-01,  9.8265e-01,\n",
            "         7.4465e-01,  9.3963e-01,  9.6892e-01,  9.9597e-01,  9.8539e-01,\n",
            "         8.7770e-01,  8.2728e-01,  9.2819e-01, -2.6911e-02, -2.5357e-01,\n",
            "         9.5791e-01,  9.4822e-01,  9.3321e-01,  9.7634e-02,  4.5033e-01,\n",
            "         7.8218e-01,  3.3588e-01, -3.6058e-01, -1.0789e-01, -1.6308e-02,\n",
            "         5.9007e-01,  9.7224e-01,  8.5648e-01,  9.1633e-01, -2.4194e-02,\n",
            "        -2.5823e-02,  6.0165e-03, -1.1828e-01,  4.7434e-01, -4.4130e-02,\n",
            "        -2.7842e-01, -2.2388e-01, -3.0953e-01, -2.5668e-01,  8.7639e-01,\n",
            "        -4.4041e-01,  8.9344e-01, -8.4202e-02, -4.0256e-01,  6.3904e-01,\n",
            "        -2.3322e-02,  9.6681e-01,  8.6841e-01,  3.6566e-01], device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 90==== Step 2 Train Loss 0.7313252687454224 ======  0.4931506849315069\n",
            "tensor([-0.0648,  0.3660, -0.0160,  0.9954,  0.0159, -0.0058,  0.5972,  0.9841,\n",
            "         0.8652,  0.0142, -0.1329, -0.0224,  0.9887, -0.0147,  0.7616,  0.9792,\n",
            "        -0.3178,  0.9650,  0.1324,  0.8861,  0.0733,  0.9721, -0.0193,  0.9664,\n",
            "         0.8624, -0.1089,  0.9253,  0.0495,  0.6559,  0.8989,  0.9528, -0.1635,\n",
            "        -0.3137,  0.9956, -0.0209, -0.0369,  0.8250,  0.0999,  0.7017,  0.0312,\n",
            "         0.9796,  0.9956,  0.8669, -0.0909, -0.1840,  0.9563,  0.8899, -0.0427,\n",
            "        -0.4166, -0.0214,  0.6263, -0.0254, -0.0825,  0.5955,  0.7765,  0.9545,\n",
            "         0.8882,  0.1659,  0.0747,  0.9867, -0.0458,  0.9982, -0.1488, -0.0206],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 91==== Step 2 Train Loss 0.6904340982437134 ======  0.5915492957746479\n",
            "tensor([ 0.9828,  0.9141, -0.0250,  0.8756,  0.8034,  0.2800, -0.3398,  0.8715,\n",
            "        -0.0708,  0.9345, -0.1568,  0.1188,  0.1794,  0.9950, -0.0871, -0.0397,\n",
            "         0.7622,  0.9413,  0.9806,  0.9807,  0.7166,  0.2283,  0.8224, -0.0860,\n",
            "         0.9480, -0.1904,  0.9556, -0.3675, -0.0168, -0.0034,  0.4056, -0.1483,\n",
            "         0.9452,  0.9617,  0.6398, -0.1028,  0.9170, -0.0273,  0.9976,  0.8361,\n",
            "        -0.0326,  0.8967, -0.3784,  0.8856, -0.0158,  0.0637,  0.9348,  0.9858,\n",
            "        -0.2342,  0.7134,  0.8829,  0.9793, -0.0526, -0.0357,  0.9871,  0.9254,\n",
            "         0.8337, -0.3164,  0.9797, -0.0213,  0.9572,  0.8757,  0.1332, -0.0148],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 92==== Step 2 Train Loss 0.6918239593505859 ======  0.6216216216216216\n",
            "tensor([ 0.6429,  0.9950, -0.0512, -0.0793,  0.9394,  0.8172,  0.9905,  0.9907,\n",
            "        -0.1088,  0.8918, -0.1918, -0.0461, -0.0450, -0.2152,  0.9552,  0.9905,\n",
            "         0.9790,  0.4540,  0.6073,  0.2172,  0.7999,  0.9747,  0.9859, -0.0166,\n",
            "        -0.4121,  0.9699, -0.2268,  0.9795,  0.6160,  0.3543,  0.9178, -0.0229,\n",
            "         0.9139, -0.0112,  0.9669,  0.2535,  0.8195, -0.1087, -0.0423, -0.0172,\n",
            "        -0.0604,  0.2523,  0.9028,  0.9471,  0.8875, -0.1190,  0.8321, -0.1073,\n",
            "         0.1520,  0.8691,  0.9919, -0.0380,  0.2621,  0.9087,  0.9632,  0.0657,\n",
            "         0.2206,  0.9613,  0.9449,  0.9946, -0.2545, -0.2695,  0.9320,  0.9085],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 93==== Step 2 Train Loss 0.7226911783218384 ======  0.40579710144927533\n",
            "tensor([-0.2073, -0.0466,  0.9857, -0.1352,  0.5029,  0.9971, -0.0718, -0.1626,\n",
            "         0.9772,  0.1008,  0.6665, -0.0767,  0.8335,  0.5983, -0.0748, -0.0458,\n",
            "        -0.0129, -0.0120,  0.0098,  0.9198, -0.0106, -0.0825,  0.9839, -0.0698,\n",
            "         0.5391,  0.9776,  0.2158, -0.0342,  0.0842,  0.9613, -0.0461,  0.9886,\n",
            "         0.9923,  0.9351, -0.0391, -0.1152, -0.1690, -0.1415,  0.9482, -0.0190,\n",
            "         0.8214,  0.9912,  0.8851,  0.2178, -0.0621, -0.0481,  0.7185,  0.2676,\n",
            "        -0.0387, -0.0117, -0.1070,  0.5358,  0.9858, -0.0803,  0.9550,  0.8307,\n",
            "         0.1749,  0.4334, -0.1166, -0.0298,  0.7318, -0.0246,  0.0346, -0.0437],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 94==== Step 2 Train Loss 0.7010603547096252 ======  0.5074626865671641\n",
            "tensor([ 0.9379, -0.0189,  0.8672,  0.7536, -0.0479,  0.1729,  0.1972,  0.6691,\n",
            "         0.9407,  0.9586,  0.3317,  0.9174,  0.1945,  0.9983, -0.0250,  0.9016,\n",
            "         0.0319,  0.9011,  0.2479,  0.9674,  0.6967,  0.5551,  0.0042,  0.0024,\n",
            "        -0.0875,  0.9575,  0.9921,  0.9939,  0.0801, -0.3851, -0.1694, -0.1358,\n",
            "        -0.0485, -0.1584, -0.2050, -0.0137, -0.0741,  0.9145, -0.0932,  0.8476,\n",
            "         0.9028,  0.6776,  0.8338, -0.0270, -0.0568,  0.9796,  0.6748, -0.0289,\n",
            "         0.8893, -0.2390,  0.7534,  0.7027,  0.8882,  0.8772, -0.0189,  0.9401,\n",
            "         0.8212,  0.9278,  0.1615,  0.8113,  0.9025, -0.0186,  0.8069,  0.9676],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 95==== Step 2 Train Loss 0.6899383664131165 ======  0.626865671641791\n",
            "tensor([ 0.8771, -0.0139,  0.1472,  0.7608,  0.8161,  0.9642, -0.1905,  0.7726,\n",
            "        -0.0314,  0.9854,  0.9732, -0.1485,  0.9957,  0.1503,  0.9712, -0.2770,\n",
            "         0.9888, -0.0237,  0.7898, -0.0840,  0.9758,  0.1065,  0.9156, -0.1251,\n",
            "         0.9831,  0.8640,  0.8154,  0.9889,  0.9779, -0.2736, -0.0657,  0.9032,\n",
            "        -0.0181, -0.0212,  0.8776,  0.0876, -0.0685,  0.1982,  0.9584,  0.1609,\n",
            "         0.9954,  0.9473,  0.1223, -0.0231,  0.7494, -0.1943,  0.3324,  0.7418,\n",
            "         0.7910,  0.9955,  0.8798, -0.1472,  0.2684,  0.9826,  0.9739,  0.0067,\n",
            "         0.8640,  0.7307,  0.3690, -0.0685,  0.9735,  0.9364,  0.8583,  0.8720],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 96==== Step 2 Train Loss 0.6924755573272705 ======  0.4848484848484848\n",
            "tensor([-0.1027, -0.0276, -0.2313,  0.9622,  0.7924, -0.0543,  0.4242, -0.1303,\n",
            "         0.9397,  0.8479,  0.8555,  0.9857, -0.0664, -0.1047,  0.9932, -0.1611,\n",
            "         0.9940,  0.9341,  0.8457,  0.9811,  0.9443,  0.9863,  0.9571, -0.0290,\n",
            "         0.9160,  0.9232,  0.9778, -0.0063,  0.8987, -0.1083,  0.7665, -0.1000,\n",
            "        -0.0517,  0.7127,  0.9944,  0.8662, -0.2810,  0.9123,  0.2963,  0.0058,\n",
            "         0.9443, -0.0260, -0.2526,  0.7638,  0.7360,  0.9902,  0.5898, -0.1288,\n",
            "        -0.1053, -0.0177, -0.0958,  0.9194,  0.8986, -0.2441,  0.8273,  0.7504,\n",
            "        -0.2587,  0.9339, -0.2848, -0.1382,  0.9921, -0.0424,  0.7544,  0.9847],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 97==== Step 2 Train Loss 0.6761484742164612 ======  0.6315789473684211\n",
            "tensor([ 0.9881,  0.2638,  0.9761,  0.9988,  0.2668,  0.9950, -0.4089, -0.2339,\n",
            "         0.8422,  0.9849,  0.9828,  0.9955, -0.0274,  0.8244,  0.8004,  0.9513,\n",
            "         0.9667, -0.2895,  0.9958, -0.0135, -0.4407,  0.9851,  0.2760,  0.2797,\n",
            "         0.4057, -0.2888,  0.9778,  0.3788, -0.0819,  0.1469, -0.1502, -0.1145,\n",
            "         0.6171,  0.9100, -0.3205, -0.0189, -0.0589,  0.5183,  0.9928,  0.8177,\n",
            "         0.7115,  0.9558,  0.9790,  0.9846,  0.9812,  0.7859,  0.7179, -0.0354,\n",
            "         0.1097,  0.1358,  0.5582,  0.9602,  0.9823, -0.1037,  0.9604, -0.4884,\n",
            "        -0.0595, -0.3866, -0.0467, -0.0507,  0.9864,  0.2054,  0.9090,  0.6255],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 98==== Step 2 Train Loss 0.6975361704826355 ======  0.5507246376811595\n",
            "tensor([ 0.5807, -0.6391,  0.1662, -0.0789,  0.7278,  0.9300, -0.0635,  0.8104,\n",
            "         0.4447, -0.0353, -0.0312, -0.0061,  0.8778, -0.1735,  0.1169,  0.8081,\n",
            "        -0.0710,  0.9006,  0.4068,  0.9191, -0.0134,  0.9572, -0.0368,  0.7208,\n",
            "         0.9574,  0.4145, -0.0813,  0.9852, -0.0268,  0.6658, -0.0201,  0.9946,\n",
            "         0.9785,  0.9618,  0.9822,  0.9901, -0.0303,  0.7583, -0.0317, -0.3954,\n",
            "        -0.0137,  0.6840,  0.7398, -0.0761, -0.0082,  0.0490, -0.2368,  0.9762,\n",
            "        -0.1622,  0.7144,  0.3984, -0.0265,  0.5051,  0.9514,  0.1760,  0.9039,\n",
            "         0.8698,  0.9396,  0.6383,  0.4456, -0.3003,  0.7994,  0.7470, -0.0284],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 99==== Step 2 Train Loss 0.6926558613777161 ======  0.6086956521739131\n",
            "  Batch   100  of  8,405.    Elapsed: 0:01:50.\n",
            "tensor([ 0.9562,  0.9047,  0.9605,  0.9722,  0.9532, -0.0039,  0.1826,  0.1241,\n",
            "         0.9940,  0.9968, -0.1156, -0.0434, -0.0569,  0.2009,  0.9501, -0.0157,\n",
            "         0.8875, -0.0542, -0.1938,  0.3871,  0.3129,  0.8963, -0.0332,  0.6826,\n",
            "         0.9361,  0.7254,  0.9726,  0.9428,  0.8369, -0.1119,  0.9898,  0.0105,\n",
            "         0.0116,  0.3312, -0.0480,  0.9817, -0.1528, -0.2892,  0.9109,  0.9977,\n",
            "         0.8972,  0.8899, -0.3834,  0.3672, -0.0143, -0.0449,  0.9594,  0.9261,\n",
            "         0.3380,  0.2201,  0.3045, -0.0536,  0.9518, -0.0148,  0.6561,  0.9973,\n",
            "        -0.0275,  0.4114, -0.0206,  0.9978,  0.9661,  0.9191,  0.9925, -0.1532],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 100==== Step 2 Train Loss 0.6982421875 ======  0.5675675675675677\n",
            "tensor([-0.0549, -0.1354,  0.9779,  0.2369,  0.9629, -0.0393,  0.5979,  0.9643,\n",
            "         0.5796,  0.7461,  0.9919,  0.9268, -0.0413,  0.9661,  0.8331,  0.7898,\n",
            "         0.9008, -0.0304,  0.2107,  0.2629, -0.0395,  0.9229, -0.1146,  0.6985,\n",
            "         0.9993,  0.9796, -0.0238, -0.0334,  0.8282,  0.0965,  0.3756,  0.1825,\n",
            "        -0.2863, -0.0915,  0.1037,  0.8940,  0.4346,  0.9229,  0.8274,  0.0985,\n",
            "         0.9945,  0.9247,  0.1519,  0.9131, -0.0323, -0.0816, -0.0748,  0.9533,\n",
            "         0.9804, -0.2750, -0.0256, -0.1252,  0.6806,  0.0388, -0.0457,  0.3861,\n",
            "        -0.2994,  0.9728,  0.8539,  0.9924,  0.3576, -0.1913,  0.1634,  0.8523],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 101==== Step 2 Train Loss 0.6922087669372559 ======  0.5538461538461539\n",
            "tensor([ 0.1346,  0.7801,  0.1050,  0.9283, -0.0327, -0.0231, -0.1669,  0.7997,\n",
            "        -0.0487,  0.0867,  0.3631, -0.0468, -0.1821,  0.9208, -0.2062,  0.9185,\n",
            "        -0.1092,  0.9929,  0.2267,  0.9383,  0.8921,  0.9632, -0.0688,  0.9628,\n",
            "         0.9300, -0.1421, -0.0218,  0.9929,  0.9672,  0.9920, -0.2858,  0.9909,\n",
            "         0.9976,  0.0425,  0.9918, -0.0154,  0.0045,  0.9464,  0.3684,  0.9130,\n",
            "         0.8459,  0.6840, -0.0132, -0.1450, -0.0196, -0.1793,  0.1209, -0.0281,\n",
            "         0.7755,  0.0690, -0.0709,  0.9697, -0.0274,  0.8979, -0.1156,  0.9193,\n",
            "         0.8174,  0.9070, -0.1498,  0.8728,  0.9488, -0.0218,  0.8365, -0.0673],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 102==== Step 2 Train Loss 0.6734999418258667 ======  0.59375\n",
            "tensor([ 0.4446, -0.0058, -0.0162, -0.0207, -0.0733,  0.9763,  0.4814,  0.9895,\n",
            "        -0.3341, -0.0651,  0.9927, -0.0135,  0.8634,  0.1010, -0.1291,  0.1599,\n",
            "         0.9238,  0.9902, -0.0217, -0.0242,  0.9755,  0.9984,  0.9009, -0.0517,\n",
            "        -0.0316,  0.8823,  0.1574, -0.0136,  0.9592,  0.3482,  0.2090,  0.5534,\n",
            "         0.8808,  0.9587,  0.7733, -0.0238,  0.7797,  0.9576, -0.1777,  0.8176,\n",
            "         0.7770,  0.9900, -0.0506,  0.7549,  0.8682, -0.4214,  0.9498,  0.0189,\n",
            "         0.3116,  0.9891,  0.9198, -0.0243, -0.2131,  0.2133, -0.1821,  0.4321,\n",
            "        -0.1579, -0.0218,  0.0063,  0.9188,  0.9884,  0.0268,  0.9512,  0.8001],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 103==== Step 2 Train Loss 0.6960837841033936 ======  0.5555555555555555\n",
            "tensor([ 0.9222, -0.0342,  0.3771,  0.2207,  0.9519,  0.9446,  0.0379,  0.6843,\n",
            "        -0.4964,  0.5794,  0.9250, -0.0295,  0.7332,  0.7699,  0.9595, -0.1489,\n",
            "        -0.3629,  0.9753, -0.0267, -0.1589, -0.0130,  0.9428,  0.9816, -0.0911,\n",
            "         0.9609,  0.2833, -0.1869,  0.1000,  0.9184,  0.8204,  0.9573, -0.2899,\n",
            "        -0.2577,  0.9213,  0.8805, -0.0118, -0.0254, -0.0312,  0.9952,  0.9933,\n",
            "        -0.1014, -0.1629, -0.0499,  0.9546,  0.3626,  0.1904,  0.9271,  0.9525,\n",
            "        -0.0250, -0.1446,  0.8334, -0.1951,  0.8892,  0.7644, -0.4837, -0.3928,\n",
            "         0.5869, -0.0176,  0.7601, -0.0738,  0.9802,  0.8985,  0.9903,  0.9729],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 104==== Step 2 Train Loss 0.6910470724105835 ======  0.53125\n",
            "tensor([ 0.3869,  0.8875,  0.8420,  0.9821,  0.5864,  0.7750,  0.9794,  0.5150,\n",
            "         0.6100, -0.3573,  0.9402, -0.0733,  0.9434,  0.9769,  0.9945, -0.2954,\n",
            "         0.8159,  0.9235,  0.8314,  0.7774, -0.0803,  0.6916,  0.2576, -0.1346,\n",
            "        -0.0240,  0.9772,  0.1204,  0.4664,  0.2291,  0.9188,  0.8997,  0.3095,\n",
            "        -0.0165,  0.9694,  0.3086,  0.9862, -0.2142, -0.3630,  0.7692, -0.1274,\n",
            "         0.0936,  0.9728, -0.0790,  0.9503,  0.5739, -0.0633,  0.9398,  0.9786,\n",
            "         0.4435,  0.5610, -0.3102, -0.1920,  0.1582, -0.1417,  0.9886,  0.6340,\n",
            "         0.7955,  0.8975, -0.2076,  0.8552,  0.9729,  0.8525,  0.9534,  0.7987],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 105==== Step 2 Train Loss 0.7067234516143799 ======  0.4999999999999999\n",
            "tensor([-0.2562,  0.9921,  0.4177,  0.8134, -0.0827,  0.9850,  0.7970,  0.6943,\n",
            "        -0.5494,  0.8911,  0.9133,  0.9106,  0.6139,  0.9678,  0.9302,  0.6207,\n",
            "        -0.0259,  0.9180,  0.9331, -0.0490,  0.9950,  0.9816,  0.9710,  0.5694,\n",
            "         0.9797, -0.0940, -0.0290,  0.8671,  0.5317,  0.6015,  0.9093,  0.0082,\n",
            "         0.9564,  0.7136,  0.8946,  0.9342, -0.0398, -0.0338, -0.0361,  0.9950,\n",
            "         0.9968,  0.4093,  0.9661,  0.9614,  0.8993, -0.1993, -0.0196,  0.9806,\n",
            "         0.6739,  0.9647,  0.7647, -0.0180, -0.0219,  0.9301,  0.9137,  0.9954,\n",
            "        -0.0639,  0.9276, -0.0132,  0.4993,  0.7733,  0.9229, -0.0335,  0.7935],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 106==== Step 2 Train Loss 0.6813234090805054 ======  0.5757575757575758\n",
            "tensor([-0.0930,  0.1142, -0.0212, -0.0296, -0.1026,  0.1275,  0.1649,  0.8335,\n",
            "         0.5559,  0.9368,  0.9843,  0.9943, -0.1818,  0.9702,  0.8607, -0.0448,\n",
            "         0.9419,  0.8995,  0.8684,  0.5762,  0.9967, -0.2443,  0.8586,  0.8986,\n",
            "         0.9421,  0.6826,  0.1427,  0.6907,  0.8924,  0.0023,  0.8557,  0.9698,\n",
            "         0.9048,  0.7465, -0.0233, -0.5677,  0.9931, -0.3208, -0.0640, -0.1230,\n",
            "         0.1069, -0.5215,  0.8920,  0.9197, -0.0262,  0.6333, -0.0850,  0.5750,\n",
            "         0.0346, -0.0685,  0.4281,  0.5963,  0.6778, -0.0543,  0.9933,  0.4713,\n",
            "         0.5271,  0.9669,  0.9445,  0.8453,  0.1643,  0.9796,  0.8616,  0.9891],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 107==== Step 2 Train Loss 0.6862658858299255 ======  0.5806451612903226\n",
            "tensor([-0.3077,  0.0896,  0.9941, -0.0711,  0.2515,  0.0511, -0.0153,  0.7900,\n",
            "        -0.0720,  0.9850, -0.0247, -0.3594,  0.9652,  0.0916,  0.2107,  0.9666,\n",
            "        -0.0299,  0.4950,  0.8715,  0.9600, -0.0956,  0.1048, -0.0694,  0.9992,\n",
            "        -0.0240,  0.7941, -0.0852,  0.8197,  0.9428,  0.9143, -0.0405,  0.8498,\n",
            "        -0.0833,  0.9866,  0.9893,  0.9296,  0.9636,  0.7191,  0.8628,  0.8067,\n",
            "         0.7406, -0.0409,  0.1252, -0.1430, -0.1159,  0.9979,  0.9598,  0.2905,\n",
            "         0.4618,  0.8840,  0.9808,  0.9495,  0.7980,  0.3003,  0.9455, -0.1618,\n",
            "         0.9807,  0.3813,  0.9942, -0.1148, -0.0150,  0.8818,  0.9243, -0.0188],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 108==== Step 2 Train Loss 0.6846111416816711 ======  0.5454545454545454\n",
            "tensor([ 8.8685e-01,  9.6047e-01,  9.7073e-01,  7.8880e-01,  7.4227e-01,\n",
            "        -2.2779e-02,  1.4824e-01,  9.9189e-01, -1.6784e-01,  9.9304e-01,\n",
            "         9.0991e-01,  2.0741e-04, -7.5206e-02,  5.4300e-01,  9.8661e-01,\n",
            "        -3.1133e-01,  5.7861e-01, -1.1457e-01,  7.0342e-01,  8.9778e-01,\n",
            "         8.6502e-01,  8.9885e-01, -2.3065e-02,  9.9669e-01,  7.1771e-01,\n",
            "        -2.7887e-02,  9.9527e-01,  9.2529e-01, -5.0080e-02, -1.6092e-02,\n",
            "        -3.5404e-02,  9.9444e-01,  6.0875e-01, -1.6947e-01,  9.7552e-01,\n",
            "         5.6735e-01, -4.6610e-01,  7.0967e-01, -8.1201e-02,  9.7471e-01,\n",
            "         9.2424e-01,  3.7924e-01, -5.8504e-01,  3.5278e-01,  2.5001e-01,\n",
            "         9.4457e-02, -2.0865e-02, -3.2874e-02, -4.0314e-02,  9.7951e-01,\n",
            "         4.0532e-01, -9.9609e-02,  9.8269e-01,  9.3463e-01,  6.0665e-01,\n",
            "         9.1564e-01, -2.6873e-02,  9.8749e-01,  8.0419e-01, -1.4182e-01,\n",
            "         8.5053e-01,  7.4218e-01,  3.7178e-02,  6.9991e-01], device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 109==== Step 2 Train Loss 0.6891487240791321 ======  0.5945945945945945\n",
            "tensor([-0.0420, -0.4275, -0.2571,  0.9593,  0.9406, -0.0556, -0.0262,  0.2878,\n",
            "        -0.1210,  0.5303,  0.6212,  0.9116, -0.0917,  0.8142,  0.0308,  0.8934,\n",
            "         0.9134,  0.8778,  0.0512, -0.0205, -0.0083,  0.6246,  0.8214, -0.1510,\n",
            "         0.8159, -0.0455, -0.0904, -0.0277,  0.7729,  0.9197, -0.6147,  0.9939,\n",
            "         0.9977,  0.5873,  0.9804, -0.0390,  0.9394,  0.4996, -0.2547, -0.0271,\n",
            "         0.9671,  0.9927,  0.9963, -0.0295,  0.4014, -0.0543, -0.0863,  0.2048,\n",
            "        -0.0317,  0.9994,  0.6864, -0.0632,  0.9165,  0.9890, -0.1673,  0.9579,\n",
            "        -0.0177, -0.0313,  0.2130, -0.1223,  0.9605, -0.1365, -0.1186, -0.1520],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 110==== Step 2 Train Loss 0.7029023170471191 ======  0.4999999999999999\n",
            "tensor([-3.1190e-02,  8.1709e-01, -5.8500e-02,  9.1995e-01, -2.3229e-01,\n",
            "         5.7715e-01, -1.0377e-01,  9.4845e-01,  9.4219e-01,  9.0255e-01,\n",
            "        -6.0202e-02, -3.3642e-01, -3.4373e-02,  8.3752e-01,  9.2785e-01,\n",
            "         9.9601e-01,  8.2047e-01,  9.8623e-01,  5.9286e-01,  2.2073e-01,\n",
            "        -8.0636e-02, -1.3844e-02,  1.1295e-02, -7.8639e-02,  9.7803e-01,\n",
            "         2.6069e-01,  9.9258e-01,  9.4455e-01,  8.8240e-01,  4.8327e-01,\n",
            "         9.7371e-01,  8.4566e-01,  4.2372e-01,  8.7899e-01, -2.1635e-02,\n",
            "         9.7706e-01, -1.3752e-02, -1.0372e-01, -7.6827e-02, -1.5195e-01,\n",
            "         8.7459e-01, -1.7933e-01,  8.3983e-01,  1.1643e-01, -9.4325e-02,\n",
            "         9.7881e-01, -3.3613e-02, -2.0595e-04,  4.0792e-01,  7.8828e-01,\n",
            "         9.7912e-01, -2.7648e-01, -7.7358e-02,  5.2796e-01,  4.5338e-01,\n",
            "        -1.5220e-02,  4.3571e-01,  9.9699e-01,  9.7422e-01,  2.4773e-02,\n",
            "         9.8682e-01,  9.5571e-01,  6.7470e-01, -4.2470e-02], device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 111==== Step 2 Train Loss 0.7098541855812073 ======  0.463768115942029\n",
            "tensor([ 0.9144,  0.8960,  0.9082,  0.7124, -0.1809,  0.8844, -0.2398,  0.7107,\n",
            "         0.9620,  0.9866, -0.0269,  0.0796, -0.2942,  0.9502, -0.0260,  0.2222,\n",
            "         0.6872,  0.9839, -0.0483,  0.2269,  0.9890,  0.8228,  0.9021,  0.9832,\n",
            "         0.1294, -0.0341,  0.9249,  0.7937, -0.0915,  0.9682,  0.8675, -0.1956,\n",
            "         0.7473, -0.3910, -0.1629, -0.0083, -0.0399,  0.8395,  0.0344,  0.9771,\n",
            "         0.1567, -0.0797, -0.0176,  0.9492,  0.9133, -0.1543, -0.0740,  0.9792,\n",
            "        -0.1138,  0.2066,  0.9983, -0.2888, -0.1198,  0.9975,  0.7823,  0.0016,\n",
            "        -0.5925,  0.9073, -0.6551, -0.0118, -0.0273,  0.9760,  0.1491,  0.7771],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 112==== Step 2 Train Loss 0.7234897017478943 ======  0.47887323943661975\n",
            "tensor([ 0.9497,  0.8572,  0.9545,  0.5655,  0.7945, -0.0882,  0.9523,  0.9884,\n",
            "         0.4276, -0.1285,  0.9937, -0.0894,  0.1205,  0.9143,  0.5996, -0.0486,\n",
            "         0.9650,  0.0949, -0.3171,  0.7767, -0.0257,  0.5076,  0.5122,  0.9630,\n",
            "         0.7993,  0.9943,  0.8012, -0.1130,  0.8852,  0.2828, -0.0289, -0.0919,\n",
            "         0.9901,  0.8980, -0.3164,  0.5501,  0.3407, -0.0323,  0.7743,  0.7967,\n",
            "         0.7236, -0.2077,  0.9901,  0.9305,  0.9942, -0.0227,  0.9978, -0.0844,\n",
            "         0.5622,  0.0400, -0.1646, -0.1191, -0.2161,  0.9681,  0.8134,  0.8381,\n",
            "         0.6967, -0.0203, -0.0147,  0.9400, -0.0272,  0.9482,  0.9818,  0.9613],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 113==== Step 2 Train Loss 0.6900580525398254 ======  0.5833333333333334\n",
            "tensor([-0.0270, -0.0296,  0.9646,  0.8177, -0.0613, -0.2368, -0.0256,  0.9844,\n",
            "        -0.3632,  0.0944,  0.8326,  0.0665,  0.9898,  0.8477,  0.6993,  0.1181,\n",
            "         0.8808,  0.6225,  0.1240, -0.0221,  0.8294,  0.0053,  0.9549, -0.1351,\n",
            "         0.5205,  0.9934, -0.0253,  0.8643,  0.9710,  0.9416,  0.2526,  0.9777,\n",
            "        -0.0569,  0.9879,  0.0277,  0.4728,  0.9725,  0.9014,  0.8339, -0.1616,\n",
            "         0.1648,  0.6451,  0.9567, -0.0697,  0.9122,  0.9786, -0.0412, -0.4114,\n",
            "        -0.0187,  0.8154,  0.9978, -0.0226,  0.8706, -0.5133, -0.0139,  0.7015,\n",
            "        -0.1960, -0.0167,  0.7307,  0.7889,  0.9187,  0.9969,  0.9418, -0.0312],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 114==== Step 2 Train Loss 0.6953044533729553 ======  0.547945205479452\n",
            "tensor([ 9.8892e-01,  9.7613e-01,  9.5857e-01, -4.9492e-02,  9.7713e-01,\n",
            "         5.5940e-01,  9.7835e-01,  2.7731e-01,  8.5020e-01, -3.5232e-02,\n",
            "         4.8139e-01,  8.4655e-01, -3.2833e-01,  9.9484e-01, -4.6687e-02,\n",
            "        -7.0473e-02,  7.7238e-01,  6.9265e-01,  7.7744e-01, -3.2176e-02,\n",
            "         9.8278e-01,  6.6186e-04,  5.0214e-01, -1.3253e-01,  8.8546e-01,\n",
            "        -4.6883e-02,  9.9065e-01, -1.2145e-01, -3.9469e-02,  9.5169e-01,\n",
            "         9.9687e-01, -1.7511e-01,  9.6400e-01,  2.0544e-02,  9.6654e-01,\n",
            "         1.1922e-01, -7.0732e-02,  4.5026e-01,  9.8907e-01,  9.0955e-01,\n",
            "         5.9230e-01,  9.3588e-01,  6.7524e-01, -1.9988e-01, -4.0528e-01,\n",
            "         7.7600e-01,  2.3832e-01,  9.3270e-01, -3.5905e-02,  9.6124e-01,\n",
            "         8.7334e-01,  4.9677e-01, -2.1405e-01, -4.1642e-02,  9.6736e-01,\n",
            "         9.0105e-01,  9.8207e-01, -3.5825e-02,  8.5636e-01, -3.6884e-02,\n",
            "        -5.1451e-01,  6.4516e-01,  9.2604e-01,  8.5018e-01], device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 115==== Step 2 Train Loss 0.6991387605667114 ======  0.5142857142857142\n",
            "tensor([ 0.3370,  0.9598, -0.2522, -0.0240,  0.9957,  0.9681,  0.9911,  0.0173,\n",
            "         0.9254, -0.2644,  0.2238, -0.0328,  0.9342,  0.1862,  0.9184,  0.9129,\n",
            "         0.6071,  0.7758, -0.0200,  0.9910,  0.9089,  0.5787,  0.6343, -0.0437,\n",
            "        -0.0040,  0.9744,  0.5808,  0.5142,  0.4533,  0.9336, -0.0065,  0.9129,\n",
            "        -0.4327,  0.2809,  0.9972,  0.9748,  0.3452, -0.1937,  0.1862,  0.9606,\n",
            "         0.9737, -0.0208, -0.0250,  0.9415,  0.9539,  0.6846, -0.5499, -0.0562,\n",
            "         0.8368,  0.9664,  0.8156,  0.9246,  0.9504,  0.0633,  0.8665, -0.1285,\n",
            "         0.8326,  0.9963,  0.9884, -0.0257,  0.9338, -0.0214,  0.9768,  0.8856],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 116==== Step 2 Train Loss 0.7065451741218567 ======  0.5753424657534246\n",
            "tensor([ 0.9388,  0.9975,  0.9470,  0.7882, -0.2655, -0.0460,  0.9443,  0.9084,\n",
            "         0.8896,  0.2537,  0.7702, -0.2603,  0.6687,  0.9937,  0.8890,  0.9315,\n",
            "         0.3506,  0.9893,  0.7936, -0.2760,  0.8880, -0.0893,  0.0325,  0.4341,\n",
            "         0.2442,  0.7756,  0.1534,  0.9909,  0.0169, -0.3152, -0.1086,  0.5902,\n",
            "        -0.0268,  0.8972,  0.9462,  0.9337,  0.9526,  0.9974, -0.0225,  0.6952,\n",
            "         0.9843,  0.9680,  0.2670,  0.2674, -0.4824,  0.9933,  0.3857,  0.4124,\n",
            "         0.6567,  0.9581,  0.9163,  0.8399, -0.0856,  0.9813,  0.3535,  0.6806,\n",
            "         0.9220,  0.8750,  0.8403,  0.9466,  0.9527, -0.2832,  0.7632, -0.0734],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 117==== Step 2 Train Loss 0.6842995882034302 ======  0.6027397260273972\n",
            "tensor([-0.0521, -0.0342,  0.8306,  0.3238,  0.8581, -0.0282,  0.0600,  0.8392,\n",
            "         0.8910, -0.1313,  0.8170, -0.0045,  0.4020, -0.0564,  0.9966,  0.9676,\n",
            "        -0.6114, -0.2562,  0.7206,  0.9944, -0.0322,  0.4715, -0.0444, -0.1338,\n",
            "         0.8284,  0.9437, -0.0196, -0.0668, -0.0195,  0.9872,  0.0901,  0.8037,\n",
            "         0.9889,  0.2842, -0.2498, -0.0481,  0.9952, -0.2538, -0.2073,  0.7794,\n",
            "         0.2259,  0.9530,  0.8510,  0.9794,  0.8232,  0.4297,  0.4303,  0.5573,\n",
            "         0.0162,  0.8676,  0.9548,  0.9965,  0.9699,  0.5841,  0.3441, -0.1414,\n",
            "         0.7722,  0.8943, -0.0227,  0.8974,  0.3104, -0.2520,  0.0027,  0.9749],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 118==== Step 2 Train Loss 0.7082238793373108 ======  0.5526315789473684\n",
            "tensor([-0.2711,  0.9716, -0.0269,  0.6251,  0.9596, -0.1913, -0.0230, -0.0958,\n",
            "        -0.4725, -0.1133,  0.9629,  0.4693, -0.0430, -0.1668,  0.5921,  0.9953,\n",
            "        -0.0517,  0.4196,  0.7185, -0.0277,  0.2345, -0.0702,  0.7501,  0.9989,\n",
            "        -0.0187,  0.8194, -0.1959, -0.2984,  0.0014,  0.9945,  0.9508, -0.0048,\n",
            "         0.1948, -0.0639,  0.9025, -0.2196, -0.0803,  0.3208,  0.6186, -0.3008,\n",
            "        -0.4791,  0.9373,  0.1766,  0.3952,  0.9657,  0.5327, -0.0424,  0.9971,\n",
            "        -0.3127,  0.9802, -0.0727, -0.0176,  0.8967, -0.1995,  0.6576, -0.0761,\n",
            "         0.9985,  0.8615,  0.9460,  0.5498,  0.5596,  0.9671,  0.9989,  0.8313],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 119==== Step 2 Train Loss 0.7072654366493225 ======  0.5333333333333333\n",
            "tensor([ 0.9950, -0.0944,  0.6372,  0.8997, -0.0426,  0.4577,  0.9041, -0.1528,\n",
            "         0.2587, -0.0762, -0.0561,  0.9964, -0.0259, -0.1051,  0.8684,  0.0556,\n",
            "         0.9368,  0.0290,  0.4815,  0.9944,  0.9004, -0.1149,  0.7497,  0.9728,\n",
            "         0.6157,  0.9412,  0.9259,  0.9945,  0.9247,  0.0367,  0.2745,  0.8128,\n",
            "         0.9438, -0.0137,  0.9620,  0.8235,  0.9174, -0.3722, -0.1262, -0.1339,\n",
            "         0.0105, -0.0711,  0.6841,  0.1438,  0.9580,  0.3922, -0.0345, -0.1878,\n",
            "         0.8122,  0.4593, -0.0343, -0.0372, -0.0489,  0.9239,  0.8537, -0.0200,\n",
            "         0.9430,  0.1073, -0.3543, -0.2127, -0.0534,  0.1654,  0.8504,  0.6999],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 120==== Step 2 Train Loss 0.7074814438819885 ======  0.525\n",
            "tensor([-0.1078, -0.0223, -0.0723,  0.7335,  0.9993, -0.0465,  0.0199,  0.6942,\n",
            "         0.9491, -0.1189,  0.9419, -0.2346,  0.9889, -0.3386,  0.2147,  0.5808,\n",
            "         0.9808,  0.6159,  0.9872,  0.5036, -0.0015,  0.5255, -0.0963,  0.9715,\n",
            "         0.8664,  0.8624,  0.3753, -0.0410, -0.1278, -0.2033,  0.9893,  0.9995,\n",
            "         0.7392,  0.9197,  0.9696,  0.3060,  0.9333, -0.0358,  0.9980, -0.3094,\n",
            "        -0.0532, -0.0558,  0.9242, -0.0493, -0.5213,  0.9423,  0.8074,  0.9657,\n",
            "        -0.3872,  0.2421,  0.5342,  0.7416,  0.6917,  0.4229,  0.8748,  0.5292,\n",
            "        -0.1038,  0.9494,  0.1960,  0.9301, -0.0295,  0.9785, -0.2224,  0.8944],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 121==== Step 2 Train Loss 0.7097507119178772 ======  0.5194805194805194\n",
            "tensor([ 0.6751, -0.0404,  0.9848, -0.0306,  0.9309, -0.4094,  0.8899,  0.1540,\n",
            "         0.9545,  0.9655,  0.9577,  0.5301,  0.4926, -0.0188, -0.0218,  0.9962,\n",
            "        -0.1373,  0.8572,  0.9298, -0.0704,  0.0241, -0.6275,  0.7563,  0.9205,\n",
            "         0.9170, -0.0345, -0.3569, -0.0384,  0.9289,  0.9292,  0.7356,  0.5860,\n",
            "         0.9866, -0.0679, -0.1645,  0.1457,  0.8456,  0.9621, -0.0217,  0.9269,\n",
            "        -0.3327,  0.9242,  0.9791,  0.1694,  0.8466,  0.1674,  0.1405,  0.9270,\n",
            "         0.9105, -0.4790, -0.0476,  0.7862,  0.5658,  0.4347,  0.8539,  0.8811,\n",
            "         0.9115,  0.9611, -0.0629,  0.8285,  0.9050,  0.6575, -0.0069,  0.1814],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 122==== Step 2 Train Loss 0.6844241619110107 ======  0.5789473684210527\n",
            "tensor([-0.0158,  0.4809,  0.4603,  0.9383,  0.5358,  0.9157,  0.8702,  0.3291,\n",
            "         0.8411, -0.0120,  0.3021,  0.4123,  0.9984,  0.3952,  0.9817,  0.6045,\n",
            "         0.9625,  0.3417, -0.5561,  0.8591,  0.7242,  0.9458,  0.9446,  0.4133,\n",
            "         0.9896,  0.6412,  0.5669,  0.8522, -0.0492, -0.0060,  0.8694, -0.1198,\n",
            "         0.0069,  0.9459,  0.8632,  0.9977,  0.2948, -0.0173, -0.0165,  0.3879,\n",
            "         0.2620,  0.8423,  0.9294,  0.4259,  0.3924,  0.9469, -0.2914,  0.9893,\n",
            "         0.9113,  0.9972,  0.9585,  0.7917,  0.8892,  0.7838,  0.9573, -0.0077,\n",
            "         0.9566, -0.0252,  0.9709,  0.9136,  0.9906,  0.7883,  0.1165,  0.9507],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 123==== Step 2 Train Loss 0.6826910972595215 ======  0.6823529411764705\n",
            "tensor([-0.2101, -0.0635, -0.0218,  0.8727, -0.2723,  0.9521,  0.5883,  0.7599,\n",
            "         0.8929,  0.9730,  0.5602,  0.5717,  0.9484,  0.2074,  0.9917,  0.9529,\n",
            "         0.7819,  0.8591,  0.7631,  0.3938,  0.8653,  0.9996,  0.8951,  0.5850,\n",
            "         0.4195,  0.9925,  0.5844, -0.0368,  0.5754,  0.9189,  0.9897, -0.0056,\n",
            "         0.0315,  0.4097,  0.8841, -0.0225,  0.9909,  0.9682, -0.1569,  0.9968,\n",
            "        -0.4560,  0.9930,  0.9839,  0.9477,  0.2597,  0.8830,  0.4887,  0.8057,\n",
            "        -0.1162, -0.0615,  0.8965,  0.9987, -0.5245,  0.9665, -0.0139,  0.5597,\n",
            "        -0.0550,  0.8964,  0.1035, -0.4804, -0.0252,  0.8442,  0.5995,  0.9391],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 124==== Step 2 Train Loss 0.6934916377067566 ======  0.6741573033707865\n",
            "tensor([-0.0167, -0.3056,  0.8259,  0.8970,  0.8256,  0.4772, -0.0513,  0.9934,\n",
            "         0.9774,  0.9244, -0.0411, -0.0288,  0.2439,  0.9012, -0.0376,  0.9379,\n",
            "         0.0141,  0.8787, -0.0261, -0.3664, -0.1073, -0.0190,  0.9949,  0.7978,\n",
            "        -0.0182,  0.9636,  0.9367,  0.9427, -0.1287,  0.9816,  0.9876,  0.9845,\n",
            "         0.9418,  0.3727, -0.0649, -0.0488,  0.8560,  0.2974, -0.0378,  0.4791,\n",
            "        -0.0828, -0.0541, -0.0272, -0.0345,  0.9898,  0.9869, -0.1419,  0.9315,\n",
            "         0.9737, -0.3170, -0.0231,  0.8673,  0.3001, -0.0263, -0.1322,  0.9814,\n",
            "         0.9668,  0.9992,  0.9625,  0.9755,  0.9719,  0.9861,  0.3833,  0.8756],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 125==== Step 2 Train Loss 0.7140295505523682 ======  0.5679012345679013\n",
            "tensor([ 0.4742, -0.1015,  0.9875,  0.2986, -0.0278,  0.9947, -0.1509,  0.9509,\n",
            "        -0.0212,  0.8892,  0.1676,  0.9851, -0.0404,  0.9485,  0.9462, -0.1574,\n",
            "         0.9834, -0.0133,  0.8887, -0.0914,  0.9231,  0.6670, -0.0129,  0.8221,\n",
            "         0.0463,  0.9094, -0.0202,  0.4310, -0.1950, -0.0785,  0.8748, -0.0258,\n",
            "         0.9892,  0.8919, -0.2807,  0.9785,  0.8851, -0.1488,  0.8354, -0.0827,\n",
            "         0.4158,  0.8746,  0.6641, -0.0631,  0.9129,  0.9878, -0.0611,  0.9791,\n",
            "         0.8170,  0.5247,  0.9289,  0.9069,  0.0145, -0.4156,  0.0193,  0.8940,\n",
            "         0.7084,  0.8970, -0.0040, -0.0339,  0.9653,  0.4503, -0.0152,  0.9726],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 126==== Step 2 Train Loss 0.688582181930542 ======  0.6075949367088608\n",
            "tensor([ 0.2098,  0.9266,  0.9747,  0.9647,  0.7141, -0.2845,  0.9824,  0.9774,\n",
            "         0.0018, -0.2839,  0.5452,  0.9504,  0.5389, -0.0479,  0.9350,  0.9988,\n",
            "         0.8496,  0.9836,  0.9327,  0.9364, -0.0227, -0.0268, -0.0163, -0.2680,\n",
            "         0.9098, -0.0710, -0.0228,  0.7420, -0.2310, -0.3799,  0.5502, -0.1604,\n",
            "         0.9163,  0.9538,  0.7749,  0.7189, -0.0604,  0.7856,  0.8981,  0.9632,\n",
            "        -0.5180,  0.8961,  0.9269,  0.4053,  0.4054, -0.0724, -0.0598,  0.9764,\n",
            "        -0.0518, -0.5095,  0.5882,  0.9098, -0.1978,  0.9532,  0.9413, -0.0267,\n",
            "         0.9965,  0.2409,  0.9733, -0.0186,  0.9390, -0.0292,  0.9460, -0.0717],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 127==== Step 2 Train Loss 0.6970463395118713 ======  0.5925925925925926\n",
            "tensor([-0.0207,  0.9900, -0.0277, -0.0698, -0.0560,  0.9016, -0.0172,  0.4111,\n",
            "         0.5859, -0.0235,  0.0363,  0.8221,  0.8242, -0.0249, -0.2882,  0.7894,\n",
            "        -0.3186,  0.8105,  0.8495,  0.1231,  0.9672,  0.9383, -0.0908, -0.0250,\n",
            "         0.6123, -0.2518, -0.2072,  0.6573, -0.2242,  0.8745,  0.9529,  0.8127,\n",
            "         0.0374,  0.9838,  0.9742,  0.7459,  0.9948,  0.8529,  0.7474,  0.8451,\n",
            "         0.9715,  0.8585, -0.3862, -0.0266,  0.9894,  0.8412, -0.1641,  0.9959,\n",
            "         0.8431,  0.0711,  0.9530,  0.8997,  0.7806,  0.9560,  0.5777, -0.1506,\n",
            "         0.9967,  0.9773,  0.9041, -0.1132,  0.3974,  0.7434,  0.0037,  0.7223],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 128==== Step 2 Train Loss 0.6840474009513855 ======  0.6813186813186813\n",
            "tensor([ 0.5590,  0.8692,  0.9204,  0.9702, -0.1524,  0.9341,  0.8360,  0.9676,\n",
            "         0.9008,  0.9929,  0.1943,  0.9958, -0.0255,  0.6525,  0.7508,  0.8076,\n",
            "         0.9774,  0.9425,  0.9569, -0.2087,  0.9720,  0.1669,  0.9104, -0.0721,\n",
            "        -0.1752,  0.9546,  0.9887, -0.1451, -0.0393,  0.9745, -0.0285, -0.0242,\n",
            "         0.9910,  0.4452,  0.8563,  0.9445, -0.0245, -0.0260, -0.0415,  0.6500,\n",
            "        -0.1380, -0.0262,  0.8921, -0.0260, -0.1828, -0.0352,  0.5751,  0.7879,\n",
            "        -0.0238, -0.1199,  0.9821,  0.8471,  0.3911,  0.3971,  0.8873, -0.0563,\n",
            "         0.9253, -0.0795,  0.9419, -0.0760,  0.7960,  0.3908,  0.7856, -0.2064],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 129==== Step 2 Train Loss 0.690517008304596 ======  0.7045454545454545\n",
            "tensor([ 0.9525, -0.0309, -0.0358, -0.2867,  0.5724,  0.9377,  0.8185,  0.8629,\n",
            "        -0.0265,  0.9968, -0.1150,  0.9817, -0.1206, -0.0500,  0.7452,  0.8468,\n",
            "         0.9876,  0.9106, -0.0287, -0.0593, -0.3480,  0.8691,  0.8333,  0.9896,\n",
            "         0.8938,  0.6246,  0.4575,  0.7663, -0.3516,  0.5914,  0.9029,  0.7803,\n",
            "         0.9960,  0.0183, -0.0211, -0.0152,  0.9507,  0.9798,  0.6311,  0.0108,\n",
            "        -0.2430,  0.3571,  0.9543,  0.5927, -0.3350,  0.7396, -0.0967,  0.9302,\n",
            "         0.9331,  0.9675,  0.8831,  0.9328,  0.9478,  0.1760,  0.9701,  0.7530,\n",
            "         0.9810,  0.0831,  0.7609,  0.9518,  0.0862, -0.0325,  0.2437,  0.9684],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 130==== Step 2 Train Loss 0.7152180671691895 ======  0.5747126436781609\n",
            "tensor([ 0.6853, -0.3406,  0.6518,  0.9021,  0.9678, -0.1135,  0.9704,  0.0763,\n",
            "        -0.5568,  0.9690, -0.1101, -0.0515, -0.1792,  0.9383,  0.5071,  0.9955,\n",
            "        -0.0158, -0.0271,  0.7773, -0.0333, -0.0278, -0.0122,  0.8088,  0.9980,\n",
            "         0.5281,  0.1076,  0.8074,  0.9406,  0.9128, -0.1382,  0.4816,  0.8221,\n",
            "         0.0338, -0.2449,  0.9325,  0.9132,  0.7349, -0.3832,  0.9936,  0.9472,\n",
            "        -0.0564,  0.9510,  0.9355,  0.9459,  0.2279, -0.0220,  0.9687,  0.6114,\n",
            "        -0.4695,  0.9966,  0.7984,  0.9589,  0.9350,  0.0069, -0.2267,  0.8962,\n",
            "         0.9733,  0.8197,  0.4907,  0.9729, -0.4618,  0.9906, -0.2804, -0.0912],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 131==== Step 2 Train Loss 0.7073030471801758 ======  0.627906976744186\n",
            "tensor([ 0.9895,  0.3757, -0.0550,  0.9247, -0.0227,  0.9521, -0.0394,  0.6349,\n",
            "        -0.1556, -0.1239,  0.1528, -0.2951, -0.4917,  0.9099,  0.8086,  0.1023,\n",
            "         0.9515,  0.3201, -0.0485,  0.7279,  0.8915,  0.8944,  0.9529,  0.9610,\n",
            "         0.3468,  0.6833,  0.4073, -0.0069,  0.9141,  0.8872,  0.9984,  0.9249,\n",
            "         0.5231, -0.0920,  0.8393,  0.8426, -0.2753,  0.9050,  0.9583,  0.9640,\n",
            "         0.9098,  0.8863,  0.9912, -0.2363,  0.9313,  0.8553,  0.9310,  0.1578,\n",
            "         0.7951, -0.0686,  0.1295,  0.9807,  0.9652,  0.5924, -0.0261,  0.9572,\n",
            "         0.9279,  0.9030, -0.3469,  0.9508,  0.8002, -0.0470,  0.9317,  0.0290],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 132==== Step 2 Train Loss 0.6904093623161316 ======  0.6136363636363638\n",
            "tensor([ 0.9461, -0.0752,  0.3201,  0.3954,  0.0305,  0.8678, -0.1048,  0.9982,\n",
            "        -0.0266,  0.9771, -0.0348,  0.7922, -0.4358,  0.4997,  0.9758, -0.1130,\n",
            "        -0.1427,  0.8512,  0.9681, -0.0388,  0.1814,  0.9958,  0.8675,  0.9958,\n",
            "         0.9532,  0.3707, -0.0221,  0.3817,  0.8625,  0.4206,  0.5135, -0.0174,\n",
            "         0.9161,  0.9941, -0.0474, -0.1315,  0.5841, -0.0457,  0.8788,  0.0515,\n",
            "         0.2875,  0.8747, -0.1035,  0.5197,  0.4111,  0.3562,  0.8651,  0.3320,\n",
            "        -0.0491, -0.0196,  0.9770,  0.8671,  0.7689,  0.9964,  0.9785, -0.0395,\n",
            "         0.9713,  0.3554,  0.9696, -0.1707,  0.1701,  0.6992,  0.1975,  0.2710],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 133==== Step 2 Train Loss 0.7103192806243896 ======  0.6436781609195402\n",
            "tensor([-0.0306, -0.1637,  0.9104,  0.8263, -0.0791,  0.0635,  0.3528, -0.0358,\n",
            "         0.2312,  0.9077,  0.4723,  0.8713, -0.4955,  0.9115,  0.9863,  0.0165,\n",
            "        -0.0497, -0.0232,  0.2636,  0.9797, -0.2334, -0.1679, -0.1619,  0.9740,\n",
            "         0.7436, -0.1509,  0.9719, -0.0374, -0.2332, -0.0474, -0.0267, -0.3145,\n",
            "         0.9684,  0.0217,  0.0322,  0.9925,  0.8705,  0.9416, -0.0683,  0.1312,\n",
            "        -0.0289,  0.9394,  0.9593, -0.0271,  0.9604, -0.3505,  0.0309, -0.3697,\n",
            "         0.8856,  0.7172, -0.0385,  0.2980,  0.9944, -0.4230,  0.0786, -0.5032,\n",
            "         0.7732,  0.1177,  0.9971,  0.9962,  0.9134,  0.6506, -0.0209,  0.2194],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 134==== Step 2 Train Loss 0.696678102016449 ======  0.6588235294117647\n",
            "tensor([-0.2285,  0.9669, -0.3161,  0.8698,  0.9956, -0.0417, -0.2677,  0.8320,\n",
            "         0.9598,  0.9979, -0.0320,  0.6451, -0.3324,  0.2517, -0.0577, -0.0156,\n",
            "         0.9480,  0.0909, -0.3097, -0.1549, -0.0285,  0.5697, -0.0596, -0.0260,\n",
            "         0.9659, -0.3881, -0.0205,  0.9569,  0.9932,  0.7008,  0.6908,  0.4025,\n",
            "        -0.0842,  0.0027,  0.8949,  0.3595, -0.1541,  0.7698,  0.9928, -0.0804,\n",
            "        -0.0385,  0.8806,  0.9512,  0.9764,  0.5157,  0.9372,  0.1929, -0.0294,\n",
            "         0.9906,  0.3829,  0.6665, -0.2649,  0.9759, -0.0763,  0.1871,  0.9027,\n",
            "         0.9595,  0.2504,  0.9618, -0.3438,  0.5611,  0.6239,  0.0799, -0.0223],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 135==== Step 2 Train Loss 0.6957297325134277 ======  0.6823529411764705\n",
            "tensor([ 0.6066,  0.9572,  0.9927,  0.7198, -0.0181,  0.9949,  0.9428,  0.9877,\n",
            "         0.2676,  0.9906,  0.9831,  0.9476,  0.8744,  0.8013,  0.9354,  0.7915,\n",
            "        -0.2108,  0.9942,  0.7852, -0.1151,  0.9555, -0.0623,  0.9783,  0.9834,\n",
            "        -0.1146, -0.1598, -0.0847,  0.8760,  0.6368, -0.0080, -0.3797,  0.8727,\n",
            "         0.9840,  0.9726,  0.9734,  0.8821,  0.9842, -0.0449, -0.4718,  0.6009,\n",
            "         0.9598, -0.0263,  0.5856, -0.1944,  0.9933,  0.6085,  0.9103,  0.7982,\n",
            "         0.9914,  0.9341,  0.6096, -0.2045,  0.9270,  0.7328, -0.2103, -0.0595,\n",
            "         0.1960, -0.2167,  0.9784, -0.0240, -0.0186,  0.5448,  0.9766,  0.8796],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 136==== Step 2 Train Loss 0.6691563129425049 ======  0.6966292134831462\n",
            "tensor([ 0.9131, -0.0459, -0.0610, -0.1946,  0.5339, -0.1920, -0.1927,  0.9708,\n",
            "         0.3798,  0.9000,  0.0958, -0.0283,  0.0682, -0.0212, -0.1112,  0.9291,\n",
            "         0.6954,  0.6198,  0.3163,  0.9848, -0.0814, -0.0968,  0.6150,  0.5666,\n",
            "         0.9855,  0.9884,  0.9621,  0.1851,  0.9367,  0.7875, -0.1138, -0.1707,\n",
            "         0.6882,  0.9701,  0.9785, -0.0260,  0.0724,  0.9857,  0.8202,  0.8431,\n",
            "         0.1478, -0.0849,  0.8733,  0.9836,  0.4111,  0.7700, -0.5183,  0.9814,\n",
            "         0.8881,  0.4191, -0.1752, -0.4317,  0.9074,  0.6346,  0.9685,  0.9260,\n",
            "         0.9971,  0.7313, -0.0318, -0.0602,  0.9697,  0.0235, -0.1623,  0.9095],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 137==== Step 2 Train Loss 0.685874879360199 ======  0.7096774193548387\n",
            "tensor([ 0.9606,  0.8895,  0.9221,  0.9764,  0.8139,  0.9594, -0.0788,  0.7883,\n",
            "        -0.0225,  0.9945, -0.0719,  0.9218,  0.9520, -0.0327,  0.9187, -0.0517,\n",
            "        -0.0848,  0.9967,  0.9686,  0.1803,  0.9207, -0.0785, -0.1162,  0.9780,\n",
            "         0.9535, -0.1345,  0.9509,  0.0155,  0.8394, -0.0395,  0.9233,  0.9338,\n",
            "         0.7234,  0.9906,  0.9844, -0.3565,  0.8545, -0.0721,  0.6270, -0.0133,\n",
            "         0.9913,  0.9678,  0.9843,  0.9957, -0.0377,  0.9345, -0.5297, -0.0399,\n",
            "         0.8484,  0.9147, -0.1072,  0.9473,  0.9983, -0.2342,  0.0772,  0.9390,\n",
            "         0.6594,  0.9786, -0.0599,  0.0432,  0.9743,  0.5147,  0.9786,  0.3600],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 138==== Step 2 Train Loss 0.6790050864219666 ======  0.7096774193548387\n",
            "tensor([ 0.5731,  0.1006,  0.4595,  0.9483,  0.9370, -0.0947,  0.0919,  0.9969,\n",
            "        -0.0464, -0.1381,  0.9256, -0.4510,  0.9551,  0.4092, -0.0275,  0.2380,\n",
            "         0.9926,  0.9151,  0.5751,  0.8688,  0.8292,  0.3108,  0.9600,  0.9319,\n",
            "         0.9961,  0.9942, -0.0224,  0.8617, -0.0460,  0.0236,  0.2112, -0.1187,\n",
            "         0.9896,  0.1696,  0.9601,  0.3987,  0.4418,  0.9658, -0.1062,  0.6233,\n",
            "        -0.2723,  0.9778, -0.0518,  0.8800,  0.8668,  0.8710,  0.9632,  0.9971,\n",
            "        -0.3780,  0.9275,  0.9363,  0.9154, -0.1044, -0.0531, -0.2517,  0.9667,\n",
            "         0.9365,  0.9230,  0.9621, -0.0753,  0.5848,  0.9611,  0.9753,  0.6575],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 139==== Step 2 Train Loss 0.7008789777755737 ======  0.627906976744186\n",
            "tensor([ 9.8049e-01,  6.9963e-04,  9.9099e-01, -1.0670e-01,  8.2548e-01,\n",
            "        -2.2165e-02,  2.8864e-01,  8.4953e-02, -1.2900e-01, -7.6379e-02,\n",
            "         5.6925e-01, -4.5010e-02, -4.9700e-02,  1.0311e-01,  5.1220e-02,\n",
            "         8.2387e-01, -3.8730e-02,  8.3117e-01,  9.7892e-01,  8.1320e-01,\n",
            "         3.0739e-01,  9.5078e-01,  8.5521e-01, -1.8562e-01, -1.6873e-01,\n",
            "         9.8656e-01,  4.3069e-02,  7.1619e-01,  9.8970e-01, -1.3108e-01,\n",
            "        -1.0203e-01,  9.4975e-01,  9.8414e-01,  1.6934e-01,  9.8858e-01,\n",
            "         9.8849e-01,  9.9736e-01, -3.3814e-02, -2.6710e-02, -1.9524e-02,\n",
            "         9.9405e-01,  1.0317e-02,  9.9199e-01,  9.4532e-01,  9.9021e-01,\n",
            "         7.0890e-01, -1.2886e-01, -9.1460e-02,  8.5846e-01, -2.0803e-02,\n",
            "        -4.9811e-02, -1.2463e-01,  7.7748e-01, -2.1821e-01,  8.5335e-01,\n",
            "         1.2918e-01,  9.9816e-01,  9.8817e-01,  9.2573e-01,  9.2825e-01,\n",
            "         4.5097e-02, -1.5025e-01, -2.5180e-01, -1.4626e-01], device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 140==== Step 2 Train Loss 0.7232540845870972 ======  0.4507042253521127\n",
            "tensor([ 0.6325,  0.0324,  0.8682,  0.9777, -0.3695, -0.0116, -0.2217, -0.0390,\n",
            "        -0.0229,  0.7345,  0.4166, -0.0980,  0.8223,  0.8952,  0.9802, -0.0643,\n",
            "         0.9637, -0.1451, -0.0315,  0.9175, -0.0978, -0.1073,  0.5859, -0.3091,\n",
            "         0.8479,  0.0636,  0.4874, -0.2314, -0.1728, -0.0044,  0.7927,  0.9606,\n",
            "         0.8926,  0.7917,  0.9526,  0.3744,  0.9825, -0.0807,  0.8909,  0.2065,\n",
            "        -0.3665, -0.0562,  0.3702,  0.6659, -0.0877,  0.9361,  0.8694, -0.0227,\n",
            "         0.9627,  0.2817,  0.9755,  0.9185,  0.9745,  0.9970,  0.9278,  0.9877,\n",
            "         0.9760, -0.1340,  0.7184, -0.0184,  0.9265,  0.1287,  0.9502,  0.7343],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 141==== Step 2 Train Loss 0.6951761245727539 ======  0.6888888888888888\n",
            "tensor([ 0.2917,  0.9439,  0.5767,  0.7430, -0.0815,  0.9387,  0.9285, -0.0683,\n",
            "         0.8681, -0.1036,  0.8632,  0.0754,  0.9359, -0.0329, -0.3268,  0.8476,\n",
            "         0.7244,  0.9451,  0.1634,  0.2737,  0.9611,  0.8906,  0.9514, -0.0623,\n",
            "        -0.1363,  0.5637, -0.0337,  0.1518,  0.8331,  0.9534, -0.0294,  0.7442,\n",
            "        -0.0227,  0.1310,  0.8856,  0.9661,  0.1583, -0.2421,  0.8835,  0.9917,\n",
            "        -0.0258,  0.9976,  0.8277, -0.0875,  0.6455,  0.8203,  0.4446, -0.0120,\n",
            "         0.9260,  0.6301,  0.6429, -0.0221,  0.2161, -0.0262, -0.2205,  0.8892,\n",
            "         0.8350,  0.9986,  0.9599, -0.0398,  0.8573,  0.4204,  0.8804,  0.7211],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 142==== Step 2 Train Loss 0.7005586624145508 ======  0.6190476190476191\n",
            "tensor([ 0.9757,  0.0616,  0.9273,  0.9503,  0.9712, -0.2269, -0.0195, -0.0595,\n",
            "         0.6354,  0.9786, -0.0460,  0.9882, -0.2973, -0.2381,  0.7712,  0.7681,\n",
            "         0.9952,  0.7424,  0.9675, -0.0377,  0.9914, -0.1508,  0.6134, -0.0964,\n",
            "        -0.1114,  0.7335,  0.8144,  0.6410,  0.0247, -0.0659, -0.0746,  0.1286,\n",
            "        -0.3928,  0.9591, -0.0447, -0.1189,  0.9846, -0.0312,  0.9427, -0.0369,\n",
            "         0.1094, -0.1410,  0.8402,  0.3154,  0.4347,  0.9273, -0.0577, -0.2673,\n",
            "        -0.0251,  0.9373,  0.9961,  0.0639,  0.7186,  0.7487,  0.0744, -0.1454,\n",
            "        -0.0333,  0.9961, -0.0150,  0.9912,  0.9746,  0.0068,  0.3831,  0.9269],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 143==== Step 2 Train Loss 0.6978413462638855 ======  0.5675675675675675\n",
            "tensor([ 0.9494,  0.8769,  0.8322,  0.0926, -0.0227, -0.0254, -0.3882,  0.7924,\n",
            "         0.7175, -0.1207, -0.0021,  0.9816, -0.0226,  0.7633,  0.3664,  0.8823,\n",
            "         0.8310,  0.9462,  0.9581,  0.9482,  0.8385,  0.9361,  0.8739,  0.7671,\n",
            "         0.9571, -0.0495, -0.0837,  0.9219,  0.9338,  0.9950,  0.8768,  0.0011,\n",
            "        -0.0502, -0.2101,  0.9662,  0.9003, -0.0119, -0.0944, -0.0233,  0.8707,\n",
            "        -0.0323, -0.1579, -0.0727,  0.9805,  0.9571,  0.1199, -0.0209,  0.9529,\n",
            "        -0.0636,  0.8767, -0.0783,  0.2294, -0.4600,  0.9044, -0.0847,  0.6976,\n",
            "         0.2504, -0.0287, -0.4160,  0.4754,  0.9299,  0.9884,  0.5527,  0.5436],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 144==== Step 2 Train Loss 0.7000453472137451 ======  0.5609756097560975\n",
            "tensor([ 0.0918,  0.1648, -0.0358,  0.9630, -0.0277, -0.1059, -0.1307,  0.9387,\n",
            "        -0.0198,  0.1480,  0.6862, -0.0504,  0.8869, -0.0308,  0.9732,  0.9846,\n",
            "         0.3486,  0.9989, -0.1849,  0.7159,  0.7858,  0.7671,  0.7752, -0.1956,\n",
            "         0.9150, -0.1020, -0.0979, -0.0361,  0.9911, -0.1910,  0.9564,  0.9966,\n",
            "         0.9084,  0.4659,  0.0178,  0.1402,  0.6467,  0.9700,  0.3574,  0.9312,\n",
            "         0.7396,  0.9051,  0.0531,  0.9832,  0.9925,  0.1238,  0.3556, -0.0925,\n",
            "        -0.0210,  0.9040,  0.9398,  0.9839,  0.9557,  0.9081,  0.8668, -0.1690,\n",
            "        -0.4986,  0.9100, -0.4196,  0.9950,  0.7837,  0.9439,  0.8438,  0.4161],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 145==== Step 2 Train Loss 0.6732200980186462 ======  0.7160493827160493\n",
            "tensor([ 0.9438, -0.1397, -0.4107,  0.7631, -0.0348, -0.0506, -0.1832, -0.1942,\n",
            "         0.8153,  0.9980,  0.1032, -0.3550, -0.0248,  0.4180,  0.7648, -0.0307,\n",
            "        -0.0305, -0.2206, -0.1446,  0.6135,  0.8399,  0.4778,  0.4723,  0.0849,\n",
            "        -0.2352,  0.9820,  0.1117,  0.0595,  0.2374,  0.9952,  0.9746,  0.9046,\n",
            "         0.0115,  0.9956, -0.2318,  0.9978,  0.7070,  0.9061, -0.0700,  0.1385,\n",
            "         0.9761, -0.0226,  0.1323,  0.9689,  0.7022, -0.1810, -0.1435,  0.9961,\n",
            "         0.9852, -0.1062, -0.4070,  0.7142,  0.9154, -0.0842,  0.9675, -0.0217,\n",
            "        -0.0907,  0.0993,  0.9850,  0.9911, -0.1200,  0.2773,  0.7056, -0.0349],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 146==== Step 2 Train Loss 0.7102037668228149 ======  0.4931506849315068\n",
            "tensor([ 0.9599, -0.2008, -0.2839, -0.3340, -0.2596,  0.7035,  0.9550,  0.7789,\n",
            "         0.0398,  0.9223,  0.9721,  0.2165,  0.0768,  0.8847, -0.0256,  0.2512,\n",
            "         0.9293, -0.2423,  0.1194,  0.8217,  0.8837, -0.1888, -0.5096,  0.7095,\n",
            "        -0.4614, -0.1710, -0.0337,  0.9285,  0.9129, -0.0185,  0.7921,  0.9569,\n",
            "        -0.0615,  0.9807,  0.0093, -0.0181,  0.9919,  0.6282, -0.0311,  0.9934,\n",
            "        -0.0325, -0.0898,  0.3205,  0.9701,  0.9790,  0.0334,  0.3018, -0.1204,\n",
            "         0.4748, -0.1780, -0.0281,  0.8613,  0.4230,  0.1738,  0.7209,  0.8325,\n",
            "        -0.0557,  0.9518, -0.0293,  0.9928,  0.2544,  0.9622,  0.9619, -0.2549],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 147==== Step 2 Train Loss 0.6778869032859802 ======  0.5797101449275363\n",
            "tensor([ 0.7466,  0.3360,  0.7996, -0.0342,  0.2450,  0.8503,  0.8152,  0.9561,\n",
            "        -0.0737,  0.9917, -0.0298,  0.4531, -0.0256, -0.2918, -0.0431,  0.9340,\n",
            "         0.7212,  0.3118,  0.8056, -0.0211,  0.8577,  0.0169,  0.2267,  0.9921,\n",
            "        -0.0480, -0.1185,  0.9122,  0.8821, -0.5146,  0.0299,  0.9851,  0.4093,\n",
            "         0.9663,  0.9130,  0.9697,  0.8128,  0.9065,  0.8803, -0.0227, -0.0596,\n",
            "         0.9239,  0.9851,  0.9825, -0.3565,  0.9027,  0.9847,  0.5215,  0.9845,\n",
            "         0.9427,  0.1119, -0.0330,  0.8481, -0.3208,  0.9206,  0.6563,  0.8429,\n",
            "         0.3756, -0.0299, -0.0134, -0.0554, -0.0191,  0.9784,  0.9415, -0.2359],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 0 Batch 148==== Step 2 Train Loss 0.6879562735557556 ======  0.6578947368421053\n",
            "tensor([ 0.0582,  0.9976,  0.2060, -0.2116, -0.0599, -0.0917,  0.9628,  0.9930,\n",
            "         0.9923,  0.9871,  0.8592,  0.3554, -0.1142, -0.0096,  0.9409,  0.7006,\n",
            "         0.9787,  0.6616, -0.0152,  0.9968,  0.5354,  0.9754, -0.3531,  0.9556,\n",
            "         0.9097,  0.7835, -0.0207,  0.9583,  0.4028, -0.0203, -0.2039, -0.0891,\n",
            "         0.8898,  0.8663,  0.9365, -0.0671,  0.9970,  0.7998,  0.3933, -0.2563,\n",
            "        -0.0158, -0.2958, -0.5221,  0.9254,  0.9966, -0.0474, -0.0604,  0.9807,\n",
            "         0.9677, -0.1313, -0.4109,  0.0864,  0.9773,  0.9567, -0.0702,  0.8976,\n",
            "        -0.0643, -0.1131,  0.9791, -0.1550,  0.9133, -0.1119,  0.7448,  0.7646],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 149==== Step 2 Train Loss 0.6856188774108887 ======  0.611111111111111\n",
            "tensor([ 0.6407, -0.0586,  0.8011, -0.3797, -0.0981,  0.9957,  0.9439,  0.7822,\n",
            "         0.9790,  0.5089,  0.7356,  0.9484,  0.9404, -0.3755,  0.7861, -0.1246,\n",
            "        -0.3282, -0.0714,  0.7938, -0.0354,  0.9391, -0.4119,  0.3086, -0.0844,\n",
            "        -0.0468,  0.9992,  0.1884, -0.1860,  0.9873,  0.9583, -0.0381, -0.3955,\n",
            "         0.9894,  0.6647,  0.8232,  0.8299,  0.9913,  0.9594,  0.7652,  0.9285,\n",
            "        -0.0844, -0.0667,  0.9528,  0.9677,  0.7853,  0.0345,  0.9931,  0.9474,\n",
            "         0.0930, -0.1514,  0.0156,  0.9232,  0.9449, -0.0425, -0.1206,  0.9809,\n",
            "        -0.0753,  0.7374,  0.8209,  0.9633,  0.7609,  0.9564,  0.4887,  0.8523],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 0 Batch 150==== Step 2 Train Loss 0.6931377053260803 ======  0.5151515151515151\n",
            "========== Epoch 0 ==== Step 2 Train Loss 0.6970478987693787 ======  0.5327408760507172\n",
            "\n",
            "  Training epoch took: 0:02:46\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-df6ed061ed57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelCLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModelFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmytrainStep2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelCLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelEmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-5ccbc1c0040a>\u001b[0m in \u001b[0;36mmytrainStep2\u001b[0;34m(model, criterion1, criterion2, embModel)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           \u001b[0;31m# Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m           \u001b[0mavg_val_loss2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavg_f1_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mavgAcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m           \u001b[0mlistOflossesValid2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_val_loss2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m           \u001b[0;31m# listOfF1Valid.append(avg_val_f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-09e9089665f3>\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(model, epoch, criterion1, validation_dataloader, modelFC, criterion2)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m#   target1 = target1.type(torch.LongTensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mb_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;31m# b_labels = b_labels.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# b_labels = torch.squeeze(b_labels,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension specified as 0 but tensor has no dimensions"
          ]
        }
      ],
      "source": [
        "modelCLS = myModelFC()\n",
        "mytrainStep2(modelCLS,criterion1,criterion2,modelEmb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAByykXEcPrG",
        "outputId": "4e6f3fd8-689f-4477-b036-bc13690da97d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "myModelFC(\n",
              "  (FC): Sequential(\n",
              "    (0): Tanh()\n",
              "    (1): Dropout(p=0.5, inplace=False)\n",
              "    (2): Linear(in_features=3072, out_features=512, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Dropout(p=0.2, inplace=False)\n",
              "    (5): Linear(in_features=512, out_features=64, bias=True)\n",
              "    (6): Linear(in_features=64, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelEmb.load_state_dict(torch.load('checkpointEmb.pt'))\n",
        "modelEmb.cuda()\n",
        "modelCLS.load_state_dict(torch.load('checkpoint.pt'))\n",
        "modelCLS.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bv_dvLaZY2t"
      },
      "source": [
        "# BERT + CNN CL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F00WdGmwZYBk"
      },
      "outputs": [],
      "source": [
        "class myModelEmbeddings(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (False,False)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModelEmbeddings, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      # self.num_unitsFC = num_unitsFC\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      \n",
        "      # for param in self.bertModel.parameters():\n",
        "      #     param.requires_grad = False\n",
        "      # modules = [self.bertModel.embeddings, *self.bertModel.encoder.layer[:4]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      num_filters = [16,16,16]\n",
        "      filter_sizes=[1,2,3] #3,4,5\n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      self.conv1d_list = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=768,\n",
        "                      out_channels=num_filters[i],\n",
        "                      kernel_size=filter_sizes[i])\n",
        "            for i in range(len(filter_sizes))\n",
        "        ])\n",
        "        # Fully-connected layer and Dropout\n",
        "      self.FC = nn.Linear(np.sum(num_filters), sum(num_filters)) #sum(num_filters)\n",
        "      self.dropout = nn.Dropout(0.8)  #0.2\n",
        "      self.dropout3 = nn.Dropout(0.1)\n",
        "      self.FC2 =  nn.Linear(sum(num_filters)*2, 32)\n",
        "      self.relu = nn.ReLU()\n",
        "      self.dropout2 = nn.Dropout(0.1)\n",
        "      self.FC3 =  nn.Linear(32, 2)\n",
        "      self.FC4 =  nn.Linear(64, 2)\n",
        "\n",
        "      self.tanh = nn.Tanh()\n",
        "     \n",
        "     \n",
        "      \n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      \n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][0:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      \n",
        "      return concatEmbeddingLayers\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      embeddings = bertOutputs[0] #last hidden states\n",
        "      return embeddings\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      # `hidden_states` is a Python list.\n",
        "      \n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      del hidden_states\n",
        "\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask, strategy='avg'):\n",
        "      if strategy == 'max':\n",
        "        #  avg_setence_embeddings = torch.mean(token_embeddings,dim=1)\n",
        "        #  print(avg_setence_embeddings.shape)\n",
        "         input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "         max_setence_embeddings = torch.max(token_embeddings, 1)[0]\n",
        "         return max_setence_embeddings\n",
        "      elif strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'sum':\n",
        "        sum_setence_embeddings = torch.sum(token_embeddings[0:len(token_embeddings)],1)\n",
        "        return sum_setence_embeddings\n",
        "\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(sent_id, attention_mask=mask)\n",
        "    \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        return embeddings \n",
        "      else:\n",
        "        embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        # embeddings = self.getCLSEmbeddings(outputs )\n",
        "        return embeddings \n",
        "      \n",
        "  \n",
        "  def forward(self, sent_id1, mask1,sent_id2, mask2,b_labels):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      # forward pass of input 2\n",
        "      output2  = self.forwardOnce(sent_id2, mask2)\n",
        "\n",
        "      # output1 = self.dropout(output1)\n",
        "      # output2 = self.dropout(output2)\n",
        "      \n",
        "      cos = nn.CosineSimilarity(dim=1)\n",
        "      output1 = output1.permute(0, 2, 1)\n",
        "      # output1 = output1.unzqueeze(1)\n",
        "      x_conv_list1 = [torch.tanh(conv1d(output1)) for conv1d in self.conv1d_list]\n",
        "      # Max pooling. Output shape: (b, num_filters[i], 1)\n",
        "      x_pool_list1 = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n",
        "            for x_conv in x_conv_list1]\n",
        "      x_fc1 = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list1],\n",
        "                         dim=1)\n",
        "\n",
        "      output2 = output2.permute(0, 2, 1)\n",
        "      # output2 = output2.unzqueeze(1)\n",
        "      x_conv_list2 = [torch.tanh(conv1d(output2)) for conv1d in self.conv1d_list]\n",
        "      x_pool_list2 = [F.max_pool1d(x_conv2, kernel_size=x_conv2.shape[2])\n",
        "            for x_conv2 in x_conv_list2]\n",
        "      x_fc2 = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list2],\n",
        "                         dim=1)\n",
        "\n",
        "     \n",
        "      logits1 = self.FC(self.dropout(x_fc1))\n",
        "     \n",
        "      logits2 = self.FC(self.dropout(x_fc2))\n",
        "      # out = cos(x_fc1, x_fc2)\n",
        "\n",
        "      # print(out)\n",
        "     \n",
        "      print(b_labels)\n",
        "\n",
        "\n",
        "      concatenated = torch.cat((logits1,logits2),dim=1)\n",
        "      concatenated = self.relu(concatenated)\n",
        "      concatenated = self.dropout3(concatenated)\n",
        "      out = self.FC2(concatenated)\n",
        "      out = self.relu(out)\n",
        "      out = self.dropout2(out)\n",
        "      out = self.FC3(out)\n",
        "      # out = self.FC4(out)\n",
        "      # plt.hist(output.cpu().data.numpy(), bins=6)\n",
        "      # plt.show()\n",
        "      return x_fc1, x_fc2,out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A05UhjRZb2_8"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion1,validation_dataloader,modelFC=None,criterion2 = None):\n",
        "    \n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.bertModel.eval()\n",
        " \n",
        "    model.eval()\n",
        "    if modelFC is not None:\n",
        "        modelFC.eval()\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # tsne = TSNE()\n",
        "    step = 0\n",
        "    concatAll = []\n",
        "    totalF1 = 0\n",
        "    # for (input1,mask1, target1),(input2,mask2, target2) in validation_dataloader:\n",
        "    totalAcc = 0\n",
        "    avg_acc = 0\n",
        "    for input1, mask1, input2, mask2,target1 in validation_dataloader:\n",
        "    # for batch in validation_dataloader:\n",
        "        # if modelFC==None:\n",
        "        step += 1\n",
        "\n",
        "        # if step == 51:\n",
        "        #     break\n",
        "        # else:\n",
        "        #   step += 1\n",
        "\n",
        "        #   if step == 71:\n",
        "        #     break\n",
        "        b_input_ids1 = input1.to(device)\n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        target1 = target1.type(torch.LongTensor)\n",
        "        # if modelFC==None:\n",
        "        #   target1 = target1.type(torch.LongTensor)\n",
        "        #   target1 = torch.where(target1==0,torch.tensor(-1),target1)\n",
        "        #   # target1 = torch.where(target1==1,torch.tensor(2),target1)\n",
        "        #   # target1 = torch.where(target1==-1,torch.tensor(1),target1)\n",
        "        #   # target1 = torch.where(target1==2,torch.tensor(0),target1)\n",
        "        # else:\n",
        "        #   target1 = target1.type(torch.LongTensor)\n",
        "        b_labels = target1.to(device)\n",
        " \n",
        "        # b_labels = b_labels.unsqueeze(1)\n",
        "        # b_labels = torch.squeeze(b_labels,1)\n",
        "        b_input_ids2 = input2.to(device)\n",
        "        b_input_mask2 = mask2.to(device)\n",
        "        # b_input_ids1 = batch[0].permute(0,2,1).squeeze(2).to(device)\n",
        "        # b_input_mask1 = batch[1].permute(0,2,1).squeeze(2).to(device)\n",
        "        # label = batch[4].type(torch.LongTensor)\n",
        "        # b_input_ids2 = batch[2].permute(0,2,1).squeeze(2).to(device)\n",
        "        # b_input_mask2 = batch[3].permute(0,2,1).squeeze(2).to(device)\n",
        "        # # label = label.type(torch.LongTensor)\n",
        "        # b_labels = label.to(device)\n",
        "        with torch.no_grad():        \n",
        "            if modelFC==None:\n",
        "\n",
        "              FC11,FC22,cos = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)             \n",
        "              \n",
        "              pos_indices = torch.where(b_labels==1)[0]\n",
        "              neg_indices = torch.where(b_labels==0)[0]\n",
        "              # print(pos_indices)\n",
        "              # print(neg_indices)\n",
        "              indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "              # loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # loss1 = criterion1(FC11,FC22,b_labels)\n",
        "              loss1 = criterion1(cos,b_labels)\n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "              \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2)\n",
        "              FC11,FC22,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels,h)\n",
        "              out = modelFC(FC11,FC22)\n",
        "              loss2 = criterion2(out,b_labels)\n",
        "            if modelFC == None:\n",
        "                total_eval_loss += loss1.item()\n",
        "                f1 = calcF1score(cos,b_labels)\n",
        "                totalF1 += f1\n",
        "                probs = F.softmax(cos, dim=1).cpu().data.numpy()\n",
        "               \n",
        "                accuracy = calcAccuracy(probs,b_labels)\n",
        "                totalAcc+=accuracy\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                print(probs) \n",
        "                \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 AVG. val Loss \"+str(loss1.item()), \"==== \",str(f1),\"==== \",str(accuracy))\n",
        "                # f1 = calcF1score(cos,b_labels)\n",
        "                # totalF1 += f1\n",
        "                # probs = F.softmax(cos, dim=1).cpu().numpy()\n",
        "                # print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                # print(probs) \n",
        "                # print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 AVG. val Loss \"+str(loss1.item()), \"==== \",str(f1))\n",
        "                # label_ids = b_labels.to('cpu').numpy()\n",
        "                \n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "            \n",
        "                total_eval_loss += loss2.item()\n",
        "                f1 = calcF1score(out,b_labels)\n",
        "                totalF1 += f1\n",
        "                probs = F.softmax(out, dim=1).cpu().numpy()\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                print(probs) \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 AVG. val Loss \"+str(loss2.item()), \"==== \",str(f1))\n",
        "                modelFC.train()\n",
        "    if modelFC==None:\n",
        "      avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "      avg_f1_val = totalF1/len(validation_dataloader)\n",
        "      avg_acc = totalAcc/len(validation_dataloader)\n",
        "    else:\n",
        "       avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "       avg_f1_val = totalF1/len(validation_dataloader)\n",
        "    \n",
        "    \n",
        "    if modelFC==None:\n",
        "       model.bertModel.train()\n",
        "       model.train()\n",
        "    else:\n",
        "       model.bertModel.eval()\n",
        "       model.eval()\n",
        "       modelFC.train()  \n",
        "    return  avg_val_loss,avg_f1_val,avg_acc #avg_val_accuracy, avg_val_f1,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lqEFDnObuhB"
      },
      "outputs": [],
      "source": [
        "def mytrainStep1(model,criterion1,criterion2):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      # reducers = criterion1.reducer.reducers\n",
        "    \n",
        "      if torch.cuda.is_available():\n",
        "          model.to(device)\n",
        "      # for param in model.bertModel.parameters():\n",
        "      #     param.requires_grad = False\n",
        "      # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[7:]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False    \n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      # optimizer = torch.optim.Adam(model.parameters(),\n",
        "      #                               lr=2e-5)\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=1e-5,\n",
        "                                    #weight_decay=0.01, #5e-5, 3e-5, 2e-5\n",
        "                                    correct_bias=False) #eps=1e-8,len(train_dataloader)\n",
        "      # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "                              \n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=4*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid = list()\n",
        "      listOfF1Valid = list()\n",
        "      epoch_stop = 0\n",
        "      model.bertModel.train()\n",
        "      model.train()\n",
        "      for epoch_i in range(0, 10):\n",
        "          \n",
        "          # if epoch_i > 0:\n",
        "\n",
        "          #     for param in model.bertModel.parameters():\n",
        "          #         param.requires_grad = False\n",
        "          # for param in model.bertModel.parameters():\n",
        "          #     param.requires_grad = False\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss = 0\n",
        "          # model.bertModel.train()\n",
        "          \n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          # for (input1,mask1, target1),(input2,mask2, target2) in train_dataloader:\n",
        "          accum_iter = 4\n",
        "          totalF1 = 0\n",
        "          for batch_idx,(input1, mask1, input2, mask2,target1) in  enumerate(train_dataloader):\n",
        "              step +=1\n",
        "              # if step ==151 :\n",
        "              #   break\n",
        "              # # Progress update every 40 batches.\n",
        "              # h = model.init_hidden(target1.size(0))\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # if torch.cuda.is_available():\n",
        "              b_input_ids1 = input1.to(device)\n",
        "              b_input_mask1 = mask1.to(device)\n",
        "              target1 = target1.type(torch.LongTensor)\n",
        "              # target1 = torch.where(target1==0,torch.tensor(-1),target1)\n",
        "            \n",
        "              b_labels = target1.to(device)\n",
        "           \n",
        "              # b_labels = b_labels.unsqueeze(1)\n",
        "              b_input_ids2 = input2.to(device)\n",
        "              b_input_mask2 = mask2.to(device)\n",
        "              # b_labels=b_labels.reshape(-1,1)\n",
        "              # b_labels = torch.squeeze(b_labels,1)\n",
        "              model.zero_grad()\n",
        "              # with torch.set_grad_enabled(True):\n",
        "                  #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              # print(a1)\n",
        "              # print(a2)\n",
        "              FC11,FC22,cos = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)\n",
        "                  # loss = criterion1(out1,out2,b_labels)\n",
        "              pos_indices = torch.where(b_labels==1)[0]\n",
        "              neg_indices = torch.where(b_labels==0)[0]\n",
        "                  # print(pos_indices)\n",
        "                  # print(neg_indices)\n",
        "              indices_tuple = (pos_indices,pos_indices,neg_indices,neg_indices)\n",
        "                  \n",
        "              loss1 = criterion1(cos,b_labels)   \n",
        "              # loss1 = criterion1(FC11,FC22,b_labels)\n",
        "              \n",
        "              # loss1 = criterion1(FC11,b_labels,indices_tuple,ref_emb=FC22, ref_labels=b_labels)\n",
        "              # loss1 = loss1 / accum_iter\n",
        "     \n",
        "          \n",
        "              total_train_loss += loss1.item()\n",
        "              f1 = calcF1score(cos,b_labels)\n",
        "              totalF1 += f1\n",
        "              probs = F.softmax(cos, dim=1).cpu().data.numpy()\n",
        "               \n",
        "              accuracy = calcAccuracy(probs,b_labels)\n",
        "              # probs = F.softmax(cos, dim=1).cpu().data.numpy()\n",
        "              # print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "              # print(probs) \n",
        "                  # print(loss.data[0])\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 1  Train Loss \"+str(loss1.item()),\" ====\",str(f1),\" ====\",str(accuracy)) #\n",
        "              # decay = 0.0001\n",
        "              # for param in model.parameters():\n",
        "              #     loss1 += decay * torch.norm(param, p='fro')\n",
        "              loss1.backward()\n",
        "              # if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_dataloader)):\n",
        "                      # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "              optimizer.step()\n",
        "                  # optimizer.zero_grad()\n",
        "              # scheduler.step()\n",
        "                      \n",
        "          avg_train_loss = total_train_loss /len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 1 AVG. Train Loss \"+str(avg_train_loss))            \n",
        "          listOflossesTrain.append(avg_train_loss)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss,avgf1,avgAcc = validation(model,epoch_i,criterion1,validation_dataloader)\n",
        "          listOflossesValid.append(avg_val_loss)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "          print(\"  Average Validation F1: {0:.2f}\".format(avgf1))\n",
        "          print(\"  Average Validation Acc: {0:.2f}\".format(avgAcc))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "       \n",
        "          early_stopping1(avg_val_loss, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping1.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              # break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain,listOflossesValid,10)\n",
        "\n",
        "      # mytrainStep2(modelCLS,criterion1,criterion2,model)\n",
        "      # torch.save(model.state_dict(), 'checkPointEmblstm.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TyZNiaEb7-B"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning.reducers import MeanReducer\n",
        "modelEmb = myModelEmbeddings(bert_emb_layer=4,startLayer=4,endLayer=5,bertModel=bertModel)\n",
        "\n",
        "# criterion1 = nn.CosineEmbeddingLoss(margin = 0.2)#margin = 0.5\n",
        "# func = distances.LpDistance\n",
        "# criterion1 = ContrastiveLossSiamese(margin1 = 0.9)\n",
        "# criterion1 = getLossFunction(device)\n",
        "# criterion1 =nn.BCELoss()\n",
        "#reducer=MeanReducer()\n",
        "criterion1 = nn.CrossEntropyLoss()\n",
        "# criterion1 = losses.ContrastiveLoss(distance = distances.CosineSimilarity(),reducer=MeanReducer(),pos_margin=1, neg_margin=0) #distance = distances.CosineSimilarity()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "early_stopping1 = EarlyStopping(patience=4, path='checkpointEmb.pt',verbose=True)\n",
        "early_stopping2 = EarlyStopping(patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGLcULNPcCh_",
        "outputId": "777ed7e7-5b89-45cc-e366-6e54e7a08414"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
            "Running Validation...\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 1==== Step 2 Probs\n",
            "[[0.46711457 0.53288543]\n",
            " [0.46428934 0.53571063]\n",
            " [0.4629332  0.5370668 ]\n",
            " [0.46136478 0.5386352 ]\n",
            " [0.46923766 0.5307623 ]\n",
            " [0.4680414  0.5319586 ]\n",
            " [0.46487445 0.53512555]\n",
            " [0.46091446 0.5390855 ]\n",
            " [0.4676137  0.5323863 ]\n",
            " [0.46820498 0.531795  ]\n",
            " [0.46499133 0.5350087 ]\n",
            " [0.4604405  0.5395595 ]\n",
            " [0.46035358 0.5396464 ]\n",
            " [0.46943548 0.5305645 ]\n",
            " [0.46681756 0.5331825 ]\n",
            " [0.46713597 0.53286403]\n",
            " [0.46716908 0.5328309 ]\n",
            " [0.4638913  0.53610873]\n",
            " [0.46958846 0.53041154]\n",
            " [0.46234202 0.5376579 ]\n",
            " [0.4701561  0.52984387]\n",
            " [0.46787044 0.5321296 ]\n",
            " [0.46204218 0.53795785]\n",
            " [0.4628085  0.53719145]\n",
            " [0.47076908 0.52923095]\n",
            " [0.46700692 0.53299314]\n",
            " [0.46749854 0.53250146]\n",
            " [0.4534919  0.5465081 ]\n",
            " [0.46621504 0.533785  ]\n",
            " [0.46644437 0.53355557]\n",
            " [0.46658778 0.5334122 ]\n",
            " [0.46924183 0.5307582 ]]\n",
            "========== Epoch 5 Batch 1==== Step 1 AVG. val Loss 0.6989235877990723 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 2==== Step 2 Probs\n",
            "[[0.46227247 0.53772753]\n",
            " [0.46901023 0.53098977]\n",
            " [0.46640635 0.53359365]\n",
            " [0.46439174 0.53560823]\n",
            " [0.47572777 0.52427226]\n",
            " [0.46392104 0.53607893]\n",
            " [0.46828774 0.53171223]\n",
            " [0.46247867 0.53752136]\n",
            " [0.46721193 0.53278804]\n",
            " [0.4684467  0.5315533 ]\n",
            " [0.46569577 0.53430426]\n",
            " [0.47070837 0.5292916 ]\n",
            " [0.46638522 0.5336148 ]\n",
            " [0.46446458 0.5355354 ]\n",
            " [0.46666133 0.5333386 ]\n",
            " [0.46020612 0.5397939 ]\n",
            " [0.47069725 0.5293028 ]\n",
            " [0.46383077 0.53616923]\n",
            " [0.4688066  0.5311934 ]\n",
            " [0.4661062  0.53389376]\n",
            " [0.4686205  0.53137946]\n",
            " [0.46050593 0.5394941 ]\n",
            " [0.46947357 0.53052646]\n",
            " [0.4660251  0.5339749 ]\n",
            " [0.46828496 0.531715  ]\n",
            " [0.4649207  0.5350793 ]\n",
            " [0.4651604  0.5348396 ]\n",
            " [0.46201777 0.5379822 ]\n",
            " [0.46706462 0.5329354 ]\n",
            " [0.46409318 0.5359068 ]\n",
            " [0.46653977 0.53346026]\n",
            " [0.47306195 0.526938  ]]\n",
            "========== Epoch 5 Batch 2==== Step 1 AVG. val Loss 0.6859338283538818 ====  0.72 ====  0.5625\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 3==== Step 2 Probs\n",
            "[[0.4572981  0.54270196]\n",
            " [0.46666917 0.5333308 ]\n",
            " [0.4749368  0.5250632 ]\n",
            " [0.47212484 0.5278751 ]\n",
            " [0.47143778 0.52856225]\n",
            " [0.47202584 0.5279741 ]\n",
            " [0.46980003 0.5302    ]\n",
            " [0.46411481 0.53588516]\n",
            " [0.47097427 0.52902573]\n",
            " [0.45837736 0.54162264]\n",
            " [0.46908376 0.5309162 ]\n",
            " [0.46205243 0.53794754]\n",
            " [0.46381226 0.53618777]\n",
            " [0.46034962 0.5396504 ]\n",
            " [0.46770477 0.5322952 ]\n",
            " [0.4606385  0.5393615 ]\n",
            " [0.46402723 0.5359727 ]\n",
            " [0.46523654 0.53476346]\n",
            " [0.46071222 0.5392878 ]\n",
            " [0.4643836  0.53561634]\n",
            " [0.46204668 0.5379533 ]\n",
            " [0.47130957 0.52869046]\n",
            " [0.46633515 0.5336648 ]\n",
            " [0.47596142 0.52403855]\n",
            " [0.46764696 0.5323531 ]\n",
            " [0.4627116  0.53728837]\n",
            " [0.47152293 0.5284771 ]\n",
            " [0.46521038 0.5347896 ]\n",
            " [0.46067533 0.5393247 ]\n",
            " [0.46590403 0.53409594]\n",
            " [0.46351668 0.5364833 ]\n",
            " [0.46487454 0.53512543]]\n",
            "========== Epoch 5 Batch 3==== Step 1 AVG. val Loss 0.7029786109924316 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 4==== Step 2 Probs\n",
            "[[0.47066653 0.5293335 ]\n",
            " [0.4687713  0.5312287 ]\n",
            " [0.46406725 0.5359327 ]\n",
            " [0.47375268 0.5262473 ]\n",
            " [0.4678049  0.5321951 ]\n",
            " [0.465035   0.53496504]\n",
            " [0.46291003 0.53709   ]\n",
            " [0.46215782 0.5378422 ]\n",
            " [0.46852323 0.53147674]\n",
            " [0.46411756 0.5358824 ]\n",
            " [0.46536648 0.5346336 ]\n",
            " [0.46446124 0.5355388 ]\n",
            " [0.4614097  0.5385903 ]\n",
            " [0.47070658 0.5292935 ]\n",
            " [0.46213505 0.537865  ]\n",
            " [0.46080637 0.5391937 ]\n",
            " [0.46699378 0.53300625]\n",
            " [0.46838337 0.5316166 ]\n",
            " [0.4679533  0.5320467 ]\n",
            " [0.46568862 0.5343114 ]\n",
            " [0.4763006  0.5236994 ]\n",
            " [0.46418807 0.5358119 ]\n",
            " [0.46807504 0.53192496]\n",
            " [0.46986818 0.5301318 ]\n",
            " [0.46271318 0.5372868 ]\n",
            " [0.46314535 0.5368547 ]\n",
            " [0.4639729  0.53602713]\n",
            " [0.4608406  0.5391594 ]\n",
            " [0.46383032 0.5361697 ]\n",
            " [0.46804672 0.5319533 ]\n",
            " [0.46374825 0.5362517 ]\n",
            " [0.46358582 0.5364142 ]]\n",
            "========== Epoch 5 Batch 4==== Step 1 AVG. val Loss 0.699624240398407 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 5==== Step 2 Probs\n",
            "[[0.46802843 0.5319716 ]\n",
            " [0.46569213 0.5343079 ]\n",
            " [0.46289924 0.53710073]\n",
            " [0.46732318 0.5326768 ]\n",
            " [0.46575934 0.53424066]\n",
            " [0.4625117  0.5374883 ]\n",
            " [0.46630105 0.5336989 ]\n",
            " [0.46937165 0.5306283 ]\n",
            " [0.46115437 0.53884566]\n",
            " [0.46477342 0.5352266 ]\n",
            " [0.47025418 0.5297458 ]\n",
            " [0.46797702 0.53202295]\n",
            " [0.4696445  0.5303555 ]\n",
            " [0.46744716 0.53255284]\n",
            " [0.46446058 0.5355394 ]\n",
            " [0.4672111  0.53278893]\n",
            " [0.46185818 0.5381418 ]\n",
            " [0.46648297 0.53351706]\n",
            " [0.46664283 0.5333572 ]\n",
            " [0.4639079  0.53609216]\n",
            " [0.4617409  0.5382591 ]\n",
            " [0.46498105 0.5350189 ]\n",
            " [0.46460438 0.5353956 ]\n",
            " [0.46661988 0.53338015]\n",
            " [0.46065903 0.539341  ]\n",
            " [0.47411105 0.5258889 ]\n",
            " [0.45951074 0.54048926]\n",
            " [0.46728453 0.53271544]\n",
            " [0.4689518  0.53104824]\n",
            " [0.46658793 0.5334121 ]\n",
            " [0.46688116 0.53311884]\n",
            " [0.46811223 0.53188777]]\n",
            "========== Epoch 5 Batch 5==== Step 1 AVG. val Loss 0.6951714754104614 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 6==== Step 2 Probs\n",
            "[[0.46927306 0.5307269 ]\n",
            " [0.4693878  0.53061223]\n",
            " [0.46735728 0.5326428 ]\n",
            " [0.46618563 0.5338144 ]\n",
            " [0.4670539  0.5329461 ]\n",
            " [0.46292457 0.53707546]\n",
            " [0.46579394 0.5342061 ]\n",
            " [0.46232218 0.53767776]\n",
            " [0.46076733 0.5392327 ]\n",
            " [0.46092182 0.5390782 ]\n",
            " [0.46512347 0.5348765 ]\n",
            " [0.46018657 0.53981346]\n",
            " [0.4668007  0.5331993 ]\n",
            " [0.4578475  0.5421525 ]\n",
            " [0.4617374  0.5382626 ]\n",
            " [0.4697536  0.53024644]\n",
            " [0.46060154 0.53939843]\n",
            " [0.46243352 0.5375665 ]\n",
            " [0.46436775 0.5356322 ]\n",
            " [0.46501964 0.53498036]\n",
            " [0.4658489  0.5341511 ]\n",
            " [0.46883947 0.53116053]\n",
            " [0.46005082 0.5399492 ]\n",
            " [0.4577703  0.5422297 ]\n",
            " [0.4738617  0.5261383 ]\n",
            " [0.4674604  0.53253955]\n",
            " [0.4609877  0.5390123 ]\n",
            " [0.469522   0.530478  ]\n",
            " [0.46600848 0.5339915 ]\n",
            " [0.46573192 0.5342681 ]\n",
            " [0.46511176 0.5348882 ]\n",
            " [0.46179622 0.5382038 ]]\n",
            "========== Epoch 5 Batch 6==== Step 1 AVG. val Loss 0.719203770160675 ====  0.5116279069767442 ====  0.34375\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 7==== Step 2 Probs\n",
            "[[0.46485358 0.5351464 ]\n",
            " [0.46564063 0.5343594 ]\n",
            " [0.47048816 0.5295118 ]\n",
            " [0.46033248 0.5396675 ]\n",
            " [0.4605773  0.5394227 ]\n",
            " [0.46688804 0.53311193]\n",
            " [0.46421045 0.53578955]\n",
            " [0.46593705 0.534063  ]\n",
            " [0.46822426 0.5317757 ]\n",
            " [0.46862882 0.5313712 ]\n",
            " [0.46405435 0.5359457 ]\n",
            " [0.46453685 0.53546315]\n",
            " [0.46193537 0.5380646 ]\n",
            " [0.4727139  0.5272862 ]\n",
            " [0.46968928 0.53031075]\n",
            " [0.4688721  0.5311279 ]\n",
            " [0.4708908  0.52910924]\n",
            " [0.4659648  0.5340352 ]\n",
            " [0.46468648 0.53531355]\n",
            " [0.4684922  0.5315078 ]\n",
            " [0.46404865 0.5359513 ]\n",
            " [0.46749339 0.5325066 ]\n",
            " [0.4688818  0.5311182 ]\n",
            " [0.45654312 0.54345685]\n",
            " [0.4713807  0.5286193 ]\n",
            " [0.47082064 0.5291794 ]\n",
            " [0.46582767 0.5341723 ]\n",
            " [0.45942742 0.54057264]\n",
            " [0.46658456 0.5334154 ]\n",
            " [0.4680063  0.5319937 ]\n",
            " [0.4612998  0.53870016]\n",
            " [0.4752004  0.5247996 ]]\n",
            "========== Epoch 5 Batch 7==== Step 1 AVG. val Loss 0.6815114617347717 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 8==== Step 2 Probs\n",
            "[[0.4583688  0.54163116]\n",
            " [0.46482873 0.53517133]\n",
            " [0.46362588 0.5363741 ]\n",
            " [0.46372893 0.5362711 ]\n",
            " [0.46358216 0.5364179 ]\n",
            " [0.46549687 0.53450316]\n",
            " [0.46890473 0.53109527]\n",
            " [0.46203446 0.5379656 ]\n",
            " [0.4692177  0.53078234]\n",
            " [0.46598312 0.5340169 ]\n",
            " [0.46757728 0.53242266]\n",
            " [0.46895453 0.53104544]\n",
            " [0.46658686 0.5334132 ]\n",
            " [0.46132106 0.53867894]\n",
            " [0.46187842 0.53812164]\n",
            " [0.4632429  0.53675705]\n",
            " [0.46836898 0.531631  ]\n",
            " [0.46287212 0.5371279 ]\n",
            " [0.45887724 0.5411228 ]\n",
            " [0.46666566 0.5333344 ]\n",
            " [0.4650179  0.5349821 ]\n",
            " [0.46278805 0.53721195]\n",
            " [0.47053638 0.5294636 ]\n",
            " [0.4620951  0.5379049 ]\n",
            " [0.46554026 0.5344597 ]\n",
            " [0.47055358 0.5294464 ]\n",
            " [0.4718208  0.5281792 ]\n",
            " [0.46555117 0.53444886]\n",
            " [0.46509278 0.53490716]\n",
            " [0.46208897 0.537911  ]\n",
            " [0.46571374 0.53428626]\n",
            " [0.473928   0.52607197]]\n",
            "========== Epoch 5 Batch 8==== Step 1 AVG. val Loss 0.6841716766357422 ====  0.72 ====  0.5625\n",
            "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 9==== Step 2 Probs\n",
            "[[0.46403456 0.5359655 ]\n",
            " [0.46822172 0.5317783 ]\n",
            " [0.47458044 0.52541953]\n",
            " [0.46446443 0.5355356 ]\n",
            " [0.46667808 0.53332186]\n",
            " [0.4664979  0.5335021 ]\n",
            " [0.463056   0.53694403]\n",
            " [0.4705331  0.5294669 ]\n",
            " [0.46667463 0.5333254 ]\n",
            " [0.46681836 0.5331816 ]\n",
            " [0.46927762 0.5307224 ]\n",
            " [0.46137914 0.5386209 ]\n",
            " [0.46610525 0.5338948 ]\n",
            " [0.46476513 0.53523487]\n",
            " [0.4583241  0.54167587]\n",
            " [0.46799582 0.5320042 ]\n",
            " [0.46806502 0.531935  ]\n",
            " [0.4664067  0.5335933 ]\n",
            " [0.47398597 0.52601403]\n",
            " [0.46490723 0.5350928 ]\n",
            " [0.4694495  0.53055054]\n",
            " [0.4643371  0.5356629 ]\n",
            " [0.47148645 0.52851355]\n",
            " [0.4663364  0.53366363]\n",
            " [0.46261612 0.53738385]\n",
            " [0.4564279  0.5435721 ]\n",
            " [0.46480292 0.5351971 ]\n",
            " [0.4725549  0.5274451 ]\n",
            " [0.4666534  0.53334665]\n",
            " [0.46543145 0.53456855]\n",
            " [0.467325   0.53267497]\n",
            " [0.46237227 0.5376277 ]]\n",
            "========== Epoch 5 Batch 9==== Step 1 AVG. val Loss 0.7046122550964355 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 10==== Step 2 Probs\n",
            "[[0.46277332 0.5372267 ]\n",
            " [0.46590036 0.5340997 ]\n",
            " [0.46680734 0.53319263]\n",
            " [0.47020963 0.52979034]\n",
            " [0.47062045 0.52937955]\n",
            " [0.46132106 0.53867894]\n",
            " [0.46313554 0.53686446]\n",
            " [0.46056852 0.53943145]\n",
            " [0.46553162 0.5344684 ]\n",
            " [0.4659474  0.5340526 ]\n",
            " [0.45983654 0.54016346]\n",
            " [0.46540976 0.53459024]\n",
            " [0.4705021  0.5294979 ]\n",
            " [0.46704885 0.5329512 ]\n",
            " [0.47057673 0.5294233 ]\n",
            " [0.46412262 0.5358774 ]\n",
            " [0.47392955 0.5260705 ]\n",
            " [0.4695054  0.5304946 ]\n",
            " [0.4624202  0.5375798 ]\n",
            " [0.46671832 0.53328174]\n",
            " [0.46536776 0.53463227]\n",
            " [0.4697918  0.53020823]\n",
            " [0.46726412 0.5327359 ]\n",
            " [0.46647605 0.533524  ]\n",
            " [0.47041443 0.5295856 ]\n",
            " [0.4685362  0.53146374]\n",
            " [0.46552107 0.53447896]\n",
            " [0.46555537 0.53444463]\n",
            " [0.463613   0.536387  ]\n",
            " [0.4666196  0.5333804 ]\n",
            " [0.4656054  0.5343946 ]\n",
            " [0.4633943  0.5366057 ]]\n",
            "========== Epoch 5 Batch 10==== Step 1 AVG. val Loss 0.7127151489257812 ====  0.5454545454545454 ====  0.375\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 11==== Step 2 Probs\n",
            "[[0.4771382  0.52286184]\n",
            " [0.4666619  0.53333807]\n",
            " [0.46715087 0.53284913]\n",
            " [0.46944022 0.5305597 ]\n",
            " [0.46454135 0.5354587 ]\n",
            " [0.46787435 0.53212565]\n",
            " [0.4609393  0.5390608 ]\n",
            " [0.46788523 0.5321148 ]\n",
            " [0.4632929  0.53670716]\n",
            " [0.4636579  0.53634214]\n",
            " [0.46764216 0.5323579 ]\n",
            " [0.46912253 0.5308775 ]\n",
            " [0.46912214 0.5308778 ]\n",
            " [0.46070847 0.53929156]\n",
            " [0.46245235 0.5375477 ]\n",
            " [0.46124342 0.53875655]\n",
            " [0.46541077 0.5345893 ]\n",
            " [0.46435967 0.53564036]\n",
            " [0.46489212 0.5351079 ]\n",
            " [0.4669856  0.53301436]\n",
            " [0.46881846 0.5311815 ]\n",
            " [0.47212824 0.5278718 ]\n",
            " [0.4635497  0.53645027]\n",
            " [0.47237512 0.52762485]\n",
            " [0.46589044 0.53410953]\n",
            " [0.46304375 0.5369562 ]\n",
            " [0.46570396 0.53429604]\n",
            " [0.4619027  0.5380973 ]\n",
            " [0.4603955  0.5396045 ]\n",
            " [0.46193537 0.5380646 ]\n",
            " [0.46849453 0.53150547]\n",
            " [0.4689059  0.5310941 ]]\n",
            "========== Epoch 5 Batch 11==== Step 1 AVG. val Loss 0.702068030834198 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 12==== Step 2 Probs\n",
            "[[0.46140376 0.5385963 ]\n",
            " [0.46259075 0.53740925]\n",
            " [0.46201858 0.53798145]\n",
            " [0.46707612 0.5329239 ]\n",
            " [0.46869817 0.53130186]\n",
            " [0.468582   0.531418  ]\n",
            " [0.4671176  0.5328824 ]\n",
            " [0.46843114 0.5315688 ]\n",
            " [0.4633871  0.53661287]\n",
            " [0.4653497  0.5346503 ]\n",
            " [0.46682215 0.5331778 ]\n",
            " [0.46583745 0.5341626 ]\n",
            " [0.46472293 0.535277  ]\n",
            " [0.47139293 0.52860713]\n",
            " [0.46370283 0.53629714]\n",
            " [0.46608847 0.5339115 ]\n",
            " [0.46584573 0.53415424]\n",
            " [0.4673101  0.5326899 ]\n",
            " [0.46940088 0.5305991 ]\n",
            " [0.46690646 0.5330936 ]\n",
            " [0.4685115  0.5314885 ]\n",
            " [0.46940023 0.5305998 ]\n",
            " [0.46441767 0.53558236]\n",
            " [0.46454915 0.5354509 ]\n",
            " [0.46361247 0.5363875 ]\n",
            " [0.4653373  0.5346627 ]\n",
            " [0.47482428 0.5251757 ]\n",
            " [0.4633832  0.53661674]\n",
            " [0.46425578 0.53574425]\n",
            " [0.46680954 0.5331904 ]\n",
            " [0.4669171  0.5330829 ]\n",
            " [0.4689629  0.5310371 ]]\n",
            "========== Epoch 5 Batch 12==== Step 1 AVG. val Loss 0.6829965114593506 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 13==== Step 2 Probs\n",
            "[[0.46094155 0.53905845]\n",
            " [0.46372464 0.5362754 ]\n",
            " [0.46439517 0.53560483]\n",
            " [0.46618912 0.53381085]\n",
            " [0.47139087 0.5286091 ]\n",
            " [0.46778023 0.5322197 ]\n",
            " [0.46465987 0.5353401 ]\n",
            " [0.47110704 0.52889293]\n",
            " [0.46606445 0.53393555]\n",
            " [0.46793434 0.5320657 ]\n",
            " [0.46511543 0.5348846 ]\n",
            " [0.4664492  0.5335508 ]\n",
            " [0.46556866 0.5344313 ]\n",
            " [0.46914154 0.53085846]\n",
            " [0.4681298  0.5318702 ]\n",
            " [0.46429572 0.5357043 ]\n",
            " [0.46499935 0.5350007 ]\n",
            " [0.46812275 0.5318772 ]\n",
            " [0.46931368 0.5306863 ]\n",
            " [0.46375877 0.53624123]\n",
            " [0.466708   0.533292  ]\n",
            " [0.46428266 0.5357173 ]\n",
            " [0.46396124 0.53603876]\n",
            " [0.47004423 0.52995574]\n",
            " [0.46481103 0.535189  ]\n",
            " [0.46184325 0.5381567 ]\n",
            " [0.45725834 0.5427417 ]\n",
            " [0.4662476  0.5337524 ]\n",
            " [0.4654631  0.5345369 ]\n",
            " [0.46603894 0.53396106]\n",
            " [0.46233267 0.53766733]\n",
            " [0.46583357 0.5341664 ]]\n",
            "========== Epoch 5 Batch 13==== Step 1 AVG. val Loss 0.6896271705627441 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 14==== Step 2 Probs\n",
            "[[0.46575946 0.53424054]\n",
            " [0.46260676 0.5373932 ]\n",
            " [0.46716908 0.53283095]\n",
            " [0.4680179  0.5319821 ]\n",
            " [0.46882737 0.5311726 ]\n",
            " [0.4701977  0.52980226]\n",
            " [0.46780026 0.53219974]\n",
            " [0.46209404 0.537906  ]\n",
            " [0.46214673 0.53785324]\n",
            " [0.47014934 0.52985066]\n",
            " [0.46819088 0.5318091 ]\n",
            " [0.4583983  0.54160166]\n",
            " [0.4655144  0.5344856 ]\n",
            " [0.46777698 0.532223  ]\n",
            " [0.46233064 0.53766936]\n",
            " [0.46443462 0.5355654 ]\n",
            " [0.46635267 0.5336473 ]\n",
            " [0.4607887  0.53921133]\n",
            " [0.46918848 0.5308115 ]\n",
            " [0.4641018  0.5358982 ]\n",
            " [0.46736187 0.5326382 ]\n",
            " [0.4665554  0.5334446 ]\n",
            " [0.46256882 0.53743124]\n",
            " [0.47170833 0.52829164]\n",
            " [0.4639829  0.5360171 ]\n",
            " [0.47165605 0.5283439 ]\n",
            " [0.46880123 0.53119874]\n",
            " [0.46388796 0.53611207]\n",
            " [0.4669087  0.53309137]\n",
            " [0.46788582 0.53211415]\n",
            " [0.464894   0.53510606]\n",
            " [0.4620951  0.5379049 ]]\n",
            "========== Epoch 5 Batch 14==== Step 1 AVG. val Loss 0.7045573592185974 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 15==== Step 2 Probs\n",
            "[[0.4596922  0.54030776]\n",
            " [0.46114677 0.5388532 ]\n",
            " [0.46672758 0.5332724 ]\n",
            " [0.46574777 0.5342522 ]\n",
            " [0.47056085 0.52943915]\n",
            " [0.46389803 0.53610194]\n",
            " [0.46508873 0.5349113 ]\n",
            " [0.46799335 0.5320066 ]\n",
            " [0.4672367  0.5327633 ]\n",
            " [0.47344613 0.5265538 ]\n",
            " [0.469149   0.530851  ]\n",
            " [0.46418625 0.53581375]\n",
            " [0.46831128 0.5316887 ]\n",
            " [0.46884376 0.53115624]\n",
            " [0.46234974 0.5376502 ]\n",
            " [0.46420497 0.53579503]\n",
            " [0.46196553 0.53803444]\n",
            " [0.45550796 0.54449207]\n",
            " [0.47057736 0.5294226 ]\n",
            " [0.46469963 0.5353003 ]\n",
            " [0.46252626 0.5374738 ]\n",
            " [0.46802035 0.5319797 ]\n",
            " [0.4639436  0.5360564 ]\n",
            " [0.46624374 0.53375626]\n",
            " [0.46697617 0.5330238 ]\n",
            " [0.47118312 0.52881694]\n",
            " [0.46849796 0.53150207]\n",
            " [0.46469513 0.5353049 ]\n",
            " [0.4657123  0.5342877 ]\n",
            " [0.46555993 0.5344401 ]\n",
            " [0.46908933 0.5309107 ]\n",
            " [0.46364364 0.53635633]]\n",
            "========== Epoch 5 Batch 15==== Step 1 AVG. val Loss 0.6830877661705017 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 16==== Step 2 Probs\n",
            "[[0.45913193 0.54086804]\n",
            " [0.46805078 0.5319492 ]\n",
            " [0.46353316 0.5364669 ]\n",
            " [0.46564907 0.53435093]\n",
            " [0.47070363 0.5292964 ]\n",
            " [0.46942163 0.53057843]\n",
            " [0.46701437 0.53298557]\n",
            " [0.4688703  0.53112966]\n",
            " [0.46554026 0.5344597 ]\n",
            " [0.46495372 0.5350463 ]\n",
            " [0.46446145 0.53553855]\n",
            " [0.46234956 0.5376504 ]\n",
            " [0.46661067 0.53338933]\n",
            " [0.46961588 0.5303841 ]\n",
            " [0.4620564  0.53794354]\n",
            " [0.46809334 0.53190666]\n",
            " [0.47258964 0.5274103 ]\n",
            " [0.4631409  0.5368591 ]\n",
            " [0.46568814 0.53431183]\n",
            " [0.46204418 0.5379558 ]\n",
            " [0.47105828 0.5289417 ]\n",
            " [0.4587021  0.541298  ]\n",
            " [0.46429795 0.53570205]\n",
            " [0.4649339  0.5350661 ]\n",
            " [0.46885967 0.5311403 ]\n",
            " [0.46748757 0.53251237]\n",
            " [0.4677407  0.5322593 ]\n",
            " [0.46012327 0.53987676]\n",
            " [0.46017632 0.5398237 ]\n",
            " [0.4691914  0.53080857]\n",
            " [0.46729344 0.53270656]\n",
            " [0.46513605 0.53486395]]\n",
            "========== Epoch 5 Batch 16==== Step 1 AVG. val Loss 0.6963944435119629 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 17==== Step 2 Probs\n",
            "[[0.46836936 0.53163064]\n",
            " [0.4684469  0.5315531 ]\n",
            " [0.46772298 0.532277  ]\n",
            " [0.4716956  0.52830446]\n",
            " [0.46652275 0.5334772 ]\n",
            " [0.4738617  0.5261383 ]\n",
            " [0.46502304 0.53497696]\n",
            " [0.46747553 0.53252447]\n",
            " [0.46601614 0.5339838 ]\n",
            " [0.46606776 0.53393227]\n",
            " [0.4666528  0.5333472 ]\n",
            " [0.46402064 0.53597933]\n",
            " [0.4653376  0.53466237]\n",
            " [0.4647675  0.5352325 ]\n",
            " [0.46966907 0.5303309 ]\n",
            " [0.46255806 0.53744197]\n",
            " [0.46735638 0.5326437 ]\n",
            " [0.46688667 0.5331133 ]\n",
            " [0.4621118  0.5378882 ]\n",
            " [0.46439108 0.5356089 ]\n",
            " [0.46495205 0.53504795]\n",
            " [0.46793586 0.5320642 ]\n",
            " [0.47114474 0.52885526]\n",
            " [0.46421805 0.535782  ]\n",
            " [0.4668465  0.53315353]\n",
            " [0.464988   0.53501207]\n",
            " [0.46758932 0.5324107 ]\n",
            " [0.46173066 0.53826934]\n",
            " [0.46215507 0.5378449 ]\n",
            " [0.46331933 0.53668064]\n",
            " [0.46829435 0.5317057 ]\n",
            " [0.46773267 0.53226733]]\n",
            "========== Epoch 5 Batch 17==== Step 1 AVG. val Loss 0.6923368573188782 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 18==== Step 2 Probs\n",
            "[[0.4678001  0.53219986]\n",
            " [0.4656454  0.53435457]\n",
            " [0.47075987 0.5292401 ]\n",
            " [0.46803242 0.53196764]\n",
            " [0.46548378 0.5345162 ]\n",
            " [0.4685094  0.5314906 ]\n",
            " [0.46370232 0.53629774]\n",
            " [0.46646982 0.5335301 ]\n",
            " [0.47705352 0.5229465 ]\n",
            " [0.46243837 0.5375616 ]\n",
            " [0.46457005 0.53542995]\n",
            " [0.4688181  0.53118193]\n",
            " [0.46926677 0.5307332 ]\n",
            " [0.47153345 0.5284666 ]\n",
            " [0.46076864 0.53923136]\n",
            " [0.46185318 0.53814685]\n",
            " [0.4686683  0.5313317 ]\n",
            " [0.46830553 0.5316944 ]\n",
            " [0.46497685 0.53502315]\n",
            " [0.45894483 0.54105514]\n",
            " [0.46870002 0.53129995]\n",
            " [0.46492884 0.53507113]\n",
            " [0.4668935  0.5331065 ]\n",
            " [0.4684469  0.5315531 ]\n",
            " [0.4657737  0.5342263 ]\n",
            " [0.46175918 0.5382408 ]\n",
            " [0.47013697 0.529863  ]\n",
            " [0.46022543 0.5397746 ]\n",
            " [0.4671377  0.5328623 ]\n",
            " [0.46309415 0.5369059 ]\n",
            " [0.46802574 0.53197426]\n",
            " [0.46957663 0.53042334]]\n",
            "========== Epoch 5 Batch 18==== Step 1 AVG. val Loss 0.6798112988471985 ====  0.7692307692307693 ====  0.625\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 19==== Step 2 Probs\n",
            "[[0.47014934 0.52985066]\n",
            " [0.46390858 0.5360914 ]\n",
            " [0.46652183 0.53347814]\n",
            " [0.46493444 0.53506553]\n",
            " [0.4706933  0.5293067 ]\n",
            " [0.46495587 0.53504413]\n",
            " [0.46331933 0.53668064]\n",
            " [0.4654081  0.5345919 ]\n",
            " [0.4681092  0.5318908 ]\n",
            " [0.46177325 0.5382268 ]\n",
            " [0.46796823 0.5320318 ]\n",
            " [0.46289784 0.53710216]\n",
            " [0.4670385  0.5329615 ]\n",
            " [0.4716956  0.52830446]\n",
            " [0.46667972 0.5333203 ]\n",
            " [0.46242926 0.5375707 ]\n",
            " [0.4588329  0.5411671 ]\n",
            " [0.45737466 0.54262537]\n",
            " [0.4660061  0.5339939 ]\n",
            " [0.46142623 0.53857374]\n",
            " [0.46499068 0.5350093 ]\n",
            " [0.46284455 0.5371555 ]\n",
            " [0.46767655 0.5323234 ]\n",
            " [0.46001083 0.5399891 ]\n",
            " [0.46900296 0.53099704]\n",
            " [0.46594462 0.5340554 ]\n",
            " [0.46779752 0.5322025 ]\n",
            " [0.46227854 0.5377214 ]\n",
            " [0.466222   0.533778  ]\n",
            " [0.46457827 0.5354218 ]\n",
            " [0.46664816 0.53335184]\n",
            " [0.4642233  0.5357767 ]]\n",
            "========== Epoch 5 Batch 19==== Step 1 AVG. val Loss 0.6872138381004333 ====  0.72 ====  0.5625\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 20==== Step 2 Probs\n",
            "[[0.46733093 0.53266907]\n",
            " [0.46809474 0.53190523]\n",
            " [0.46833977 0.5316602 ]\n",
            " [0.4658244  0.5341756 ]\n",
            " [0.4627436  0.53725636]\n",
            " [0.46573192 0.5342681 ]\n",
            " [0.46523818 0.5347618 ]\n",
            " [0.46459746 0.53540254]\n",
            " [0.46369943 0.53630054]\n",
            " [0.46561408 0.5343859 ]\n",
            " [0.4675122  0.53248775]\n",
            " [0.4629327  0.5370673 ]\n",
            " [0.46450153 0.53549844]\n",
            " [0.4649259  0.5350742 ]\n",
            " [0.46935746 0.5306425 ]\n",
            " [0.46056542 0.53943455]\n",
            " [0.4695111  0.53048897]\n",
            " [0.4674798  0.53252023]\n",
            " [0.45824057 0.5417595 ]\n",
            " [0.47243568 0.5275643 ]\n",
            " [0.4628281  0.5371719 ]\n",
            " [0.46438438 0.5356157 ]\n",
            " [0.46312562 0.53687435]\n",
            " [0.46628103 0.533719  ]\n",
            " [0.46565798 0.534342  ]\n",
            " [0.46427798 0.53572196]\n",
            " [0.46201932 0.5379807 ]\n",
            " [0.46375945 0.5362405 ]\n",
            " [0.46498665 0.5350133 ]\n",
            " [0.4693855  0.53061444]\n",
            " [0.4689389  0.5310611 ]\n",
            " [0.46026644 0.5397335 ]]\n",
            "========== Epoch 5 Batch 20==== Step 1 AVG. val Loss 0.6855870485305786 ====  0.72 ====  0.5625\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 21==== Step 2 Probs\n",
            "[[0.4599913  0.5400087 ]\n",
            " [0.46555474 0.5344453 ]\n",
            " [0.4668467  0.53315336]\n",
            " [0.46712467 0.53287536]\n",
            " [0.46460834 0.5353917 ]\n",
            " [0.46330237 0.5366977 ]\n",
            " [0.46272317 0.5372768 ]\n",
            " [0.46396694 0.53603303]\n",
            " [0.46676007 0.53323996]\n",
            " [0.46512696 0.534873  ]\n",
            " [0.4667297  0.5332703 ]\n",
            " [0.46500587 0.5349941 ]\n",
            " [0.45739293 0.54260707]\n",
            " [0.46988574 0.53011423]\n",
            " [0.47373942 0.5262606 ]\n",
            " [0.46449515 0.5355048 ]\n",
            " [0.46646813 0.53353184]\n",
            " [0.46792242 0.5320776 ]\n",
            " [0.46915513 0.53084487]\n",
            " [0.46597844 0.53402156]\n",
            " [0.46790007 0.53209996]\n",
            " [0.46593705 0.534063  ]\n",
            " [0.4656948  0.5343052 ]\n",
            " [0.4691995  0.53080046]\n",
            " [0.4666903  0.53330976]\n",
            " [0.45999694 0.5400031 ]\n",
            " [0.47192883 0.52807117]\n",
            " [0.4603855  0.5396145 ]\n",
            " [0.4675758  0.5324242 ]\n",
            " [0.46410403 0.535896  ]\n",
            " [0.4671565  0.5328435 ]\n",
            " [0.46141735 0.5385827 ]]\n",
            "========== Epoch 5 Batch 21==== Step 1 AVG. val Loss 0.6971923112869263 ====  0.6666666666666666 ====  0.5\n",
            "tensor([0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 22==== Step 2 Probs\n",
            "[[0.46500432 0.5349957 ]\n",
            " [0.4744453  0.5255547 ]\n",
            " [0.4574418  0.5425582 ]\n",
            " [0.4685725  0.53142744]\n",
            " [0.4583169  0.5416831 ]\n",
            " [0.46000993 0.5399901 ]\n",
            " [0.46835697 0.53164303]\n",
            " [0.46935546 0.53064454]\n",
            " [0.46347448 0.53652555]\n",
            " [0.46698612 0.5330139 ]\n",
            " [0.45664263 0.5433574 ]\n",
            " [0.46183345 0.5381666 ]\n",
            " [0.46633798 0.533662  ]\n",
            " [0.46421605 0.5357839 ]\n",
            " [0.46602747 0.5339725 ]\n",
            " [0.46289995 0.5371001 ]\n",
            " [0.4672143  0.5327857 ]\n",
            " [0.46395132 0.53604865]\n",
            " [0.46090657 0.53909343]\n",
            " [0.46277356 0.53722644]\n",
            " [0.46571216 0.5342878 ]\n",
            " [0.46779618 0.53220385]\n",
            " [0.466708   0.53329206]\n",
            " [0.47410688 0.52589315]\n",
            " [0.4570974  0.5429026 ]\n",
            " [0.46997017 0.53002983]\n",
            " [0.46373433 0.5362656 ]\n",
            " [0.46619782 0.53380215]\n",
            " [0.4693805  0.5306195 ]\n",
            " [0.46987066 0.5301294 ]\n",
            " [0.4732196  0.52678037]\n",
            " [0.4604573  0.5395427 ]]\n",
            "========== Epoch 5 Batch 22==== Step 1 AVG. val Loss 0.6760854721069336 ====  0.7692307692307693 ====  0.625\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 5 Batch 23==== Step 2 Probs\n",
            "[[0.46737164 0.53262836]\n",
            " [0.46522686 0.5347732 ]\n",
            " [0.46728623 0.53271383]\n",
            " [0.47205287 0.5279471 ]\n",
            " [0.46395627 0.53604376]\n",
            " [0.46716395 0.5328361 ]\n",
            " [0.47082457 0.5291754 ]\n",
            " [0.4661025  0.53389746]\n",
            " [0.46033317 0.53966683]\n",
            " [0.4670133  0.5329867 ]\n",
            " [0.46212038 0.5378796 ]\n",
            " [0.46998304 0.53001696]\n",
            " [0.4578414  0.54215854]\n",
            " [0.46965784 0.53034216]\n",
            " [0.4652752  0.5347248 ]\n",
            " [0.46302757 0.5369724 ]\n",
            " [0.46095353 0.53904647]\n",
            " [0.47177833 0.52822167]\n",
            " [0.4609437  0.5390563 ]\n",
            " [0.46635112 0.53364885]\n",
            " [0.46612212 0.5338779 ]\n",
            " [0.46827957 0.5317204 ]\n",
            " [0.46824306 0.53175694]\n",
            " [0.46388322 0.53611684]\n",
            " [0.46866325 0.5313367 ]\n",
            " [0.46622854 0.53377146]\n",
            " [0.4707556  0.52924436]\n",
            " [0.4684603  0.53153974]\n",
            " [0.47034335 0.52965665]\n",
            " [0.46656877 0.53343123]\n",
            " [0.46023256 0.53976744]\n",
            " [0.46275628 0.5372437 ]]\n",
            "========== Epoch 5 Batch 23==== Step 1 AVG. val Loss 0.6961424350738525 ====  0.6666666666666666 ====  0.5\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 24==== Step 2 Probs\n",
            "[[0.45968884 0.5403112 ]\n",
            " [0.4675183  0.5324817 ]\n",
            " [0.46605545 0.53394455]\n",
            " [0.46781784 0.5321821 ]\n",
            " [0.46451882 0.53548115]\n",
            " [0.46928686 0.53071314]\n",
            " [0.46823528 0.53176475]\n",
            " [0.4663494  0.53365064]\n",
            " [0.4655767  0.5344233 ]\n",
            " [0.46154705 0.5384529 ]\n",
            " [0.46769243 0.53230757]\n",
            " [0.4721519  0.5278481 ]\n",
            " [0.4665174  0.5334826 ]\n",
            " [0.46480134 0.5351987 ]\n",
            " [0.45873758 0.5412624 ]\n",
            " [0.46497834 0.53502166]\n",
            " [0.4701351  0.5298649 ]\n",
            " [0.46671185 0.5332882 ]\n",
            " [0.46617573 0.53382427]\n",
            " [0.46303037 0.53696966]\n",
            " [0.46417877 0.5358212 ]\n",
            " [0.46960595 0.5303941 ]\n",
            " [0.46556813 0.5344319 ]\n",
            " [0.4650215  0.53497857]\n",
            " [0.46955985 0.53044015]\n",
            " [0.46667376 0.5333262 ]\n",
            " [0.4641604  0.5358396 ]\n",
            " [0.46118194 0.53881806]\n",
            " [0.46549386 0.53450614]\n",
            " [0.45967206 0.54032797]\n",
            " [0.46230537 0.53769463]\n",
            " [0.469671   0.530329  ]]\n",
            "========== Epoch 5 Batch 24==== Step 1 AVG. val Loss 0.7002128958702087 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 5 Batch 25==== Step 2 Probs\n",
            "[[0.4696646  0.5303354 ]\n",
            " [0.46282065 0.53717935]\n",
            " [0.4648152  0.5351848 ]\n",
            " [0.4693396  0.53066045]\n",
            " [0.4643844  0.5356156 ]\n",
            " [0.46618918 0.5338108 ]\n",
            " [0.46005026 0.5399498 ]\n",
            " [0.46890378 0.53109616]\n",
            " [0.4657119  0.5342881 ]\n",
            " [0.47011828 0.5298817 ]\n",
            " [0.4671653  0.53283465]\n",
            " [0.46374962 0.53625035]\n",
            " [0.46633685 0.53366315]\n",
            " [0.46426806 0.5357319 ]\n",
            " [0.4685119  0.53148806]\n",
            " [0.4692325  0.53076756]\n",
            " [0.4660759  0.5339241 ]\n",
            " [0.4656965  0.5343035 ]\n",
            " [0.46187705 0.53812295]\n",
            " [0.47231406 0.5276859 ]\n",
            " [0.46280748 0.5371925 ]\n",
            " [0.4666197  0.53338027]\n",
            " [0.46416563 0.5358344 ]\n",
            " [0.4694256  0.53057444]\n",
            " [0.47003257 0.5299674 ]\n",
            " [0.46885377 0.5311462 ]\n",
            " [0.46867833 0.5313217 ]\n",
            " [0.4695031  0.5304969 ]\n",
            " [0.4699787  0.5300213 ]\n",
            " [0.46017325 0.5398267 ]\n",
            " [0.46160024 0.53839976]\n",
            " [0.46283972 0.5371602 ]]\n",
            "========== Epoch 5 Batch 25==== Step 1 AVG. val Loss 0.7166573405265808 ====  0.5116279069767442 ====  0.34375\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
            "       device='cuda:0')\n",
            "========== Epoch 5 Batch 26==== Step 2 Probs\n",
            "[[0.46836767 0.5316323 ]\n",
            " [0.46687317 0.53312683]\n",
            " [0.46724167 0.5327583 ]\n",
            " [0.46790966 0.5320903 ]\n",
            " [0.46774086 0.53225917]\n",
            " [0.46539548 0.53460455]\n",
            " [0.4661046  0.53389543]\n",
            " [0.46592137 0.5340786 ]\n",
            " [0.46249112 0.5375089 ]\n",
            " [0.47050315 0.52949685]\n",
            " [0.46500224 0.5349977 ]\n",
            " [0.46480134 0.5351987 ]\n",
            " [0.46448165 0.53551835]\n",
            " [0.46642658 0.53357345]\n",
            " [0.4682781  0.5317219 ]\n",
            " [0.46610057 0.5338994 ]\n",
            " [0.4718117  0.5281883 ]\n",
            " [0.46571463 0.53428537]\n",
            " [0.46507543 0.53492457]\n",
            " [0.46592742 0.5340725 ]\n",
            " [0.46756873 0.53243124]]\n",
            "========== Epoch 5 Batch 26==== Step 1 AVG. val Loss 0.7125715017318726 ====  0.5517241379310345 ====  0.38095238095238093\n",
            "  Average Validation Loss: 0.70\n",
            "  Average Validation F1: 0.66\n",
            "  Average Validation Acc: 0.50\n",
            "  Validation took: 0:00:19\n",
            "EarlyStopping counter: 1 out of 4\n",
            "\n",
            "======== Epoch 7 / 2 ========\n",
            "Training...\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 1==== Step 1  Train Loss 0.6788520216941833  ==== 0.7142857142857143  ==== 0.625\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 2==== Step 1  Train Loss 0.6989490389823914  ==== 0.5641025641025642  ==== 0.46875\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 3==== Step 1  Train Loss 0.6690226793289185  ==== 0.6111111111111113  ==== 0.5625\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 4==== Step 1  Train Loss 0.7152379751205444  ==== 0.5  ==== 0.4375\n",
            "tensor([1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 5==== Step 1  Train Loss 0.6798213124275208  ==== 0.5499999999999999  ==== 0.4375\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 6==== Step 1  Train Loss 0.7126052379608154  ==== 0.5365853658536586  ==== 0.40625\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 7==== Step 1  Train Loss 0.7374250292778015  ==== 0.20689655172413793  ==== 0.28125\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 8==== Step 1  Train Loss 0.7164591550827026  ==== 0.5263157894736842  ==== 0.4375\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 9==== Step 1  Train Loss 0.6892383098602295  ==== 0.6511627906976745  ==== 0.53125\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 10==== Step 1  Train Loss 0.7100446224212646  ==== 0.4444444444444444  ==== 0.375\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 11==== Step 1  Train Loss 0.6663923859596252  ==== 0.6341463414634146  ==== 0.53125\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 12==== Step 1  Train Loss 0.691186249256134  ==== 0.631578947368421  ==== 0.5625\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 13==== Step 1  Train Loss 0.7070174813270569  ==== 0.6222222222222222  ==== 0.46875\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 14==== Step 1  Train Loss 0.7094045281410217  ==== 0.5263157894736842  ==== 0.4375\n",
            "tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 15==== Step 1  Train Loss 0.7097583413124084  ==== 0.6046511627906976  ==== 0.46875\n",
            "tensor([1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 16==== Step 1  Train Loss 0.7201381921768188  ==== 0.4615384615384615  ==== 0.34375\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 17==== Step 1  Train Loss 0.7033851146697998  ==== 0.5142857142857142  ==== 0.46875\n",
            "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 18==== Step 1  Train Loss 0.6609680652618408  ==== 0.6857142857142857  ==== 0.65625\n",
            "tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 19==== Step 1  Train Loss 0.694165050983429  ==== 0.5945945945945945  ==== 0.53125\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 20==== Step 1  Train Loss 0.7048800587654114  ==== 0.5641025641025642  ==== 0.46875\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 21==== Step 1  Train Loss 0.7052668929100037  ==== 0.5641025641025642  ==== 0.46875\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 22==== Step 1  Train Loss 0.6816288232803345  ==== 0.5945945945945946  ==== 0.53125\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 23==== Step 1  Train Loss 0.7058635950088501  ==== 0.5142857142857143  ==== 0.46875\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 24==== Step 1  Train Loss 0.7226789593696594  ==== 0.2857142857142857  ==== 0.375\n",
            "tensor([1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 25==== Step 1  Train Loss 0.7015920877456665  ==== 0.5333333333333333  ==== 0.5625\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1],\n",
            "       device='cuda:0')\n",
            "========== Epoch 6 Batch 26==== Step 1  Train Loss 0.6876389384269714  ==== 0.5833333333333334  ==== 0.5238095238095238\n",
            "========== Epoch 6 ==== Step 1 AVG. Train Loss 0.6992161594904386\n",
            "\n",
            "  Training epoch took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 1==== Step 2 Probs\n",
            "[[0.4870393  0.51296073]\n",
            " [0.47635743 0.5236426 ]\n",
            " [0.4799014  0.5200986 ]\n",
            " [0.4793169  0.5206831 ]\n",
            " [0.48562363 0.5143764 ]\n",
            " [0.48560968 0.51439035]\n",
            " [0.48030162 0.5196984 ]\n",
            " [0.47362557 0.5263744 ]\n",
            " [0.47717702 0.522823  ]\n",
            " [0.47431394 0.5256861 ]\n",
            " [0.47733146 0.52266854]\n",
            " [0.48032576 0.5196742 ]\n",
            " [0.47557318 0.5244268 ]\n",
            " [0.47630677 0.52369326]\n",
            " [0.48606247 0.51393753]\n",
            " [0.47130424 0.52869576]\n",
            " [0.4747484  0.52525157]\n",
            " [0.48055765 0.5194423 ]\n",
            " [0.47938827 0.52061176]\n",
            " [0.47646272 0.5235373 ]\n",
            " [0.48042735 0.5195727 ]\n",
            " [0.4683331  0.53166693]\n",
            " [0.47627893 0.5237211 ]\n",
            " [0.47888    0.52111995]\n",
            " [0.48062396 0.519376  ]\n",
            " [0.48433405 0.51566595]\n",
            " [0.47955692 0.5204431 ]\n",
            " [0.48179713 0.5182029 ]\n",
            " [0.46821758 0.5317824 ]\n",
            " [0.47392875 0.52607125]\n",
            " [0.47918507 0.5208149 ]\n",
            " [0.4718315  0.5281685 ]]\n",
            "========== Epoch 6 Batch 1==== Step 1 AVG. val Loss 0.6861011385917664 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 2==== Step 2 Probs\n",
            "[[0.48567662 0.51432335]\n",
            " [0.47438046 0.5256195 ]\n",
            " [0.48113617 0.5188638 ]\n",
            " [0.48389608 0.5161039 ]\n",
            " [0.46871185 0.5312882 ]\n",
            " [0.4834492  0.5165508 ]\n",
            " [0.48512408 0.51487595]\n",
            " [0.47181317 0.5281868 ]\n",
            " [0.4781815  0.5218185 ]\n",
            " [0.48228848 0.5177115 ]\n",
            " [0.4679191  0.53208095]\n",
            " [0.47938848 0.5206115 ]\n",
            " [0.46914846 0.53085154]\n",
            " [0.47404242 0.5259576 ]\n",
            " [0.47935292 0.5206471 ]\n",
            " [0.46879745 0.53120255]\n",
            " [0.47904357 0.5209564 ]\n",
            " [0.47564644 0.52435356]\n",
            " [0.4830688  0.5169312 ]\n",
            " [0.47981066 0.52018934]\n",
            " [0.4743971  0.52560294]\n",
            " [0.47185552 0.52814454]\n",
            " [0.47959134 0.52040863]\n",
            " [0.4858482  0.5141518 ]\n",
            " [0.47940168 0.52059835]\n",
            " [0.4819021  0.5180979 ]\n",
            " [0.47729233 0.52270764]\n",
            " [0.47383437 0.5261656 ]\n",
            " [0.4751919  0.52480817]\n",
            " [0.47636938 0.5236306 ]\n",
            " [0.4705192  0.52948076]\n",
            " [0.47716197 0.52283806]]\n",
            "========== Epoch 6 Batch 2==== Step 1 AVG. val Loss 0.6949896216392517 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 3==== Step 2 Probs\n",
            "[[0.47848913 0.52151084]\n",
            " [0.4764277  0.52357227]\n",
            " [0.48734945 0.51265055]\n",
            " [0.4838766  0.5161234 ]\n",
            " [0.4756484  0.5243516 ]\n",
            " [0.4781462  0.5218538 ]\n",
            " [0.48087853 0.51912147]\n",
            " [0.47276354 0.5272365 ]\n",
            " [0.48123026 0.51876974]\n",
            " [0.48419178 0.5158082 ]\n",
            " [0.48265877 0.51734126]\n",
            " [0.48818585 0.5118142 ]\n",
            " [0.47631732 0.5236827 ]\n",
            " [0.47425574 0.52574426]\n",
            " [0.47675166 0.5232483 ]\n",
            " [0.47583285 0.5241671 ]\n",
            " [0.48525876 0.51474124]\n",
            " [0.48178038 0.51821965]\n",
            " [0.47278076 0.52721924]\n",
            " [0.47745153 0.5225485 ]\n",
            " [0.47552153 0.52447844]\n",
            " [0.48470506 0.51529497]\n",
            " [0.47717366 0.5228264 ]\n",
            " [0.4746467  0.5253533 ]\n",
            " [0.4769102  0.5230898 ]\n",
            " [0.4731492  0.5268508 ]\n",
            " [0.47799456 0.5220054 ]\n",
            " [0.47809398 0.521906  ]\n",
            " [0.47476813 0.5252319 ]\n",
            " [0.48344582 0.5165542 ]\n",
            " [0.47926226 0.52073777]\n",
            " [0.4795249  0.52047515]]\n",
            "========== Epoch 6 Batch 3==== Step 1 AVG. val Loss 0.6922405362129211 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 4==== Step 2 Probs\n",
            "[[0.47842333 0.52157664]\n",
            " [0.47199136 0.52800864]\n",
            " [0.47345224 0.52654773]\n",
            " [0.48030207 0.5196979 ]\n",
            " [0.47395694 0.52604306]\n",
            " [0.47522625 0.5247737 ]\n",
            " [0.4739833  0.5260167 ]\n",
            " [0.47580954 0.5241905 ]\n",
            " [0.474949   0.525051  ]\n",
            " [0.47360295 0.52639705]\n",
            " [0.47681817 0.5231818 ]\n",
            " [0.47505525 0.5249447 ]\n",
            " [0.47682092 0.5231791 ]\n",
            " [0.4731667  0.52683324]\n",
            " [0.47604764 0.52395236]\n",
            " [0.475113   0.52488697]\n",
            " [0.47696856 0.5230315 ]\n",
            " [0.4755494  0.5244506 ]\n",
            " [0.47966784 0.52033216]\n",
            " [0.4755539  0.52444607]\n",
            " [0.4717526  0.52824736]\n",
            " [0.4804233  0.5195767 ]\n",
            " [0.47049874 0.52950126]\n",
            " [0.47498983 0.52501017]\n",
            " [0.47792268 0.5220773 ]\n",
            " [0.4695237  0.53047633]\n",
            " [0.48256263 0.51743734]\n",
            " [0.4764376  0.52356243]\n",
            " [0.4684869  0.5315131 ]\n",
            " [0.48018587 0.51981413]\n",
            " [0.46676576 0.53323424]\n",
            " [0.47343048 0.52656955]]\n",
            "========== Epoch 6 Batch 4==== Step 1 AVG. val Loss 0.6837605834007263 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 5==== Step 2 Probs\n",
            "[[0.47311834 0.52688164]\n",
            " [0.47797468 0.52202535]\n",
            " [0.46730444 0.5326956 ]\n",
            " [0.47672608 0.5232739 ]\n",
            " [0.48142228 0.51857775]\n",
            " [0.47760668 0.5223933 ]\n",
            " [0.47582322 0.5241768 ]\n",
            " [0.47891465 0.5210853 ]\n",
            " [0.47860482 0.52139515]\n",
            " [0.47033027 0.52966976]\n",
            " [0.47960585 0.52039415]\n",
            " [0.48124278 0.5187573 ]\n",
            " [0.48442245 0.5155775 ]\n",
            " [0.47177032 0.52822965]\n",
            " [0.47748825 0.5225118 ]\n",
            " [0.47741905 0.522581  ]\n",
            " [0.47481474 0.5251852 ]\n",
            " [0.47840956 0.5215904 ]\n",
            " [0.4787253  0.5212746 ]\n",
            " [0.47447243 0.5255276 ]\n",
            " [0.47992322 0.52007675]\n",
            " [0.48319665 0.5168033 ]\n",
            " [0.47471306 0.525287  ]\n",
            " [0.47027093 0.52972907]\n",
            " [0.47592193 0.524078  ]\n",
            " [0.4828248  0.5171752 ]\n",
            " [0.47697803 0.52302194]\n",
            " [0.48400003 0.516     ]\n",
            " [0.4808573  0.5191427 ]\n",
            " [0.47552854 0.5244715 ]\n",
            " [0.47794375 0.5220562 ]\n",
            " [0.48206517 0.5179348 ]]\n",
            "========== Epoch 6 Batch 5==== Step 1 AVG. val Loss 0.7015097141265869 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 6==== Step 2 Probs\n",
            "[[0.47624803 0.523752  ]\n",
            " [0.4735191  0.5264809 ]\n",
            " [0.4776642  0.52233577]\n",
            " [0.48092178 0.51907825]\n",
            " [0.47606796 0.52393204]\n",
            " [0.4797871  0.5202129 ]\n",
            " [0.47265816 0.52734184]\n",
            " [0.48391792 0.51608205]\n",
            " [0.4818151  0.5181849 ]\n",
            " [0.47557312 0.52442694]\n",
            " [0.47558618 0.5244138 ]\n",
            " [0.4654996  0.5345004 ]\n",
            " [0.4758155  0.5241845 ]\n",
            " [0.47687507 0.5231249 ]\n",
            " [0.47579426 0.52420574]\n",
            " [0.47970337 0.5202967 ]\n",
            " [0.4782987  0.52170134]\n",
            " [0.48327345 0.51672655]\n",
            " [0.48771313 0.5122869 ]\n",
            " [0.4719592  0.52804077]\n",
            " [0.4720659  0.52793413]\n",
            " [0.48061895 0.5193811 ]\n",
            " [0.47601184 0.5239881 ]\n",
            " [0.4811508  0.51884925]\n",
            " [0.4807912  0.5192088 ]\n",
            " [0.47530195 0.5246981 ]\n",
            " [0.47977138 0.5202287 ]\n",
            " [0.47157034 0.5284296 ]\n",
            " [0.47727963 0.52272034]\n",
            " [0.47191352 0.5280865 ]\n",
            " [0.47092748 0.5290725 ]\n",
            " [0.4774857  0.52251434]]\n",
            "========== Epoch 6 Batch 6==== Step 1 AVG. val Loss 0.7060201168060303 ====  0.5454545454545454 ====  0.375\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 7==== Step 2 Probs\n",
            "[[0.47675833 0.5232417 ]\n",
            " [0.4702491  0.52975094]\n",
            " [0.4795948  0.5204052 ]\n",
            " [0.477208   0.522792  ]\n",
            " [0.48172718 0.5182728 ]\n",
            " [0.47625515 0.5237449 ]\n",
            " [0.48308012 0.5169199 ]\n",
            " [0.47118798 0.528812  ]\n",
            " [0.46991614 0.5300839 ]\n",
            " [0.4712541  0.52874595]\n",
            " [0.47855097 0.521449  ]\n",
            " [0.48588294 0.51411706]\n",
            " [0.47794223 0.5220578 ]\n",
            " [0.4706518  0.5293482 ]\n",
            " [0.47945604 0.52054393]\n",
            " [0.47762412 0.5223759 ]\n",
            " [0.4723002  0.5276998 ]\n",
            " [0.4703428  0.5296572 ]\n",
            " [0.48201036 0.51798964]\n",
            " [0.478406   0.52159405]\n",
            " [0.47586    0.52414   ]\n",
            " [0.47408947 0.52591056]\n",
            " [0.47893876 0.52106124]\n",
            " [0.47734335 0.5226567 ]\n",
            " [0.48155004 0.5184499 ]\n",
            " [0.47460687 0.5253932 ]\n",
            " [0.47945687 0.5205431 ]\n",
            " [0.46752718 0.5324728 ]\n",
            " [0.48479983 0.51520014]\n",
            " [0.4759868  0.52401316]\n",
            " [0.48424962 0.5157504 ]\n",
            " [0.47058284 0.52941716]]\n",
            "========== Epoch 6 Batch 7==== Step 1 AVG. val Loss 0.6917092204093933 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 8==== Step 2 Probs\n",
            "[[0.4760202  0.52397984]\n",
            " [0.47800574 0.52199423]\n",
            " [0.4761232  0.5238768 ]\n",
            " [0.47927058 0.5207294 ]\n",
            " [0.4766018  0.5233982 ]\n",
            " [0.47711742 0.5228826 ]\n",
            " [0.47792828 0.5220717 ]\n",
            " [0.47276935 0.5272306 ]\n",
            " [0.47252163 0.52747834]\n",
            " [0.4695392  0.5304608 ]\n",
            " [0.46860382 0.5313962 ]\n",
            " [0.47888848 0.5211115 ]\n",
            " [0.4769216  0.52307844]\n",
            " [0.48036364 0.51963633]\n",
            " [0.47297218 0.52702785]\n",
            " [0.47268194 0.52731806]\n",
            " [0.47649723 0.5235028 ]\n",
            " [0.47785726 0.52214277]\n",
            " [0.47403526 0.52596474]\n",
            " [0.47377983 0.52622014]\n",
            " [0.4818925  0.5181075 ]\n",
            " [0.47485802 0.52514195]\n",
            " [0.47400576 0.52599424]\n",
            " [0.4736165  0.5263835 ]\n",
            " [0.47217137 0.52782863]\n",
            " [0.47754598 0.52245396]\n",
            " [0.46850914 0.5314909 ]\n",
            " [0.48055294 0.519447  ]\n",
            " [0.4790043  0.5209957 ]\n",
            " [0.48488715 0.5151128 ]\n",
            " [0.4717526  0.52824736]\n",
            " [0.47271243 0.5272876 ]]\n",
            "========== Epoch 6 Batch 8==== Step 1 AVG. val Loss 0.6895208358764648 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 9==== Step 2 Probs\n",
            "[[0.47402722 0.5259728 ]\n",
            " [0.47382793 0.5261721 ]\n",
            " [0.47672787 0.52327216]\n",
            " [0.4760804  0.52391964]\n",
            " [0.47814962 0.52185035]\n",
            " [0.47094023 0.5290597 ]\n",
            " [0.47426343 0.5257365 ]\n",
            " [0.47699034 0.52300966]\n",
            " [0.4799881  0.5200119 ]\n",
            " [0.47557023 0.52442974]\n",
            " [0.47952053 0.52047944]\n",
            " [0.47659802 0.523402  ]\n",
            " [0.47482094 0.5251791 ]\n",
            " [0.4820285  0.51797146]\n",
            " [0.47895145 0.5210485 ]\n",
            " [0.4748415  0.52515846]\n",
            " [0.47207382 0.52792615]\n",
            " [0.47541526 0.5245848 ]\n",
            " [0.4783441  0.5216559 ]\n",
            " [0.48331007 0.5166899 ]\n",
            " [0.47437492 0.5256251 ]\n",
            " [0.47869566 0.52130437]\n",
            " [0.47614667 0.52385336]\n",
            " [0.4792873  0.52071273]\n",
            " [0.47575852 0.52424145]\n",
            " [0.47676277 0.5232372 ]\n",
            " [0.47665018 0.5233498 ]\n",
            " [0.48029396 0.5197061 ]\n",
            " [0.4748214  0.52517855]\n",
            " [0.47846442 0.5215356 ]\n",
            " [0.47830874 0.5216912 ]\n",
            " [0.48762357 0.5123764 ]]\n",
            "========== Epoch 6 Batch 9==== Step 1 AVG. val Loss 0.6935213804244995 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 10==== Step 2 Probs\n",
            "[[0.47315413 0.5268459 ]\n",
            " [0.48556677 0.51443326]\n",
            " [0.47505262 0.52494735]\n",
            " [0.48062623 0.5193738 ]\n",
            " [0.4799313  0.5200687 ]\n",
            " [0.47525695 0.5247431 ]\n",
            " [0.4771742  0.5228258 ]\n",
            " [0.47645178 0.52354825]\n",
            " [0.47784892 0.5221511 ]\n",
            " [0.46602446 0.5339756 ]\n",
            " [0.4851424  0.5148576 ]\n",
            " [0.4701941  0.5298059 ]\n",
            " [0.47944894 0.5205511 ]\n",
            " [0.4701136  0.52988636]\n",
            " [0.48581117 0.5141888 ]\n",
            " [0.47902244 0.52097756]\n",
            " [0.4784265  0.52157354]\n",
            " [0.47587627 0.5241237 ]\n",
            " [0.4773719  0.5226281 ]\n",
            " [0.47336966 0.5266304 ]\n",
            " [0.4776759  0.5223241 ]\n",
            " [0.47145918 0.52854085]\n",
            " [0.46768704 0.5323129 ]\n",
            " [0.4755339  0.5244661 ]\n",
            " [0.48124263 0.5187574 ]\n",
            " [0.47704104 0.522959  ]\n",
            " [0.4778529  0.5221471 ]\n",
            " [0.48294556 0.51705444]\n",
            " [0.4728074  0.5271926 ]\n",
            " [0.47555634 0.5244436 ]\n",
            " [0.4595702  0.5404298 ]\n",
            " [0.4751962  0.5248038 ]]\n",
            "========== Epoch 6 Batch 10==== Step 1 AVG. val Loss 0.7009438276290894 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 11==== Step 2 Probs\n",
            "[[0.47826666 0.52173334]\n",
            " [0.4763375  0.52366257]\n",
            " [0.47998473 0.5200153 ]\n",
            " [0.48197094 0.5180291 ]\n",
            " [0.4774766  0.5225234 ]\n",
            " [0.4693722  0.5306277 ]\n",
            " [0.48142353 0.51857644]\n",
            " [0.48037708 0.5196229 ]\n",
            " [0.4776378  0.52236223]\n",
            " [0.47746357 0.5225364 ]\n",
            " [0.4713616  0.5286384 ]\n",
            " [0.48649052 0.51350945]\n",
            " [0.47421736 0.5257826 ]\n",
            " [0.47000578 0.5299942 ]\n",
            " [0.47660455 0.5233954 ]\n",
            " [0.47999582 0.52000415]\n",
            " [0.47639692 0.52360314]\n",
            " [0.47873583 0.5212642 ]\n",
            " [0.4753485  0.52465147]\n",
            " [0.4766422  0.5233578 ]\n",
            " [0.47393748 0.5260625 ]\n",
            " [0.47194648 0.52805346]\n",
            " [0.48006538 0.51993465]\n",
            " [0.47844076 0.52155924]\n",
            " [0.47967622 0.5203238 ]\n",
            " [0.47600308 0.52399695]\n",
            " [0.47639662 0.5236034 ]\n",
            " [0.47899455 0.52100545]\n",
            " [0.4824071  0.51759285]\n",
            " [0.47993296 0.52006704]\n",
            " [0.47877175 0.52122825]\n",
            " [0.46789643 0.5321036 ]]\n",
            "========== Epoch 6 Batch 11==== Step 1 AVG. val Loss 0.7046285271644592 ====  0.5454545454545454 ====  0.375\n",
            "tensor([0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 12==== Step 2 Probs\n",
            "[[0.48244944 0.5175505 ]\n",
            " [0.48277277 0.5172272 ]\n",
            " [0.48673964 0.51326036]\n",
            " [0.47307628 0.5269237 ]\n",
            " [0.48393488 0.5160651 ]\n",
            " [0.48512208 0.5148779 ]\n",
            " [0.47475287 0.5252471 ]\n",
            " [0.48184723 0.51815283]\n",
            " [0.466628   0.533372  ]\n",
            " [0.47477883 0.52522117]\n",
            " [0.47960258 0.5203974 ]\n",
            " [0.4836459  0.51635414]\n",
            " [0.46838808 0.53161186]\n",
            " [0.46987382 0.53012615]\n",
            " [0.47360462 0.52639544]\n",
            " [0.48537552 0.51462454]\n",
            " [0.47157097 0.52842903]\n",
            " [0.48310888 0.5168911 ]\n",
            " [0.4799895  0.5200105 ]\n",
            " [0.4851623  0.5148377 ]\n",
            " [0.4777257  0.52227426]\n",
            " [0.47444645 0.5255536 ]\n",
            " [0.4767051  0.52329487]\n",
            " [0.4705152  0.5294848 ]\n",
            " [0.4787905  0.52120954]\n",
            " [0.4736732  0.5263268 ]\n",
            " [0.48691404 0.51308596]\n",
            " [0.4792347  0.52076536]\n",
            " [0.47543034 0.5245697 ]\n",
            " [0.48182923 0.5181708 ]\n",
            " [0.46780524 0.53219473]\n",
            " [0.47378534 0.52621466]]\n",
            "========== Epoch 6 Batch 12==== Step 1 AVG. val Loss 0.6983235478401184 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 13==== Step 2 Probs\n",
            "[[0.47750056 0.5224995 ]\n",
            " [0.4763104  0.52368957]\n",
            " [0.4784513  0.5215487 ]\n",
            " [0.47870445 0.5212955 ]\n",
            " [0.47444874 0.5255512 ]\n",
            " [0.47470665 0.52529335]\n",
            " [0.48038346 0.51961654]\n",
            " [0.47812232 0.52187765]\n",
            " [0.47699293 0.5230071 ]\n",
            " [0.4785399  0.52146006]\n",
            " [0.4707857  0.5292143 ]\n",
            " [0.47368705 0.52631295]\n",
            " [0.4820255  0.5179745 ]\n",
            " [0.48309308 0.516907  ]\n",
            " [0.48444322 0.5155568 ]\n",
            " [0.4779188  0.52208126]\n",
            " [0.48128137 0.5187186 ]\n",
            " [0.4767008  0.5232992 ]\n",
            " [0.47763577 0.52236426]\n",
            " [0.4748229  0.5251771 ]\n",
            " [0.47322118 0.5267788 ]\n",
            " [0.47528136 0.5247186 ]\n",
            " [0.48021874 0.5197813 ]\n",
            " [0.4760996  0.5239004 ]\n",
            " [0.47852057 0.5214794 ]\n",
            " [0.4653059  0.5346941 ]\n",
            " [0.4773806  0.5226194 ]\n",
            " [0.47463673 0.52536327]\n",
            " [0.47678307 0.52321696]\n",
            " [0.47626707 0.52373296]\n",
            " [0.4763547  0.52364534]\n",
            " [0.46434027 0.53565973]]\n",
            "========== Epoch 6 Batch 13==== Step 1 AVG. val Loss 0.6888930201530457 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 14==== Step 2 Probs\n",
            "[[0.47792876 0.5220713 ]\n",
            " [0.46771133 0.5322886 ]\n",
            " [0.47776034 0.5222396 ]\n",
            " [0.4727676  0.5272324 ]\n",
            " [0.48564124 0.51435876]\n",
            " [0.4711661  0.5288339 ]\n",
            " [0.47798225 0.5220177 ]\n",
            " [0.47356015 0.52643985]\n",
            " [0.47659907 0.5234009 ]\n",
            " [0.48467603 0.515324  ]\n",
            " [0.4694571  0.5305429 ]\n",
            " [0.47607085 0.5239291 ]\n",
            " [0.48162276 0.51837724]\n",
            " [0.47898674 0.52101326]\n",
            " [0.4729215  0.52707857]\n",
            " [0.48136982 0.51863015]\n",
            " [0.48030806 0.519692  ]\n",
            " [0.48095614 0.51904386]\n",
            " [0.47538847 0.52461153]\n",
            " [0.48137832 0.5186217 ]\n",
            " [0.4803658  0.51963425]\n",
            " [0.473759   0.526241  ]\n",
            " [0.46850914 0.5314909 ]\n",
            " [0.48351082 0.51648915]\n",
            " [0.47342148 0.52657855]\n",
            " [0.47437388 0.5256261 ]\n",
            " [0.480821   0.51917905]\n",
            " [0.47777689 0.5222231 ]\n",
            " [0.47686514 0.5231349 ]\n",
            " [0.4743145  0.52568555]\n",
            " [0.46894452 0.53105545]\n",
            " [0.4762672  0.52373284]]\n",
            "========== Epoch 6 Batch 14==== Step 1 AVG. val Loss 0.7050415277481079 ====  0.5454545454545454 ====  0.375\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 15==== Step 2 Probs\n",
            "[[0.47101945 0.5289805 ]\n",
            " [0.46527395 0.534726  ]\n",
            " [0.46684676 0.53315324]\n",
            " [0.48043853 0.51956147]\n",
            " [0.4723807  0.52761924]\n",
            " [0.4798755  0.5201245 ]\n",
            " [0.4838139  0.51618606]\n",
            " [0.47474733 0.52525264]\n",
            " [0.47843868 0.5215613 ]\n",
            " [0.47972897 0.52027106]\n",
            " [0.47728616 0.52271384]\n",
            " [0.48215997 0.51784   ]\n",
            " [0.47959793 0.5204021 ]\n",
            " [0.474891   0.525109  ]\n",
            " [0.47773498 0.522265  ]\n",
            " [0.47639003 0.52360994]\n",
            " [0.4759168  0.5240832 ]\n",
            " [0.48596516 0.5140348 ]\n",
            " [0.47792384 0.52207613]\n",
            " [0.47766432 0.5223357 ]\n",
            " [0.4799923  0.52000767]\n",
            " [0.48778445 0.51221555]\n",
            " [0.47430387 0.52569616]\n",
            " [0.47588387 0.5241161 ]\n",
            " [0.47063696 0.52936304]\n",
            " [0.48796585 0.5120342 ]\n",
            " [0.478791   0.521209  ]\n",
            " [0.47350562 0.5264943 ]\n",
            " [0.47588518 0.5241148 ]\n",
            " [0.47518545 0.5248146 ]\n",
            " [0.4732226  0.52677745]\n",
            " [0.48550385 0.51449615]]\n",
            "========== Epoch 6 Batch 15==== Step 1 AVG. val Loss 0.6994320750236511 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 16==== Step 2 Probs\n",
            "[[0.47505948 0.52494055]\n",
            " [0.47851786 0.5214821 ]\n",
            " [0.48194423 0.5180558 ]\n",
            " [0.48285475 0.5171453 ]\n",
            " [0.474295   0.52570504]\n",
            " [0.475496   0.524504  ]\n",
            " [0.4694377  0.5305623 ]\n",
            " [0.47610074 0.52389926]\n",
            " [0.47073677 0.52926326]\n",
            " [0.47286958 0.5271304 ]\n",
            " [0.4723403  0.5276598 ]\n",
            " [0.47872895 0.52127105]\n",
            " [0.48116395 0.518836  ]\n",
            " [0.47423562 0.52576435]\n",
            " [0.47919962 0.5208004 ]\n",
            " [0.47824067 0.52175933]\n",
            " [0.47865742 0.5213426 ]\n",
            " [0.4794666  0.52053344]\n",
            " [0.48504016 0.5149599 ]\n",
            " [0.47248554 0.52751446]\n",
            " [0.47220144 0.52779853]\n",
            " [0.48139477 0.51860523]\n",
            " [0.47502175 0.5249782 ]\n",
            " [0.47874197 0.521258  ]\n",
            " [0.47722045 0.5227795 ]\n",
            " [0.47420004 0.5258    ]\n",
            " [0.48129588 0.51870406]\n",
            " [0.46683863 0.5331614 ]\n",
            " [0.48171118 0.5182888 ]\n",
            " [0.47712225 0.52287775]\n",
            " [0.47745022 0.5225498 ]\n",
            " [0.47986796 0.52013206]]\n",
            "========== Epoch 6 Batch 16==== Step 1 AVG. val Loss 0.7090812921524048 ====  0.5116279069767442 ====  0.34375\n",
            "tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 17==== Step 2 Probs\n",
            "[[0.47592226 0.5240777 ]\n",
            " [0.47294402 0.527056  ]\n",
            " [0.4717422  0.5282578 ]\n",
            " [0.48013467 0.5198654 ]\n",
            " [0.47078732 0.5292127 ]\n",
            " [0.4834067  0.5165933 ]\n",
            " [0.4800757  0.51992434]\n",
            " [0.48179287 0.51820713]\n",
            " [0.47463924 0.52536076]\n",
            " [0.4797424  0.5202576 ]\n",
            " [0.4766097  0.5233903 ]\n",
            " [0.4748742  0.5251258 ]\n",
            " [0.48204666 0.51795334]\n",
            " [0.46206293 0.5379371 ]\n",
            " [0.4748828  0.5251172 ]\n",
            " [0.48285723 0.5171428 ]\n",
            " [0.47786427 0.52213573]\n",
            " [0.479439   0.520561  ]\n",
            " [0.4707607  0.5292393 ]\n",
            " [0.47182447 0.5281756 ]\n",
            " [0.46984944 0.53015053]\n",
            " [0.47818634 0.5218137 ]\n",
            " [0.47604874 0.5239513 ]\n",
            " [0.48013273 0.51986724]\n",
            " [0.47507992 0.52492005]\n",
            " [0.47976878 0.5202312 ]\n",
            " [0.4708641  0.5291359 ]\n",
            " [0.47475708 0.525243  ]\n",
            " [0.47576177 0.52423817]\n",
            " [0.47891396 0.52108604]\n",
            " [0.48085442 0.51914555]\n",
            " [0.4743894  0.5256106 ]]\n",
            "========== Epoch 6 Batch 17==== Step 1 AVG. val Loss 0.6941568851470947 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 18==== Step 2 Probs\n",
            "[[0.4779309  0.5220691 ]\n",
            " [0.47074303 0.529257  ]\n",
            " [0.4726026  0.5273974 ]\n",
            " [0.48361084 0.51638913]\n",
            " [0.4853149  0.5146851 ]\n",
            " [0.47724703 0.52275294]\n",
            " [0.48579654 0.5142034 ]\n",
            " [0.47923678 0.5207632 ]\n",
            " [0.4885005  0.51149946]\n",
            " [0.47433084 0.5256692 ]\n",
            " [0.4693973  0.5306027 ]\n",
            " [0.48569742 0.51430255]\n",
            " [0.476733   0.523267  ]\n",
            " [0.4776508  0.5223492 ]\n",
            " [0.46823645 0.53176355]\n",
            " [0.47584778 0.5241522 ]\n",
            " [0.47774178 0.52225816]\n",
            " [0.47796902 0.52203095]\n",
            " [0.46999386 0.5300062 ]\n",
            " [0.4817104  0.51828957]\n",
            " [0.47177577 0.5282243 ]\n",
            " [0.46877632 0.5312237 ]\n",
            " [0.47624043 0.52375954]\n",
            " [0.4691557  0.5308443 ]\n",
            " [0.4806528  0.5193472 ]\n",
            " [0.4787824  0.5212176 ]\n",
            " [0.48211446 0.5178855 ]\n",
            " [0.4805942  0.5194058 ]\n",
            " [0.48920318 0.51079684]\n",
            " [0.46573728 0.5342628 ]\n",
            " [0.47504458 0.5249554 ]\n",
            " [0.47781533 0.52218467]]\n",
            "========== Epoch 6 Batch 18==== Step 1 AVG. val Loss 0.6860530376434326 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 19==== Step 2 Probs\n",
            "[[0.46813193 0.5318681 ]\n",
            " [0.47589356 0.52410644]\n",
            " [0.4781571  0.5218429 ]\n",
            " [0.4696734  0.5303266 ]\n",
            " [0.47892725 0.52107275]\n",
            " [0.47692934 0.52307063]\n",
            " [0.47217515 0.5278248 ]\n",
            " [0.47599205 0.524008  ]\n",
            " [0.47559935 0.5244006 ]\n",
            " [0.47309253 0.52690744]\n",
            " [0.47837955 0.52162045]\n",
            " [0.46701097 0.532989  ]\n",
            " [0.47209042 0.5279096 ]\n",
            " [0.47228637 0.5277136 ]\n",
            " [0.47168773 0.52831227]\n",
            " [0.47638765 0.5236123 ]\n",
            " [0.47534645 0.52465355]\n",
            " [0.4762359  0.52376413]\n",
            " [0.47539485 0.5246051 ]\n",
            " [0.47864732 0.52135265]\n",
            " [0.47613052 0.5238695 ]\n",
            " [0.4745905  0.52540946]\n",
            " [0.4733745  0.5266255 ]\n",
            " [0.4750433  0.5249567 ]\n",
            " [0.47910354 0.5208965 ]\n",
            " [0.47448492 0.5255151 ]\n",
            " [0.47724694 0.52275306]\n",
            " [0.4804572  0.5195428 ]\n",
            " [0.47649735 0.52350265]\n",
            " [0.47735935 0.52264065]\n",
            " [0.46694925 0.5330508 ]\n",
            " [0.47192115 0.5280788 ]]\n",
            "========== Epoch 6 Batch 19==== Step 1 AVG. val Loss 0.6950512528419495 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 20==== Step 2 Probs\n",
            "[[0.477599   0.52240103]\n",
            " [0.47615513 0.52384484]\n",
            " [0.47597712 0.5240228 ]\n",
            " [0.47187275 0.52812725]\n",
            " [0.480067   0.519933  ]\n",
            " [0.47373417 0.52626586]\n",
            " [0.4841251  0.5158749 ]\n",
            " [0.47955313 0.5204469 ]\n",
            " [0.4756189  0.52438116]\n",
            " [0.47530934 0.5246907 ]\n",
            " [0.47448054 0.5255195 ]\n",
            " [0.48011816 0.51988184]\n",
            " [0.4814908  0.5185092 ]\n",
            " [0.47966042 0.52033955]\n",
            " [0.4798457  0.5201543 ]\n",
            " [0.48985928 0.5101407 ]\n",
            " [0.4779892  0.5220108 ]\n",
            " [0.4743118  0.52568823]\n",
            " [0.48484644 0.5151535 ]\n",
            " [0.48458993 0.51541007]\n",
            " [0.47185227 0.52814776]\n",
            " [0.47038066 0.52961934]\n",
            " [0.47399628 0.5260037 ]\n",
            " [0.48210967 0.5178904 ]\n",
            " [0.48076612 0.5192339 ]\n",
            " [0.47835597 0.521644  ]\n",
            " [0.47937688 0.5206231 ]\n",
            " [0.47742403 0.522576  ]\n",
            " [0.47804537 0.52195466]\n",
            " [0.48107633 0.51892364]\n",
            " [0.48009273 0.51990724]\n",
            " [0.48257273 0.51742727]]\n",
            "========== Epoch 6 Batch 20==== Step 1 AVG. val Loss 0.6868239641189575 ====  0.7692307692307693 ====  0.625\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 21==== Step 2 Probs\n",
            "[[0.47000596 0.5299941 ]\n",
            " [0.47420666 0.5257934 ]\n",
            " [0.48258108 0.5174189 ]\n",
            " [0.47594255 0.5240575 ]\n",
            " [0.48084813 0.5191519 ]\n",
            " [0.48503035 0.5149697 ]\n",
            " [0.47812155 0.5218785 ]\n",
            " [0.48109752 0.5189025 ]\n",
            " [0.47449186 0.5255081 ]\n",
            " [0.47947535 0.5205247 ]\n",
            " [0.47162914 0.52837086]\n",
            " [0.47325668 0.5267433 ]\n",
            " [0.4778326  0.5221674 ]\n",
            " [0.47931933 0.52068067]\n",
            " [0.4704593  0.52954066]\n",
            " [0.47465137 0.52534866]\n",
            " [0.4778529  0.5221471 ]\n",
            " [0.47785747 0.5221425 ]\n",
            " [0.48554838 0.5144516 ]\n",
            " [0.4815342  0.5184658 ]\n",
            " [0.4766176  0.5233824 ]\n",
            " [0.47877702 0.521223  ]\n",
            " [0.4704497  0.52955025]\n",
            " [0.479697   0.520303  ]\n",
            " [0.4815398  0.5184602 ]\n",
            " [0.4877253  0.5122747 ]\n",
            " [0.47669578 0.5233042 ]\n",
            " [0.4896321  0.51036793]\n",
            " [0.4775413  0.52245873]\n",
            " [0.47826275 0.5217373 ]\n",
            " [0.47865695 0.52134305]\n",
            " [0.4741064  0.52589357]]\n",
            "========== Epoch 6 Batch 21==== Step 1 AVG. val Loss 0.69333815574646 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 22==== Step 2 Probs\n",
            "[[0.48333415 0.5166659 ]\n",
            " [0.47620144 0.5237985 ]\n",
            " [0.47915414 0.52084583]\n",
            " [0.47938344 0.52061653]\n",
            " [0.4841835  0.5158165 ]\n",
            " [0.47216445 0.5278355 ]\n",
            " [0.4721144  0.52788556]\n",
            " [0.47688514 0.52311486]\n",
            " [0.47634315 0.52365685]\n",
            " [0.47695374 0.5230463 ]\n",
            " [0.48274854 0.5172515 ]\n",
            " [0.46867168 0.5313284 ]\n",
            " [0.47352138 0.5264786 ]\n",
            " [0.47511795 0.524882  ]\n",
            " [0.4798575  0.52014256]\n",
            " [0.47733992 0.52266014]\n",
            " [0.47906265 0.5209374 ]\n",
            " [0.4805983  0.5194017 ]\n",
            " [0.47409195 0.52590805]\n",
            " [0.47535002 0.52465004]\n",
            " [0.47768414 0.52231586]\n",
            " [0.4793014  0.52069855]\n",
            " [0.47130644 0.52869356]\n",
            " [0.4773956  0.5226044 ]\n",
            " [0.47763318 0.5223668 ]\n",
            " [0.47704753 0.5229525 ]\n",
            " [0.48219812 0.5178019 ]\n",
            " [0.46975505 0.53024495]\n",
            " [0.48433828 0.5156617 ]\n",
            " [0.47435498 0.525645  ]\n",
            " [0.4811649  0.5188351 ]\n",
            " [0.48242158 0.5175784 ]]\n",
            "========== Epoch 6 Batch 22==== Step 1 AVG. val Loss 0.6973509788513184 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 23==== Step 2 Probs\n",
            "[[0.47935265 0.52064735]\n",
            " [0.47440174 0.5255982 ]\n",
            " [0.47678977 0.52321017]\n",
            " [0.47760075 0.52239925]\n",
            " [0.48018634 0.51981366]\n",
            " [0.47600716 0.52399284]\n",
            " [0.48093802 0.51906204]\n",
            " [0.4868411  0.51315886]\n",
            " [0.46823996 0.53176004]\n",
            " [0.47572422 0.5242758 ]\n",
            " [0.47843862 0.5215613 ]\n",
            " [0.47858763 0.5214124 ]\n",
            " [0.47328547 0.5267145 ]\n",
            " [0.4813718  0.51862824]\n",
            " [0.4864713  0.5135287 ]\n",
            " [0.48537832 0.5146217 ]\n",
            " [0.47683212 0.5231679 ]\n",
            " [0.47669652 0.52330345]\n",
            " [0.4743757  0.5256243 ]\n",
            " [0.4776875  0.52231246]\n",
            " [0.47048572 0.5295143 ]\n",
            " [0.4740811  0.5259189 ]\n",
            " [0.47384498 0.526155  ]\n",
            " [0.48359182 0.5164082 ]\n",
            " [0.46992397 0.530076  ]\n",
            " [0.47289923 0.52710074]\n",
            " [0.47544917 0.52455086]\n",
            " [0.47697186 0.5230281 ]\n",
            " [0.47538698 0.524613  ]\n",
            " [0.4678586  0.5321413 ]\n",
            " [0.4793718  0.5206283 ]\n",
            " [0.47707495 0.5229251 ]]\n",
            "========== Epoch 6 Batch 23==== Step 1 AVG. val Loss 0.6935369968414307 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 6 Batch 24==== Step 2 Probs\n",
            "[[0.4723742  0.52762574]\n",
            " [0.47824067 0.52175933]\n",
            " [0.47623554 0.5237645 ]\n",
            " [0.47673997 0.52326   ]\n",
            " [0.4726157  0.52738434]\n",
            " [0.47775614 0.52224386]\n",
            " [0.47744346 0.5225566 ]\n",
            " [0.47728357 0.5227164 ]\n",
            " [0.48247412 0.51752585]\n",
            " [0.47854736 0.52145267]\n",
            " [0.48161596 0.518384  ]\n",
            " [0.47392485 0.5260751 ]\n",
            " [0.4803766  0.5196234 ]\n",
            " [0.48333853 0.5166614 ]\n",
            " [0.46767533 0.5323246 ]\n",
            " [0.48052317 0.5194769 ]\n",
            " [0.4764281  0.52357197]\n",
            " [0.47735828 0.5226417 ]\n",
            " [0.47784728 0.5221527 ]\n",
            " [0.47966236 0.5203376 ]\n",
            " [0.47894058 0.5210594 ]\n",
            " [0.4806344  0.5193656 ]\n",
            " [0.4719772  0.5280228 ]\n",
            " [0.48065063 0.5193494 ]\n",
            " [0.48751044 0.51248956]\n",
            " [0.47797585 0.52202415]\n",
            " [0.47866565 0.5213344 ]\n",
            " [0.47558913 0.5244109 ]\n",
            " [0.4863523  0.5136477 ]\n",
            " [0.47546265 0.5245373 ]\n",
            " [0.4738561  0.5261439 ]\n",
            " [0.47100723 0.5289928 ]]\n",
            "========== Epoch 6 Batch 24==== Step 1 AVG. val Loss 0.6914050579071045 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 6 Batch 25==== Step 2 Probs\n",
            "[[0.4761189  0.52388114]\n",
            " [0.47499928 0.5250007 ]\n",
            " [0.47351715 0.5264828 ]\n",
            " [0.47159144 0.5284086 ]\n",
            " [0.47375202 0.5262479 ]\n",
            " [0.4793019  0.52069813]\n",
            " [0.47881562 0.5211844 ]\n",
            " [0.4856733  0.5143267 ]\n",
            " [0.47509906 0.524901  ]\n",
            " [0.47459865 0.5254014 ]\n",
            " [0.47491765 0.5250824 ]\n",
            " [0.47342148 0.52657855]\n",
            " [0.4793056  0.5206944 ]\n",
            " [0.48242944 0.51757056]\n",
            " [0.48346686 0.51653314]\n",
            " [0.47557023 0.52442974]\n",
            " [0.4730522  0.5269478 ]\n",
            " [0.47948092 0.5205191 ]\n",
            " [0.47331873 0.52668124]\n",
            " [0.47669813 0.5233019 ]\n",
            " [0.47823402 0.521766  ]\n",
            " [0.47683752 0.5231625 ]\n",
            " [0.47419107 0.52580893]\n",
            " [0.46898696 0.5310131 ]\n",
            " [0.46831006 0.53168994]\n",
            " [0.48157433 0.5184257 ]\n",
            " [0.474266   0.525734  ]\n",
            " [0.47913212 0.5208679 ]\n",
            " [0.47568497 0.52431506]\n",
            " [0.48247766 0.5175224 ]\n",
            " [0.4789824  0.5210176 ]\n",
            " [0.4768059  0.5231941 ]]\n",
            "========== Epoch 6 Batch 25==== Step 1 AVG. val Loss 0.68655925989151 ====  0.7450980392156863 ====  0.59375\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0],\n",
            "       device='cuda:0')\n",
            "========== Epoch 6 Batch 26==== Step 2 Probs\n",
            "[[0.48763105 0.512369  ]\n",
            " [0.48069063 0.5193094 ]\n",
            " [0.4741575  0.52584255]\n",
            " [0.47496915 0.52503085]\n",
            " [0.46979728 0.5302027 ]\n",
            " [0.47212398 0.527876  ]\n",
            " [0.47279173 0.52720827]\n",
            " [0.4762165  0.52378345]\n",
            " [0.4798798  0.5201202 ]\n",
            " [0.48036355 0.51963645]\n",
            " [0.4778614  0.5221386 ]\n",
            " [0.47846958 0.5215304 ]\n",
            " [0.46674794 0.53325206]\n",
            " [0.47502115 0.5249788 ]\n",
            " [0.47837272 0.52162725]\n",
            " [0.48152965 0.5184703 ]\n",
            " [0.47389093 0.52610904]\n",
            " [0.47825345 0.5217465 ]\n",
            " [0.47237733 0.5276227 ]\n",
            " [0.4760075  0.5239925 ]\n",
            " [0.4804706  0.5195294 ]]\n",
            "========== Epoch 6 Batch 26==== Step 1 AVG. val Loss 0.6906386613845825 ====  0.6875000000000001 ====  0.5238095238095238\n",
            "  Average Validation Loss: 0.69\n",
            "  Average Validation F1: 0.66\n",
            "  Average Validation Acc: 0.50\n",
            "  Validation took: 0:00:19\n",
            "EarlyStopping counter: 2 out of 4\n",
            "\n",
            "======== Epoch 8 / 2 ========\n",
            "Training...\n",
            "tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 1==== Step 1  Train Loss 0.7005140781402588  ==== 0.5789473684210527  ==== 0.5\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 2==== Step 1  Train Loss 0.6865814328193665  ==== 0.42857142857142855  ==== 0.5\n",
            "tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 3==== Step 1  Train Loss 0.6888290047645569  ==== 0.5294117647058824  ==== 0.5\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 4==== Step 1  Train Loss 0.6952143907546997  ==== 0.6666666666666666  ==== 0.59375\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 5==== Step 1  Train Loss 0.6942198872566223  ==== 0.6111111111111112  ==== 0.5625\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 6==== Step 1  Train Loss 0.6883529424667358  ==== 0.5555555555555556  ==== 0.5\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 7==== Step 1  Train Loss 0.6786903738975525  ==== 0.6666666666666666  ==== 0.59375\n",
            "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 8==== Step 1  Train Loss 0.6777213215827942  ==== 0.6666666666666666  ==== 0.5625\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 9==== Step 1  Train Loss 0.6979241967201233  ==== 0.6153846153846153  ==== 0.53125\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 10==== Step 1  Train Loss 0.6967465877532959  ==== 0.5714285714285715  ==== 0.53125\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 11==== Step 1  Train Loss 0.70367431640625  ==== 0.48484848484848486  ==== 0.46875\n",
            "tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 12==== Step 1  Train Loss 0.6745271682739258  ==== 0.5384615384615385  ==== 0.625\n",
            "tensor([0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 13==== Step 1  Train Loss 0.6819155216217041  ==== 0.6829268292682926  ==== 0.59375\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 14==== Step 1  Train Loss 0.6880612969398499  ==== 0.6  ==== 0.5\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 15==== Step 1  Train Loss 0.6975988745689392  ==== 0.5  ==== 0.5625\n",
            "tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 16==== Step 1  Train Loss 0.7059018611907959  ==== 0.5142857142857143  ==== 0.46875\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 17==== Step 1  Train Loss 0.6736461520195007  ==== 0.5161290322580646  ==== 0.53125\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 18==== Step 1  Train Loss 0.6859491467475891  ==== 0.5161290322580646  ==== 0.53125\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 19==== Step 1  Train Loss 0.6812717318534851  ==== 0.5714285714285715  ==== 0.53125\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 20==== Step 1  Train Loss 0.7032068371772766  ==== 0.5714285714285714  ==== 0.53125\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 21==== Step 1  Train Loss 0.6679888963699341  ==== 0.65  ==== 0.5625\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 22==== Step 1  Train Loss 0.6964157819747925  ==== 0.42857142857142855  ==== 0.5\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 23==== Step 1  Train Loss 0.6879317164421082  ==== 0.6341463414634148  ==== 0.53125\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 24==== Step 1  Train Loss 0.6998518109321594  ==== 0.33333333333333337  ==== 0.375\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 25==== Step 1  Train Loss 0.6864917278289795  ==== 0.717948717948718  ==== 0.65625\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0],\n",
            "       device='cuda:0')\n",
            "========== Epoch 7 Batch 26==== Step 1  Train Loss 0.7052356600761414  ==== 0.4545454545454546  ==== 0.42857142857142855\n",
            "========== Epoch 7 ==== Step 1 AVG. Train Loss 0.6901716429453629\n",
            "\n",
            "  Training epoch took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 1==== Step 2 Probs\n",
            "[[0.48696023 0.5130397 ]\n",
            " [0.483449   0.516551  ]\n",
            " [0.4835348  0.5164652 ]\n",
            " [0.47596312 0.52403694]\n",
            " [0.4907654  0.50923467]\n",
            " [0.48843867 0.51156133]\n",
            " [0.4938481  0.5061519 ]\n",
            " [0.48646694 0.51353306]\n",
            " [0.4912102  0.50878984]\n",
            " [0.47848052 0.52151954]\n",
            " [0.48870343 0.5112965 ]\n",
            " [0.48073378 0.5192662 ]\n",
            " [0.48560697 0.51439303]\n",
            " [0.48734578 0.5126542 ]\n",
            " [0.48387045 0.51612955]\n",
            " [0.48220068 0.5177993 ]\n",
            " [0.48598197 0.514018  ]\n",
            " [0.4924397  0.50756025]\n",
            " [0.48027727 0.51972276]\n",
            " [0.4798007  0.5201993 ]\n",
            " [0.48052695 0.5194731 ]\n",
            " [0.4862238  0.5137762 ]\n",
            " [0.49453852 0.50546145]\n",
            " [0.48886657 0.5111335 ]\n",
            " [0.4893917  0.5106083 ]\n",
            " [0.48747647 0.51252353]\n",
            " [0.4802801  0.51971984]\n",
            " [0.48065206 0.51934797]\n",
            " [0.49453753 0.50546247]\n",
            " [0.4902859  0.50971407]\n",
            " [0.4765372  0.5234628 ]\n",
            " [0.48808518 0.51191485]]\n",
            "========== Epoch 7 Batch 1==== Step 1 AVG. val Loss 0.6881377100944519 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 2==== Step 2 Probs\n",
            "[[0.48275286 0.51724714]\n",
            " [0.48609623 0.5139038 ]\n",
            " [0.4822857  0.5177143 ]\n",
            " [0.47634298 0.523657  ]\n",
            " [0.4873207  0.5126793 ]\n",
            " [0.48028776 0.5197122 ]\n",
            " [0.48286414 0.51713586]\n",
            " [0.4799854  0.52001464]\n",
            " [0.4799785  0.5200215 ]\n",
            " [0.48707542 0.51292455]\n",
            " [0.47325942 0.52674055]\n",
            " [0.4752612  0.5247388 ]\n",
            " [0.49692363 0.5030764 ]\n",
            " [0.4768141  0.5231859 ]\n",
            " [0.47709817 0.52290183]\n",
            " [0.48021507 0.519785  ]\n",
            " [0.48245072 0.51754934]\n",
            " [0.4799977  0.5200023 ]\n",
            " [0.48895854 0.5110414 ]\n",
            " [0.4845693  0.5154307 ]\n",
            " [0.48261645 0.5173835 ]\n",
            " [0.49418527 0.5058147 ]\n",
            " [0.4890207  0.5109793 ]\n",
            " [0.48018405 0.519816  ]\n",
            " [0.47349298 0.526507  ]\n",
            " [0.48535657 0.5146434 ]\n",
            " [0.48161322 0.5183867 ]\n",
            " [0.4867722  0.51322776]\n",
            " [0.474786   0.52521396]\n",
            " [0.475466   0.524534  ]\n",
            " [0.48007807 0.5199219 ]\n",
            " [0.48735982 0.5126402 ]]\n",
            "========== Epoch 7 Batch 2==== Step 1 AVG. val Loss 0.6925144791603088 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 3==== Step 2 Probs\n",
            "[[0.4898397  0.51016027]\n",
            " [0.48207897 0.51792103]\n",
            " [0.47916937 0.52083063]\n",
            " [0.48097998 0.51902   ]\n",
            " [0.4953758  0.5046242 ]\n",
            " [0.48229828 0.51770175]\n",
            " [0.47458923 0.5254108 ]\n",
            " [0.47791958 0.52208036]\n",
            " [0.49640995 0.50359   ]\n",
            " [0.47383922 0.5261607 ]\n",
            " [0.48542476 0.51457524]\n",
            " [0.48537856 0.5146215 ]\n",
            " [0.4901671  0.5098329 ]\n",
            " [0.50024253 0.49975747]\n",
            " [0.48667115 0.51332885]\n",
            " [0.48620886 0.51379114]\n",
            " [0.4818575  0.5181425 ]\n",
            " [0.49033514 0.50966483]\n",
            " [0.4781766  0.52182335]\n",
            " [0.486917   0.51308304]\n",
            " [0.4937558  0.50624424]\n",
            " [0.49325395 0.50674605]\n",
            " [0.48343018 0.51656985]\n",
            " [0.48808235 0.51191765]\n",
            " [0.4782404  0.5217596 ]\n",
            " [0.47957075 0.5204292 ]\n",
            " [0.47756374 0.5224362 ]\n",
            " [0.48368028 0.51631975]\n",
            " [0.48481068 0.5151893 ]\n",
            " [0.4759169  0.52408314]\n",
            " [0.49243388 0.50756615]\n",
            " [0.49148503 0.508515  ]]\n",
            "========== Epoch 7 Batch 3==== Step 1 AVG. val Loss 0.6960878372192383 ====  0.6222222222222222 ====  0.46875\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 4==== Step 2 Probs\n",
            "[[0.48221213 0.5177879 ]\n",
            " [0.4888252  0.51117486]\n",
            " [0.48142773 0.5185723 ]\n",
            " [0.48436174 0.5156383 ]\n",
            " [0.4809637  0.51903635]\n",
            " [0.48687342 0.51312655]\n",
            " [0.47841653 0.52158344]\n",
            " [0.4879332  0.51206684]\n",
            " [0.48707625 0.5129238 ]\n",
            " [0.47320065 0.5267994 ]\n",
            " [0.480138   0.51986194]\n",
            " [0.48888543 0.51111454]\n",
            " [0.4851541  0.5148459 ]\n",
            " [0.47734955 0.5226504 ]\n",
            " [0.48614734 0.51385266]\n",
            " [0.4887335  0.51126647]\n",
            " [0.4847448  0.5152553 ]\n",
            " [0.48053995 0.51946   ]\n",
            " [0.49334803 0.50665194]\n",
            " [0.48470753 0.51529247]\n",
            " [0.4742335  0.5257665 ]\n",
            " [0.48426455 0.51573545]\n",
            " [0.48857248 0.5114275 ]\n",
            " [0.48351744 0.5164826 ]\n",
            " [0.48870805 0.5112919 ]\n",
            " [0.48518986 0.51481014]\n",
            " [0.47773942 0.5222606 ]\n",
            " [0.4863957  0.51360434]\n",
            " [0.47752157 0.52247846]\n",
            " [0.4749696  0.5250304 ]\n",
            " [0.4883356  0.51166433]\n",
            " [0.47844413 0.5215559 ]]\n",
            "========== Epoch 7 Batch 4==== Step 1 AVG. val Loss 0.6939587593078613 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 5==== Step 2 Probs\n",
            "[[0.48529705 0.5147029 ]\n",
            " [0.4780369  0.5219631 ]\n",
            " [0.4924916  0.50750846]\n",
            " [0.49413848 0.5058615 ]\n",
            " [0.4790984  0.52090156]\n",
            " [0.4884248  0.51157516]\n",
            " [0.47163078 0.5283692 ]\n",
            " [0.4755966  0.5244034 ]\n",
            " [0.47674507 0.52325493]\n",
            " [0.48582003 0.51417994]\n",
            " [0.48491555 0.51508445]\n",
            " [0.4840388  0.51596117]\n",
            " [0.48603216 0.5139679 ]\n",
            " [0.47626483 0.52373517]\n",
            " [0.49024323 0.5097568 ]\n",
            " [0.47881368 0.52118635]\n",
            " [0.48398924 0.51601076]\n",
            " [0.48694128 0.51305866]\n",
            " [0.4753313  0.5246687 ]\n",
            " [0.48224187 0.5177581 ]\n",
            " [0.48244804 0.517552  ]\n",
            " [0.48051858 0.5194814 ]\n",
            " [0.47685474 0.52314526]\n",
            " [0.48152572 0.5184743 ]\n",
            " [0.4903651  0.5096349 ]\n",
            " [0.4932131  0.50678694]\n",
            " [0.47486317 0.5251368 ]\n",
            " [0.48168814 0.51831186]\n",
            " [0.48434624 0.5156538 ]\n",
            " [0.48841715 0.51158285]\n",
            " [0.48936406 0.510636  ]\n",
            " [0.4802599  0.51974016]]\n",
            "========== Epoch 7 Batch 5==== Step 1 AVG. val Loss 0.69086754322052 ====  0.72 ====  0.5625\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 6==== Step 2 Probs\n",
            "[[0.49329078 0.50670916]\n",
            " [0.49187464 0.50812536]\n",
            " [0.47504106 0.5249589 ]\n",
            " [0.48380768 0.5161923 ]\n",
            " [0.49543875 0.50456125]\n",
            " [0.479061   0.52093893]\n",
            " [0.47538835 0.52461165]\n",
            " [0.47953543 0.5204646 ]\n",
            " [0.4853585  0.5146415 ]\n",
            " [0.48436642 0.5156336 ]\n",
            " [0.47981757 0.52018243]\n",
            " [0.49015293 0.5098471 ]\n",
            " [0.47768342 0.5223166 ]\n",
            " [0.4846318  0.5153682 ]\n",
            " [0.4895635  0.51043653]\n",
            " [0.49082834 0.50917166]\n",
            " [0.48098496 0.519015  ]\n",
            " [0.48315975 0.5168403 ]\n",
            " [0.48175132 0.5182487 ]\n",
            " [0.48238224 0.51761776]\n",
            " [0.48174965 0.51825035]\n",
            " [0.49084106 0.50915897]\n",
            " [0.48042458 0.51957536]\n",
            " [0.49110606 0.5088939 ]\n",
            " [0.4977817  0.5022183 ]\n",
            " [0.48408672 0.5159133 ]\n",
            " [0.47462565 0.52537435]\n",
            " [0.48046577 0.5195342 ]\n",
            " [0.4892279  0.5107721 ]\n",
            " [0.47353762 0.5264624 ]\n",
            " [0.49179778 0.5082022 ]\n",
            " [0.48372564 0.51627433]]\n",
            "========== Epoch 7 Batch 6==== Step 1 AVG. val Loss 0.6864271759986877 ====  0.72 ====  0.5625\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 7==== Step 2 Probs\n",
            "[[0.4823509  0.5176492 ]\n",
            " [0.48928088 0.5107191 ]\n",
            " [0.4682865  0.5317135 ]\n",
            " [0.48557615 0.51442385]\n",
            " [0.47437233 0.5256276 ]\n",
            " [0.48853984 0.5114602 ]\n",
            " [0.48291674 0.5170833 ]\n",
            " [0.4856564  0.5143436 ]\n",
            " [0.4850824  0.5149177 ]\n",
            " [0.4827519  0.5172481 ]\n",
            " [0.4967323  0.5032677 ]\n",
            " [0.48354664 0.5164533 ]\n",
            " [0.482053   0.51794696]\n",
            " [0.49088493 0.50911504]\n",
            " [0.4853572  0.5146428 ]\n",
            " [0.47923788 0.5207621 ]\n",
            " [0.48414403 0.51585597]\n",
            " [0.48076132 0.51923865]\n",
            " [0.4853581  0.51464194]\n",
            " [0.48835567 0.5116443 ]\n",
            " [0.4831084  0.5168916 ]\n",
            " [0.4796294  0.5203706 ]\n",
            " [0.48215014 0.51784986]\n",
            " [0.49425858 0.5057414 ]\n",
            " [0.48702076 0.51297927]\n",
            " [0.4795723  0.5204277 ]\n",
            " [0.47916207 0.5208379 ]\n",
            " [0.4891264  0.51087356]\n",
            " [0.4835549  0.5164451 ]\n",
            " [0.48723635 0.5127636 ]\n",
            " [0.49058083 0.50941914]\n",
            " [0.48332664 0.5166734 ]]\n",
            "========== Epoch 7 Batch 7==== Step 1 AVG. val Loss 0.6936520934104919 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 8==== Step 2 Probs\n",
            "[[0.47563207 0.5243679 ]\n",
            " [0.48900446 0.5109955 ]\n",
            " [0.49153793 0.5084621 ]\n",
            " [0.48943096 0.5105691 ]\n",
            " [0.48129782 0.51870215]\n",
            " [0.4860958  0.5139042 ]\n",
            " [0.48810405 0.51189595]\n",
            " [0.48740885 0.5125911 ]\n",
            " [0.4853819  0.51461816]\n",
            " [0.4871154  0.5128846 ]\n",
            " [0.48263636 0.51736367]\n",
            " [0.48060402 0.51939595]\n",
            " [0.4716343  0.5283657 ]\n",
            " [0.49413085 0.5058691 ]\n",
            " [0.48216912 0.51783085]\n",
            " [0.49196336 0.5080366 ]\n",
            " [0.488554   0.511446  ]\n",
            " [0.48167682 0.5183231 ]\n",
            " [0.48376098 0.51623905]\n",
            " [0.48440707 0.515593  ]\n",
            " [0.4964573  0.5035427 ]\n",
            " [0.48636767 0.5136323 ]\n",
            " [0.48083586 0.5191641 ]\n",
            " [0.47999948 0.5200005 ]\n",
            " [0.48914915 0.51085085]\n",
            " [0.47854784 0.5214521 ]\n",
            " [0.49124694 0.5087531 ]\n",
            " [0.49509764 0.5049024 ]\n",
            " [0.48780307 0.5121969 ]\n",
            " [0.4760681  0.52393186]\n",
            " [0.48637688 0.5136231 ]\n",
            " [0.49223715 0.50776285]]\n",
            "========== Epoch 7 Batch 8==== Step 1 AVG. val Loss 0.6961514353752136 ====  0.5777777777777777 ====  0.40625\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 9==== Step 2 Probs\n",
            "[[0.48539647 0.5146035 ]\n",
            " [0.48252952 0.51747054]\n",
            " [0.47400287 0.52599716]\n",
            " [0.47814858 0.5218514 ]\n",
            " [0.49229723 0.50770277]\n",
            " [0.4879977  0.5120022 ]\n",
            " [0.48148742 0.5185126 ]\n",
            " [0.4854793  0.5145207 ]\n",
            " [0.49008954 0.5099105 ]\n",
            " [0.47908714 0.5209128 ]\n",
            " [0.47691375 0.52308625]\n",
            " [0.4898358  0.51016414]\n",
            " [0.4823906  0.5176094 ]\n",
            " [0.47313434 0.52686566]\n",
            " [0.48148134 0.5185187 ]\n",
            " [0.48224634 0.51775366]\n",
            " [0.48179406 0.51820594]\n",
            " [0.4829276  0.51707244]\n",
            " [0.4850304  0.51496965]\n",
            " [0.48402306 0.5159769 ]\n",
            " [0.47419694 0.5258031 ]\n",
            " [0.47848052 0.52151954]\n",
            " [0.47661    0.52339   ]\n",
            " [0.48243883 0.51756114]\n",
            " [0.47384205 0.526158  ]\n",
            " [0.48784873 0.5121513 ]\n",
            " [0.48499045 0.5150096 ]\n",
            " [0.48940685 0.5105932 ]\n",
            " [0.49465474 0.5053453 ]\n",
            " [0.48430187 0.51569813]\n",
            " [0.4849639  0.51503617]\n",
            " [0.4782599  0.52174014]]\n",
            "========== Epoch 7 Batch 9==== Step 1 AVG. val Loss 0.6900522112846375 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 10==== Step 2 Probs\n",
            "[[0.48256144 0.5174386 ]\n",
            " [0.47447482 0.5255252 ]\n",
            " [0.486976   0.51302403]\n",
            " [0.47494704 0.52505296]\n",
            " [0.48100635 0.5189937 ]\n",
            " [0.481904   0.518096  ]\n",
            " [0.48425254 0.5157474 ]\n",
            " [0.48912132 0.5108786 ]\n",
            " [0.47616744 0.5238326 ]\n",
            " [0.49287847 0.5071215 ]\n",
            " [0.48008144 0.5199186 ]\n",
            " [0.49626824 0.5037317 ]\n",
            " [0.485864   0.514136  ]\n",
            " [0.47887453 0.52112544]\n",
            " [0.4750313  0.52496874]\n",
            " [0.48761204 0.51238793]\n",
            " [0.4876964  0.5123036 ]\n",
            " [0.49192563 0.50807434]\n",
            " [0.47918186 0.5208181 ]\n",
            " [0.48849666 0.51150334]\n",
            " [0.48160496 0.51839507]\n",
            " [0.47984758 0.52015245]\n",
            " [0.47427297 0.52572703]\n",
            " [0.48828503 0.511715  ]\n",
            " [0.4821355  0.5178645 ]\n",
            " [0.48773545 0.51226455]\n",
            " [0.47867662 0.5213234 ]\n",
            " [0.4831223  0.5168777 ]\n",
            " [0.48386487 0.51613516]\n",
            " [0.48362184 0.51637816]\n",
            " [0.48712647 0.51287353]\n",
            " [0.48345312 0.51654685]]\n",
            "========== Epoch 7 Batch 10==== Step 1 AVG. val Loss 0.6924302577972412 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 11==== Step 2 Probs\n",
            "[[0.49454087 0.50545913]\n",
            " [0.48628634 0.51371366]\n",
            " [0.48558885 0.51441115]\n",
            " [0.48034346 0.51965654]\n",
            " [0.4889383  0.51106167]\n",
            " [0.49553013 0.5044699 ]\n",
            " [0.4933434  0.5066565 ]\n",
            " [0.49238938 0.5076106 ]\n",
            " [0.49367377 0.50632626]\n",
            " [0.49557886 0.5044212 ]\n",
            " [0.47484094 0.52515906]\n",
            " [0.48607853 0.51392144]\n",
            " [0.47848052 0.52151954]\n",
            " [0.47664142 0.5233586 ]\n",
            " [0.4888567  0.5111433 ]\n",
            " [0.49371138 0.5062886 ]\n",
            " [0.495415   0.50458497]\n",
            " [0.4964015  0.5035985 ]\n",
            " [0.4781245  0.5218755 ]\n",
            " [0.4902485  0.5097515 ]\n",
            " [0.49114376 0.50885624]\n",
            " [0.48558742 0.5144126 ]\n",
            " [0.48620144 0.51379853]\n",
            " [0.48516202 0.514838  ]\n",
            " [0.48286375 0.5171362 ]\n",
            " [0.47057077 0.5294292 ]\n",
            " [0.47814655 0.5218535 ]\n",
            " [0.49240378 0.50759625]\n",
            " [0.48660335 0.5133967 ]\n",
            " [0.48138228 0.5186177 ]\n",
            " [0.4859338  0.51406616]\n",
            " [0.48559767 0.51440233]]\n",
            "========== Epoch 7 Batch 11==== Step 1 AVG. val Loss 0.6918625831604004 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 12==== Step 2 Probs\n",
            "[[0.49712822 0.5028718 ]\n",
            " [0.48326463 0.5167354 ]\n",
            " [0.48537344 0.5146266 ]\n",
            " [0.4835462  0.5164538 ]\n",
            " [0.47954163 0.52045834]\n",
            " [0.47840267 0.5215973 ]\n",
            " [0.48482388 0.5151761 ]\n",
            " [0.4855645  0.5144355 ]\n",
            " [0.48738608 0.51261395]\n",
            " [0.49081305 0.5091869 ]\n",
            " [0.4861153  0.51388466]\n",
            " [0.48075545 0.51924455]\n",
            " [0.4886379  0.51136214]\n",
            " [0.481554   0.518446  ]\n",
            " [0.485389   0.514611  ]\n",
            " [0.48380062 0.51619935]\n",
            " [0.49156564 0.5084343 ]\n",
            " [0.4769856  0.52301437]\n",
            " [0.48012552 0.5198745 ]\n",
            " [0.48121503 0.51878494]\n",
            " [0.4680913  0.53190863]\n",
            " [0.47993746 0.5200625 ]\n",
            " [0.491878   0.508122  ]\n",
            " [0.4781019  0.5218981 ]\n",
            " [0.49195823 0.5080418 ]\n",
            " [0.47755367 0.5224464 ]\n",
            " [0.48566228 0.5143377 ]\n",
            " [0.47009176 0.5299082 ]\n",
            " [0.47922543 0.5207746 ]\n",
            " [0.49536684 0.5046332 ]\n",
            " [0.48026875 0.5197313 ]\n",
            " [0.49290967 0.50709033]]\n",
            "========== Epoch 7 Batch 12==== Step 1 AVG. val Loss 0.702605128288269 ====  0.5777777777777777 ====  0.40625\n",
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 13==== Step 2 Probs\n",
            "[[0.47762582 0.5223742 ]\n",
            " [0.49632198 0.5036781 ]\n",
            " [0.4882057  0.51179427]\n",
            " [0.47799912 0.5220009 ]\n",
            " [0.48573005 0.51426995]\n",
            " [0.4909925  0.5090075 ]\n",
            " [0.49026817 0.5097318 ]\n",
            " [0.48201463 0.51798534]\n",
            " [0.4786794  0.52132064]\n",
            " [0.4944803  0.5055197 ]\n",
            " [0.48310634 0.5168936 ]\n",
            " [0.48222435 0.5177756 ]\n",
            " [0.48174813 0.51825184]\n",
            " [0.4853714  0.5146286 ]\n",
            " [0.47882965 0.5211704 ]\n",
            " [0.49640355 0.5035965 ]\n",
            " [0.4972682  0.5027318 ]\n",
            " [0.48647055 0.5135294 ]\n",
            " [0.48333243 0.51666754]\n",
            " [0.49082053 0.5091795 ]\n",
            " [0.47753277 0.52246726]\n",
            " [0.48263848 0.5173616 ]\n",
            " [0.4882986  0.5117014 ]\n",
            " [0.49020046 0.50979954]\n",
            " [0.49544802 0.50455195]\n",
            " [0.4888913  0.51110876]\n",
            " [0.48582634 0.5141737 ]\n",
            " [0.47818232 0.5218177 ]\n",
            " [0.48400816 0.5159918 ]\n",
            " [0.49095517 0.5090448 ]\n",
            " [0.4848969  0.5151031 ]\n",
            " [0.49066046 0.5093395 ]]\n",
            "========== Epoch 7 Batch 13==== Step 1 AVG. val Loss 0.6955278515815735 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 14==== Step 2 Probs\n",
            "[[0.4816277  0.51837236]\n",
            " [0.4777835  0.5222165 ]\n",
            " [0.4958513  0.5041487 ]\n",
            " [0.48123673 0.51876324]\n",
            " [0.4815162  0.5184838 ]\n",
            " [0.48532954 0.5146705 ]\n",
            " [0.48814112 0.51185894]\n",
            " [0.4766169  0.5233831 ]\n",
            " [0.49265027 0.5073497 ]\n",
            " [0.48455235 0.5154476 ]\n",
            " [0.48356882 0.51643115]\n",
            " [0.47982222 0.5201777 ]\n",
            " [0.49521497 0.504785  ]\n",
            " [0.4874302  0.5125698 ]\n",
            " [0.49526364 0.50473636]\n",
            " [0.47889143 0.52110857]\n",
            " [0.48988768 0.51011235]\n",
            " [0.47622934 0.52377063]\n",
            " [0.47920033 0.5207997 ]\n",
            " [0.49263194 0.5073681 ]\n",
            " [0.4818575  0.5181425 ]\n",
            " [0.49135503 0.508645  ]\n",
            " [0.47957423 0.5204258 ]\n",
            " [0.48952344 0.5104765 ]\n",
            " [0.48912928 0.5108707 ]\n",
            " [0.48308492 0.516915  ]\n",
            " [0.47587186 0.52412814]\n",
            " [0.4739983  0.5260017 ]\n",
            " [0.49254188 0.50745815]\n",
            " [0.47846332 0.52153665]\n",
            " [0.47887316 0.52112687]\n",
            " [0.4809207  0.5190793 ]]\n",
            "========== Epoch 7 Batch 14==== Step 1 AVG. val Loss 0.6856257319450378 ====  0.7692307692307693 ====  0.625\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 15==== Step 2 Probs\n",
            "[[0.48503658 0.51496345]\n",
            " [0.48585194 0.5141481 ]\n",
            " [0.47618753 0.52381253]\n",
            " [0.48291174 0.51708823]\n",
            " [0.4927536  0.5072464 ]\n",
            " [0.47499165 0.5250083 ]\n",
            " [0.48531622 0.5146838 ]\n",
            " [0.4754647  0.52453524]\n",
            " [0.4788987  0.52110136]\n",
            " [0.4943314  0.50566864]\n",
            " [0.48314983 0.5168502 ]\n",
            " [0.47765476 0.5223452 ]\n",
            " [0.4893631  0.51063687]\n",
            " [0.486237   0.513763  ]\n",
            " [0.47788584 0.52211416]\n",
            " [0.4836964  0.5163036 ]\n",
            " [0.48297855 0.5170215 ]\n",
            " [0.48889047 0.51110953]\n",
            " [0.4766999  0.5233001 ]\n",
            " [0.49246338 0.50753665]\n",
            " [0.47368646 0.52631354]\n",
            " [0.48488635 0.5151137 ]\n",
            " [0.47873378 0.5212662 ]\n",
            " [0.4815892  0.5184108 ]\n",
            " [0.4907414  0.5092586 ]\n",
            " [0.49088493 0.50911504]\n",
            " [0.49400085 0.50599915]\n",
            " [0.48480263 0.5151974 ]\n",
            " [0.48448095 0.515519  ]\n",
            " [0.49118066 0.50881934]\n",
            " [0.48209432 0.5179057 ]\n",
            " [0.48135126 0.51864874]]\n",
            "========== Epoch 7 Batch 15==== Step 1 AVG. val Loss 0.6953123211860657 ====  0.6666666666666666 ====  0.5\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 16==== Step 2 Probs\n",
            "[[0.48560885 0.5143912 ]\n",
            " [0.48075044 0.5192495 ]\n",
            " [0.47790146 0.5220986 ]\n",
            " [0.4838068  0.5161933 ]\n",
            " [0.49308515 0.50691485]\n",
            " [0.48923028 0.5107697 ]\n",
            " [0.4831571  0.51684296]\n",
            " [0.47905648 0.5209435 ]\n",
            " [0.4820395  0.5179605 ]\n",
            " [0.47154036 0.5284596 ]\n",
            " [0.48660013 0.5133999 ]\n",
            " [0.48495516 0.5150448 ]\n",
            " [0.48355883 0.51644117]\n",
            " [0.47722498 0.522775  ]\n",
            " [0.48882887 0.5111711 ]\n",
            " [0.48091006 0.5190899 ]\n",
            " [0.48595843 0.5140416 ]\n",
            " [0.49704933 0.50295067]\n",
            " [0.4890878  0.5109122 ]\n",
            " [0.49920103 0.50079894]\n",
            " [0.48402953 0.51597047]\n",
            " [0.48126298 0.5187371 ]\n",
            " [0.49010596 0.509894  ]\n",
            " [0.48965317 0.51034683]\n",
            " [0.4767149  0.52328503]\n",
            " [0.48484415 0.51515585]\n",
            " [0.49772757 0.50227237]\n",
            " [0.49171174 0.50828826]\n",
            " [0.48006144 0.5199386 ]\n",
            " [0.49093196 0.5090681 ]\n",
            " [0.48639554 0.51360446]\n",
            " [0.48938912 0.5106109 ]]\n",
            "========== Epoch 7 Batch 16==== Step 1 AVG. val Loss 0.6953383684158325 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 17==== Step 2 Probs\n",
            "[[0.49324578 0.5067542 ]\n",
            " [0.48538902 0.514611  ]\n",
            " [0.48827633 0.51172364]\n",
            " [0.49209458 0.5079054 ]\n",
            " [0.4734266  0.5265734 ]\n",
            " [0.49005044 0.50994956]\n",
            " [0.48549318 0.51450676]\n",
            " [0.47586718 0.52413285]\n",
            " [0.4732167  0.5267833 ]\n",
            " [0.48579496 0.5142051 ]\n",
            " [0.4893261  0.51067394]\n",
            " [0.4816587  0.5183413 ]\n",
            " [0.47439358 0.5256064 ]\n",
            " [0.48418844 0.51581156]\n",
            " [0.4924306  0.5075694 ]\n",
            " [0.48092324 0.51907676]\n",
            " [0.48707625 0.5129238 ]\n",
            " [0.48628452 0.5137155 ]\n",
            " [0.4797573  0.52024263]\n",
            " [0.48411548 0.5158846 ]\n",
            " [0.4853187  0.51468134]\n",
            " [0.48142844 0.5185716 ]\n",
            " [0.4884082  0.5115918 ]\n",
            " [0.48382205 0.51617795]\n",
            " [0.4935711  0.5064289 ]\n",
            " [0.4890912  0.51090884]\n",
            " [0.48282877 0.5171712 ]\n",
            " [0.4916476  0.5083524 ]\n",
            " [0.48595873 0.51404124]\n",
            " [0.49026293 0.5097371 ]\n",
            " [0.48296696 0.51703304]\n",
            " [0.48277843 0.51722157]]\n",
            "========== Epoch 7 Batch 17==== Step 1 AVG. val Loss 0.6838913559913635 ====  0.7924528301886793 ====  0.65625\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 18==== Step 2 Probs\n",
            "[[0.47718355 0.5228164 ]\n",
            " [0.47791353 0.52208644]\n",
            " [0.47421777 0.5257822 ]\n",
            " [0.49154347 0.5084566 ]\n",
            " [0.4819227  0.5180773 ]\n",
            " [0.48165256 0.51834744]\n",
            " [0.49490246 0.5050975 ]\n",
            " [0.48873162 0.5112684 ]\n",
            " [0.47806066 0.52193934]\n",
            " [0.4915477  0.50845224]\n",
            " [0.4777466  0.5222534 ]\n",
            " [0.48163223 0.5183677 ]\n",
            " [0.49693045 0.5030695 ]\n",
            " [0.4936165  0.50638354]\n",
            " [0.48080695 0.51919305]\n",
            " [0.49975574 0.50024426]\n",
            " [0.48116827 0.5188318 ]\n",
            " [0.48215967 0.5178404 ]\n",
            " [0.48354858 0.5164514 ]\n",
            " [0.49242136 0.5075786 ]\n",
            " [0.48499423 0.51500577]\n",
            " [0.48644495 0.51355505]\n",
            " [0.4930272  0.5069728 ]\n",
            " [0.4942273  0.5057727 ]\n",
            " [0.4756278  0.5243722 ]\n",
            " [0.48100647 0.5189935 ]\n",
            " [0.48980394 0.5101961 ]\n",
            " [0.47833478 0.5216652 ]\n",
            " [0.47907162 0.5209283 ]\n",
            " [0.48180804 0.51819193]\n",
            " [0.49239373 0.50760627]\n",
            " [0.49088037 0.50911963]]\n",
            "========== Epoch 7 Batch 18==== Step 1 AVG. val Loss 0.6912506818771362 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 19==== Step 2 Probs\n",
            "[[0.48156458 0.51843536]\n",
            " [0.48683125 0.5131688 ]\n",
            " [0.48152965 0.51847035]\n",
            " [0.4827373  0.51726276]\n",
            " [0.48391882 0.51608115]\n",
            " [0.48431385 0.51568615]\n",
            " [0.49362528 0.5063747 ]\n",
            " [0.48546338 0.5145366 ]\n",
            " [0.48096815 0.5190318 ]\n",
            " [0.47442147 0.5255785 ]\n",
            " [0.4814061  0.5185939 ]\n",
            " [0.4817014  0.5182986 ]\n",
            " [0.48519808 0.5148019 ]\n",
            " [0.48827538 0.5117246 ]\n",
            " [0.49644297 0.503557  ]\n",
            " [0.4877449  0.5122551 ]\n",
            " [0.48350757 0.5164924 ]\n",
            " [0.4806159  0.519384  ]\n",
            " [0.4823721  0.5176279 ]\n",
            " [0.4818727  0.5181273 ]\n",
            " [0.48451757 0.5154824 ]\n",
            " [0.481529   0.518471  ]\n",
            " [0.4862907  0.5137093 ]\n",
            " [0.4749045  0.5250955 ]\n",
            " [0.4896907  0.5103093 ]\n",
            " [0.48255184 0.5174481 ]\n",
            " [0.49183068 0.5081693 ]\n",
            " [0.49005637 0.50994366]\n",
            " [0.47494677 0.5250532 ]\n",
            " [0.48473987 0.5152601 ]\n",
            " [0.4784762  0.5215238 ]\n",
            " [0.48568612 0.5143139 ]]\n",
            "========== Epoch 7 Batch 19==== Step 1 AVG. val Loss 0.6999294757843018 ====  0.5777777777777777 ====  0.40625\n",
            "tensor([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 20==== Step 2 Probs\n",
            "[[0.47433728 0.5256627 ]\n",
            " [0.4756879  0.52431214]\n",
            " [0.473671   0.52632904]\n",
            " [0.49464163 0.5053584 ]\n",
            " [0.48210755 0.5178924 ]\n",
            " [0.4820129  0.5179871 ]\n",
            " [0.47842962 0.52157044]\n",
            " [0.4874493  0.5125507 ]\n",
            " [0.4776247  0.5223753 ]\n",
            " [0.48203978 0.5179602 ]\n",
            " [0.47625104 0.523749  ]\n",
            " [0.48165047 0.5183495 ]\n",
            " [0.48881462 0.5111854 ]\n",
            " [0.4879434  0.5120566 ]\n",
            " [0.48182744 0.51817256]\n",
            " [0.47556072 0.5244393 ]\n",
            " [0.4872155  0.51278454]\n",
            " [0.48398924 0.51601076]\n",
            " [0.48253942 0.5174606 ]\n",
            " [0.48182553 0.51817447]\n",
            " [0.487625   0.512375  ]\n",
            " [0.4814087  0.51859134]\n",
            " [0.4763626  0.5236375 ]\n",
            " [0.49411356 0.50588644]\n",
            " [0.47696126 0.52303874]\n",
            " [0.49101263 0.50898737]\n",
            " [0.4757412  0.5242588 ]\n",
            " [0.49125773 0.5087422 ]\n",
            " [0.47297135 0.5270287 ]\n",
            " [0.49119893 0.5088011 ]\n",
            " [0.48312286 0.5168772 ]\n",
            " [0.48903468 0.5109653 ]]\n",
            "========== Epoch 7 Batch 20==== Step 1 AVG. val Loss 0.6899183988571167 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 21==== Step 2 Probs\n",
            "[[0.48382393 0.5161761 ]\n",
            " [0.4860795  0.5139205 ]\n",
            " [0.49091142 0.5090886 ]\n",
            " [0.47917938 0.52082056]\n",
            " [0.47824183 0.5217582 ]\n",
            " [0.49114376 0.50885624]\n",
            " [0.47801083 0.52198917]\n",
            " [0.47884527 0.5211547 ]\n",
            " [0.4869603  0.51303965]\n",
            " [0.48696023 0.5130397 ]\n",
            " [0.4907602  0.5092398 ]\n",
            " [0.47480345 0.52519655]\n",
            " [0.49164996 0.50835   ]\n",
            " [0.47298476 0.5270152 ]\n",
            " [0.4764136  0.5235864 ]\n",
            " [0.47754055 0.52245945]\n",
            " [0.47721452 0.5227855 ]\n",
            " [0.48326546 0.51673454]\n",
            " [0.47628334 0.5237166 ]\n",
            " [0.49031734 0.50968266]\n",
            " [0.48532453 0.5146755 ]\n",
            " [0.49935642 0.50064355]\n",
            " [0.48440045 0.5155995 ]\n",
            " [0.49197933 0.5080207 ]\n",
            " [0.48255622 0.5174438 ]\n",
            " [0.47775766 0.5222423 ]\n",
            " [0.4866905  0.5133095 ]\n",
            " [0.4739433  0.5260567 ]\n",
            " [0.49196473 0.50803524]\n",
            " [0.48498166 0.5150184 ]\n",
            " [0.48686427 0.51313573]\n",
            " [0.47518834 0.5248116 ]]\n",
            "========== Epoch 7 Batch 21==== Step 1 AVG. val Loss 0.6958702802658081 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 22==== Step 2 Probs\n",
            "[[0.4859798  0.5140202 ]\n",
            " [0.4799262  0.52007383]\n",
            " [0.48996335 0.51003665]\n",
            " [0.47082493 0.52917504]\n",
            " [0.4801166  0.5198834 ]\n",
            " [0.4810524  0.5189476 ]\n",
            " [0.47085315 0.52914685]\n",
            " [0.4854244  0.5145756 ]\n",
            " [0.4821913  0.5178087 ]\n",
            " [0.4847611  0.51523894]\n",
            " [0.49059594 0.5094041 ]\n",
            " [0.4893783  0.51062167]\n",
            " [0.491433   0.50856704]\n",
            " [0.4841405  0.5158595 ]\n",
            " [0.4735647  0.52643526]\n",
            " [0.48922887 0.5107711 ]\n",
            " [0.4998202  0.50017977]\n",
            " [0.49243215 0.5075679 ]\n",
            " [0.4831141  0.5168859 ]\n",
            " [0.48928735 0.5107127 ]\n",
            " [0.4779373  0.5220627 ]\n",
            " [0.47220072 0.5277993 ]\n",
            " [0.4763973  0.5236027 ]\n",
            " [0.48891768 0.51108235]\n",
            " [0.47586095 0.52413905]\n",
            " [0.48475346 0.5152465 ]\n",
            " [0.4818741  0.5181259 ]\n",
            " [0.48950168 0.5104983 ]\n",
            " [0.4915301  0.5084699 ]\n",
            " [0.48770353 0.5122965 ]\n",
            " [0.4735128  0.52648723]\n",
            " [0.47579205 0.52420795]]\n",
            "========== Epoch 7 Batch 22==== Step 1 AVG. val Loss 0.6951798796653748 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 23==== Step 2 Probs\n",
            "[[0.4799381  0.5200619 ]\n",
            " [0.4766182  0.52338177]\n",
            " [0.48961318 0.5103868 ]\n",
            " [0.48211324 0.5178867 ]\n",
            " [0.4889126  0.51108736]\n",
            " [0.48474288 0.51525706]\n",
            " [0.49097574 0.50902426]\n",
            " [0.48448426 0.51551574]\n",
            " [0.47848052 0.52151954]\n",
            " [0.48013872 0.5198613 ]\n",
            " [0.4822649  0.51773506]\n",
            " [0.47730312 0.52269685]\n",
            " [0.49060285 0.50939715]\n",
            " [0.47430423 0.5256958 ]\n",
            " [0.49105605 0.508944  ]\n",
            " [0.4896302  0.51036984]\n",
            " [0.4879567  0.5120433 ]\n",
            " [0.48408666 0.5159133 ]\n",
            " [0.487673   0.51232696]\n",
            " [0.4884063  0.5115937 ]\n",
            " [0.47990197 0.52009803]\n",
            " [0.48958045 0.51041955]\n",
            " [0.48356503 0.51643497]\n",
            " [0.48102385 0.51897615]\n",
            " [0.48540902 0.514591  ]\n",
            " [0.4797573  0.52024263]\n",
            " [0.48700088 0.5129991 ]\n",
            " [0.488693   0.511307  ]\n",
            " [0.48154572 0.5184543 ]\n",
            " [0.4861478  0.5138522 ]\n",
            " [0.48793238 0.51206756]\n",
            " [0.4836823  0.5163177 ]]\n",
            "========== Epoch 7 Batch 23==== Step 1 AVG. val Loss 0.6978181600570679 ====  0.6086956521739131 ====  0.4375\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 7 Batch 24==== Step 2 Probs\n",
            "[[0.49265924 0.5073407 ]\n",
            " [0.4793913  0.5206087 ]\n",
            " [0.4793162  0.52068377]\n",
            " [0.49351728 0.5064827 ]\n",
            " [0.48013833 0.5198617 ]\n",
            " [0.4892799  0.5107201 ]\n",
            " [0.49088654 0.5091135 ]\n",
            " [0.47836637 0.5216336 ]\n",
            " [0.49039596 0.5096041 ]\n",
            " [0.4824654  0.5175346 ]\n",
            " [0.48939124 0.5106088 ]\n",
            " [0.47937974 0.5206203 ]\n",
            " [0.48783165 0.51216835]\n",
            " [0.48094368 0.5190563 ]\n",
            " [0.46910927 0.5308907 ]\n",
            " [0.48833355 0.5116664 ]\n",
            " [0.47525084 0.5247491 ]\n",
            " [0.49289092 0.5071091 ]\n",
            " [0.48848954 0.5115105 ]\n",
            " [0.4889627  0.5110373 ]\n",
            " [0.4880454  0.5119546 ]\n",
            " [0.4957477  0.5042523 ]\n",
            " [0.4810926  0.51890737]\n",
            " [0.48129293 0.5187071 ]\n",
            " [0.48310462 0.51689535]\n",
            " [0.49406478 0.50593525]\n",
            " [0.4859273  0.5140727 ]\n",
            " [0.4892001  0.5107999 ]\n",
            " [0.4795699  0.5204301 ]\n",
            " [0.48098394 0.519016  ]\n",
            " [0.48409617 0.51590383]\n",
            " [0.4820482  0.5179518 ]]\n",
            "========== Epoch 7 Batch 24==== Step 1 AVG. val Loss 0.7033255696296692 ====  0.5777777777777777 ====  0.40625\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 7 Batch 25==== Step 2 Probs\n",
            "[[0.47556704 0.52443296]\n",
            " [0.48145577 0.5185442 ]\n",
            " [0.48696253 0.5130375 ]\n",
            " [0.48745364 0.51254636]\n",
            " [0.47467622 0.52532375]\n",
            " [0.48215777 0.51784223]\n",
            " [0.47502407 0.5249759 ]\n",
            " [0.48775557 0.5122444 ]\n",
            " [0.49231014 0.5076899 ]\n",
            " [0.48442295 0.5155771 ]\n",
            " [0.48211926 0.51788074]\n",
            " [0.48590267 0.51409733]\n",
            " [0.4780868  0.5219132 ]\n",
            " [0.4874076  0.5125924 ]\n",
            " [0.4912156  0.5087845 ]\n",
            " [0.47702906 0.5229709 ]\n",
            " [0.4818964  0.5181036 ]\n",
            " [0.48129866 0.5187014 ]\n",
            " [0.48344398 0.516556  ]\n",
            " [0.49566868 0.5043313 ]\n",
            " [0.4850488  0.51495117]\n",
            " [0.48114234 0.5188577 ]\n",
            " [0.4920546  0.5079454 ]\n",
            " [0.47931743 0.5206826 ]\n",
            " [0.48478258 0.5152175 ]\n",
            " [0.481536   0.51846397]\n",
            " [0.4768115  0.5231885 ]\n",
            " [0.4827373  0.51726276]\n",
            " [0.48638922 0.5136107 ]\n",
            " [0.48335725 0.51664275]\n",
            " [0.48938435 0.51061565]\n",
            " [0.4867917  0.5132083 ]]\n",
            "========== Epoch 7 Batch 25==== Step 1 AVG. val Loss 0.6870328783988953 ====  0.7692307692307693 ====  0.625\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
            "       device='cuda:0')\n",
            "========== Epoch 7 Batch 26==== Step 2 Probs\n",
            "[[0.48332003 0.51668   ]\n",
            " [0.48263815 0.5173619 ]\n",
            " [0.48330787 0.51669216]\n",
            " [0.48634765 0.5136523 ]\n",
            " [0.47644997 0.52355003]\n",
            " [0.48678547 0.5132145 ]\n",
            " [0.49727178 0.5027282 ]\n",
            " [0.4832065  0.51679355]\n",
            " [0.4852662  0.51473385]\n",
            " [0.4906638  0.5093362 ]\n",
            " [0.49266    0.5073401 ]\n",
            " [0.4825151  0.5174849 ]\n",
            " [0.48325536 0.5167446 ]\n",
            " [0.4905742  0.5094258 ]\n",
            " [0.4905086  0.50949144]\n",
            " [0.48529595 0.514704  ]\n",
            " [0.4927426  0.5072574 ]\n",
            " [0.48431855 0.51568145]\n",
            " [0.4796155  0.5203845 ]\n",
            " [0.48834524 0.51165473]\n",
            " [0.48993117 0.51006883]]\n",
            "========== Epoch 7 Batch 26==== Step 1 AVG. val Loss 0.6956111788749695 ====  0.6451612903225806 ====  0.47619047619047616\n",
            "  Average Validation Loss: 0.69\n",
            "  Average Validation F1: 0.67\n",
            "  Average Validation Acc: 0.50\n",
            "  Validation took: 0:00:19\n",
            "EarlyStopping counter: 3 out of 4\n",
            "\n",
            "======== Epoch 9 / 2 ========\n",
            "Training...\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 1==== Step 1  Train Loss 0.705603837966919  ==== 0.6341463414634146  ==== 0.53125\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 2==== Step 1  Train Loss 0.7022223472595215  ==== 0.5945945945945945  ==== 0.53125\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 3==== Step 1  Train Loss 0.7103541493415833  ==== 0.5641025641025642  ==== 0.46875\n",
            "tensor([0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 4==== Step 1  Train Loss 0.689595103263855  ==== 0.631578947368421  ==== 0.5625\n",
            "tensor([0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 5==== Step 1  Train Loss 0.691041886806488  ==== 0.5625  ==== 0.5625\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 6==== Step 1  Train Loss 0.7198383808135986  ==== 0.3225806451612903  ==== 0.34375\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 7==== Step 1  Train Loss 0.7343930602073669  ==== 0.35294117647058826  ==== 0.3125\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 8==== Step 1  Train Loss 0.711938202381134  ==== 0.5238095238095238  ==== 0.375\n",
            "tensor([1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 9==== Step 1  Train Loss 0.6981523633003235  ==== 0.65  ==== 0.5625\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 10==== Step 1  Train Loss 0.7042612433433533  ==== 0.4827586206896552  ==== 0.53125\n",
            "tensor([1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 11==== Step 1  Train Loss 0.7035573720932007  ==== 0.38709677419354843  ==== 0.40625\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 12==== Step 1  Train Loss 0.690547525882721  ==== 0.6111111111111112  ==== 0.5625\n",
            "tensor([1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 13==== Step 1  Train Loss 0.6886712908744812  ==== 0.48484848484848486  ==== 0.46875\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 14==== Step 1  Train Loss 0.7289254069328308  ==== 0.5128205128205129  ==== 0.40625\n",
            "tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 15==== Step 1  Train Loss 0.6796713471412659  ==== 0.6808510638297872  ==== 0.53125\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 16==== Step 1  Train Loss 0.6956424117088318  ==== 0.6315789473684211  ==== 0.5625\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 17==== Step 1  Train Loss 0.6855143904685974  ==== 0.5945945945945946  ==== 0.53125\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 18==== Step 1  Train Loss 0.7033998370170593  ==== 0.4848484848484848  ==== 0.46875\n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 19==== Step 1  Train Loss 0.6737483143806458  ==== 0.631578947368421  ==== 0.5625\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 20==== Step 1  Train Loss 0.6782904863357544  ==== 0.5789473684210527  ==== 0.5\n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 21==== Step 1  Train Loss 0.69400954246521  ==== 0.6000000000000001  ==== 0.625\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 22==== Step 1  Train Loss 0.704056978225708  ==== 0.5  ==== 0.4375\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 23==== Step 1  Train Loss 0.7051609754562378  ==== 0.5142857142857143  ==== 0.46875\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 24==== Step 1  Train Loss 0.7032008171081543  ==== 0.4571428571428572  ==== 0.40625\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 25==== Step 1  Train Loss 0.7047701478004456  ==== 0.42424242424242425  ==== 0.40625\n",
            "tensor([0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
            "       device='cuda:0')\n",
            "========== Epoch 8 Batch 26==== Step 1  Train Loss 0.7010965347290039  ==== 0.39999999999999997  ==== 0.42857142857142855\n",
            "========== Epoch 8 ==== Step 1 AVG. Train Loss 0.7002947674347804\n",
            "\n",
            "  Training epoch took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 1==== Step 2 Probs\n",
            "[[0.48302275 0.5169773 ]\n",
            " [0.4889431  0.5110569 ]\n",
            " [0.4870422  0.5129578 ]\n",
            " [0.48409724 0.51590276]\n",
            " [0.4824596  0.5175404 ]\n",
            " [0.48951247 0.5104875 ]\n",
            " [0.48075366 0.51924634]\n",
            " [0.49195418 0.50804585]\n",
            " [0.48934227 0.5106577 ]\n",
            " [0.48495424 0.51504576]\n",
            " [0.4851357  0.5148643 ]\n",
            " [0.48683867 0.5131613 ]\n",
            " [0.49665266 0.50334734]\n",
            " [0.48405772 0.51594234]\n",
            " [0.48650235 0.51349765]\n",
            " [0.48397258 0.51602745]\n",
            " [0.49439824 0.50560176]\n",
            " [0.48584354 0.5141564 ]\n",
            " [0.49881628 0.50118375]\n",
            " [0.49985987 0.5001402 ]\n",
            " [0.4865232  0.5134768 ]\n",
            " [0.4826862  0.5173138 ]\n",
            " [0.50357366 0.4964263 ]\n",
            " [0.48308438 0.5169156 ]\n",
            " [0.48527426 0.51472574]\n",
            " [0.4901241  0.5098759 ]\n",
            " [0.4895694  0.5104306 ]\n",
            " [0.48538214 0.51461786]\n",
            " [0.49152437 0.5084756 ]\n",
            " [0.49875593 0.50124407]\n",
            " [0.4860837  0.5139163 ]\n",
            " [0.49761057 0.5023895 ]]\n",
            "========== Epoch 8 Batch 1==== Step 1 AVG. val Loss 0.6941059231758118 ====  0.6382978723404255 ====  0.46875\n",
            "tensor([1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 2==== Step 2 Probs\n",
            "[[0.4890636  0.5109364 ]\n",
            " [0.49979085 0.50020915]\n",
            " [0.48524225 0.51475775]\n",
            " [0.49275997 0.50724006]\n",
            " [0.4929775  0.5070225 ]\n",
            " [0.4867449  0.5132551 ]\n",
            " [0.4973744  0.5026256 ]\n",
            " [0.47359696 0.52640307]\n",
            " [0.4867427  0.51325727]\n",
            " [0.49070108 0.50929886]\n",
            " [0.48626706 0.5137329 ]\n",
            " [0.49033192 0.50966805]\n",
            " [0.49996987 0.5000301 ]\n",
            " [0.49106774 0.5089323 ]\n",
            " [0.48697585 0.5130241 ]\n",
            " [0.48260432 0.5173957 ]\n",
            " [0.4800795  0.51992047]\n",
            " [0.49151924 0.5084808 ]\n",
            " [0.49032483 0.50967515]\n",
            " [0.48742703 0.51257294]\n",
            " [0.49154937 0.5084506 ]\n",
            " [0.48800722 0.51199275]\n",
            " [0.48818326 0.51181674]\n",
            " [0.4821349  0.5178651 ]\n",
            " [0.48251528 0.5174847 ]\n",
            " [0.4818321  0.5181679 ]\n",
            " [0.48943204 0.51056796]\n",
            " [0.48383182 0.5161682 ]\n",
            " [0.49990296 0.50009704]\n",
            " [0.4829538  0.5170462 ]\n",
            " [0.4979737  0.5020263 ]\n",
            " [0.4933505  0.5066495 ]]\n",
            "========== Epoch 8 Batch 2==== Step 1 AVG. val Loss 0.6994566321372986 ====  0.6666666666666666 ====  0.5\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 3==== Step 2 Probs\n",
            "[[0.49298108 0.5070189 ]\n",
            " [0.48287725 0.51712275]\n",
            " [0.48534685 0.5146532 ]\n",
            " [0.48451358 0.5154864 ]\n",
            " [0.48050198 0.51949805]\n",
            " [0.49339524 0.5066048 ]\n",
            " [0.492757   0.507243  ]\n",
            " [0.48998436 0.5100156 ]\n",
            " [0.48374093 0.5162591 ]\n",
            " [0.4886583  0.51134175]\n",
            " [0.48228994 0.5177101 ]\n",
            " [0.4929117  0.50708824]\n",
            " [0.49044216 0.50955784]\n",
            " [0.48744655 0.5125534 ]\n",
            " [0.48614824 0.51385176]\n",
            " [0.48320478 0.51679516]\n",
            " [0.49181747 0.50818247]\n",
            " [0.496489   0.503511  ]\n",
            " [0.48008597 0.51991403]\n",
            " [0.47838226 0.52161777]\n",
            " [0.48494783 0.5150522 ]\n",
            " [0.48350963 0.51649034]\n",
            " [0.48297557 0.5170244 ]\n",
            " [0.49232784 0.5076722 ]\n",
            " [0.49049145 0.50950855]\n",
            " [0.48955858 0.51044136]\n",
            " [0.48205167 0.5179483 ]\n",
            " [0.48689565 0.5131044 ]\n",
            " [0.4936846  0.50631547]\n",
            " [0.490348   0.50965196]\n",
            " [0.4844914  0.5155086 ]\n",
            " [0.5021834  0.49781662]]\n",
            "========== Epoch 8 Batch 3==== Step 1 AVG. val Loss 0.683199405670166 ====  0.8076923076923077 ====  0.6875\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 4==== Step 2 Probs\n",
            "[[0.49339482 0.5066052 ]\n",
            " [0.48813406 0.5118659 ]\n",
            " [0.4874056  0.51259446]\n",
            " [0.47465655 0.5253434 ]\n",
            " [0.48473144 0.51526856]\n",
            " [0.48660576 0.51339424]\n",
            " [0.48683524 0.51316476]\n",
            " [0.48750618 0.51249385]\n",
            " [0.48625845 0.51374155]\n",
            " [0.4922796  0.5077205 ]\n",
            " [0.48893875 0.51106125]\n",
            " [0.49401003 0.50598997]\n",
            " [0.4950853  0.50491476]\n",
            " [0.48117542 0.5188246 ]\n",
            " [0.49236968 0.50763035]\n",
            " [0.48344958 0.5165504 ]\n",
            " [0.5002501  0.4997499 ]\n",
            " [0.47914422 0.52085584]\n",
            " [0.4838073  0.5161927 ]\n",
            " [0.49540874 0.5045913 ]\n",
            " [0.49336    0.50664   ]\n",
            " [0.48608342 0.5139166 ]\n",
            " [0.48924562 0.51075435]\n",
            " [0.49866918 0.5013308 ]\n",
            " [0.47462243 0.5253776 ]\n",
            " [0.4852778  0.5147222 ]\n",
            " [0.49380183 0.5061981 ]\n",
            " [0.49044752 0.5095524 ]\n",
            " [0.4823965  0.51760346]\n",
            " [0.4815956  0.5184044 ]\n",
            " [0.47742516 0.5225748 ]\n",
            " [0.485558   0.514442  ]]\n",
            "========== Epoch 8 Batch 4==== Step 1 AVG. val Loss 0.6947407722473145 ====  0.5777777777777778 ====  0.40625\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 5==== Step 2 Probs\n",
            "[[0.5021819  0.49781814]\n",
            " [0.48139668 0.5186033 ]\n",
            " [0.49566755 0.5043324 ]\n",
            " [0.4846543  0.5153457 ]\n",
            " [0.50036216 0.49963784]\n",
            " [0.48384777 0.51615226]\n",
            " [0.48246658 0.5175334 ]\n",
            " [0.49371365 0.5062864 ]\n",
            " [0.48949057 0.51050943]\n",
            " [0.48999292 0.5100071 ]\n",
            " [0.48686767 0.5131324 ]\n",
            " [0.47955453 0.52044547]\n",
            " [0.4973668  0.5026332 ]\n",
            " [0.47913474 0.52086526]\n",
            " [0.50192237 0.4980777 ]\n",
            " [0.4856868  0.51431316]\n",
            " [0.48175403 0.51824594]\n",
            " [0.48697323 0.5130267 ]\n",
            " [0.49483266 0.50516737]\n",
            " [0.49624443 0.5037556 ]\n",
            " [0.4803939  0.5196062 ]\n",
            " [0.49858987 0.5014101 ]\n",
            " [0.48472857 0.5152715 ]\n",
            " [0.48877928 0.51122075]\n",
            " [0.4850461  0.5149539 ]\n",
            " [0.48672718 0.5132728 ]\n",
            " [0.50616556 0.49383444]\n",
            " [0.48647088 0.5135291 ]\n",
            " [0.4997681  0.5002319 ]\n",
            " [0.49752942 0.50247055]\n",
            " [0.4856852  0.5143148 ]\n",
            " [0.48457137 0.5154286 ]]\n",
            "========== Epoch 8 Batch 5==== Step 1 AVG. val Loss 0.6941660642623901 ====  0.6190476190476191 ====  0.5\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 6==== Step 2 Probs\n",
            "[[0.4994234  0.5005766 ]\n",
            " [0.49403632 0.5059637 ]\n",
            " [0.48213592 0.5178641 ]\n",
            " [0.4892743  0.5107257 ]\n",
            " [0.48224002 0.51776   ]\n",
            " [0.4924984  0.50750154]\n",
            " [0.49388716 0.5061129 ]\n",
            " [0.48522118 0.5147788 ]\n",
            " [0.49737233 0.5026276 ]\n",
            " [0.48621476 0.51378524]\n",
            " [0.4859352  0.5140648 ]\n",
            " [0.5028501  0.49714985]\n",
            " [0.48739797 0.51260203]\n",
            " [0.47825378 0.5217462 ]\n",
            " [0.48169604 0.518304  ]\n",
            " [0.48458552 0.5154145 ]\n",
            " [0.5082381  0.49176192]\n",
            " [0.49742624 0.5025737 ]\n",
            " [0.5058228  0.49417728]\n",
            " [0.49138403 0.508616  ]\n",
            " [0.4787895  0.52121043]\n",
            " [0.49534833 0.50465167]\n",
            " [0.50549805 0.49450192]\n",
            " [0.48386326 0.5161367 ]\n",
            " [0.4852049  0.5147951 ]\n",
            " [0.49090528 0.5090947 ]\n",
            " [0.4903032  0.50969684]\n",
            " [0.4821263  0.5178737 ]\n",
            " [0.48276058 0.51723945]\n",
            " [0.4758147  0.52418524]\n",
            " [0.49071097 0.509289  ]\n",
            " [0.48129398 0.518706  ]]\n",
            "========== Epoch 8 Batch 6==== Step 1 AVG. val Loss 0.6899638175964355 ====  0.6666666666666667 ====  0.53125\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 7==== Step 2 Probs\n",
            "[[0.50037783 0.49962217]\n",
            " [0.48483595 0.5151641 ]\n",
            " [0.49313343 0.50686663]\n",
            " [0.4871507  0.51284933]\n",
            " [0.47585696 0.524143  ]\n",
            " [0.48296094 0.51703906]\n",
            " [0.49050108 0.50949895]\n",
            " [0.4928282  0.50717175]\n",
            " [0.50382805 0.49617195]\n",
            " [0.4862383  0.51376164]\n",
            " [0.49156225 0.50843775]\n",
            " [0.4872767  0.5127233 ]\n",
            " [0.48858574 0.5114143 ]\n",
            " [0.48394272 0.51605725]\n",
            " [0.5067257  0.49327427]\n",
            " [0.489892   0.510108  ]\n",
            " [0.48475546 0.51524454]\n",
            " [0.48905206 0.510948  ]\n",
            " [0.48224348 0.5177566 ]\n",
            " [0.48164988 0.5183501 ]\n",
            " [0.4893948  0.51060516]\n",
            " [0.4898154  0.5101846 ]\n",
            " [0.4953543  0.5046457 ]\n",
            " [0.4975729  0.5024271 ]\n",
            " [0.49606565 0.5039343 ]\n",
            " [0.494293   0.505707  ]\n",
            " [0.48931918 0.51068085]\n",
            " [0.4885854  0.5114145 ]\n",
            " [0.49962088 0.5003791 ]\n",
            " [0.49631834 0.50368166]\n",
            " [0.49785686 0.50214314]\n",
            " [0.484974   0.515026  ]]\n",
            "========== Epoch 8 Batch 7==== Step 1 AVG. val Loss 0.6914200782775879 ====  0.6521739130434783 ====  0.5\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 8==== Step 2 Probs\n",
            "[[0.48905715 0.5109428 ]\n",
            " [0.48835793 0.51164204]\n",
            " [0.48816267 0.5118373 ]\n",
            " [0.49232686 0.50767314]\n",
            " [0.4873249  0.51267517]\n",
            " [0.50193393 0.49806607]\n",
            " [0.48466235 0.51533765]\n",
            " [0.493098   0.50690204]\n",
            " [0.4823877  0.5176123 ]\n",
            " [0.49573562 0.5042644 ]\n",
            " [0.49685282 0.5031472 ]\n",
            " [0.49139237 0.5086076 ]\n",
            " [0.48064038 0.5193596 ]\n",
            " [0.49972284 0.50027716]\n",
            " [0.50160915 0.49839085]\n",
            " [0.4867245  0.5132755 ]\n",
            " [0.4872055  0.5127945 ]\n",
            " [0.48998302 0.510017  ]\n",
            " [0.48258922 0.51741076]\n",
            " [0.48776826 0.5122317 ]\n",
            " [0.47927618 0.52072376]\n",
            " [0.48756558 0.5124344 ]\n",
            " [0.49779853 0.50220144]\n",
            " [0.4782781  0.5217219 ]\n",
            " [0.49506482 0.50493515]\n",
            " [0.5039395  0.4960605 ]\n",
            " [0.48597613 0.51402384]\n",
            " [0.4878129  0.5121871 ]\n",
            " [0.48375157 0.51624846]\n",
            " [0.4859377  0.5140623 ]\n",
            " [0.48890516 0.51109487]\n",
            " [0.49093953 0.5090605 ]]\n",
            "========== Epoch 8 Batch 8==== Step 1 AVG. val Loss 0.699388861656189 ====  0.5777777777777777 ====  0.40625\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 9==== Step 2 Probs\n",
            "[[0.49723232 0.5027677 ]\n",
            " [0.48312926 0.51687074]\n",
            " [0.4875974  0.51240265]\n",
            " [0.47791296 0.52208704]\n",
            " [0.4924583  0.5075417 ]\n",
            " [0.4805173  0.51948273]\n",
            " [0.48601574 0.51398426]\n",
            " [0.4901051  0.5098949 ]\n",
            " [0.4959314  0.5040686 ]\n",
            " [0.48030296 0.519697  ]\n",
            " [0.48838037 0.5116197 ]\n",
            " [0.4904521  0.5095479 ]\n",
            " [0.47936875 0.5206312 ]\n",
            " [0.4943826  0.50561744]\n",
            " [0.486123   0.513877  ]\n",
            " [0.4869127  0.51308733]\n",
            " [0.49588525 0.5041147 ]\n",
            " [0.4828854  0.51711464]\n",
            " [0.48677945 0.5132206 ]\n",
            " [0.49298257 0.50701743]\n",
            " [0.48437378 0.5156262 ]\n",
            " [0.49672353 0.50327647]\n",
            " [0.49162716 0.50837284]\n",
            " [0.48594466 0.5140554 ]\n",
            " [0.48558277 0.51441723]\n",
            " [0.48868862 0.5113114 ]\n",
            " [0.48159522 0.5184048 ]\n",
            " [0.48529357 0.51470643]\n",
            " [0.48078012 0.5192199 ]\n",
            " [0.48915774 0.51084226]\n",
            " [0.48951262 0.5104874 ]\n",
            " [0.48577186 0.5142281 ]]\n",
            "========== Epoch 8 Batch 9==== Step 1 AVG. val Loss 0.6921635270118713 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 10==== Step 2 Probs\n",
            "[[0.48243633 0.5175637 ]\n",
            " [0.48040754 0.51959246]\n",
            " [0.48488674 0.51511323]\n",
            " [0.48447475 0.5155252 ]\n",
            " [0.4805853  0.51941466]\n",
            " [0.48416337 0.51583666]\n",
            " [0.5041145  0.49588543]\n",
            " [0.49852458 0.5014754 ]\n",
            " [0.49995247 0.5000475 ]\n",
            " [0.503763   0.496237  ]\n",
            " [0.47923157 0.52076846]\n",
            " [0.4967278  0.50327224]\n",
            " [0.4900668  0.5099332 ]\n",
            " [0.48573586 0.51426417]\n",
            " [0.4848418  0.51515824]\n",
            " [0.4818899  0.5181101 ]\n",
            " [0.48520437 0.5147956 ]\n",
            " [0.49194255 0.5080574 ]\n",
            " [0.4796572  0.52034277]\n",
            " [0.4894797  0.5105203 ]\n",
            " [0.5005648  0.49943525]\n",
            " [0.49413526 0.5058648 ]\n",
            " [0.4837285  0.5162715 ]\n",
            " [0.49008423 0.50991577]\n",
            " [0.4847599  0.5152401 ]\n",
            " [0.49081385 0.5091861 ]\n",
            " [0.50155973 0.4984402 ]\n",
            " [0.48986125 0.5101388 ]\n",
            " [0.48664388 0.5133561 ]\n",
            " [0.48786244 0.51213753]\n",
            " [0.4864653  0.5135347 ]\n",
            " [0.49605867 0.50394136]]\n",
            "========== Epoch 8 Batch 10==== Step 1 AVG. val Loss 0.68826824426651 ====  0.7234042553191489 ====  0.59375\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 11==== Step 2 Probs\n",
            "[[0.4967032  0.5032968 ]\n",
            " [0.48004648 0.5199535 ]\n",
            " [0.4858346  0.51416546]\n",
            " [0.49871355 0.50128645]\n",
            " [0.48406807 0.5159319 ]\n",
            " [0.47954854 0.5204515 ]\n",
            " [0.48861378 0.5113862 ]\n",
            " [0.48737928 0.51262075]\n",
            " [0.4982869  0.5017131 ]\n",
            " [0.48018575 0.51981425]\n",
            " [0.48297492 0.5170251 ]\n",
            " [0.48225787 0.5177421 ]\n",
            " [0.4808107  0.5191893 ]\n",
            " [0.48731375 0.51268625]\n",
            " [0.48916987 0.51083016]\n",
            " [0.4749348  0.52506524]\n",
            " [0.49788943 0.5021106 ]\n",
            " [0.4814192  0.5185808 ]\n",
            " [0.4949854  0.50501454]\n",
            " [0.48358452 0.5164155 ]\n",
            " [0.5025708  0.4974292 ]\n",
            " [0.47954974 0.5204503 ]\n",
            " [0.49236748 0.5076325 ]\n",
            " [0.48849502 0.511505  ]\n",
            " [0.49175462 0.5082454 ]\n",
            " [0.4950569  0.5049431 ]\n",
            " [0.48653895 0.5134611 ]\n",
            " [0.48711947 0.51288056]\n",
            " [0.4892915  0.5107085 ]\n",
            " [0.49969822 0.5003018 ]\n",
            " [0.49757296 0.50242704]\n",
            " [0.4881403  0.51185966]]\n",
            "========== Epoch 8 Batch 11==== Step 1 AVG. val Loss 0.6979902982711792 ====  0.608695652173913 ====  0.4375\n",
            "tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 12==== Step 2 Probs\n",
            "[[0.47488427 0.5251157 ]\n",
            " [0.48117954 0.51882046]\n",
            " [0.47961617 0.52038383]\n",
            " [0.48240313 0.5175969 ]\n",
            " [0.49996987 0.5000301 ]\n",
            " [0.49008736 0.50991267]\n",
            " [0.4853617  0.51463836]\n",
            " [0.48712125 0.5128788 ]\n",
            " [0.48834708 0.51165295]\n",
            " [0.48473534 0.51526463]\n",
            " [0.4837201  0.51627994]\n",
            " [0.4917257  0.50827426]\n",
            " [0.48311657 0.51688343]\n",
            " [0.48102662 0.5189734 ]\n",
            " [0.49526313 0.50473684]\n",
            " [0.4850744  0.5149256 ]\n",
            " [0.48249325 0.5175068 ]\n",
            " [0.47368348 0.5263166 ]\n",
            " [0.49386188 0.5061381 ]\n",
            " [0.49411348 0.5058865 ]\n",
            " [0.48061812 0.5193819 ]\n",
            " [0.49001494 0.509985  ]\n",
            " [0.4825477  0.5174523 ]\n",
            " [0.48293704 0.51706296]\n",
            " [0.4919782  0.5080218 ]\n",
            " [0.49207902 0.5079209 ]\n",
            " [0.49405128 0.5059487 ]\n",
            " [0.47341797 0.526582  ]\n",
            " [0.48721188 0.51278806]\n",
            " [0.48588678 0.51411325]\n",
            " [0.48224774 0.51775223]\n",
            " [0.48062158 0.51937836]]\n",
            "========== Epoch 8 Batch 12==== Step 1 AVG. val Loss 0.6848304271697998 ====  0.7924528301886793 ====  0.65625\n",
            "tensor([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 13==== Step 2 Probs\n",
            "[[0.48433226 0.51566774]\n",
            " [0.4943335  0.50566643]\n",
            " [0.48955965 0.5104404 ]\n",
            " [0.4829285  0.5170715 ]\n",
            " [0.48249978 0.5175002 ]\n",
            " [0.495795   0.5042049 ]\n",
            " [0.4918704  0.5081296 ]\n",
            " [0.4928576  0.5071424 ]\n",
            " [0.4839183  0.51608163]\n",
            " [0.49554205 0.50445795]\n",
            " [0.47385666 0.5261434 ]\n",
            " [0.5011321  0.49886793]\n",
            " [0.4936316  0.50636846]\n",
            " [0.48643458 0.5135654 ]\n",
            " [0.48119822 0.5188018 ]\n",
            " [0.49162483 0.50837517]\n",
            " [0.4940819  0.505918  ]\n",
            " [0.49401587 0.5059841 ]\n",
            " [0.486108   0.513892  ]\n",
            " [0.4843014  0.5156986 ]\n",
            " [0.4911291  0.5088709 ]\n",
            " [0.48546416 0.51453584]\n",
            " [0.47965917 0.5203408 ]\n",
            " [0.4939875  0.5060125 ]\n",
            " [0.4901459  0.5098541 ]\n",
            " [0.4933658  0.5066342 ]\n",
            " [0.48948053 0.5105195 ]\n",
            " [0.47819284 0.5218072 ]\n",
            " [0.4926711  0.50732887]\n",
            " [0.48948824 0.5105117 ]\n",
            " [0.50372523 0.49627474]\n",
            " [0.47767332 0.52232665]]\n",
            "========== Epoch 8 Batch 13==== Step 1 AVG. val Loss 0.6951238512992859 ====  0.6521739130434783 ====  0.5\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 14==== Step 2 Probs\n",
            "[[0.499005   0.500995  ]\n",
            " [0.49315798 0.5068421 ]\n",
            " [0.49349242 0.5065076 ]\n",
            " [0.48268464 0.5173154 ]\n",
            " [0.48119262 0.5188074 ]\n",
            " [0.48987883 0.51012117]\n",
            " [0.4939699  0.50603014]\n",
            " [0.5057314  0.49426866]\n",
            " [0.4894079  0.5105921 ]\n",
            " [0.49642512 0.50357485]\n",
            " [0.4840547  0.5159453 ]\n",
            " [0.4901965  0.50980353]\n",
            " [0.48446077 0.5155392 ]\n",
            " [0.48679256 0.5132075 ]\n",
            " [0.49828997 0.50171006]\n",
            " [0.48773885 0.5122612 ]\n",
            " [0.4867008  0.5132992 ]\n",
            " [0.488266   0.511734  ]\n",
            " [0.47963342 0.5203666 ]\n",
            " [0.49980912 0.5001909 ]\n",
            " [0.4929091  0.5070909 ]\n",
            " [0.49846336 0.50153667]\n",
            " [0.48920557 0.51079446]\n",
            " [0.4965197  0.5034803 ]\n",
            " [0.48486376 0.5151363 ]\n",
            " [0.48677754 0.51322246]\n",
            " [0.4826457  0.5173543 ]\n",
            " [0.48734573 0.51265424]\n",
            " [0.4938052  0.5061948 ]\n",
            " [0.4883725  0.51162744]\n",
            " [0.48893932 0.51106066]\n",
            " [0.49291837 0.5070816 ]]\n",
            "========== Epoch 8 Batch 14==== Step 1 AVG. val Loss 0.6955344676971436 ====  0.5909090909090909 ====  0.4375\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 15==== Step 2 Probs\n",
            "[[0.48587698 0.514123  ]\n",
            " [0.48231107 0.5176889 ]\n",
            " [0.4958363  0.5041637 ]\n",
            " [0.49233097 0.50766903]\n",
            " [0.48468378 0.5153162 ]\n",
            " [0.49666962 0.5033304 ]\n",
            " [0.484657   0.515343  ]\n",
            " [0.4889618  0.51103824]\n",
            " [0.4975677  0.50243235]\n",
            " [0.49961254 0.5003875 ]\n",
            " [0.48534614 0.51465386]\n",
            " [0.47825384 0.5217461 ]\n",
            " [0.50109553 0.49890444]\n",
            " [0.48392206 0.51607794]\n",
            " [0.48530185 0.51469815]\n",
            " [0.48564732 0.5143527 ]\n",
            " [0.49557844 0.50442153]\n",
            " [0.48740077 0.51259923]\n",
            " [0.48717672 0.51282334]\n",
            " [0.5001304  0.49986964]\n",
            " [0.50048625 0.49951372]\n",
            " [0.49344367 0.5065564 ]\n",
            " [0.49446064 0.50553936]\n",
            " [0.4917543  0.5082457 ]\n",
            " [0.48478156 0.51521844]\n",
            " [0.48982474 0.5101753 ]\n",
            " [0.48549944 0.5145006 ]\n",
            " [0.4811694  0.5188306 ]\n",
            " [0.47708565 0.5229143 ]\n",
            " [0.49232343 0.50767654]\n",
            " [0.47905457 0.5209454 ]\n",
            " [0.49169832 0.5083017 ]]\n",
            "========== Epoch 8 Batch 15==== Step 1 AVG. val Loss 0.6975312232971191 ====  0.5000000000000001 ====  0.375\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 16==== Step 2 Probs\n",
            "[[0.48660576 0.51339424]\n",
            " [0.487722   0.512278  ]\n",
            " [0.47704285 0.52295715]\n",
            " [0.49140152 0.5085985 ]\n",
            " [0.48940617 0.51059383]\n",
            " [0.49285394 0.50714606]\n",
            " [0.48187602 0.5181239 ]\n",
            " [0.48187563 0.5181244 ]\n",
            " [0.48360094 0.5163991 ]\n",
            " [0.47806633 0.5219337 ]\n",
            " [0.48777905 0.5122209 ]\n",
            " [0.48955467 0.5104453 ]\n",
            " [0.48788068 0.5121193 ]\n",
            " [0.48804775 0.5119523 ]\n",
            " [0.49907568 0.50092435]\n",
            " [0.4902636  0.5097364 ]\n",
            " [0.48929775 0.51070225]\n",
            " [0.50318134 0.4968186 ]\n",
            " [0.49692917 0.50307083]\n",
            " [0.48229083 0.5177092 ]\n",
            " [0.4801612  0.5198388 ]\n",
            " [0.4981955  0.5018045 ]\n",
            " [0.47770438 0.52229565]\n",
            " [0.48751444 0.5124855 ]\n",
            " [0.48978448 0.5102156 ]\n",
            " [0.48180452 0.51819545]\n",
            " [0.50641197 0.49358806]\n",
            " [0.48286462 0.5171354 ]\n",
            " [0.47896233 0.5210377 ]\n",
            " [0.50466394 0.49533606]\n",
            " [0.48844966 0.5115503 ]\n",
            " [0.48924854 0.5107515 ]]\n",
            "========== Epoch 8 Batch 16==== Step 1 AVG. val Loss 0.6934108138084412 ====  0.6382978723404256 ====  0.46875\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 17==== Step 2 Probs\n",
            "[[0.50106114 0.4989389 ]\n",
            " [0.4953003  0.5046997 ]\n",
            " [0.47469357 0.52530646]\n",
            " [0.48879606 0.511204  ]\n",
            " [0.48192376 0.51807624]\n",
            " [0.4797823  0.52021766]\n",
            " [0.4851811  0.5148189 ]\n",
            " [0.49042183 0.5095781 ]\n",
            " [0.4857663  0.5142337 ]\n",
            " [0.4836813  0.5163187 ]\n",
            " [0.48730302 0.512697  ]\n",
            " [0.4925126  0.50748736]\n",
            " [0.4952101  0.5047899 ]\n",
            " [0.48915675 0.5108432 ]\n",
            " [0.49562323 0.50437677]\n",
            " [0.4943335  0.50566643]\n",
            " [0.49284518 0.5071548 ]\n",
            " [0.47570089 0.52429914]\n",
            " [0.48749265 0.5125073 ]\n",
            " [0.49823657 0.50176346]\n",
            " [0.48450828 0.5154917 ]\n",
            " [0.4952078  0.5047922 ]\n",
            " [0.48279217 0.5172078 ]\n",
            " [0.485776   0.514224  ]\n",
            " [0.47935027 0.52064973]\n",
            " [0.49554572 0.50445426]\n",
            " [0.48571903 0.514281  ]\n",
            " [0.49329025 0.5067098 ]\n",
            " [0.48564717 0.5143528 ]\n",
            " [0.4799104  0.5200896 ]\n",
            " [0.48557106 0.5144289 ]\n",
            " [0.48310736 0.5168926 ]]\n",
            "========== Epoch 8 Batch 17==== Step 1 AVG. val Loss 0.6884990334510803 ====  0.72 ====  0.5625\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 18==== Step 2 Probs\n",
            "[[0.48328704 0.51671296]\n",
            " [0.47619087 0.5238092 ]\n",
            " [0.48982444 0.5101756 ]\n",
            " [0.48336455 0.5166354 ]\n",
            " [0.49897295 0.50102705]\n",
            " [0.48122153 0.5187785 ]\n",
            " [0.4835594  0.51644063]\n",
            " [0.48174965 0.51825035]\n",
            " [0.49077073 0.50922924]\n",
            " [0.49148074 0.5085193 ]\n",
            " [0.48954007 0.51045996]\n",
            " [0.47863364 0.52136636]\n",
            " [0.48335883 0.51664114]\n",
            " [0.4816539  0.51834613]\n",
            " [0.48607907 0.5139209 ]\n",
            " [0.47941437 0.52058566]\n",
            " [0.48210448 0.5178956 ]\n",
            " [0.48158118 0.5184188 ]\n",
            " [0.48172858 0.51827145]\n",
            " [0.47967297 0.52032703]\n",
            " [0.48251334 0.5174867 ]\n",
            " [0.48028353 0.5197165 ]\n",
            " [0.49636018 0.5036398 ]\n",
            " [0.4889336  0.5110664 ]\n",
            " [0.5014357  0.49856427]\n",
            " [0.47778648 0.5222135 ]\n",
            " [0.48788986 0.5121102 ]\n",
            " [0.48957703 0.51042295]\n",
            " [0.48375818 0.5162418 ]\n",
            " [0.4895313  0.51046866]\n",
            " [0.4842018  0.5157982 ]\n",
            " [0.49589047 0.50410956]]\n",
            "========== Epoch 8 Batch 18==== Step 1 AVG. val Loss 0.690622866153717 ====  0.6938775510204082 ====  0.53125\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 8 Batch 19==== Step 2 Probs\n",
            "[[0.49211222 0.5078878 ]\n",
            " [0.49580902 0.504191  ]\n",
            " [0.47932363 0.5206764 ]\n",
            " [0.48806515 0.5119349 ]\n",
            " [0.47802168 0.5219783 ]\n",
            " [0.4806535  0.51934654]\n",
            " [0.48382577 0.5161742 ]\n",
            " [0.48593783 0.5140622 ]\n",
            " [0.4939817  0.50601834]\n",
            " [0.48835433 0.5116457 ]\n",
            " [0.4898468  0.51015323]\n",
            " [0.48048058 0.5195194 ]\n",
            " [0.49622148 0.5037785 ]\n",
            " [0.4856708  0.5143292 ]\n",
            " [0.49468288 0.50531715]\n",
            " [0.49020585 0.5097941 ]\n",
            " [0.49716067 0.5028393 ]\n",
            " [0.4848849  0.51511514]\n",
            " [0.49006563 0.50993437]\n",
            " [0.49259174 0.50740826]\n",
            " [0.48016182 0.5198382 ]\n",
            " [0.48608747 0.5139125 ]\n",
            " [0.49356213 0.50643784]\n",
            " [0.483574   0.51642597]\n",
            " [0.49196324 0.50803673]\n",
            " [0.4870656  0.5129344 ]\n",
            " [0.47707275 0.5229272 ]\n",
            " [0.490039   0.509961  ]\n",
            " [0.49789926 0.5021007 ]\n",
            " [0.48270065 0.51729935]\n",
            " [0.48771223 0.5122878 ]\n",
            " [0.48216474 0.51783526]]\n",
            "========== Epoch 8 Batch 19==== Step 1 AVG. val Loss 0.6870347261428833 ====  0.72 ====  0.5625\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 20==== Step 2 Probs\n",
            "[[0.49155068 0.5084494 ]\n",
            " [0.49639085 0.5036091 ]\n",
            " [0.48912847 0.5108715 ]\n",
            " [0.5049954  0.49500462]\n",
            " [0.48676506 0.513235  ]\n",
            " [0.4798522  0.5201478 ]\n",
            " [0.49163848 0.5083615 ]\n",
            " [0.4985174  0.5014826 ]\n",
            " [0.48788157 0.51211846]\n",
            " [0.47967547 0.52032447]\n",
            " [0.49374846 0.5062516 ]\n",
            " [0.4891979  0.51080215]\n",
            " [0.4991319  0.50086814]\n",
            " [0.4871895  0.5128105 ]\n",
            " [0.49335176 0.5066483 ]\n",
            " [0.48391113 0.51608884]\n",
            " [0.4795     0.5205    ]\n",
            " [0.4858005  0.51419955]\n",
            " [0.48547566 0.51452434]\n",
            " [0.49289775 0.50710225]\n",
            " [0.49265188 0.5073482 ]\n",
            " [0.49723208 0.5027679 ]\n",
            " [0.4854444  0.51455563]\n",
            " [0.47992554 0.5200744 ]\n",
            " [0.47881448 0.5211855 ]\n",
            " [0.48678425 0.5132158 ]\n",
            " [0.50277364 0.49722642]\n",
            " [0.48978367 0.51021636]\n",
            " [0.4781032  0.5218968 ]\n",
            " [0.49721876 0.5027813 ]\n",
            " [0.4876402  0.51235974]\n",
            " [0.48409116 0.51590884]]\n",
            "========== Epoch 8 Batch 20==== Step 1 AVG. val Loss 0.6900261640548706 ====  0.6666666666666666 ====  0.53125\n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 21==== Step 2 Probs\n",
            "[[0.48919895 0.510801  ]\n",
            " [0.50418997 0.49581006]\n",
            " [0.49529934 0.50470066]\n",
            " [0.48973942 0.5102606 ]\n",
            " [0.48527908 0.51472086]\n",
            " [0.4865007  0.51349926]\n",
            " [0.49156418 0.5084358 ]\n",
            " [0.49833256 0.50166744]\n",
            " [0.50255436 0.4974456 ]\n",
            " [0.49411514 0.5058849 ]\n",
            " [0.48774648 0.5122535 ]\n",
            " [0.49701917 0.5029808 ]\n",
            " [0.48727646 0.5127235 ]\n",
            " [0.49082735 0.5091726 ]\n",
            " [0.48250362 0.5174964 ]\n",
            " [0.48203248 0.5179675 ]\n",
            " [0.49228713 0.50771284]\n",
            " [0.4805004  0.5194996 ]\n",
            " [0.49398285 0.50601715]\n",
            " [0.50280297 0.49719706]\n",
            " [0.48376757 0.51623243]\n",
            " [0.50071454 0.49928546]\n",
            " [0.47527683 0.5247231 ]\n",
            " [0.4930899  0.5069101 ]\n",
            " [0.49807447 0.5019255 ]\n",
            " [0.4819821  0.5180178 ]\n",
            " [0.4832171  0.5167829 ]\n",
            " [0.48669794 0.513302  ]\n",
            " [0.5012078  0.49879214]\n",
            " [0.491478   0.50852203]\n",
            " [0.48624536 0.5137546 ]\n",
            " [0.49261916 0.50738084]]\n",
            "========== Epoch 8 Batch 21==== Step 1 AVG. val Loss 0.6934622526168823 ====  0.5853658536585367 ====  0.46875\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 22==== Step 2 Probs\n",
            "[[0.4900147  0.5099853 ]\n",
            " [0.49278024 0.50721973]\n",
            " [0.48293358 0.5170664 ]\n",
            " [0.4857475  0.51425254]\n",
            " [0.48670754 0.5132925 ]\n",
            " [0.4957898  0.5042102 ]\n",
            " [0.48502728 0.51497275]\n",
            " [0.4891048  0.5108952 ]\n",
            " [0.50016826 0.49983174]\n",
            " [0.48942405 0.5105759 ]\n",
            " [0.4868858  0.5131142 ]\n",
            " [0.4762806  0.52371943]\n",
            " [0.49789184 0.5021082 ]\n",
            " [0.4911159  0.5088841 ]\n",
            " [0.48237893 0.5176211 ]\n",
            " [0.48276964 0.5172304 ]\n",
            " [0.49110493 0.50889504]\n",
            " [0.49163622 0.5083638 ]\n",
            " [0.4937237  0.50627625]\n",
            " [0.4833178  0.51668227]\n",
            " [0.48603258 0.51396745]\n",
            " [0.48954296 0.51045704]\n",
            " [0.48690134 0.51309866]\n",
            " [0.4822078  0.5177922 ]\n",
            " [0.49848878 0.5015112 ]\n",
            " [0.50220805 0.49779195]\n",
            " [0.49321413 0.50678587]\n",
            " [0.49298367 0.5070163 ]\n",
            " [0.48782203 0.51217794]\n",
            " [0.49247974 0.5075203 ]\n",
            " [0.48015192 0.5198481 ]\n",
            " [0.48228797 0.517712  ]]\n",
            "========== Epoch 8 Batch 22==== Step 1 AVG. val Loss 0.703843891620636 ====  0.4 ====  0.25\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 23==== Step 2 Probs\n",
            "[[0.48038918 0.5196109 ]\n",
            " [0.47643447 0.52356553]\n",
            " [0.49198544 0.5080145 ]\n",
            " [0.49277648 0.5072235 ]\n",
            " [0.49037275 0.5096273 ]\n",
            " [0.48192286 0.51807714]\n",
            " [0.47669187 0.52330816]\n",
            " [0.4942808  0.50571924]\n",
            " [0.4916642  0.50833577]\n",
            " [0.4910121  0.5089879 ]\n",
            " [0.48080295 0.51919705]\n",
            " [0.48996776 0.51003224]\n",
            " [0.47614828 0.52385175]\n",
            " [0.48963496 0.51036507]\n",
            " [0.50420344 0.49579656]\n",
            " [0.48961958 0.5103804 ]\n",
            " [0.48601928 0.51398075]\n",
            " [0.4867008  0.5132992 ]\n",
            " [0.4932905  0.5067095 ]\n",
            " [0.4880018  0.51199824]\n",
            " [0.49280253 0.5071975 ]\n",
            " [0.49734685 0.5026531 ]\n",
            " [0.49223718 0.5077628 ]\n",
            " [0.49694118 0.50305885]\n",
            " [0.48557806 0.51442194]\n",
            " [0.47700313 0.5229969 ]\n",
            " [0.50011915 0.49988085]\n",
            " [0.49000645 0.50999355]\n",
            " [0.4825832  0.51741683]\n",
            " [0.4792144  0.52078557]\n",
            " [0.49179956 0.50820047]\n",
            " [0.50004315 0.49995688]]\n",
            "========== Epoch 8 Batch 23==== Step 1 AVG. val Loss 0.6931537985801697 ====  0.6666666666666667 ====  0.53125\n",
            "tensor([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 24==== Step 2 Probs\n",
            "[[0.4911314  0.50886863]\n",
            " [0.48732212 0.51267785]\n",
            " [0.48383406 0.5161659 ]\n",
            " [0.490108   0.509892  ]\n",
            " [0.50027573 0.49972433]\n",
            " [0.48566067 0.5143393 ]\n",
            " [0.49064764 0.5093523 ]\n",
            " [0.5017343  0.49826568]\n",
            " [0.48241794 0.51758206]\n",
            " [0.48637533 0.51362467]\n",
            " [0.4849815  0.5150185 ]\n",
            " [0.4841554  0.5158446 ]\n",
            " [0.4893613  0.5106387 ]\n",
            " [0.49379164 0.50620836]\n",
            " [0.49203518 0.5079648 ]\n",
            " [0.49777955 0.50222045]\n",
            " [0.47965416 0.5203458 ]\n",
            " [0.48167658 0.5183234 ]\n",
            " [0.48905587 0.5109441 ]\n",
            " [0.48839653 0.5116034 ]\n",
            " [0.49274036 0.50725967]\n",
            " [0.49787885 0.50212115]\n",
            " [0.50134367 0.49865627]\n",
            " [0.48300567 0.51699436]\n",
            " [0.4993072  0.50069284]\n",
            " [0.5010988  0.4989012 ]\n",
            " [0.4864881  0.5135119 ]\n",
            " [0.4920365  0.50796354]\n",
            " [0.5037126  0.49628738]\n",
            " [0.49320477 0.5067952 ]\n",
            " [0.48413196 0.51586807]\n",
            " [0.50164753 0.4983525 ]]\n",
            "========== Epoch 8 Batch 24==== Step 1 AVG. val Loss 0.6901512145996094 ====  0.6363636363636364 ====  0.5\n",
            "tensor([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 8 Batch 25==== Step 2 Probs\n",
            "[[0.48428306 0.5157169 ]\n",
            " [0.47889897 0.521101  ]\n",
            " [0.48930225 0.5106978 ]\n",
            " [0.48054504 0.519455  ]\n",
            " [0.4861847  0.51381534]\n",
            " [0.49410585 0.5058942 ]\n",
            " [0.49757853 0.5024215 ]\n",
            " [0.5011999  0.4988001 ]\n",
            " [0.4851168  0.5148832 ]\n",
            " [0.49412885 0.5058712 ]\n",
            " [0.49548522 0.50451475]\n",
            " [0.49513495 0.50486505]\n",
            " [0.48108104 0.518919  ]\n",
            " [0.49181426 0.50818574]\n",
            " [0.49267343 0.5073266 ]\n",
            " [0.4887288  0.5112712 ]\n",
            " [0.49264574 0.50735426]\n",
            " [0.48668936 0.5133107 ]\n",
            " [0.48494852 0.5150515 ]\n",
            " [0.48627663 0.51372343]\n",
            " [0.48184523 0.51815474]\n",
            " [0.4999881  0.5000119 ]\n",
            " [0.48236492 0.5176351 ]\n",
            " [0.49223492 0.50776505]\n",
            " [0.48776066 0.5122393 ]\n",
            " [0.4984825  0.50151753]\n",
            " [0.48827884 0.51172113]\n",
            " [0.49132374 0.5086763 ]\n",
            " [0.48630393 0.513696  ]\n",
            " [0.48009026 0.51990974]\n",
            " [0.4865881  0.5134119 ]\n",
            " [0.50167364 0.49832636]]\n",
            "========== Epoch 8 Batch 25==== Step 1 AVG. val Loss 0.7012423276901245 ====  0.5581395348837209 ====  0.40625\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1],\n",
            "       device='cuda:0')\n",
            "========== Epoch 8 Batch 26==== Step 2 Probs\n",
            "[[0.47869468 0.52130526]\n",
            " [0.47799197 0.52200806]\n",
            " [0.4867427  0.51325727]\n",
            " [0.49629995 0.5037001 ]\n",
            " [0.48458388 0.5154161 ]\n",
            " [0.4966505  0.5033495 ]\n",
            " [0.49317214 0.50682783]\n",
            " [0.49311516 0.5068849 ]\n",
            " [0.48418304 0.515817  ]\n",
            " [0.4810605  0.5189395 ]\n",
            " [0.48190904 0.51809096]\n",
            " [0.48380017 0.5161998 ]\n",
            " [0.47982138 0.5201786 ]\n",
            " [0.48958218 0.5104178 ]\n",
            " [0.49780527 0.5021947 ]\n",
            " [0.49134892 0.5086511 ]\n",
            " [0.50062305 0.49937698]\n",
            " [0.49961978 0.5003802 ]\n",
            " [0.49318147 0.50681853]\n",
            " [0.49282348 0.5071765 ]\n",
            " [0.47319642 0.5268036 ]]\n",
            "========== Epoch 8 Batch 26==== Step 1 AVG. val Loss 0.6906193494796753 ====  0.6666666666666666 ====  0.5238095238095238\n",
            "  Average Validation Loss: 0.69\n",
            "  Average Validation F1: 0.64\n",
            "  Average Validation Acc: 0.49\n",
            "  Validation took: 0:00:19\n",
            "EarlyStopping counter: 4 out of 4\n",
            "Early stopping\n",
            "\n",
            "======== Epoch 10 / 2 ========\n",
            "Training...\n",
            "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 1==== Step 1  Train Loss 0.7071424722671509  ==== 0.28571428571428575  ==== 0.375\n",
            "tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 2==== Step 1  Train Loss 0.6803162097930908  ==== 0.6315789473684211  ==== 0.5625\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 3==== Step 1  Train Loss 0.6603389382362366  ==== 0.7333333333333334  ==== 0.75\n",
            "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 4==== Step 1  Train Loss 0.6966963410377502  ==== 0.6341463414634146  ==== 0.53125\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 5==== Step 1  Train Loss 0.6972439289093018  ==== 0.4827586206896552  ==== 0.53125\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 6==== Step 1  Train Loss 0.6884200572967529  ==== 0.45161290322580644  ==== 0.46875\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 7==== Step 1  Train Loss 0.6995245218276978  ==== 0.3703703703703704  ==== 0.46875\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 8==== Step 1  Train Loss 0.7051754593849182  ==== 0.39999999999999997  ==== 0.4375\n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 9==== Step 1  Train Loss 0.7046431303024292  ==== 0.47368421052631576  ==== 0.375\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 10==== Step 1  Train Loss 0.6881728172302246  ==== 0.5945945945945946  ==== 0.53125\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 11==== Step 1  Train Loss 0.6972634196281433  ==== 0.5142857142857143  ==== 0.46875\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 12==== Step 1  Train Loss 0.6906237006187439  ==== 0.5714285714285714  ==== 0.53125\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 13==== Step 1  Train Loss 0.6792986989021301  ==== 0.6829268292682926  ==== 0.59375\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 14==== Step 1  Train Loss 0.6922307014465332  ==== 0.5  ==== 0.5\n",
            "tensor([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 15==== Step 1  Train Loss 0.7025323510169983  ==== 0.29629629629629634  ==== 0.40625\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 16==== Step 1  Train Loss 0.6811558604240417  ==== 0.5925925925925926  ==== 0.65625\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 17==== Step 1  Train Loss 0.7069541215896606  ==== 0.48484848484848486  ==== 0.46875\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 18==== Step 1  Train Loss 0.6907774209976196  ==== 0.4666666666666667  ==== 0.5\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 19==== Step 1  Train Loss 0.6759599447250366  ==== 0.7000000000000001  ==== 0.625\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 20==== Step 1  Train Loss 0.6961038708686829  ==== 0.5517241379310345  ==== 0.59375\n",
            "tensor([1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 21==== Step 1  Train Loss 0.6815546154975891  ==== 0.625  ==== 0.625\n",
            "tensor([0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 22==== Step 1  Train Loss 0.6923421621322632  ==== 0.4827586206896552  ==== 0.53125\n",
            "tensor([0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 23==== Step 1  Train Loss 0.6906007528305054  ==== 0.588235294117647  ==== 0.5625\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 24==== Step 1  Train Loss 0.7028402090072632  ==== 0.4137931034482759  ==== 0.46875\n",
            "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 25==== Step 1  Train Loss 0.7010451555252075  ==== 0.46153846153846156  ==== 0.5625\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
            "       device='cuda:0')\n",
            "========== Epoch 9 Batch 26==== Step 1  Train Loss 0.7046710252761841  ==== 0.5714285714285713  ==== 0.5714285714285714\n",
            "========== Epoch 9 ==== Step 1 AVG. Train Loss 0.6928318417989291\n",
            "\n",
            "  Training epoch took: 0:00:12\n",
            "\n",
            "Running Validation...\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 1==== Step 2 Probs\n",
            "[[0.5016713  0.49832866]\n",
            " [0.50643027 0.4935697 ]\n",
            " [0.5002664  0.49973363]\n",
            " [0.5001296  0.49987042]\n",
            " [0.50887996 0.49112004]\n",
            " [0.50253695 0.49746305]\n",
            " [0.5046766  0.49532345]\n",
            " [0.50504184 0.4949581 ]\n",
            " [0.5088001  0.49119997]\n",
            " [0.49688375 0.50311625]\n",
            " [0.5030075  0.49699253]\n",
            " [0.5126967  0.48730332]\n",
            " [0.50347465 0.49652535]\n",
            " [0.49612737 0.50387263]\n",
            " [0.49754217 0.50245786]\n",
            " [0.5143812  0.48561886]\n",
            " [0.49429646 0.50570357]\n",
            " [0.4958718  0.5041282 ]\n",
            " [0.49152264 0.5084774 ]\n",
            " [0.5079282  0.49207184]\n",
            " [0.5098025  0.49019745]\n",
            " [0.50150466 0.4984954 ]\n",
            " [0.49621284 0.50378716]\n",
            " [0.5060722  0.49392775]\n",
            " [0.4987873  0.50121266]\n",
            " [0.5100496  0.48995042]\n",
            " [0.4945604  0.50543964]\n",
            " [0.49911937 0.5008806 ]\n",
            " [0.5025357  0.4974643 ]\n",
            " [0.5083317  0.49166825]\n",
            " [0.49461135 0.5053886 ]\n",
            " [0.51151884 0.48848116]]\n",
            "========== Epoch 9 Batch 1==== Step 1 AVG. val Loss 0.6910696625709534 ====  0.41666666666666663 ====  0.5625\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 2==== Step 2 Probs\n",
            "[[0.5119138  0.48808625]\n",
            " [0.5011721  0.49882784]\n",
            " [0.50165963 0.4983404 ]\n",
            " [0.51384395 0.48615608]\n",
            " [0.50212497 0.49787506]\n",
            " [0.49774316 0.5022569 ]\n",
            " [0.5084097  0.49159035]\n",
            " [0.5073703  0.4926297 ]\n",
            " [0.50838757 0.49161243]\n",
            " [0.5003229  0.4996771 ]\n",
            " [0.5077849  0.4922151 ]\n",
            " [0.50291836 0.49708167]\n",
            " [0.50547    0.49453005]\n",
            " [0.5085538  0.4914462 ]\n",
            " [0.49816632 0.5018337 ]\n",
            " [0.5117937  0.4882064 ]\n",
            " [0.5087025  0.4912975 ]\n",
            " [0.50932926 0.4906707 ]\n",
            " [0.50438946 0.49561056]\n",
            " [0.50957924 0.49042073]\n",
            " [0.50319684 0.49680322]\n",
            " [0.5129125  0.4870875 ]\n",
            " [0.49905545 0.50094455]\n",
            " [0.50135845 0.49864158]\n",
            " [0.5055866  0.49441338]\n",
            " [0.5134591  0.48654088]\n",
            " [0.5033265  0.49667355]\n",
            " [0.49739203 0.502608  ]\n",
            " [0.49689564 0.5031043 ]\n",
            " [0.4983998  0.5016002 ]\n",
            " [0.4949819  0.5050181 ]\n",
            " [0.5031754  0.49682462]]\n",
            "========== Epoch 9 Batch 2==== Step 1 AVG. val Loss 0.6956440210342407 ====  0.37037037037037035 ====  0.46875\n",
            "tensor([1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 3==== Step 2 Probs\n",
            "[[0.502587   0.49741295]\n",
            " [0.50039005 0.49960998]\n",
            " [0.5025292  0.4974708 ]\n",
            " [0.4978631  0.5021369 ]\n",
            " [0.50624156 0.4937584 ]\n",
            " [0.5090408  0.4909592 ]\n",
            " [0.5110695  0.48893052]\n",
            " [0.5008392  0.4991609 ]\n",
            " [0.5096439  0.49035615]\n",
            " [0.49939084 0.5006091 ]\n",
            " [0.5042346  0.49576545]\n",
            " [0.5067019  0.49329814]\n",
            " [0.49854782 0.5014522 ]\n",
            " [0.50104994 0.49895003]\n",
            " [0.5088734  0.49112657]\n",
            " [0.5050817  0.49491826]\n",
            " [0.5027156  0.4972844 ]\n",
            " [0.51474684 0.48525316]\n",
            " [0.5040774  0.49592257]\n",
            " [0.5034832  0.49651676]\n",
            " [0.5106598  0.48934013]\n",
            " [0.5042092  0.4957908 ]\n",
            " [0.49662158 0.5033784 ]\n",
            " [0.5045975  0.4954025 ]\n",
            " [0.5120898  0.4879102 ]\n",
            " [0.50267005 0.49732998]\n",
            " [0.5013912  0.4986088 ]\n",
            " [0.5000424  0.49995765]\n",
            " [0.50757515 0.4924249 ]\n",
            " [0.5110336  0.4889664 ]\n",
            " [0.50323397 0.49676606]\n",
            " [0.5084172  0.49158284]]\n",
            "========== Epoch 9 Batch 3==== Step 1 AVG. val Loss 0.6908872127532959 ====  0.11764705882352941 ====  0.53125\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 4==== Step 2 Probs\n",
            "[[0.49924994 0.50075006]\n",
            " [0.5059043  0.49409568]\n",
            " [0.4966489  0.5033511 ]\n",
            " [0.5002358  0.4997642 ]\n",
            " [0.50864834 0.49135166]\n",
            " [0.51200414 0.48799583]\n",
            " [0.49617037 0.5038296 ]\n",
            " [0.5017612  0.4982388 ]\n",
            " [0.49751908 0.5024809 ]\n",
            " [0.5114795  0.48852056]\n",
            " [0.5071352  0.4928648 ]\n",
            " [0.5063308  0.49366924]\n",
            " [0.49975765 0.50024235]\n",
            " [0.5066507  0.49334937]\n",
            " [0.505819   0.49418095]\n",
            " [0.49893793 0.50106204]\n",
            " [0.5096041  0.4903959 ]\n",
            " [0.5111357  0.48886427]\n",
            " [0.5005245  0.49947545]\n",
            " [0.51306283 0.4869372 ]\n",
            " [0.5047     0.4953    ]\n",
            " [0.50710785 0.49289212]\n",
            " [0.49762315 0.5023769 ]\n",
            " [0.4982748  0.5017252 ]\n",
            " [0.49431846 0.5056815 ]\n",
            " [0.50721943 0.4927806 ]\n",
            " [0.5136016  0.4863984 ]\n",
            " [0.504941   0.49505904]\n",
            " [0.50813043 0.49186963]\n",
            " [0.49821585 0.50178415]\n",
            " [0.4961401  0.50385994]\n",
            " [0.5092609  0.4907391 ]]\n",
            "========== Epoch 9 Batch 4==== Step 1 AVG. val Loss 0.6981960535049438 ====  0.39999999999999997 ====  0.4375\n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 5==== Step 2 Probs\n",
            "[[0.5090155  0.49098447]\n",
            " [0.50449216 0.49550787]\n",
            " [0.49436754 0.50563246]\n",
            " [0.5145781  0.4854219 ]\n",
            " [0.5009027  0.4990973 ]\n",
            " [0.5000641  0.4999359 ]\n",
            " [0.4962187  0.50378126]\n",
            " [0.5053602  0.4946398 ]\n",
            " [0.51000565 0.48999435]\n",
            " [0.508419   0.491581  ]\n",
            " [0.49446023 0.5055398 ]\n",
            " [0.5027097  0.49729028]\n",
            " [0.4933288  0.5066712 ]\n",
            " [0.50465703 0.49534294]\n",
            " [0.5042842  0.4957158 ]\n",
            " [0.50423205 0.49576792]\n",
            " [0.50422645 0.49577355]\n",
            " [0.504346   0.495654  ]\n",
            " [0.49769402 0.502306  ]\n",
            " [0.49807763 0.5019224 ]\n",
            " [0.50080514 0.49919486]\n",
            " [0.49868128 0.5013187 ]\n",
            " [0.4988863  0.5011137 ]\n",
            " [0.50368696 0.49631307]\n",
            " [0.5104014  0.48959857]\n",
            " [0.5122267  0.4877733 ]\n",
            " [0.50615674 0.49384326]\n",
            " [0.50926834 0.49073163]\n",
            " [0.49816477 0.5018353 ]\n",
            " [0.50974536 0.49025464]\n",
            " [0.50450504 0.49549496]\n",
            " [0.50668156 0.49331847]]\n",
            "========== Epoch 9 Batch 5==== Step 1 AVG. val Loss 0.6925849914550781 ====  0.3846153846153846 ====  0.5\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 6==== Step 2 Probs\n",
            "[[0.5097601  0.4902399 ]\n",
            " [0.51183975 0.48816022]\n",
            " [0.49712658 0.5028734 ]\n",
            " [0.5014167  0.49858335]\n",
            " [0.4978871  0.50211287]\n",
            " [0.5059384  0.4940616 ]\n",
            " [0.50512266 0.4948774 ]\n",
            " [0.50892466 0.4910754 ]\n",
            " [0.5027437  0.4972563 ]\n",
            " [0.500754   0.499246  ]\n",
            " [0.5119696  0.48803034]\n",
            " [0.508632   0.49136797]\n",
            " [0.50985825 0.49014175]\n",
            " [0.49404004 0.50596   ]\n",
            " [0.49831674 0.5016833 ]\n",
            " [0.50735784 0.4926422 ]\n",
            " [0.49780768 0.5021924 ]\n",
            " [0.50066483 0.4993352 ]\n",
            " [0.49830535 0.5016947 ]\n",
            " [0.4957082  0.5042918 ]\n",
            " [0.514104   0.48589602]\n",
            " [0.50682753 0.4931725 ]\n",
            " [0.5037951  0.49620494]\n",
            " [0.5117422  0.48825777]\n",
            " [0.50703824 0.49296173]\n",
            " [0.49509117 0.50490886]\n",
            " [0.5050841  0.49491596]\n",
            " [0.5066221  0.49337792]\n",
            " [0.511819   0.48818097]\n",
            " [0.50288427 0.49711573]\n",
            " [0.50830156 0.49169844]\n",
            " [0.50415605 0.49584398]]\n",
            "========== Epoch 9 Batch 6==== Step 1 AVG. val Loss 0.6968832015991211 ====  0.30769230769230765 ====  0.4375\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 7==== Step 2 Probs\n",
            "[[0.50356096 0.49643898]\n",
            " [0.50503576 0.49496424]\n",
            " [0.5081932  0.4918068 ]\n",
            " [0.5006527  0.49934727]\n",
            " [0.4948465  0.5051535 ]\n",
            " [0.5018817  0.49811828]\n",
            " [0.51432014 0.48567986]\n",
            " [0.515635   0.48436496]\n",
            " [0.5029476  0.49705234]\n",
            " [0.4993141  0.5006859 ]\n",
            " [0.51151884 0.48848116]\n",
            " [0.50055873 0.49944127]\n",
            " [0.5082384  0.49176162]\n",
            " [0.5079089  0.49209115]\n",
            " [0.49741903 0.50258094]\n",
            " [0.5096091  0.49039087]\n",
            " [0.5064747  0.4935254 ]\n",
            " [0.499384   0.5006161 ]\n",
            " [0.492781   0.507219  ]\n",
            " [0.5108138  0.48918626]\n",
            " [0.5061361  0.49386385]\n",
            " [0.49699378 0.5030062 ]\n",
            " [0.49887386 0.5011261 ]\n",
            " [0.50822145 0.49177855]\n",
            " [0.5020716  0.49792832]\n",
            " [0.5023342  0.49766582]\n",
            " [0.50638986 0.4936101 ]\n",
            " [0.5111661  0.48883393]\n",
            " [0.5031235  0.49687642]\n",
            " [0.5085234  0.49147663]\n",
            " [0.49940488 0.5005951 ]\n",
            " [0.5053917  0.49460825]]\n",
            "========== Epoch 9 Batch 7==== Step 1 AVG. val Loss 0.6969298124313354 ====  0.28571428571428575 ====  0.375\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 8==== Step 2 Probs\n",
            "[[0.5066024  0.4933976 ]\n",
            " [0.5008373  0.49916267]\n",
            " [0.5054281  0.49457192]\n",
            " [0.4997924  0.5002076 ]\n",
            " [0.50097567 0.49902433]\n",
            " [0.5071983  0.49280173]\n",
            " [0.5069042  0.49309587]\n",
            " [0.5060667  0.49393335]\n",
            " [0.5125593  0.48744074]\n",
            " [0.4985353  0.50146466]\n",
            " [0.50400734 0.49599266]\n",
            " [0.5075831  0.49241692]\n",
            " [0.50368494 0.49631506]\n",
            " [0.51166785 0.48833212]\n",
            " [0.49608192 0.5039181 ]\n",
            " [0.50526345 0.49473655]\n",
            " [0.5137777  0.48622236]\n",
            " [0.5034079  0.49659207]\n",
            " [0.5059153  0.49408472]\n",
            " [0.51066166 0.48933834]\n",
            " [0.5009464  0.49905357]\n",
            " [0.50487393 0.49512604]\n",
            " [0.5045938  0.4954062 ]\n",
            " [0.4997238  0.50027627]\n",
            " [0.5101026  0.48989734]\n",
            " [0.5095804  0.49041963]\n",
            " [0.51661295 0.48338708]\n",
            " [0.5039653  0.49603465]\n",
            " [0.5062651  0.49373493]\n",
            " [0.5013566  0.49864346]\n",
            " [0.5127326  0.48726735]\n",
            " [0.5025427  0.4974573 ]]\n",
            "========== Epoch 9 Batch 8==== Step 1 AVG. val Loss 0.6922253966331482 ====  0.2857142857142857 ====  0.53125\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 9==== Step 2 Probs\n",
            "[[0.5032875  0.4967125 ]\n",
            " [0.50664693 0.4933531 ]\n",
            " [0.5149773  0.4850227 ]\n",
            " [0.500089   0.49991095]\n",
            " [0.50490314 0.49509683]\n",
            " [0.49798596 0.50201404]\n",
            " [0.50747    0.49253002]\n",
            " [0.5051391  0.49486092]\n",
            " [0.50734204 0.49265802]\n",
            " [0.49822408 0.50177586]\n",
            " [0.5012052  0.49879485]\n",
            " [0.5100315  0.48996848]\n",
            " [0.5148125  0.48518747]\n",
            " [0.5042985  0.4957015 ]\n",
            " [0.49610966 0.50389034]\n",
            " [0.5039021  0.4960979 ]\n",
            " [0.5095321  0.49046797]\n",
            " [0.5145972  0.4854028 ]\n",
            " [0.50756675 0.49243325]\n",
            " [0.50728303 0.49271697]\n",
            " [0.5057978  0.49420223]\n",
            " [0.5017899  0.49821004]\n",
            " [0.49629954 0.5037005 ]\n",
            " [0.4973649  0.50263506]\n",
            " [0.48935094 0.510649  ]\n",
            " [0.5038241  0.49617586]\n",
            " [0.5118175  0.4881825 ]\n",
            " [0.50303084 0.49696913]\n",
            " [0.49909058 0.50090945]\n",
            " [0.50653815 0.49346182]\n",
            " [0.5021576  0.49784243]\n",
            " [0.49987167 0.5001283 ]]\n",
            "========== Epoch 9 Batch 9==== Step 1 AVG. val Loss 0.6899170875549316 ====  0.44444444444444436 ====  0.53125\n",
            "tensor([1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 10==== Step 2 Probs\n",
            "[[0.5051214  0.49487862]\n",
            " [0.5013585  0.49864146]\n",
            " [0.5045561  0.4954439 ]\n",
            " [0.50627404 0.49372596]\n",
            " [0.50579077 0.49420923]\n",
            " [0.5089526  0.49104735]\n",
            " [0.50296146 0.49703857]\n",
            " [0.49899432 0.50100565]\n",
            " [0.5067206  0.4932794 ]\n",
            " [0.51016825 0.48983178]\n",
            " [0.508402   0.491598  ]\n",
            " [0.4974661  0.50253385]\n",
            " [0.5063534  0.49364656]\n",
            " [0.50020146 0.49979854]\n",
            " [0.5110789  0.48892108]\n",
            " [0.49656978 0.50343025]\n",
            " [0.5097708  0.49022922]\n",
            " [0.50524884 0.49475113]\n",
            " [0.49566534 0.5043347 ]\n",
            " [0.50304884 0.4969512 ]\n",
            " [0.50836384 0.4916362 ]\n",
            " [0.5053992  0.49460077]\n",
            " [0.50113124 0.49886873]\n",
            " [0.5070127  0.49298728]\n",
            " [0.50593424 0.49406573]\n",
            " [0.5047998  0.49520028]\n",
            " [0.50359356 0.4964064 ]\n",
            " [0.5092082  0.4907918 ]\n",
            " [0.5019864  0.49801362]\n",
            " [0.50126    0.49874002]\n",
            " [0.5056732  0.49432674]\n",
            " [0.49523145 0.50476855]]\n",
            "========== Epoch 9 Batch 10==== Step 1 AVG. val Loss 0.6924740672111511 ====  0.3478260869565218 ====  0.53125\n",
            "tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 11==== Step 2 Probs\n",
            "[[0.50055754 0.49944246]\n",
            " [0.49095318 0.5090468 ]\n",
            " [0.5078456  0.49215436]\n",
            " [0.5018145  0.49818555]\n",
            " [0.504791   0.49520895]\n",
            " [0.5091258  0.49087417]\n",
            " [0.49722412 0.5027759 ]\n",
            " [0.5050328  0.49496722]\n",
            " [0.49617702 0.503823  ]\n",
            " [0.49499384 0.5050062 ]\n",
            " [0.50293136 0.49706864]\n",
            " [0.5075438  0.4924562 ]\n",
            " [0.505961   0.494039  ]\n",
            " [0.5115945  0.4884056 ]\n",
            " [0.51000667 0.4899933 ]\n",
            " [0.5124463  0.48755378]\n",
            " [0.50024277 0.4997572 ]\n",
            " [0.5058681  0.49413195]\n",
            " [0.49853548 0.5014645 ]\n",
            " [0.50559497 0.494405  ]\n",
            " [0.5024969  0.49750313]\n",
            " [0.51068676 0.48931324]\n",
            " [0.50561357 0.49438643]\n",
            " [0.5037881  0.49621186]\n",
            " [0.50310796 0.496892  ]\n",
            " [0.5057607  0.49423927]\n",
            " [0.51010823 0.4898918 ]\n",
            " [0.50851554 0.49148446]\n",
            " [0.49397978 0.50602025]\n",
            " [0.5075672  0.49243277]\n",
            " [0.5062492  0.49375084]\n",
            " [0.49990368 0.5000963 ]]\n",
            "========== Epoch 9 Batch 11==== Step 1 AVG. val Loss 0.6985641121864319 ====  0.14814814814814817 ====  0.28125\n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 12==== Step 2 Probs\n",
            "[[0.50607985 0.4939201 ]\n",
            " [0.5147013  0.48529866]\n",
            " [0.49529496 0.504705  ]\n",
            " [0.51305693 0.48694304]\n",
            " [0.5048044  0.4951956 ]\n",
            " [0.51282424 0.48717576]\n",
            " [0.5133867  0.48661327]\n",
            " [0.5021748  0.49782518]\n",
            " [0.4946515  0.50534856]\n",
            " [0.49717426 0.5028257 ]\n",
            " [0.510638   0.48936203]\n",
            " [0.51066905 0.489331  ]\n",
            " [0.51004255 0.4899574 ]\n",
            " [0.5101989  0.4898011 ]\n",
            " [0.5004192  0.49958083]\n",
            " [0.500944   0.499056  ]\n",
            " [0.49748343 0.50251657]\n",
            " [0.49748152 0.5025185 ]\n",
            " [0.4982784  0.5017216 ]\n",
            " [0.49580404 0.5041959 ]\n",
            " [0.50634116 0.49365884]\n",
            " [0.5105312  0.48946887]\n",
            " [0.4981132  0.50188684]\n",
            " [0.51078737 0.48921263]\n",
            " [0.50617355 0.4938264 ]\n",
            " [0.503664   0.496336  ]\n",
            " [0.5155392  0.4844608 ]\n",
            " [0.5043605  0.49563947]\n",
            " [0.5028459  0.4971541 ]\n",
            " [0.5043174  0.4956826 ]\n",
            " [0.5018337  0.4981663 ]\n",
            " [0.5021747  0.4978253 ]]\n",
            "========== Epoch 9 Batch 12==== Step 1 AVG. val Loss 0.6911948323249817 ====  0.36363636363636365 ====  0.5625\n",
            "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 13==== Step 2 Probs\n",
            "[[0.5073707  0.4926293 ]\n",
            " [0.5067691  0.4932309 ]\n",
            " [0.5080972  0.49190286]\n",
            " [0.50662524 0.49337476]\n",
            " [0.50851214 0.49148786]\n",
            " [0.506516   0.49348402]\n",
            " [0.49721447 0.5027855 ]\n",
            " [0.50895363 0.49104637]\n",
            " [0.5083023  0.4916977 ]\n",
            " [0.4997816  0.5002184 ]\n",
            " [0.49516976 0.50483024]\n",
            " [0.49832478 0.50167525]\n",
            " [0.5034678  0.49653223]\n",
            " [0.5053642  0.4946358 ]\n",
            " [0.49740556 0.5025945 ]\n",
            " [0.5055345  0.4944655 ]\n",
            " [0.50130516 0.49869487]\n",
            " [0.50449306 0.49550688]\n",
            " [0.5018031  0.4981969 ]\n",
            " [0.49921826 0.5007817 ]\n",
            " [0.5079802  0.4920197 ]\n",
            " [0.505046   0.49495396]\n",
            " [0.5088115  0.4911885 ]\n",
            " [0.48985267 0.5101473 ]\n",
            " [0.49702692 0.5029731 ]\n",
            " [0.4993947  0.5006053 ]\n",
            " [0.5111433  0.48885664]\n",
            " [0.5090091  0.49099088]\n",
            " [0.5142543  0.4857457 ]\n",
            " [0.48987302 0.510127  ]\n",
            " [0.5025206  0.4974794 ]\n",
            " [0.5114274  0.4885726 ]]\n",
            "========== Epoch 9 Batch 13==== Step 1 AVG. val Loss 0.6951174139976501 ====  0.3076923076923077 ====  0.4375\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 14==== Step 2 Probs\n",
            "[[0.5034832  0.49651682]\n",
            " [0.5044382  0.49556178]\n",
            " [0.49931455 0.50068545]\n",
            " [0.51023585 0.48976412]\n",
            " [0.506189   0.49381104]\n",
            " [0.4990437  0.50095636]\n",
            " [0.5087821  0.49121794]\n",
            " [0.4927197  0.5072803 ]\n",
            " [0.5081996  0.49180043]\n",
            " [0.5153369  0.4846632 ]\n",
            " [0.4973656  0.5026344 ]\n",
            " [0.49351326 0.5064867 ]\n",
            " [0.5086727  0.49132726]\n",
            " [0.5010729  0.49892718]\n",
            " [0.5024303  0.4975697 ]\n",
            " [0.505308   0.49469203]\n",
            " [0.49257693 0.50742304]\n",
            " [0.505829   0.494171  ]\n",
            " [0.49793905 0.50206095]\n",
            " [0.49057952 0.5094205 ]\n",
            " [0.49622965 0.50377035]\n",
            " [0.49664995 0.50335   ]\n",
            " [0.5049354  0.49506462]\n",
            " [0.48981345 0.51018655]\n",
            " [0.4944184  0.50558156]\n",
            " [0.49707645 0.50292355]\n",
            " [0.50476795 0.49523205]\n",
            " [0.5107378  0.48926222]\n",
            " [0.50696784 0.49303216]\n",
            " [0.5110695  0.48893052]\n",
            " [0.50394744 0.49605253]\n",
            " [0.50355244 0.49644753]]\n",
            "========== Epoch 9 Batch 14==== Step 1 AVG. val Loss 0.6901412606239319 ====  0.5599999999999999 ====  0.65625\n",
            "tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 15==== Step 2 Probs\n",
            "[[0.5098431  0.49015686]\n",
            " [0.4950876  0.5049124 ]\n",
            " [0.504444   0.49555597]\n",
            " [0.49790984 0.50209016]\n",
            " [0.51101273 0.48898724]\n",
            " [0.49856773 0.5014323 ]\n",
            " [0.5011786  0.4988214 ]\n",
            " [0.49724948 0.5027505 ]\n",
            " [0.49747851 0.50252146]\n",
            " [0.50296676 0.49703327]\n",
            " [0.50109655 0.49890342]\n",
            " [0.50737953 0.49262047]\n",
            " [0.50841004 0.49158996]\n",
            " [0.50945675 0.49054328]\n",
            " [0.50460535 0.49539468]\n",
            " [0.51027817 0.4897218 ]\n",
            " [0.4993114  0.5006886 ]\n",
            " [0.5071257  0.49287432]\n",
            " [0.5059137  0.49408635]\n",
            " [0.4957401  0.5042599 ]\n",
            " [0.5135955  0.48640445]\n",
            " [0.49914953 0.5008505 ]\n",
            " [0.50632274 0.49367732]\n",
            " [0.5081533  0.49184665]\n",
            " [0.49476227 0.5052377 ]\n",
            " [0.49802148 0.5019785 ]\n",
            " [0.5115695  0.48843047]\n",
            " [0.50914824 0.49085176]\n",
            " [0.5026196  0.4973804 ]\n",
            " [0.50403136 0.4959686 ]\n",
            " [0.5071956  0.4928044 ]\n",
            " [0.50329477 0.49670523]]\n",
            "========== Epoch 9 Batch 15==== Step 1 AVG. val Loss 0.6975734829902649 ====  0.22222222222222224 ====  0.34375\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 16==== Step 2 Probs\n",
            "[[0.49945986 0.50054014]\n",
            " [0.50260055 0.49739945]\n",
            " [0.50142795 0.49857208]\n",
            " [0.51785344 0.48214656]\n",
            " [0.5123697  0.4876303 ]\n",
            " [0.50228614 0.49771392]\n",
            " [0.5059992  0.49400076]\n",
            " [0.5013997  0.49860036]\n",
            " [0.5119568  0.48804322]\n",
            " [0.50063974 0.49936032]\n",
            " [0.49255562 0.5074443 ]\n",
            " [0.48984426 0.51015574]\n",
            " [0.51288384 0.48711613]\n",
            " [0.5131923  0.48680776]\n",
            " [0.5061272  0.4938728 ]\n",
            " [0.5064332  0.49356684]\n",
            " [0.5130343  0.48696575]\n",
            " [0.49781448 0.5021855 ]\n",
            " [0.5039509  0.4960491 ]\n",
            " [0.49777755 0.5022225 ]\n",
            " [0.50160444 0.4983956 ]\n",
            " [0.509221   0.49077898]\n",
            " [0.5027975  0.49720252]\n",
            " [0.5181822  0.4818178 ]\n",
            " [0.5013074  0.49869254]\n",
            " [0.49647233 0.5035277 ]\n",
            " [0.5047084  0.4952916 ]\n",
            " [0.5097389  0.49026108]\n",
            " [0.5085386  0.4914614 ]\n",
            " [0.49287087 0.50712913]\n",
            " [0.49447522 0.50552475]\n",
            " [0.5052418  0.49475813]]\n",
            "========== Epoch 9 Batch 16==== Step 1 AVG. val Loss 0.6880340576171875 ====  0.5217391304347827 ====  0.65625\n",
            "tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 17==== Step 2 Probs\n",
            "[[0.50608623 0.4939138 ]\n",
            " [0.5097642  0.49023578]\n",
            " [0.50396943 0.49603057]\n",
            " [0.50412804 0.495872  ]\n",
            " [0.49704883 0.5029512 ]\n",
            " [0.49833524 0.50166476]\n",
            " [0.50041956 0.49958047]\n",
            " [0.50779194 0.4922081 ]\n",
            " [0.50550956 0.49449044]\n",
            " [0.5025738  0.49742624]\n",
            " [0.5074294  0.49257055]\n",
            " [0.5052362  0.4947638 ]\n",
            " [0.5077628  0.49223718]\n",
            " [0.504472   0.495528  ]\n",
            " [0.5103227  0.48967728]\n",
            " [0.50245845 0.49754152]\n",
            " [0.50429946 0.49570054]\n",
            " [0.50044054 0.49955952]\n",
            " [0.5112151  0.48878488]\n",
            " [0.5030654  0.49693456]\n",
            " [0.49448788 0.5055121 ]\n",
            " [0.5071264  0.4928736 ]\n",
            " [0.5020755  0.4979245 ]\n",
            " [0.50149906 0.4985009 ]\n",
            " [0.5076697  0.49233034]\n",
            " [0.50615984 0.49384022]\n",
            " [0.51174724 0.4882528 ]\n",
            " [0.50265026 0.49734974]\n",
            " [0.50740623 0.49259382]\n",
            " [0.50409716 0.49590284]\n",
            " [0.4966412  0.50335884]\n",
            " [0.49613088 0.5038691 ]]\n",
            "========== Epoch 9 Batch 17==== Step 1 AVG. val Loss 0.6955459713935852 ====  0.09523809523809523 ====  0.40625\n",
            "tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 18==== Step 2 Probs\n",
            "[[0.4992468  0.5007532 ]\n",
            " [0.5127497  0.48725033]\n",
            " [0.51282865 0.48717138]\n",
            " [0.49852455 0.50147545]\n",
            " [0.51098156 0.4890185 ]\n",
            " [0.49499476 0.50500524]\n",
            " [0.5052249  0.49477518]\n",
            " [0.49730048 0.50269955]\n",
            " [0.50799733 0.49200267]\n",
            " [0.50317425 0.49682578]\n",
            " [0.49869558 0.5013044 ]\n",
            " [0.50712544 0.49287456]\n",
            " [0.5110537  0.48894635]\n",
            " [0.49688148 0.5031185 ]\n",
            " [0.5184392  0.48156083]\n",
            " [0.4933919  0.5066081 ]\n",
            " [0.50324297 0.49675706]\n",
            " [0.5075742  0.49242577]\n",
            " [0.5125293  0.4874707 ]\n",
            " [0.500126   0.49987406]\n",
            " [0.49853    0.50146997]\n",
            " [0.5060856  0.49391448]\n",
            " [0.5053118  0.4946882 ]\n",
            " [0.51484704 0.4851529 ]\n",
            " [0.50015724 0.49984273]\n",
            " [0.51130855 0.48869148]\n",
            " [0.5091242  0.49087572]\n",
            " [0.5014639  0.4985361 ]\n",
            " [0.5097375  0.49026248]\n",
            " [0.5107508  0.48924926]\n",
            " [0.50935906 0.49064094]\n",
            " [0.5015316  0.4984684 ]]\n",
            "========== Epoch 9 Batch 18==== Step 1 AVG. val Loss 0.6919549703598022 ====  0.45454545454545453 ====  0.625\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 19==== Step 2 Probs\n",
            "[[0.5063946  0.49360535]\n",
            " [0.5000029  0.4999971 ]\n",
            " [0.504195   0.49580503]\n",
            " [0.49613023 0.5038698 ]\n",
            " [0.49698022 0.50301975]\n",
            " [0.50250113 0.4974989 ]\n",
            " [0.49841717 0.5015828 ]\n",
            " [0.5061404  0.49385962]\n",
            " [0.4969596  0.5030404 ]\n",
            " [0.497546   0.50245404]\n",
            " [0.498542   0.501458  ]\n",
            " [0.4941125  0.5058875 ]\n",
            " [0.5139332  0.48606676]\n",
            " [0.50606406 0.49393594]\n",
            " [0.5127728  0.4872272 ]\n",
            " [0.50401586 0.4959841 ]\n",
            " [0.50898606 0.49101397]\n",
            " [0.50945246 0.4905475 ]\n",
            " [0.5082909  0.4917091 ]\n",
            " [0.5064233  0.4935767 ]\n",
            " [0.5076863  0.49231368]\n",
            " [0.50496066 0.49503934]\n",
            " [0.5136001  0.48639986]\n",
            " [0.5071391  0.49286088]\n",
            " [0.5014515  0.4985485 ]\n",
            " [0.5022983  0.4977017 ]\n",
            " [0.5038449  0.49615508]\n",
            " [0.50879323 0.4912068 ]\n",
            " [0.5047382  0.49526182]\n",
            " [0.5055681  0.49443188]\n",
            " [0.4943569  0.5056431 ]\n",
            " [0.50045663 0.4995434 ]]\n",
            "========== Epoch 9 Batch 19==== Step 1 AVG. val Loss 0.6890687346458435 ====  0.5263157894736842 ====  0.71875\n",
            "tensor([1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 20==== Step 2 Probs\n",
            "[[0.5013283  0.49867174]\n",
            " [0.50347435 0.49652565]\n",
            " [0.5074543  0.49254572]\n",
            " [0.50521904 0.49478096]\n",
            " [0.49485236 0.50514764]\n",
            " [0.5049165  0.4950835 ]\n",
            " [0.50519806 0.49480194]\n",
            " [0.50060016 0.4993998 ]\n",
            " [0.50161767 0.49838233]\n",
            " [0.49802098 0.50197905]\n",
            " [0.50111806 0.49888197]\n",
            " [0.50620776 0.49379227]\n",
            " [0.5115922  0.48840776]\n",
            " [0.5014167  0.49858335]\n",
            " [0.50453746 0.49546254]\n",
            " [0.4987114  0.5012886 ]\n",
            " [0.493517   0.506483  ]\n",
            " [0.5083553  0.4916447 ]\n",
            " [0.5008127  0.49918726]\n",
            " [0.50955945 0.49044052]\n",
            " [0.5048557  0.4951443 ]\n",
            " [0.5066925  0.49330747]\n",
            " [0.5008319  0.4991681 ]\n",
            " [0.51297396 0.48702604]\n",
            " [0.50075245 0.49924752]\n",
            " [0.5045539  0.4954461 ]\n",
            " [0.511327   0.48867303]\n",
            " [0.50362825 0.4963718 ]\n",
            " [0.4975775  0.5024225 ]\n",
            " [0.506451   0.49354902]\n",
            " [0.5055966  0.4944034 ]\n",
            " [0.50125724 0.49874273]]\n",
            "========== Epoch 9 Batch 20==== Step 1 AVG. val Loss 0.6951496601104736 ====  0.2727272727272727 ====  0.5\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 21==== Step 2 Probs\n",
            "[[0.49737617 0.5026238 ]\n",
            " [0.5035396  0.49646032]\n",
            " [0.4996561  0.5003439 ]\n",
            " [0.5033235  0.49667653]\n",
            " [0.51569784 0.48430213]\n",
            " [0.50785357 0.49214646]\n",
            " [0.5094248  0.49057513]\n",
            " [0.49994746 0.5000526 ]\n",
            " [0.5092274  0.49077258]\n",
            " [0.50057936 0.4994206 ]\n",
            " [0.51308435 0.48691568]\n",
            " [0.5030672  0.4969328 ]\n",
            " [0.512372   0.48762798]\n",
            " [0.51149595 0.48850408]\n",
            " [0.5037681  0.4962319 ]\n",
            " [0.4864658  0.5135342 ]\n",
            " [0.4988339  0.5011661 ]\n",
            " [0.5078337  0.49216628]\n",
            " [0.50173324 0.4982667 ]\n",
            " [0.5043138  0.49568617]\n",
            " [0.51173866 0.48826137]\n",
            " [0.49936867 0.50063133]\n",
            " [0.50105184 0.49894813]\n",
            " [0.49786216 0.50213784]\n",
            " [0.49858728 0.5014127 ]\n",
            " [0.49957913 0.50042087]\n",
            " [0.51156205 0.48843792]\n",
            " [0.49162757 0.5083724 ]\n",
            " [0.50381756 0.49618238]\n",
            " [0.49246708 0.50753295]\n",
            " [0.49568322 0.5043168 ]\n",
            " [0.5090794  0.4909206 ]]\n",
            "========== Epoch 9 Batch 21==== Step 1 AVG. val Loss 0.695433497428894 ====  0.35714285714285715 ====  0.4375\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 22==== Step 2 Probs\n",
            "[[0.49890214 0.5010978 ]\n",
            " [0.5058029  0.49419704]\n",
            " [0.503339   0.496661  ]\n",
            " [0.50647444 0.4935256 ]\n",
            " [0.50730664 0.49269336]\n",
            " [0.50694704 0.49305296]\n",
            " [0.4992257  0.5007743 ]\n",
            " [0.508555   0.491445  ]\n",
            " [0.49741346 0.50258654]\n",
            " [0.49797872 0.5020213 ]\n",
            " [0.50149393 0.49850613]\n",
            " [0.5087222  0.49127784]\n",
            " [0.5045349  0.49546507]\n",
            " [0.50899756 0.4910024 ]\n",
            " [0.50188196 0.49811798]\n",
            " [0.51358885 0.48641115]\n",
            " [0.4965111  0.5034889 ]\n",
            " [0.5170805  0.4829195 ]\n",
            " [0.5018419  0.49815804]\n",
            " [0.5028545  0.4971455 ]\n",
            " [0.51106036 0.4889396 ]\n",
            " [0.50737846 0.49262157]\n",
            " [0.51007617 0.4899238 ]\n",
            " [0.51297396 0.48702604]\n",
            " [0.5057826  0.49421743]\n",
            " [0.5090631  0.4909369 ]\n",
            " [0.5001968  0.49980316]\n",
            " [0.49639407 0.50360596]\n",
            " [0.51534337 0.48465666]\n",
            " [0.50769305 0.49230692]\n",
            " [0.50711423 0.49288574]\n",
            " [0.50256616 0.49743387]]\n",
            "========== Epoch 9 Batch 22==== Step 1 AVG. val Loss 0.6954762935638428 ====  0.09523809523809522 ====  0.40625\n",
            "tensor([1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 23==== Step 2 Probs\n",
            "[[0.49822295 0.50177705]\n",
            " [0.50156164 0.49843836]\n",
            " [0.5025768  0.49742317]\n",
            " [0.500471   0.499529  ]\n",
            " [0.51691884 0.48308116]\n",
            " [0.5028458  0.49715415]\n",
            " [0.50446105 0.49553892]\n",
            " [0.5093821  0.49061784]\n",
            " [0.5050694  0.49493065]\n",
            " [0.5127852  0.48721477]\n",
            " [0.50117517 0.4988248 ]\n",
            " [0.50570446 0.49429554]\n",
            " [0.4988828  0.5011172 ]\n",
            " [0.5046461  0.4953539 ]\n",
            " [0.50629115 0.49370888]\n",
            " [0.5037149  0.4962851 ]\n",
            " [0.5090864  0.4909136 ]\n",
            " [0.50987035 0.49012968]\n",
            " [0.50108576 0.4989142 ]\n",
            " [0.5069956  0.49300432]\n",
            " [0.50413156 0.49586847]\n",
            " [0.503207   0.49679294]\n",
            " [0.5039942  0.49600574]\n",
            " [0.50837684 0.49162313]\n",
            " [0.5016551  0.49834493]\n",
            " [0.51232964 0.48767033]\n",
            " [0.50724804 0.49275196]\n",
            " [0.5027585  0.49724147]\n",
            " [0.50230765 0.49769232]\n",
            " [0.49977472 0.5002253 ]\n",
            " [0.50627387 0.4937261 ]\n",
            " [0.5069367  0.4930632 ]]\n",
            "========== Epoch 9 Batch 23==== Step 1 AVG. val Loss 0.6916525959968567 ====  0.11764705882352941 ====  0.53125\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n",
            "========== Epoch 9 Batch 24==== Step 2 Probs\n",
            "[[0.49345687 0.5065431 ]\n",
            " [0.50958514 0.49041483]\n",
            " [0.49389797 0.506102  ]\n",
            " [0.4970327  0.5029673 ]\n",
            " [0.509185   0.49081498]\n",
            " [0.5014356  0.49856445]\n",
            " [0.50435835 0.49564165]\n",
            " [0.5062414  0.49375865]\n",
            " [0.5126143  0.48738572]\n",
            " [0.5015873  0.49841267]\n",
            " [0.50428444 0.49571556]\n",
            " [0.5083221  0.49167788]\n",
            " [0.49191037 0.50808966]\n",
            " [0.5089714  0.49102864]\n",
            " [0.49653366 0.5034663 ]\n",
            " [0.51429915 0.48570085]\n",
            " [0.5020299  0.49797013]\n",
            " [0.5076207  0.4923793 ]\n",
            " [0.50614744 0.49385256]\n",
            " [0.5020309  0.4979691 ]\n",
            " [0.50474244 0.49525762]\n",
            " [0.5048086  0.4951914 ]\n",
            " [0.5070047  0.49299532]\n",
            " [0.49516058 0.50483936]\n",
            " [0.49608845 0.50391155]\n",
            " [0.49922875 0.5007712 ]\n",
            " [0.50518155 0.49481848]\n",
            " [0.49527088 0.5047291 ]\n",
            " [0.5060353  0.49396473]\n",
            " [0.49590835 0.5040916 ]\n",
            " [0.51084304 0.48915696]\n",
            " [0.49850297 0.50149703]]\n",
            "========== Epoch 9 Batch 24==== Step 1 AVG. val Loss 0.6940326690673828 ====  0.42857142857142855 ====  0.5\n",
            "tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
            "========== Epoch 9 Batch 25==== Step 2 Probs\n",
            "[[0.5074526  0.49254733]\n",
            " [0.50720125 0.49279875]\n",
            " [0.5001413  0.49985868]\n",
            " [0.4982838  0.5017162 ]\n",
            " [0.49047595 0.50952405]\n",
            " [0.4941125  0.5058875 ]\n",
            " [0.50196195 0.49803805]\n",
            " [0.50788593 0.492114  ]\n",
            " [0.50853366 0.49146637]\n",
            " [0.48564342 0.5143566 ]\n",
            " [0.5028758  0.4971242 ]\n",
            " [0.4906086  0.50939137]\n",
            " [0.5109272  0.4890728 ]\n",
            " [0.50248677 0.49751323]\n",
            " [0.5047355  0.49526447]\n",
            " [0.5052452  0.49475485]\n",
            " [0.5064288  0.49357125]\n",
            " [0.513196   0.48680407]\n",
            " [0.5091517  0.49084827]\n",
            " [0.49995562 0.50004435]\n",
            " [0.5003949  0.49960515]\n",
            " [0.50143445 0.49856552]\n",
            " [0.5075133  0.4924867 ]\n",
            " [0.5075843  0.49241576]\n",
            " [0.50827    0.49172997]\n",
            " [0.5022961  0.49770388]\n",
            " [0.500051   0.49994895]\n",
            " [0.5059562  0.49404377]\n",
            " [0.50061387 0.4993862 ]\n",
            " [0.50608784 0.4939122 ]\n",
            " [0.5110337  0.48896626]\n",
            " [0.49988496 0.50011504]]\n",
            "========== Epoch 9 Batch 25==== Step 1 AVG. val Loss 0.6951913237571716 ====  0.2857142857142857 ====  0.375\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n",
            "       device='cuda:0')\n",
            "========== Epoch 9 Batch 26==== Step 2 Probs\n",
            "[[0.50506437 0.4949356 ]\n",
            " [0.509358   0.49064195]\n",
            " [0.50156015 0.49843985]\n",
            " [0.5033903  0.49660966]\n",
            " [0.49905318 0.5009468 ]\n",
            " [0.49862233 0.5013777 ]\n",
            " [0.50882185 0.49117818]\n",
            " [0.50989187 0.49010813]\n",
            " [0.5024865  0.49751344]\n",
            " [0.505724   0.494276  ]\n",
            " [0.5009057  0.4990943 ]\n",
            " [0.49952987 0.5004701 ]\n",
            " [0.49685505 0.503145  ]\n",
            " [0.50912356 0.49087647]\n",
            " [0.49776468 0.5022353 ]\n",
            " [0.500075   0.49992502]\n",
            " [0.5008543  0.4991457 ]\n",
            " [0.50849813 0.4915019 ]\n",
            " [0.5053249  0.49467504]\n",
            " [0.5068445  0.4931555 ]\n",
            " [0.5059462  0.49405372]]\n",
            "========== Epoch 9 Batch 26==== Step 1 AVG. val Loss 0.691116988658905 ====  0.3076923076923077 ====  0.5714285714285714\n",
            "  Average Validation Loss: 0.69\n",
            "  Average Validation F1: 0.32\n",
            "  Average Validation Acc: 0.50\n",
            "  Validation took: 0:00:19\n",
            "EarlyStopping counter: 5 out of 4\n",
            "Early stopping\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:03:09 (h:mm:ss)\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "mytrainStep1(modelEmb,criterion1,criterion2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9bIburFmosz"
      },
      "source": [
        "BERT + CNN min avg loss 0.59 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.2 <br>\n",
        "FC output = 2 <br>\n",
        "bert layer = 5 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.51 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.51 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 2e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.6 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.3 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "relu <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 16 <br>\n",
        "relu <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 4 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [32,32,32] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [32,32,32] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 2e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,32,64] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.52 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16,16,16,16] <br>\n",
        "filter_sizes=[2,3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [16] <br>\n",
        "filter_sizes=[7] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.66 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [2,2,2,2] <br>\n",
        "filter_sizes=[2,3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [100,100,100] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.54 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [400] <br>\n",
        "filter_sizes=[7] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.52 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [400,400,400] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.53 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [400,400,400] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 2e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.52 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [600,600,600] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>\n",
        "\n",
        "====================================\n",
        "\n",
        "BERT + CNN min avg loss 0.51 <br>\n",
        "6 epochs <br>\n",
        "num_filters = [200,200,200] <br>\n",
        "filter_sizes=[3,4,5] <br>\n",
        "lr = 1e-5 <br>\n",
        "dropout = 0.5 <br>\n",
        "FC output = 2 <br>\n",
        "tanh <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9534BCPWm76"
      },
      "outputs": [],
      "source": [
        "class myModelFC(nn.Module):\n",
        "  def __init__(self):\n",
        "      super(myModelFC, self).__init__()\n",
        "      inputFeatures = 48 #32\n",
        "      self.FC = nn.Sequential(\n",
        "        # nn.ReLU(),\n",
        "        # nn.Tanh(),\n",
        "        # nn.Dropout(0.5),  \n",
        "        nn.Linear(in_features = inputFeatures*2,out_features = 2),\n",
        "        # nn.Tanh(),\n",
        "        # nn.ReLU(),\n",
        "        # nn.Dropout(0.2), #0.5\n",
        "        # nn.Linear(in_features = 512,out_features = 64),\n",
        "        # nn.Linear(in_features = 64,out_features = 2),\n",
        "      \n",
        "      )\n",
        "\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "      concatenated = torch.cat((input1,input1),dim=1)\n",
        "      \n",
        "      out = self.FC(concatenated)\n",
        "      # print(out)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBl4FNaoW12w"
      },
      "outputs": [],
      "source": [
        "def mytrainStep2(model,criterion1,criterion2,embModel):\n",
        "      # loss = nn.CosineEmbeddingLoss()\n",
        "      embModel.bertModel.eval()\n",
        "      embModel.eval()\n",
        "      for param in embModel.bertModel.parameters():\n",
        "          param.requires_grad = False\n",
        "      for param2 in embModel.parameters():\n",
        "          param2.requires_grad = False    \n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "         model.to(device)\n",
        "      \n",
        "      \n",
        "      # embModel.bias.requires_grad = False\n",
        "      # loss = nn.CrossEntropyLoss()\n",
        "        # PyTorch scheduler\n",
        "      optimizer2 = torch.optim.Adam(model.parameters(),\n",
        "                                    lr=0.00001\n",
        "                                    )\n",
        "      # scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=3*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain2 = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid2 = list()\n",
        "      listOfF1Valid = list()\n",
        "      totalf1 = 0\n",
        "      epoch_stop = 0\n",
        "      for epoch_i in range(0, 20):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss2 = 0\n",
        "\n",
        "          model.train()\n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "          totalf1 = 0\n",
        "          for (input1,mask1,input2,mask2,target1) in train_dataloader:\n",
        "              step +=1\n",
        "              # if step ==171 :\n",
        "              #   break\n",
        "              # # Progress update every 40 batches.\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              if torch.cuda.is_available():\n",
        "                  b_input_ids1 = input1.to(device)\n",
        "                  b_input_mask1 = mask1.to(device)\n",
        "                  target1 = target1.type(torch.LongTensor)\n",
        "                  b_labels = target1.to(device)\n",
        "                 \n",
        "                  # b_labels = torch.squeeze(b_labels,1)\n",
        "                  b_input_ids2 = input2.to(device)\n",
        "                  b_input_mask2 = mask2.to(device)\n",
        "              \n",
        "              model.zero_grad()  \n",
        "\n",
        "              # with embModel.parameters() == False:\n",
        "              # for p in embModel.parameters():\n",
        "              #     p.requires_grad = False            \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2) \n",
        "              FC11,FC22,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,b_labels)\n",
        "              out = model(FC11,FC22)\n",
        "             \n",
        "              loss = criterion2(out,b_labels)\n",
        "\n",
        "              total_train_loss2 += loss.item()\n",
        "              f1 = calcF1score(out,b_labels)\n",
        "              totalf1+=f1\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 2 Train Loss \"+str(loss.item()),\"====== \",str(f1))\n",
        "\n",
        "              loss.backward()\n",
        "\n",
        "              optimizer2.step()\n",
        "              # scheduler.step()\n",
        "              \n",
        "          avg_train_loss2 = total_train_loss2 /len(train_dataloader)\n",
        "          avg_f1_train = totalf1/len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 2 Train Loss \"+str(avg_train_loss2),\"====== \",str(avg_f1_train))            \n",
        "          listOflossesTrain2.append(avg_train_loss2)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss2))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss2,avg_f1_val = validation(embModel,epoch_i,criterion1,validation_dataloader,model,criterion2)\n",
        "          listOflossesValid2.append(avg_val_loss2)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss2))\n",
        "          print(\"  Validation avg-F1: {0:.2f}\".format(avg_f1_val))\n",
        "          # print(\"  Validation avg-Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "          # Record all statistics from this epoch.\n",
        "          training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss2,\n",
        "                'Validation Loss': avg_val_loss2,\n",
        "                # 'Valid. avg F1.': avg_val_f1,\n",
        "                # 'Valid. avg Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "          )\n",
        "          early_stopping2(avg_val_loss2, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping2.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              \n",
        "              break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain2,listOflossesValid2,epoch_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59AP2hMmXHNa",
        "outputId": "b7a66ffd-0d5e-4a2e-fcae-e9d123c268c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "myModelEmbeddings(\n",
              "  (bertModel): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (conv1d_list): ModuleList(\n",
              "    (0): Conv1d(768, 400, kernel_size=(3,), stride=(1,))\n",
              "    (1): Conv1d(768, 400, kernel_size=(4,), stride=(1,))\n",
              "  )\n",
              "  (FC): Linear(in_features=800, out_features=800, bias=True)\n",
              "  (dropout): Dropout(p=0.9, inplace=False)\n",
              "  (dropout3): Dropout(p=0.5, inplace=False)\n",
              "  (FC2): Linear(in_features=1600, out_features=256, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dropout2): Dropout(p=0.2, inplace=False)\n",
              "  (FC3): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (FC4): Linear(in_features=64, out_features=2, bias=True)\n",
              "  (tanh): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelEmb.load_state_dict(torch.load('checkpointEmb.pt'))\n",
        "\n",
        "modelEmb.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zUnSg8lLXDJl",
        "outputId": "be4a7df2-9028-4633-bcc8-7dd71d65e51d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 2 ========\n",
            "Training...\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.2617,  1.0294,  0.1573,  ...,  0.1007, -0.2208, -0.4087],\n",
            "        [ 0.7404, -0.9041, -0.0749,  ...,  0.2106,  0.5767, -0.0246],\n",
            "        [ 0.3998, -1.0484,  0.0884,  ...,  0.2143,  0.5157,  0.2009],\n",
            "        ...,\n",
            "        [-0.2617,  1.0294,  0.1573,  ...,  0.1007, -0.2208, -0.4087],\n",
            "        [-1.0678,  1.1359,  0.3060,  ..., -0.0355, -0.2742, -0.4013],\n",
            "        [-0.8356,  1.3510,  0.4815,  ...,  0.0966, -0.1293, -0.4623]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8824, -0.7078, -0.5958,  0.5099,  0.7426,  0.9327,  0.9756,  0.8965,\n",
            "         0.4512,  0.2548,  0.9880, -0.1023, -0.0247,  0.7827,  0.7596,  0.9902,\n",
            "         0.9443, -0.3342,  0.9339,  0.2258, -0.7864,  0.9898,  0.9814,  0.9355,\n",
            "         0.9501,  0.3618,  0.1924,  0.2143,  0.6467,  0.9807,  0.9636,  0.9700,\n",
            "         0.9928, -0.5287,  0.4933,  0.7654,  0.3957,  0.8266, -0.0241,  0.9929,\n",
            "         0.9683,  0.9498,  0.8210,  0.5905,  0.6544,  0.8754,  0.8436,  0.5896,\n",
            "         0.7572,  0.9857,  0.6167, -0.7929,  0.8972,  0.8109,  0.7862,  0.4105,\n",
            "         0.9007,  0.9719,  0.8961,  0.9882,  0.9212,  0.5591, -0.7554,  0.9786],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 1==== Step 2 Train Loss 0.7024617791175842 ======  0.4482758620689655\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.9991,  1.3487,  0.4281,  ...,  0.0453, -0.1767, -0.3482],\n",
            "        [ 0.3175,  0.4783, -0.2652,  ...,  0.0937, -0.6684, -0.3663],\n",
            "        [ 0.5658, -0.0485, -0.1112,  ..., -0.1453,  0.2649, -0.1790],\n",
            "        ...,\n",
            "        [-0.4139,  1.2744,  0.3755,  ...,  0.0124, -0.1431, -0.5804],\n",
            "        [-0.0521,  1.0964,  0.1750,  ..., -0.0223, -0.0144, -0.4478],\n",
            "        [-0.2312,  1.0967, -0.0069,  ..., -0.0793, -0.4564, -0.4430]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9884, -0.3526,  0.8850,  0.0981,  0.9911, -0.3037, -0.6403,  0.9616,\n",
            "         0.9612,  0.6784,  0.8714,  0.8726,  0.2121,  0.5259,  0.7800,  0.9676,\n",
            "        -0.5451,  0.9762,  0.9914,  0.9600,  0.9799,  0.8944,  0.9478, -0.2611,\n",
            "        -0.7046,  0.8983,  0.8668, -0.4004, -0.1577, -0.6809,  0.2511,  0.9143,\n",
            "         0.8476,  0.9586,  0.8345,  0.9391,  0.9392, -0.1684,  0.9882,  0.7657,\n",
            "         0.7141,  0.9935,  0.9647,  0.7486,  0.9690,  0.9716,  0.5555,  0.1851,\n",
            "        -0.7227,  0.5624,  0.9542,  0.3562,  0.9633,  0.9829,  0.6053,  0.3367,\n",
            "        -0.0140, -0.8183,  0.9894, -0.5039,  0.9940,  0.6907,  0.9474,  0.8497],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 2==== Step 2 Train Loss 0.7286109328269958 ======  0.40740740740740744\n",
            "torch.Size([64, 48])\n",
            "tensor([[-8.8904e-01,  1.2255e+00,  3.6094e-01,  ..., -8.9244e-02,\n",
            "         -2.1415e-01, -6.9753e-01],\n",
            "        [ 4.5173e-01,  2.0100e-01, -2.8778e-01,  ...,  3.6282e-01,\n",
            "         -5.5715e-01, -1.7955e-01],\n",
            "        [ 5.0823e-01, -4.5213e-01, -1.1456e-01,  ...,  1.8860e-01,\n",
            "          5.3247e-01, -1.3404e-01],\n",
            "        ...,\n",
            "        [-9.9532e-01,  1.3825e+00,  5.2117e-01,  ...,  3.3899e-02,\n",
            "         -1.5260e-01, -4.4832e-01],\n",
            "        [ 4.6149e-01, -8.1123e-01,  9.6725e-02,  ...,  1.9848e-01,\n",
            "          6.8729e-01, -1.2190e-03],\n",
            "        [ 5.6440e-01, -1.1077e-02, -1.3260e-01,  ...,  1.2938e-01,\n",
            "          2.3470e-01, -2.8810e-01]], device='cuda:0')\n",
            "tensor([ 0.9682, -0.0592,  0.9184,  0.9907,  0.8519,  0.9547,  0.0799,  0.7889,\n",
            "         0.2269,  0.9004,  0.9466, -0.7788, -0.3238,  0.9825,  0.9765,  0.9827,\n",
            "         0.9881,  0.1603,  0.9647, -0.0624,  0.9851,  0.7596,  0.7961,  0.9896,\n",
            "         0.8718,  0.8987, -0.4983,  0.9952, -0.0051, -0.1570,  0.7850,  0.8472,\n",
            "         0.8323, -0.0937, -0.4277, -0.3579,  0.9093,  0.6281,  0.9216,  0.7908,\n",
            "         0.9283,  0.5130,  0.1348,  0.8552, -0.5001,  0.9154,  0.0367,  0.1575,\n",
            "        -0.5907,  0.4316, -0.3315,  0.5866, -0.3565,  0.7605,  0.1748,  0.9399,\n",
            "         0.9654,  0.4939, -0.5152,  0.3101,  0.0933,  0.9744,  0.9863,  0.7538],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 3==== Step 2 Train Loss 0.7262365818023682 ======  0.4838709677419355\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.5240,  1.1403,  0.2868,  ..., -0.1296, -0.2523, -0.5639],\n",
            "        [-1.0283,  1.1775,  0.3013,  ...,  0.0269, -0.1209, -0.5962],\n",
            "        [-1.2911,  1.0738,  0.5832,  ..., -0.1055, -0.3588, -0.3891],\n",
            "        ...,\n",
            "        [-1.1540,  1.0509,  0.3702,  ...,  0.0580, -0.5099, -0.3729],\n",
            "        [ 0.3833,  0.9066, -0.2889,  ...,  0.0269, -0.4725, -0.3010],\n",
            "        [-1.3342,  1.0919,  0.5945,  ...,  0.0202, -0.5763, -0.2895]],\n",
            "       device='cuda:0')\n",
            "tensor([-3.8016e-01, -6.2478e-01,  9.9241e-01,  6.9294e-01, -1.9055e-01,\n",
            "         9.4324e-01,  4.2186e-01, -7.0043e-01,  5.2703e-01,  9.8826e-01,\n",
            "         9.9281e-01,  9.6146e-01,  9.0246e-01,  9.7466e-01,  8.4321e-01,\n",
            "        -2.7813e-01,  9.6235e-01,  8.0970e-01, -6.0878e-01,  7.6960e-01,\n",
            "         7.5424e-01,  9.9504e-01, -8.8607e-02,  9.7635e-01,  9.9034e-01,\n",
            "         6.0270e-01, -4.3859e-02,  3.0404e-04, -7.9552e-01, -8.0122e-01,\n",
            "         9.8825e-01,  9.9187e-01,  9.8844e-01,  9.7094e-01,  7.1028e-01,\n",
            "         4.4686e-01,  2.2083e-02, -2.3573e-01, -7.2560e-01, -1.6515e-02,\n",
            "         4.0942e-01,  9.7516e-01,  9.7758e-01,  5.0991e-01,  8.3835e-01,\n",
            "         9.8931e-01,  7.9485e-01,  9.8883e-01,  4.3961e-01,  6.1093e-01,\n",
            "        -8.2610e-02,  9.6776e-01,  9.9305e-01,  2.0284e-01,  9.8947e-01,\n",
            "        -2.0581e-01,  2.7812e-02,  1.6878e-01,  9.3534e-01,  9.9340e-01,\n",
            "        -4.8523e-01,  8.4013e-01,  9.4217e-01, -5.0718e-01], device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 4==== Step 2 Train Loss 0.7586920857429504 ======  0.35714285714285715\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5569,  0.2474, -0.4614,  ...,  0.1131, -0.8210, -0.1078],\n",
            "        [ 0.6224, -0.5825,  0.0156,  ...,  0.1775,  0.7018, -0.1850],\n",
            "        [-1.2283,  1.2088,  0.4402,  ...,  0.0137, -0.2309, -0.4243],\n",
            "        ...,\n",
            "        [ 0.9208, -0.3942, -0.2488,  ...,  0.1091,  0.4275, -0.0849],\n",
            "        [-0.7144,  1.3005,  0.3446,  ..., -0.0782, -0.3563, -0.5837],\n",
            "        [ 0.2653,  0.6336, -0.0046,  ...,  0.0809,  0.0868, -0.3045]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.0509, -0.4193, -0.2609,  0.7409, -0.6932, -0.1281,  0.9619,  0.3355,\n",
            "        -0.7511, -0.0181, -0.2005, -0.4786,  0.9494,  0.6154, -0.4154,  0.7183,\n",
            "        -0.5320, -0.7516,  0.1683, -0.3492, -0.7251, -0.3656, -0.3578, -0.3106,\n",
            "         0.9726,  0.9317,  0.9755,  0.7208, -0.3677,  0.9442, -0.3501,  0.0899,\n",
            "         0.4312,  0.9521,  0.2533,  0.9955,  0.1129,  0.7773,  0.9635, -0.7184,\n",
            "        -0.8790,  0.9918,  0.9075,  0.9913,  0.3805, -0.1458, -0.7193,  0.7334,\n",
            "         0.1504, -0.2952, -0.8378,  0.7631, -0.3443,  0.6905,  0.4820,  0.9841,\n",
            "         0.9883,  0.7395,  0.2640, -0.6688,  0.9618,  0.9210,  0.9664,  0.7061],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 5==== Step 2 Train Loss 0.6573202610015869 ======  0.5416666666666667\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.7912,  0.2414, -0.0117,  ...,  0.0406,  0.3236, -0.0156],\n",
            "        [ 0.6590, -0.2958,  0.0304,  ...,  0.1312,  0.6065, -0.3795],\n",
            "        [ 0.3638,  0.4704, -0.1656,  ...,  0.1800, -0.3423, -0.1010],\n",
            "        ...,\n",
            "        [-1.2960,  1.0792,  0.5453,  ..., -0.0307, -0.3771, -0.4643],\n",
            "        [-1.1660,  1.2503,  0.2727,  ..., -0.0540, -0.2665, -0.6168],\n",
            "        [ 0.6150,  0.4884, -0.4694,  ...,  0.2197, -0.7162, -0.1866]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2457, -0.3270,  0.2022,  0.8691,  0.9028,  0.2019, -0.4112, -0.2673,\n",
            "         0.9103,  0.9162,  0.9166, -0.2262,  0.9864,  0.9799,  0.7744,  0.9913,\n",
            "         0.0194,  0.9746,  0.4914,  0.9876,  0.9934,  0.9682,  0.9803,  0.9628,\n",
            "         0.5572,  0.7461, -0.7638,  0.9928,  0.9440, -0.0478,  0.9149,  0.9523,\n",
            "        -0.1474,  0.9026,  0.9068,  0.5677,  0.9660,  0.9500,  0.4078, -0.7308,\n",
            "         0.9857,  0.8605,  0.4167,  0.3128, -0.6244,  0.9491, -0.1735, -0.2515,\n",
            "         0.8290,  0.9281,  0.9876,  0.9791,  0.8790,  0.3710,  0.9861,  0.9792,\n",
            "         0.7165,  0.9883, -0.4227,  0.9796,  0.9972,  0.6312, -0.5351,  0.4361],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 6==== Step 2 Train Loss 0.7361865043640137 ======  0.4067796610169492\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2986,  1.0662,  0.3883,  ...,  0.1176, -0.5177, -0.2900],\n",
            "        [-1.0031,  0.9289,  0.3501,  ..., -0.0564, -0.2396, -0.4708],\n",
            "        [ 0.5862, -0.3358, -0.0459,  ...,  0.0875,  0.7939, -0.3583],\n",
            "        ...,\n",
            "        [-0.6442,  1.1782,  0.2434,  ...,  0.0233, -0.4369, -0.5772],\n",
            "        [ 0.5415, -0.4392, -0.0600,  ...,  0.0117,  0.4557, -0.0193],\n",
            "        [-1.1503,  1.3181,  0.4642,  ...,  0.0609, -0.4010, -0.4159]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.5984,  0.9491,  0.8464,  0.4377, -0.7558,  0.5844,  0.9222,  0.9808,\n",
            "         0.9900,  0.9326,  0.0637,  0.1521,  0.6325,  0.9720,  0.3571,  0.2766,\n",
            "         0.7955,  0.8479,  0.9780,  0.9593,  0.2700, -0.1881,  0.9077,  0.9794,\n",
            "         0.9416,  0.9440,  0.9576,  0.7691,  0.8496,  0.9193,  0.6871,  0.6148,\n",
            "         0.9834,  0.4272, -0.0928, -0.7093,  0.9482, -0.5521,  0.8737, -0.1244,\n",
            "         0.9875,  0.9695,  0.6801, -0.7410,  0.8264,  0.6162,  0.9884, -0.2641,\n",
            "         0.9161,  0.9329,  0.9695,  0.3719,  0.6053,  0.8198, -0.0543,  0.5626,\n",
            "         0.9756,  0.0406,  0.6798,  0.9876,  0.7894,  0.9416, -0.5912, -0.4051],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 7==== Step 2 Train Loss 0.6952616572380066 ======  0.4313725490196078\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2688,  1.0855,  0.6017,  ..., -0.0183, -0.2832, -0.3602],\n",
            "        [ 0.5898,  0.8838, -0.3752,  ...,  0.1571, -0.4498, -0.3748],\n",
            "        [ 0.1742,  0.6754, -0.2077,  ...,  0.0399, -0.6339, -0.2681],\n",
            "        ...,\n",
            "        [-1.2087,  1.2789,  0.3959,  ..., -0.1254, -0.4096, -0.3585],\n",
            "        [-0.4226,  0.9329,  0.0784,  ..., -0.0586, -0.2757, -0.6474],\n",
            "        [-0.2660,  0.9232, -0.0969,  ..., -0.0990, -0.5275, -0.4730]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9922,  0.8916,  0.7522, -0.3486,  0.8124,  0.0244,  0.8285,  0.9611,\n",
            "         0.9312,  0.9554,  0.9704,  0.9853,  0.9890,  0.3165,  0.9612,  0.2379,\n",
            "        -0.0718,  0.9793,  0.9905, -0.3481,  0.9199, -0.5424,  0.9901, -0.2936,\n",
            "         0.9852, -0.6963,  0.3983, -0.1183,  0.9930,  0.1066,  0.9951,  0.6533,\n",
            "        -0.5754,  0.5338,  0.9901,  0.9365,  0.9531, -0.6585,  0.9809,  0.9630,\n",
            "        -0.3022, -0.5714,  0.9847, -0.7642,  0.9952,  0.5504,  0.9622,  0.9716,\n",
            "         0.1999,  0.2571, -0.7983,  0.7448, -0.1361, -0.7763,  0.8451,  0.8915,\n",
            "        -0.6166,  0.9851,  0.9426,  0.8575,  0.4718, -0.3631,  0.7759,  0.8127],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 8==== Step 2 Train Loss 0.7222496867179871 ======  0.21739130434782608\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.0257,  1.1614, -0.1430,  ..., -0.0429, -0.4442, -0.5735],\n",
            "        [ 0.5829, -0.0029,  0.0511,  ..., -0.1490,  0.6723, -0.2594],\n",
            "        [ 0.5980,  0.8060, -0.0940,  ..., -0.0178, -0.1036, -0.3299],\n",
            "        ...,\n",
            "        [-0.7776,  1.3167,  0.3063,  ..., -0.0636, -0.4296, -0.3785],\n",
            "        [-1.1692,  1.3348,  0.5814,  ...,  0.0220, -0.2940, -0.3555],\n",
            "        [ 0.6437, -0.1672,  0.0330,  ..., -0.0622,  0.6299, -0.1721]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1357, -0.5072,  0.7621,  0.9921,  0.9222, -0.6143,  0.8303,  0.9439,\n",
            "        -0.5874,  0.2819,  0.3355,  0.8358,  0.4233,  0.9469,  0.3853,  0.9666,\n",
            "         0.9072,  0.8745, -0.4324, -0.2297, -0.7460,  0.9726,  0.1252, -0.0594,\n",
            "         0.3824,  0.9936,  0.9932,  0.8811,  0.8132,  0.6275,  0.9919,  0.6982,\n",
            "         0.7402,  0.9922,  0.8223,  0.7388,  0.9300,  0.9066, -0.6856, -0.2648,\n",
            "         0.3591, -0.4001, -0.5104,  0.9291,  0.9167,  0.7318,  0.9507, -0.2478,\n",
            "         0.2282,  0.9844,  0.9654,  0.9923,  0.9774, -0.8490,  0.8840, -0.0902,\n",
            "         0.7725,  0.3882,  0.1427, -0.0531,  0.8317, -0.3303, -0.7379, -0.2462],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 9==== Step 2 Train Loss 0.6818116903305054 ======  0.5106382978723405\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2710e+00,  1.0923e+00,  4.3920e-01,  ...,  1.1882e-02,\n",
            "         -5.5370e-01, -3.8831e-01],\n",
            "        [-7.9786e-01,  1.0310e+00,  2.9281e-01,  ..., -1.0729e-01,\n",
            "         -2.5838e-01, -4.4771e-01],\n",
            "        [ 6.9066e-01, -7.1530e-01,  2.1379e-02,  ...,  2.1377e-01,\n",
            "          5.4684e-01, -1.1332e-01],\n",
            "        ...,\n",
            "        [-1.1165e+00,  1.0646e+00,  5.7593e-01,  ...,  4.9428e-04,\n",
            "         -2.0331e-01, -3.4027e-01],\n",
            "        [-7.1816e-01,  1.0253e+00,  3.3431e-01,  ..., -1.5712e-01,\n",
            "          1.0382e-02, -6.8870e-01],\n",
            "        [-1.2373e+00,  1.0514e+00,  4.5474e-01,  ...,  6.3037e-02,\n",
            "         -3.4974e-01, -5.3496e-01]], device='cuda:0')\n",
            "tensor([ 0.9939,  0.3837,  0.9829,  0.9615,  0.8308, -0.4914,  0.9918,  0.9540,\n",
            "        -0.4331,  0.9907,  0.9912,  0.9893,  0.9894,  0.9814,  0.9744, -0.3899,\n",
            "         0.6913,  0.9930,  0.0873,  0.8845, -0.2103,  0.9883,  0.9916,  0.8784,\n",
            "         0.9935,  0.9380,  0.9732,  0.7878,  0.9934,  0.9909,  0.9273,  0.9762,\n",
            "        -0.0886, -0.0875,  0.9874, -0.6154, -0.7914,  0.9581,  0.0879,  0.4452,\n",
            "         0.9339,  0.7827,  0.7068,  0.5626,  0.8673,  0.8198,  0.9353,  0.9940,\n",
            "         0.3658,  0.8257,  0.9824, -0.1713,  0.7155,  0.9276,  0.5372,  0.9039,\n",
            "        -0.7575,  0.3412,  0.9817,  0.9124, -0.8163,  0.9768,  0.9755,  0.9548],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 10==== Step 2 Train Loss 0.740455150604248 ======  0.44067796610169485\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.5155,  1.0566,  0.6494,  ...,  0.0953, -0.3893, -0.4249],\n",
            "        [-0.9794,  1.1208,  0.3896,  ...,  0.2332, -0.3649, -0.4267],\n",
            "        [-1.2618,  0.9934,  0.5118,  ...,  0.0750, -0.2198, -0.4760],\n",
            "        ...,\n",
            "        [-0.5596,  1.2857,  0.2469,  ...,  0.0497, -0.4647, -0.4925],\n",
            "        [ 0.5304, -0.5254,  0.0628,  ...,  0.1543,  0.7044, -0.1635],\n",
            "        [-0.3616,  1.0822, -0.0092,  ...,  0.0136, -0.7408, -0.4458]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9899,  0.9727,  0.9860,  0.2334,  0.9117,  0.7725,  0.7281, -0.7197,\n",
            "        -0.3968,  0.8909,  0.9873, -0.2714, -0.8043, -0.1304,  0.3032,  0.9868,\n",
            "         0.9872,  0.9516,  0.3321, -0.2169,  0.0662,  0.0166,  0.5674,  0.9930,\n",
            "         0.9774,  0.9862,  0.9568,  0.9901,  0.9914,  0.8732,  0.8408, -0.0830,\n",
            "         0.6536,  0.9462,  0.9026,  0.4086,  0.8784,  0.9856,  0.2423,  0.4399,\n",
            "        -0.3736,  0.7842,  0.8612,  0.6984,  0.9657,  0.9084, -0.2101,  0.9295,\n",
            "         0.3460,  0.3946,  0.9830,  0.9586,  0.4230,  0.8724,  0.9794, -0.3065,\n",
            "         0.9350,  0.7862,  0.9133,  0.6501, -0.0093,  0.9721,  0.9849,  0.8823],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ/klEQVR4nO3dfaxkdX3H8ffHXR5s1bLIDd2CuqC0hLRxMbdbWhsf8AmxEUyJXVLt2tKsWm002laQP6qmptBUaZs26irItrUIXSVsfahdYYkxUexFF1igyIKY7nZlryIqaUoFvv1jzpXxcu/O7J2Ze/nZ9yuZ3HN+55yZz56d/ey5Z87MpKqQJLXnCSsdQJK0NBa4JDXKApekRlngktQoC1ySGrV6OR/smGOOqXXr1i3nQ0pS82688cZvV9XU/PFlLfB169YxMzOznA8pSc1L8s2Fxoc+hZJkVZKvJflUN39CkhuS7ElyZZLDxxVWkjTYoZwDfwtwe9/8xcAlVfUs4LvAeeMMJkk6uKEKPMnxwCuAj3TzAU4HtnWrbAXOnkRASdLChj0C/yvgT4BHuvmnAvdX1UPd/F7guIU2TLI5yUySmdnZ2ZHCSpIeNbDAk/wGcKCqblzKA1TVlqqarqrpqanHvIgqSVqiYa5CeS7wyiRnAkcCTwH+GjgqyeruKPx4YN/kYkqS5ht4BF5VF1TV8VW1DtgIXFdVvw3sBM7pVtsEXDOxlJKkxxjlnZjvAN6WZA+9c+KXjieSJGkYh/RGnqq6Hri+m74b2DD+SJKkYSzrOzEl6VCsO//TKx1hbO656BVjv08/zEqSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNbDAkxyZ5CtJbkpya5J3d+OXJ/lGkl3dbf3k40qS5gzzlWoPAqdX1QNJDgO+mOSz3bI/rqptk4snSVrMwAKvqgIe6GYP6241yVCSpMGGOgeeZFWSXcABYEdV3dAtem+Sm5NckuSIRbbdnGQmyczs7OyYYkuShirwqnq4qtYDxwMbkvwicAFwMvDLwNHAOxbZdktVTVfV9NTU1JhiS5IO6SqUqrof2AmcUVX7q+dB4KPAhkkElCQtbJirUKaSHNVNPxF4CfAfSdZ2YwHOBnZPMqgk6ccNcxXKWmBrklX0Cv+qqvpUkuuSTAEBdgFvmGBOSdI8w1yFcjNw6gLjp08kkSRpKL4TU5IaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1zHdiHpnkK0luSnJrknd34yckuSHJniRXJjl88nElSXOGOQJ/EDi9qp4NrAfOSHIacDFwSVU9C/gucN7kYkqS5htY4NXzQDd7WHcr4HRgWze+ld4300uSlslQ58CTrEqyCzgA7ADuAu6vqoe6VfYCx00moiRpIUMVeFU9XFXrgeOBDcDJwz5Aks1JZpLMzM7OLjGmJGm+Q7oKparuB3YCvwoclWR1t+h4YN8i22ypqumqmp6amhoprCTpUcNchTKV5Khu+onAS4Db6RX5Od1qm4BrJhVSkvRYqwevwlpga5JV9Ar/qqr6VJLbgI8n+TPga8ClE8wpSZpnYIFX1c3AqQuM303vfLgkaQX4TkxJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY0a5kuNn5ZkZ5Lbktya5C3d+LuS7Euyq7udOfm4kqQ5w3yp8UPA26vqq0meDNyYZEe37JKq+svJxZMkLWaYLzXeD+zvpn+Q5HbguEkHkyQd3CGdA0+yjt431N/QDb05yc1JLkuyZpFtNieZSTIzOzs7UlhJ0qOGLvAkTwI+Aby1qr4PfAB4JrCe3hH6+xbarqq2VNV0VU1PTU2NIbIkCYYs8CSH0Svvj1XVJwGq6t6qeriqHgE+DGyYXExJ0nzDXIUS4FLg9qp6f9/42r7VXgXsHn88SdJihrkK5bnAa4Fbkuzqxt4JnJtkPVDAPcDrJ5JQkrSgYa5C+SKQBRZ9ZvxxJEnD8p2YktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KhhvhPzaUl2Jrktya1J3tKNH51kR5I7u59rJh9XkjRnmCPwh4C3V9UpwGnAm5KcApwPXFtVJwHXdvOSpGUysMCran9VfbWb/gFwO3AccBawtVttK3D2pEJKkh7rkM6BJ1kHnArcABxbVfu7Rd8Cjh1rMknSQQ1d4EmeBHwCeGtVfb9/WVUVUItstznJTJKZ2dnZkcJKkh41VIEnOYxeeX+sqj7ZDd+bZG23fC1wYKFtq2pLVU1X1fTU1NQ4MkuSGO4qlACXArdX1fv7Fm0HNnXTm4Brxh9PkrSY1UOs81zgtcAtSXZ1Y+8ELgKuSnIe8E3g1ZOJKElayMACr6ovAllk8YvGG0eSNCzfiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHDfKnxZUkOJNndN/auJPuS7OpuZ042piRpvmGOwC8Hzlhg/JKqWt/dPjPeWJKkQQYWeFV9AbhvGbJIkg7BKOfA35zk5u4Uy5rFVkqyOclMkpnZ2dkRHk6S1G+pBf4B4JnAemA/8L7FVqyqLVU1XVXTU1NTS3w4SdJ8Syrwqrq3qh6uqkeADwMbxhtLkjTIkgo8ydq+2VcBuxdbV5I0GasHrZDkCuAFwDFJ9gJ/CrwgyXqggHuA108woyRpAQMLvKrOXWD40glkkSQdAt+JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUQMLPMllSQ4k2d03dnSSHUnu7H6umWxMSdJ8wxyBXw6cMW/sfODaqjoJuLablyQto4EFXlVfAO6bN3wWsLWb3gqcPeZckqQBlnoO/Niq2t9Nfws4drEVk2xOMpNkZnZ2dokPJ0mab+QXMauqgDrI8i1VNV1V01NTU6M+nCSps9QCvzfJWoDu54HxRZIkDWOpBb4d2NRNbwKuGU8cSdKwhrmM8ArgS8AvJNmb5DzgIuAlSe4EXtzNS5KW0epBK1TVuYssetGYs0iSDoHvxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqIFvpZfUlnXnf3qlI2iZeAQuSY2ywCWpURa4JDXKApekRvki5gr4SXqR6Z6LXrHSEaT/tzwCl6RGjXQEnuQe4AfAw8BDVTU9jlCSpMHGcQrlhVX17THcjyTpEHgKRZIaNeoReAH/lqSAD1XVlvkrJNkMbAZ4+tOfPuLD6fHmJ+kFWak1ox6B/3pVPQd4OfCmJM+bv0JVbamq6aqanpqaGvHhJElzRirwqtrX/TwAXA1sGEcoSdJgSy7wJD+d5Mlz08BLgd3jCiZJOrhRzoEfC1ydZO5+/qmq/nUsqSRJAy25wKvqbuDZY8wiSToEXkYoSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHNfKmxnzstST/OI3BJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1UoEnOSPJHUn2JDl/XKEkSYON8q30q4C/A14OnAKcm+SUcQWTJB3cKEfgG4A9VXV3Vf0v8HHgrPHEkiQNMspb6Y8D/rNvfi/wK/NXSrIZ2NzNPpDkjhEec5BjgG9P8P7HqZWs5hy/VrKac4xyMbD0rM9YaHDin4VSVVuALZN+HIAkM1U1vRyPNapWsppz/FrJas7xG3fWUU6h7AOe1jd/fDcmSVoGoxT4vwMnJTkhyeHARmD7eGJJkgZZ8imUqnooyZuBzwGrgMuq6taxJVuaZTlVMyatZDXn+LWS1ZzjN9asqapx3p8kaZn4TkxJapQFLkmNaq7AkxydZEeSO7ufaxZY54VJdvXd/ifJ2d2yy5N8o2/Z+pXM2q33cF+e7X3jJyS5ofuogiu7F4tXJGeS9Um+lOTWJDcn+a2+ZRPdp4M+siHJEd3+2dPtr3V9yy7oxu9I8rJx5lpCzrclua3bf9cmeUbfsgWfAyuY9XVJZvsy/X7fsk3dc+XOJJtWOOclfRm/nuT+vmXLtk+TXJbkQJLdiyxPkr/p/hw3J3lO37Kl78+qauoG/AVwfjd9PnDxgPWPBu4Dfqqbvxw45/GUFXhgkfGrgI3d9AeBN65UTuDngZO66Z8D9gNHTXqf0nuB/C7gROBw4CbglHnr/AHwwW56I3BlN31Kt/4RwAnd/axawZwv7HsevnEu58GeAyuY9XXA3y6w7dHA3d3PNd30mpXKOW/9P6R3McVK7NPnAc8Bdi+y/Ezgs0CA04AbxrE/mzsCp/d2/a3d9Fbg7AHrnwN8tqr+e6KpFnaoWX8kSYDTgW1L2f4QDcxZVV+vqju76f8CDgBTE8rTb5iPbOjPvw14Ubf/zgI+XlUPVtU3gD3d/a1Izqra2fc8/DK9906shFE+BuNlwI6quq+qvgvsAM54nOQ8F7hiQlkOqqq+QO9AcTFnAX9fPV8GjkqylhH3Z4sFfmxV7e+mvwUcO2D9jTz2L/W93a8xlyQ5YuwJHzVs1iOTzCT58typHuCpwP1V9VA3v5fexxesZE4Akmygd0R0V9/wpPbpQh/ZMH8//Gidbn99j97+G2bb5czZ7zx6R2RzFnoOTMqwWX+z+zvdlmTuTXuPy33anY46Abiub3g59+kgi/1ZRtqfE38r/VIk+TzwswssurB/pqoqyaLXQXb/w/0SvWvV51xAr6QOp3dN5juA96xw1mdU1b4kJwLXJbmFXgmNzZj36T8Am6rqkW54rPv0J12S1wDTwPP7hh/zHKiquxa+h2XxL8AVVfVgktfT+w3n9BXMM8hGYFtVPdw39njbp2P3uCzwqnrxYsuS3JtkbVXt78rkwEHu6tXA1VX1w777njvSfDDJR4E/WumsVbWv+3l3kuuBU4FP0Ps1a3V3VDnSRxWMI2eSpwCfBi7sfg2cu++x7tN5hvnIhrl19iZZDfwM8J0ht13OnCR5Mb3/NJ9fVQ/OjS/yHJhU2QzMWlXf6Zv9CL3XSea2fcG8ba8fe8JHH2vYv7+NwJv6B5Z5nw6y2J9lpP3Z4imU7cDcK7WbgGsOsu5jzol1BTV3jvlsYMFXjcdkYNYka+ZOOSQ5BngucFv1XuHYSe8c/qLbL2POw4Gr6Z3H2zZv2ST36TAf2dCf/xzgum7/bQc2pneVygnAScBXxpjtkHImORX4EPDKqjrQN77gc2BCOYfNurZv9pXA7d3054CXdpnXAC/lx3/DXdacXdaT6b0A+KW+seXep4NsB36nuxrlNOB73YHPaPtzuV6lHdeN3rnNa4E7gc8DR3fj08BH+tZbR+9/tyfM2/464BZ6JfOPwJNWMivwa12em7qf5/VtfyK9wtkD/DNwxArmfA3wQ2BX3239cuxTeq/gf53e0dOF3dh76BUhwJHd/tnT7a8T+7a9sNvuDuDlE35uDsr5eeDevv23fdBzYAWz/jlwa5dpJ3By37a/1+3rPcDvrmTObv5dwEXztlvWfUrvQHF/929kL73XON4AvKFbHnpfgHNXl2d6HPvTt9JLUqNaPIUiScICl6RmWeCS1CgLXJIaZYFLUqMscElqlAUuSY36P6hiKT39xiwZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 11==== Step 2 Train Loss 0.7518513202667236 ======  0.35087719298245607\n",
            "torch.Size([64, 48])\n",
            "tensor([[-4.6859e-01,  8.4208e-01,  1.8003e-01,  ..., -1.6802e-01,\n",
            "          3.0917e-01, -6.8794e-01],\n",
            "        [-1.1165e+00,  1.0646e+00,  5.7593e-01,  ...,  4.9428e-04,\n",
            "         -2.0331e-01, -3.4027e-01],\n",
            "        [-1.5466e+00,  1.0789e+00,  6.0078e-01,  ...,  5.2502e-02,\n",
            "         -6.7530e-01, -3.0805e-01],\n",
            "        ...,\n",
            "        [ 3.6508e-01, -8.3677e-01, -2.2780e-02,  ...,  1.8117e-01,\n",
            "          3.4452e-01,  1.3451e-01],\n",
            "        [-1.2760e+00,  1.1292e+00,  6.2194e-01,  ..., -7.6299e-02,\n",
            "         -3.2565e-01, -4.3216e-01],\n",
            "        [-1.2911e+00,  1.0738e+00,  5.8322e-01,  ..., -1.0555e-01,\n",
            "         -3.5878e-01, -3.8912e-01]], device='cuda:0')\n",
            "tensor([-0.2251, -0.2220,  0.9144,  0.4416,  0.9828,  0.6853, -0.2952,  0.8691,\n",
            "         0.9675,  0.9807,  0.4725, -0.2928,  0.3412,  0.9903,  0.9928,  0.7727,\n",
            "        -0.7050,  0.8466,  0.5834,  0.9869,  0.9381,  0.8717,  0.9866,  0.9918,\n",
            "         0.6412,  0.9673,  0.8317,  0.3189, -0.2040,  0.1619,  0.8239,  0.2389,\n",
            "         0.9940,  0.9622,  0.5828,  0.9874,  0.8136,  0.6368, -0.0581,  0.0545,\n",
            "         0.8440,  0.7375,  0.9806,  0.7215,  0.9639, -0.7127,  0.8356,  0.0758,\n",
            "        -0.7459,  0.6562,  0.9824,  0.8516,  0.9709, -0.3894,  0.8144,  0.8780,\n",
            "         0.7472,  0.9870, -0.1905,  0.9483,  0.9540,  0.4200, -0.7543, -0.5719],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 12==== Step 2 Train Loss 0.6804673671722412 ======  0.391304347826087\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3627,  1.0719,  0.5094,  ...,  0.0520, -0.6434, -0.2880],\n",
            "        [ 0.0209,  1.0102, -0.1147,  ..., -0.0147, -0.4269, -0.3338],\n",
            "        [ 0.4420, -0.5949,  0.0961,  ...,  0.0957,  0.6955, -0.0704],\n",
            "        ...,\n",
            "        [ 0.7064,  0.1182, -0.3985,  ...,  0.2611, -0.6184, -0.1245],\n",
            "        [ 0.4423,  0.5765, -0.4362,  ...,  0.0632, -0.7167, -0.2471],\n",
            "        [-0.0198,  0.8874, -0.1421,  ...,  0.1852, -0.2916, -0.4161]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9963, -0.4221,  0.9740, -0.3069,  0.9155,  0.5089,  0.9692,  0.9470,\n",
            "         0.9889, -0.0187, -0.1752, -0.3901,  0.5547, -0.7357,  0.9902,  0.9866,\n",
            "         0.9948,  0.7298, -0.1337,  0.9631,  0.3144,  0.9567,  0.9823,  0.0357,\n",
            "        -0.3721,  0.3469,  0.9932,  0.6498,  0.3999,  0.9448, -0.2838,  0.8209,\n",
            "         0.3434,  0.2532,  0.4032, -0.1829,  0.9626,  0.1727, -0.7382, -0.3693,\n",
            "         0.9776,  0.9865,  0.9454,  0.0950,  0.9826,  0.0723,  0.2655, -0.2945,\n",
            "        -0.4393,  0.5214,  0.9651,  0.9755, -0.1727,  0.9879,  0.8925,  0.9820,\n",
            "        -0.0265, -0.2033,  0.4569,  0.9349, -0.7843,  0.8740,  0.7112,  0.7804],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 13==== Step 2 Train Loss 0.7089555859565735 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5544,  0.1826, -0.4099,  ...,  0.2853, -0.6645,  0.0094],\n",
            "        [-0.8989,  1.3933,  0.4126,  ...,  0.0661, -0.3699, -0.3615],\n",
            "        [-1.2689,  1.2059,  0.5439,  ..., -0.0313, -0.6080, -0.3506],\n",
            "        ...,\n",
            "        [ 0.8068, -0.1543, -0.3726,  ...,  0.1219, -0.2660, -0.0454],\n",
            "        [-1.3364,  1.1315,  0.5472,  ...,  0.0434, -0.6048, -0.3285],\n",
            "        [-1.0123,  1.2382,  0.3965,  ..., -0.0490, -0.3938, -0.3546]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7835,  0.6335,  0.9911,  0.7711,  0.6788,  0.7971,  0.5576, -0.8074,\n",
            "         0.3514,  0.3978,  0.9912,  0.8311,  0.9637,  0.9781,  0.9681,  0.8633,\n",
            "        -0.1444, -0.0607,  0.7605, -0.3822,  0.9805, -0.7196,  0.9711,  0.9391,\n",
            "         0.8761,  0.6348,  0.3773,  0.9883, -0.3889,  0.9796, -0.7491,  0.5416,\n",
            "         0.9744, -0.2276,  0.9626,  0.9325, -0.6240,  0.1652,  0.5615,  0.8165,\n",
            "        -0.7379,  0.4167,  0.6983,  0.9482, -0.7131, -0.0666,  0.6186,  0.9465,\n",
            "         0.9838, -0.8810,  0.7947,  0.9894,  0.2728,  0.9803, -0.5353,  0.9625,\n",
            "         0.8239,  0.9768,  0.7631, -0.8035,  0.9096,  0.8559,  0.2811,  0.9731],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 14==== Step 2 Train Loss 0.656638503074646 ======  0.4745762711864407\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743],\n",
            "        [ 0.6329,  0.5249, -0.5279,  ...,  0.1956, -0.5113, -0.1256],\n",
            "        [ 0.4765,  0.0325, -0.4026,  ...,  0.2945, -0.6460, -0.1506],\n",
            "        ...,\n",
            "        [ 0.5694,  0.6209, -0.1958,  ...,  0.0625,  0.0063, -0.4595],\n",
            "        [-0.8030,  0.9445,  0.3620,  ..., -0.0834, -0.2357, -0.3524],\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9915, -0.2073,  0.9472,  0.7927,  0.9786,  0.9879,  0.9317,  0.9395,\n",
            "         0.9845,  0.0919,  0.9658,  0.3321,  0.8534,  0.8886,  0.9453,  0.8471,\n",
            "         0.8398,  0.7848, -0.2502,  0.8104,  0.9868, -0.2075,  0.9894,  0.9561,\n",
            "         0.9606,  0.9903, -0.0187,  0.2742,  0.9508,  0.9921,  0.8049, -0.7107,\n",
            "        -0.5415,  0.9243,  0.6815,  0.8013,  0.9550,  0.2778,  0.9883,  0.9838,\n",
            "         0.8183, -0.0814,  0.6798,  0.0479,  0.8891,  0.1513,  0.0119,  0.7635,\n",
            "         0.9621,  0.9917,  0.8087,  0.8918,  0.9658, -0.8401,  0.9663,  0.7633,\n",
            "         0.9914,  0.9007,  0.7374,  0.9553, -0.4608,  0.8588,  0.9768,  0.9622],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAONElEQVR4nO3df6zd9V3H8ed77QqaZdKOm1rpwi0ZSpoYYblBlMQ5YBsbBppIZonTTmvq5jQz07gi/+iisfiHqNFkNoDUH+GHnQt1ZFlKKVlMgHlx/CoE2rItFgu9GzAlRqTs7R/nc9nh9t6ec+89P/ouz0dyc78/z/fVz7l53e/9nvM9jcxEklTP28YdQJK0NBa4JBVlgUtSURa4JBVlgUtSUStHebCzzz47JycnR3lISSrv4Ycf/nZmTsxdPtICn5ycZHp6epSHlKTyIuJb8y33EookFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFWWBS1JRFrgkFTXSOzElaTEmt98z7ggD8c0dVw3lcT0Dl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6SiLHBJKsoCl6Si+i7wiFgREV+PiC+1+Q0R8VBEHIqIOyNi1fBiSpLmWswZ+KeBp7rmbwRuysz3AC8BWwcZTJJ0cn0VeESsB64Cbm7zAVwG7G6b7AI2DSOgJGl+/Z6B/znwe8D32vy7gJcz83ibPwKcM9+OEbEtIqYjYnpmZmZZYSVJ39ezwCPi54BjmfnwUg6QmTszcyozpyYmJpbyEJKkeazsY5tLgasj4iPAmcA7gb8AzoqIle0sfD3w3PBiSpLm6nkGnpnXZ+b6zJwENgP3ZeYvAvuBa9tmW4C7h5ZSknSC5bwP/LPAZyLiEJ1r4rcMJpIkqR/9XEJ5Q2beD9zfpp8FLh58JElSP7wTU5KKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqSgLXJKKssAlqaieBR4RZ0bE1yLi0Yg4EBF/2JZviIiHIuJQRNwZEauGH1eSNKufM/BXgcsy8yeAC4ErI+IS4Ebgpsx8D/ASsHV4MSVJc/Us8Ox4pc2+vX0lcBmwuy3fBWwaSkJJ0rz6ugYeESsi4hHgGLAXOAy8nJnH2yZHgHMW2HdbRExHxPTMzMwgMkuS6LPAM/P1zLwQWA9cDFzQ7wEyc2dmTmXm1MTExBJjSpLmWtS7UDLzZWA/8FPAWRGxsq1aDzw34GySpJPo510oExFxVpv+AeADwFN0ivzattkW4O5hhZQknWhl701YB+yKiBV0Cv+uzPxSRDwJ3BERfwR8HbhliDklSXP0LPDMfAy4aJ7lz9K5Hi5JGgPvxJSkoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSrKApekoixwSSqqZ4FHxLsjYn9EPBkRByLi0235mojYGxEH2/fVw48rSZrVzxn4ceB3MnMjcAnwqYjYCGwH9mXm+cC+Ni9JGpGeBZ6ZRzPz39v0fwNPAecA1wC72ma7gE3DCilJOtGiroFHxCRwEfAQsDYzj7ZVzwNrF9hnW0RMR8T0zMzMMqJKkrr1XeAR8Q7gC8BvZ+Z/da/LzARyvv0yc2dmTmXm1MTExLLCSpK+r68Cj4i30ynvf8zMf26LX4iIdW39OuDYcCJKkubTz7tQArgFeCoz/6xr1R5gS5veAtw9+HiSpIWs7GObS4FfAh6PiEfast8HdgB3RcRW4FvAR4cTUZI0n54Fnpn/CsQCqy8fbBxJUr+8E1OSirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJakoC1ySirLAJamongUeEbdGxLGIeKJr2ZqI2BsRB9v31cONKUmaq58z8NuAK+cs2w7sy8zzgX1tXpI0Qj0LPDO/Crw4Z/E1wK42vQvYNOBckqQelnoNfG1mHm3TzwNrF9owIrZFxHRETM/MzCzxcJKkuZb9ImZmJpAnWb8zM6cyc2piYmK5h5MkNUst8BciYh1A+35scJEkSf1YaoHvAba06S3A3YOJI0nqVz9vI7wdeAD4sYg4EhFbgR3AByLiIHBFm5ckjdDKXhtk5nULrLp8wFkkSYvgnZiSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklF9fwwK2khk9vvGXeEgfnmjqvGHUFaNM/AJakoC1ySirLAJakor4FLeD1fNXkGLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVJQFLklFWeCSVFSZG3m80UKS3swzcEkqygKXpKIscEkqygKXpKLKvIh5OjmdXpDVqcefr7cOz8AlqSgLXJKKssAlqSgLXJKKssAlqahlFXhEXBkRT0fEoYjYPqhQkqTellzgEbEC+Gvgw8BG4LqI2DioYJKkk1vOGfjFwKHMfDYz/w+4A7hmMLEkSb0s50aec4D/6Jo/Avzk3I0iYhuwrc2+EhFPL+OYS3U28O0xHHexKuSskBFq5KyQEWrkPKUzxo1vTC4157nzLRz6nZiZuRPYOezjnExETGfm1Dgz9KNCzgoZoUbOChmhRs4KGWHwOZdzCeU54N1d8+vbMknSCCynwP8NOD8iNkTEKmAzsGcwsSRJvSz5EkpmHo+I3wS+AqwAbs3MAwNLNlhjvYSzCBVyVsgINXJWyAg1clbICAPOGZk5yMeTJI2Id2JKUlEWuCQVddoUeESsiYi9EXGwfV89zzbvj4hHur7+NyI2tXW3RcQ3utZdOI6MbbvXu3Ls6Vq+ISIeah9dcGd78Xjg+hzLCyPigYg4EBGPRcQvdK0b2lj2+viGiDijjc2hNlaTXeuub8ufjogPDSrTEnN+JiKebGO3LyLO7Vo37/M/howfj4iZriy/1rVuS/v5OBgRW4aVsc+cN3VlfCYiXu5aN6qxvDUijkXEEwusj4j4y/ZveCwi3tu1buljmZmnxRfwp8D2Nr0duLHH9muAF4EfbPO3AdeeChmBVxZYfhewuU1/HvjkuHICPwqc36Z/BDgKnDXMsaTzYvlh4DxgFfAosHHONr8BfL5NbwbubNMb2/ZnABva46wY0vj1k/P9XT97n5zNebLnfwwZPw781Tz7rgGebd9Xt+nV48o5Z/vfovOGipGNZTvOzwDvBZ5YYP1HgC8DAVwCPDSIsTxtzsDp3Ma/q03vAjb12P5a4MuZ+T9DTfVmi834hogI4DJg91L2X6SeOTPzmcw82Kb/EzgGTAwpz6x+Pr6hO/tu4PI2dtcAd2Tmq5n5DeBQe7yx5MzM/V0/ew/SuY9ilJbzURgfAvZm5ouZ+RKwF7jyFMl5HXD7kLIsKDO/SueEcCHXAH+XHQ8CZ0XEOpY5lqdTga/NzKNt+nlgbY/tN3PiE/3H7c+bmyLijIEn7D/jmRExHREPzl7iAd4FvJyZx9v8ETofZzAMixrLiLiYztnR4a7FwxjL+T6+Ye4YvLFNG6vv0hm7fvYdlMUeayuds7NZ8z3/g9Zvxp9vz+PuiJi9ce+UHMt2GWoDcF/X4lGMZT8W+ncsayxL/afGEXEv8MPzrLqheyYzMyIWfH9k+83343Tewz7rejpltYrOezU/C3xuTBnPzcznIuI84L6IeJxOEQ3MgMfy74Etmfm9tnggY/lWEBEfA6aA93UtPuH5z8zD8z/CUP0LcHtmvhoRv07nL5vLxpCjX5uB3Zn5eteyU2Ush6JUgWfmFQuti4gXImJdZh5tpXLsJA/1UeCLmfla12PPnnG+GhF/C/zuuDJm5nPt+7MRcT9wEfAFOn92rWxnlsv66IJB5IyIdwL3ADe0PwtnH3sgYzmPfj6+YXabIxGxEvgh4Dt97jsofR0rIq6g8wvzfZn56uzyBZ7/QZdOz4yZ+Z2u2ZvpvDYyu+/Pztn3/gHnm7WY520z8KnuBSMay34s9O9Y1lieTpdQ9gCzr+BuAe4+ybYnXCdrRTV7rXkTMO+rycPOGBGrZy85RMTZwKXAk9l5xWM/nWv3C+4/wpyrgC/Sua63e866YY1lPx/f0J39WuC+NnZ7gM3ReZfKBuB84GsDyrXonBFxEfA3wNWZeaxr+bzP/5gyruuavRp4qk1/Bfhgy7oa+CBv/mt2pDlb1gvovAj4QNeyUY1lP/YAv9zejXIJ8N12orO8sRzFK7Sj+KJznXMfcBC4F1jTlk8BN3dtN0nnt97b5ux/H/A4nbL5B+Ad48gI/HTL8Wj7vrVr//PolM4h4J+AM8Y1lsDHgNeAR7q+Lhz2WNJ5Nf8ZOmdRN7Rln6NThABntrE51MbqvK59b2j7PQ18eMg/j71y3gu80DV2e3o9/2PI+CfAgZZlP3BB176/2sb4EPAr4xzLNv8HwI45+41yLG+n806s1+hcx94KfAL4RFsfdP4DnMMty9QgxtJb6SWpqNPpEookvaVY4JJUlAUuSUVZ4JJUlAUuSUVZ4JJUlAUuSUX9P2KLB+zBQ2dTAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 15==== Step 2 Train Loss 0.7349309325218201 ======  0.43636363636363634\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5840, -0.5251,  0.0705,  ...,  0.0889,  0.6569, -0.1970],\n",
            "        [-0.1240,  1.1540,  0.3027,  ..., -0.0436,  0.0227, -0.6169],\n",
            "        [ 0.4848, -0.0429, -0.4726,  ...,  0.1856, -0.8957,  0.0595],\n",
            "        ...,\n",
            "        [-1.0156,  1.2767,  0.3918,  ..., -0.0145, -0.2219, -0.4080],\n",
            "        [ 0.0997,  0.9889, -0.0663,  ..., -0.0116, -0.2888, -0.1708],\n",
            "        [ 0.9516, -0.1719, -0.4969,  ...,  0.0532, -0.1281,  0.0665]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9821,  0.8419,  0.1839,  0.9687,  0.9401,  0.9876,  0.5541,  0.7386,\n",
            "         0.8904,  0.9941,  0.6447, -0.5966, -0.1895,  0.9858,  0.9935,  0.5776,\n",
            "        -0.0597,  0.9922,  0.9884, -0.3857,  0.9779,  0.3443,  0.8372,  0.8976,\n",
            "         0.8998,  0.9675, -0.7655,  0.9353,  0.9309, -0.8494, -0.5055, -0.7722,\n",
            "         0.9814,  0.4401,  0.9102,  0.8300, -0.2205,  0.9850,  0.4489,  0.9804,\n",
            "         0.9507,  0.7238,  0.4455,  0.3769, -0.0296, -0.5871, -0.4923, -0.2729,\n",
            "         0.9006,  0.8111,  0.9920, -0.6000, -0.2041,  0.6875, -0.5521,  0.8445,\n",
            "         0.1392, -0.2132,  0.5306,  0.0033,  0.4471,  0.9669,  0.6253,  0.7233],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 16==== Step 2 Train Loss 0.7606855034828186 ======  0.2692307692307692\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.3684,  0.4131,  0.1742,  ...,  0.1391,  0.3064, -0.4075],\n",
            "        [-1.4876,  1.0979,  0.5174,  ...,  0.0869, -0.6326, -0.4616],\n",
            "        [ 0.5794, -0.4463,  0.1689,  ...,  0.0571,  0.6394, -0.0601],\n",
            "        ...,\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [-1.1925,  1.0694,  0.5852,  ...,  0.0109, -0.2746, -0.4163],\n",
            "        [ 0.0094,  0.0777, -0.0512,  ...,  0.2431, -0.4782, -0.2792]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2853,  0.9876, -0.3121,  0.7134,  0.8628,  0.3320,  0.8797,  0.9188,\n",
            "         0.9074,  0.5576,  0.7958, -0.0611,  0.9295, -0.1942,  0.9297,  0.7548,\n",
            "         0.9013, -0.6730,  0.9002,  0.1873,  0.9667,  0.5593,  0.9881,  0.9864,\n",
            "         0.4876,  0.9857,  0.7460, -0.3467, -0.1266,  0.6955, -0.7777,  0.9668,\n",
            "         0.9918,  0.9907,  0.9865, -0.8183, -0.5061, -0.5705, -0.3301,  0.1663,\n",
            "         0.9666,  0.8883,  0.7898,  0.9859,  0.9717,  0.9943,  0.9716,  0.9862,\n",
            "         0.6485,  0.9833, -0.2562,  0.9917,  0.9917, -0.2831,  0.7704, -0.6135,\n",
            "         0.9913,  0.9295,  0.7806,  0.7280, -0.8701,  0.9922,  0.6130,  0.8553],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARA0lEQVR4nO3df4zkdX3H8efLO37YasshG3oF8UBpCWnjYbZXWhp/4C/ERjAl9ki1Z0tzarXRaFtB/qiamkJTpW3aqKcg19Yi9JRw9Uct8iPGRLGLHr+LHIgp9ORWEZU0pYLv/jHf1XFv92Z2Z2b3PvB8JJP9/px53Wc2r/vud74zk6pCktSeJ612AEnS8ljgktQoC1ySGmWBS1KjLHBJatTalXywI444ojZs2LCSDylJzbvxxhu/VVVT85evaIFv2LCBmZmZlXxISWpekm8stNxTKJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRQxd4kjVJvprkk938sUluSLI7yeVJDp5cTEnSfEs5An8zcEff/IXARVX1LOA7wDnjDCZJ2r+hCjzJ0cDLgQ938wFOBXZ0m2wHzpxEQEnSwoZ9J+ZfA38KPLWbfxrwUFU92s3fBxy10I5JtgJbAY455pjlJ5X0hLPh3E+tdoSxufeCl4/9PgcegSf5TWBvVd24nAeoqm1VNV1V01NT+7yVX5K0TMMcgZ8CvCLJ6cChwM8AfwMclmRtdxR+NHD/5GJKkuYbeAReVedV1dFVtQHYDFxbVb8DXAec1W22BbhqYiklSfsY5TrwtwNvTbKb3jnxi8cTSZI0jCV9nGxVXQ9c303fA2wafyRJ0jB8J6YkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHDfKnxoUm+nOSmJLcleVe3/NIkX0+yq7ttnHxcSdKcYb6R5xHg1Kp6OMlBwBeSfKZb9ydVtWNy8SRJixlY4FVVwMPd7EHdrSYZSpI02FDnwJOsSbIL2AtcXVU3dKvek+TmJBclOWRiKSVJ+xiqwKvqsaraCBwNbEryS8B5wAnArwCH0/uW+n0k2ZpkJsnM7OzsmGJLkpZ0FUpVPQRcB5xWVXuq5xHgIyzyDfVVta2qpqtqempqavTEkiRguKtQppIc1k0/GXgx8J9J1nfLApwJ3DrJoJKknzTMVSjrge1J1tAr/Cuq6pNJrk0yBQTYBbx+gjklSfMMcxXKzcBJCyw/dSKJJElD8Z2YktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KhhvhPz0CRfTnJTktuSvKtbfmySG5LsTnJ5koMnH1eSNGeYI/BHgFOr6tnARuC0JCcDFwIXVdWzgO8A50wupiRpvoEFXj0Pd7MHdbcCTgV2dMu30/tmeknSChnqHHiSNUl2AXuBq4G7gYeq6tFuk/uAoxbZd2uSmSQzs7Oz48gsSWLIAq+qx6pqI3A0sAk4YdgHqKptVTVdVdNTU1PLjClJmm9JV6FU1UPAdcCvAYclWdutOhq4f8zZJEn7McxVKFNJDuumnwy8GLiDXpGf1W22BbhqUiElSftaO3gT1gPbk6yhV/hXVNUnk9wOfCzJnwNfBS6eYE5J0jwDC7yqbgZOWmD5PfTOh0uSVoHvxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDfOdmE9Pcl2S25PcluTN3fJ3Jrk/ya7udvrk40qS5gzznZiPAm+rqq8keSpwY5Kru3UXVdVfTS6eJGkxw3wn5h5gTzf9/SR3AEdNOpgkaf+WdA48yQZ6X3B8Q7foTUluTnJJknWL7LM1yUySmdnZ2ZHCSpJ+bOgCT/IU4OPAW6rqe8D7gWcCG+kdob93of2qaltVTVfV9NTU1BgiS5JgyAJPchC98v5oVX0CoKoeqKrHquqHwIeATZOLKUmab5irUAJcDNxRVe/rW76+b7NXAreOP54kaTHDXIVyCvAa4JYku7pl7wDOTrIRKOBe4HUTSShJWtAwV6F8AcgCqz49/jiSpGH5TkxJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1DDfifn0JNcluT3JbUne3C0/PMnVSe7qfq6bfFxJ0pxhjsAfBd5WVScCJwNvTHIicC5wTVUdD1zTzUuSVsjAAq+qPVX1lW76+8AdwFHAGcD2brPtwJmTCilJ2teSzoEn2QCcBNwAHFlVe7pV3wSOXGSfrUlmkszMzs6OEFWS1G/oAk/yFODjwFuq6nv966qqgFpov6raVlXTVTU9NTU1UlhJ0o8NVeBJDqJX3h+tqk90ix9Isr5bvx7YO5mIkqSFDHMVSoCLgTuq6n19q3YCW7rpLcBV448nSVrM2iG2OQV4DXBLkl3dsncAFwBXJDkH+AbwqslElCQtZGCBV9UXgCyy+oXjjSNJGpbvxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDfOdmJck2Zvk1r5l70xyf5Jd3e30ycaUJM03zBH4pcBpCyy/qKo2drdPjzeWJGmQgQVeVZ8HHlyBLJKkJRjlHPibktzcnWJZt9hGSbYmmUkyMzs7O8LDSZL6LbfA3w88E9gI7AHeu9iGVbWtqqaranpqamqZDydJmm9ZBV5VD1TVY1X1Q+BDwKbxxpIkDbKsAk+yvm/2lcCti20rSZqMtYM2SHIZ8HzgiCT3AX8GPD/JRqCAe4HXTTCjJGkBAwu8qs5eYPHFE8giSVoC34kpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRpY4EkuSbI3ya19yw5PcnWSu7qf6yYbU5I03zBH4JcCp81bdi5wTVUdD1zTzUuSVtDAAq+qzwMPzlt8BrC9m94OnDnmXJKkAZZ7DvzIqtrTTX8TOHKxDZNsTTKTZGZ2dnaZDydJmm/kFzGrqoDaz/ptVTVdVdNTU1OjPpwkqbPcAn8gyXqA7ufe8UWSJA1juQW+E9jSTW8BrhpPHEnSsIa5jPAy4IvALya5L8k5wAXAi5PcBbyom5ckraC1gzaoqrMXWfXCMWeRJC2B78SUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUQM/D/xAseHcT612BC3g3gtevtoRpCcsj8AlqVEjHYEnuRf4PvAY8GhVTY8jlCRpsHGcQnlBVX1rDPcjSVoCT6FIUqNGPQIv4N+TFPDBqto2f4MkW4GtAMccc8yIDydpEF/wf+IY9Qj8N6rqOcDLgDcmee78DapqW1VNV9X01NTUiA8nSZozUoFX1f3dz73AlcCmcYSSJA227AJP8tNJnjo3DbwEuHVcwSRJ+zfKOfAjgSuTzN3PP1fVv40llSRpoGUXeFXdAzx7jFmkVeMLf2qRlxFKUqMscElqlAUuSY2ywCWpUc18nKwOTL74J60ej8AlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KiRCjzJaUnuTLI7ybnjCiVJGmyULzVeA/w98DLgRODsJCeOK5gkaf9GOQLfBOyuqnuq6v+AjwFnjCeWJGmQUT4P/Cjgv/rm7wN+df5GSbYCW7vZh5PcOcJjjtMRwLdWO8QAB3rGAz0fmHFczDiiXDhSvmcstHDiX+hQVduAbZN+nKVKMlNV06udY38O9IwHej4w47iYcXSTyDfKKZT7gaf3zR/dLZMkrYBRCvw/gOOTHJvkYGAzsHM8sSRJgyz7FEpVPZrkTcBngTXAJVV129iSTd4Bd1pnAQd6xgM9H5hxXMw4urHnS1WN+z4lSSvAd2JKUqMscElq1OO6wJMcnuTqJHd1P9ctsM0Lkuzqu/1vkjO7dZcm+Xrfuo0rna/b7rG+DDv7lh+b5Ibuowwu715MHqshx3Bjki8muS3JzUl+u2/dxMZw0Ec5JDmkG5fd3Tht6Ft3Xrf8ziQvHVemZWR8a5Lbu3G7Jskz+tYt+LyvcL7XJpnty/EHfeu2dL8XdyXZMol8Q2a8qC/f15I81Ldu4mPYPc4lSfYmuXWR9Unyt92/4eYkz+lbt/xxrKrH7Q34S+Dcbvpc4MIB2x8OPAj8VDd/KXDWaucDHl5k+RXA5m76A8AbViMj8AvA8d30zwN7gMMmOYb0Xji/GzgOOBi4CThx3jZ/CHygm94MXN5Nn9htfwhwbHc/a1Yp4wv6ft/eMJdxf8/7Cud7LfB3C+x7OHBP93NdN71uNTLO2/6P6F1QsSJj2Pc4zwWeA9y6yPrTgc8AAU4GbhjHOD6uj8DpvbV/eze9HThzwPZnAZ+pqv+ZaKofW2q+H0kS4FRgx3L2X4KBGavqa1V1Vzf938BeYGoCWfoN81EO/dl3AC/sxu0M4GNV9UhVfR3Y3d3fimesquv6ft++RO/9FCtllI/DeClwdVU9WFXfAa4GTjsAMp4NXDaBHPtVVZ+nd/C3mDOAf6ieLwGHJVnPiOP4eC/wI6tqTzf9TeDIAdtvZt8n/z3dnzwXJTlklfIdmmQmyZfmTu8ATwMeqqpHu/n76H28wbgtaQyTbKJ3pHR33+JJjOFCH+Uw/9//o226cfouvXEbZt+VytjvHHpHaXMWet5XI99vdc/fjiRzb9474MawO/10LHBt3+JJj+GwFvt3jDSOE38r/aQl+RzwcwusOr9/pqoqyaLXTHb/G/4yveva55xHr7QOpncN59uBd69CvmdU1f1JjgOuTXILvTIaizGP4T8CW6rqh93ikcfwiSDJq4Fp4Hl9i/d53qvq7oXvYWL+Fbisqh5J8jp6f9GcusIZhrUZ2FFVj/UtOxDGcGKaL/CqetFi65I8kGR9Ve3pymXvfu7qVcCVVfWDvvueO/J8JMlHgD9ejXxVdX/3854k1wMnAR+n92fY2u7octkfZTCOjEl+BvgUcH73J+LcfY88hosY5qMc5ra5L8la4GeBbw+570plJMmL6P1n+byqemRu+SLP+zjLZ2C+qvp23+yH6b0mMrfv8+fte/0Ys81ZynO1GXhj/4IVGMNhLfbvGGkcH++nUHYCc6/qbgGu2s+2+5w76wpr7nzzmcCCrzBPMl+SdXOnHZIcAZwC3F69V0Cuo3feftH9VyjjwcCV9M7x7Zi3blJjOMxHOfRnPwu4thu3ncDm9K5SORY4HvjymHItKWOSk4APAq+oqr19yxd83lch3/q+2VcAd3TTnwVe0uVcB7yEn/zrdcUydjlPoPci4Bf7lq3EGA5rJ/C73dUoJwPf7Q5uRhvHlXiFdrVu9M53XgPcBXwOOLxbPg18uG+7DfT+J3zSvP2vBW6hVzr/BDxlpfMBv95luKn7eU7f/sfRK57dwL8Ah6zGGAKvBn4A7Oq7bZz0GNJ7Zf9r9I6ozu+WvZteGQIc2o3L7m6cjuvb9/xuvzuBl03wd3BQxs8BD/SN285Bz/sK5/sL4LYux3XACX37/n43truB31utMezm3wlcMG+/FRnD7rEuo3f11Q/oncc+B3g98Ppufeh9Ac7dXZbpcYyjb6WXpEY93k+hSNLjlgUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGvX/eNskqxx82vUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 17==== Step 2 Train Loss 0.7175687551498413 ======  0.4642857142857143\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3356,  1.0630,  0.6518,  ...,  0.0380, -0.3062, -0.3769],\n",
            "        [-1.2960,  1.0792,  0.5453,  ..., -0.0307, -0.3771, -0.4643],\n",
            "        [-0.5950,  1.0603, -0.0045,  ..., -0.2072, -0.0055, -0.6379],\n",
            "        ...,\n",
            "        [-1.0829,  1.1355,  0.4428,  ...,  0.0443, -0.5413, -0.3498],\n",
            "        [-1.2607,  1.1923,  0.5572,  ..., -0.0862, -0.2458, -0.4959],\n",
            "        [-0.1117,  1.0713, -0.0091,  ..., -0.0382, -0.5868, -0.3688]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5140,  0.9620, -0.1218, -0.8378, -0.2001,  0.9694,  0.9851,  0.9896,\n",
            "         0.6694,  0.9880,  0.9877,  0.1667,  0.9948,  0.2998, -0.6817, -0.0551,\n",
            "        -0.6139,  0.9890,  0.9504,  0.6679, -0.1419,  0.9937,  0.9248,  0.9340,\n",
            "         0.8327,  0.3410, -0.4011, -0.8031,  0.7132,  0.9592,  0.7837,  0.9773,\n",
            "         0.5535,  0.9868, -0.3311, -0.7137, -0.5690,  0.0582,  0.9158,  0.8872,\n",
            "         0.8055,  0.2729, -0.5217,  0.9885,  0.8223,  0.8933,  0.8505,  0.9943,\n",
            "        -0.2150,  0.3468,  0.9748, -0.1816,  0.9515,  0.8397, -0.6179,  0.8344,\n",
            "         0.9865, -0.3444,  0.8818,  0.8686, -0.4748,  0.6337, -0.4723,  0.8221],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 18==== Step 2 Train Loss 0.7053250670433044 ======  0.47058823529411764\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2788,  1.0906,  0.5091,  ..., -0.0722, -0.2141, -0.4788],\n",
            "        [-0.5215,  1.0631, -0.0059,  ..., -0.0762, -0.4281, -0.4321],\n",
            "        [-0.2249,  0.9508, -0.1857,  ...,  0.0933, -0.5021, -0.6406],\n",
            "        ...,\n",
            "        [ 0.4073,  0.5618, -0.3719,  ...,  0.1893, -0.3844, -0.2963],\n",
            "        [-1.5155,  1.0566,  0.6494,  ...,  0.0953, -0.3893, -0.4249],\n",
            "        [ 0.5728, -0.7652,  0.0266,  ...,  0.0875,  0.4782,  0.0216]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.6394, -0.7957,  0.8869, -0.6819,  0.9837,  0.1147, -0.7456,  0.8419,\n",
            "        -0.2599,  0.1830,  0.9628, -0.1837,  0.9824,  0.6974,  0.9902,  0.4716,\n",
            "         0.8875,  0.9924,  0.9247, -0.0111,  0.4470,  0.5070,  0.0606, -0.3091,\n",
            "         0.2743,  0.9885, -0.6379,  0.3881, -0.0014,  0.9795,  0.7908, -0.3495,\n",
            "         0.6980,  0.9401,  0.7087,  0.9774,  0.9918, -0.1325,  0.9101, -0.8013,\n",
            "         0.9826,  0.2811,  0.8728,  0.8389,  0.8847,  0.7763, -0.2411,  0.4252,\n",
            "        -0.8089,  0.9788,  0.8420,  0.9947,  0.7487,  0.0406,  0.1187,  0.6905,\n",
            "         0.4167,  0.3806,  0.7241,  0.7237,  0.9083,  0.1443,  0.9648,  0.7165],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 19==== Step 2 Train Loss 0.7070392370223999 ======  0.5\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743],\n",
            "        [-0.5308,  1.2628,  0.2311,  ...,  0.0910, -0.2654, -0.4449],\n",
            "        [ 0.5651, -0.3555, -0.1069,  ...,  0.1122,  0.6507, -0.3559],\n",
            "        ...,\n",
            "        [ 0.5273, -0.7904,  0.0646,  ...,  0.0945,  0.5784,  0.1131],\n",
            "        [-1.3200,  1.0510,  0.6014,  ...,  0.0297, -0.4826, -0.3896],\n",
            "        [-0.4487,  0.9212, -0.0880,  ..., -0.2330, -0.5489, -0.4732]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9823,  0.8766, -0.3346,  0.9784,  0.0642,  0.5471,  0.9877,  0.8062,\n",
            "         0.9816, -0.1372,  0.8361,  0.9396,  0.5610,  0.9913,  0.9545,  0.8201,\n",
            "         0.6630,  0.8645,  0.8497,  0.9714, -0.4987,  0.9044, -0.7054,  0.6667,\n",
            "        -0.1804, -0.1026,  0.7697, -0.7384, -0.1532,  0.9371,  0.9946,  0.9120,\n",
            "         0.5774,  0.3165,  0.8492,  0.9422,  0.9600,  0.7638, -0.2670,  0.7229,\n",
            "         0.0283,  0.1904,  0.9846, -0.8439,  0.9889,  0.1839,  0.9851,  0.9934,\n",
            "         0.9796,  0.9869,  0.4173,  0.1246,  0.0805,  0.8611, -0.5139, -0.8870,\n",
            "        -0.5673,  0.9834,  0.6258,  0.9213,  0.9607, -0.6654,  0.9847,  0.6328],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 20==== Step 2 Train Loss 0.7306628823280334 ======  0.41379310344827586\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3567,  1.1212,  0.5071,  ...,  0.0908, -0.4530, -0.4381],\n",
            "        [ 0.1445,  0.8534, -0.0934,  ..., -0.0512, -0.4518, -0.2236],\n",
            "        [ 0.0997,  0.9889, -0.0663,  ..., -0.0116, -0.2888, -0.1708],\n",
            "        ...,\n",
            "        [-0.8639,  1.2702,  0.4311,  ..., -0.0020, -0.2367, -0.4605],\n",
            "        [-1.1646,  1.1628,  0.4900,  ..., -0.0530, -0.1701, -0.4644],\n",
            "        [-0.9794,  1.1208,  0.3896,  ...,  0.2332, -0.3649, -0.4267]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.4646, -0.5684,  0.3069,  0.9857,  0.4588,  0.5155,  0.9825,  0.9885,\n",
            "         0.2409,  0.9401,  0.6529,  0.9720,  0.2964,  0.0236,  0.1131,  0.7027,\n",
            "         0.9878,  0.5294,  0.6961, -0.6964,  0.9901,  0.2169,  0.9794, -0.4962,\n",
            "         0.0560,  0.9199,  0.5079, -0.3602,  0.9784, -0.4745,  0.9286,  0.5689,\n",
            "        -0.0653,  0.3718,  0.9934,  0.0089, -0.1443, -0.2197,  0.9759,  0.8839,\n",
            "         0.7884,  0.9842, -0.7793,  0.9422, -0.4077,  0.9924,  0.9833,  0.3428,\n",
            "        -0.6915,  0.9817,  0.0581,  0.9438,  0.9784, -0.4327,  0.9268, -0.6357,\n",
            "         0.8207, -0.1457,  0.9929,  0.9764,  0.9806,  0.9423, -0.5482,  0.9585],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPSUlEQVR4nO3df4zkdX3H8ecLDrAtthxlc73ijwOlNSSNh9lQWhp/ICpqIpgSeyTas6U5tNBoapOe8kepaVNsqiRNjfYU5NpalIKEa9FaBIwxUexi+XFA8A7ElOvBrSIqaUoF3v1jvivTZfZmdmdm9z76fCSb/c7n+/3OvPazk9d99zvfmUtVIUlqz2FrHUCStDIWuCQ1ygKXpEZZ4JLUKAtckhq1bjUf7LjjjqtNmzat5kNKUvNuu+22b1fVzOLxVS3wTZs2MTc3t5oPKUnNS/KtQeOeQpGkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNGlrgSZ6T5GtJ7khyd5I/7cZPSHJrkr1JPp3kyOnHlSQtGOUI/AngjKp6KbAZOCvJacAHgMuq6sXAd4HzpxdTkrTY0AKvnse7m0d0XwWcAVzTje8EzplKQknSQCO9EzPJ4cBtwIuBDwP3A49V1ZPdJg8Bxy+x7zZgG8ALXvCCcfNK+gmyafsNax1hYh689I0Tv8+RXsSsqqeqajPwPOBU4CWjPkBV7aiq2aqanZl51lv5JUkrtKyrUKrqMeAW4NeAY5IsHME/D9g34WySpIMY5SqUmSTHdMs/BbwGuJdekZ/bbbYVuH5aISVJzzbKOfCNwM7uPPhhwNVV9S9J7gE+leTPgP8ALp9iTknSIkMLvKruBE4ZMP4AvfPhkqQ14DsxJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRQws8yfOT3JLkniR3J3lXN35Jkn1Jbu++3jD9uJKkBetG2OZJ4D1V9fUkzwVuS3Jjt+6yqvqr6cWTJC1laIFX1X5gf7f8gyT3AsdPO5gk6eCWdQ48ySbgFODWbuiiJHcmuSLJ+iX22ZZkLsnc/Pz8WGElSc8YucCTHA1cC7y7qr4PfAR4EbCZ3hH6BwftV1U7qmq2qmZnZmYmEFmSBCMWeJIj6JX3J6vqMwBV9UhVPVVVTwMfA06dXkxJ0mKjXIUS4HLg3qr6UN/4xr7N3gzsnnw8SdJSRrkK5XTgbcBdSW7vxt4HnJdkM1DAg8AFU0koSRpolKtQvgxkwKrPTj6OJGlUvhNTkhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSo4YWeJLnJ7klyT1J7k7yrm782CQ3JtnTfV8//biSpAWjHIE/Cbynqk4GTgMuTHIysB24qapOAm7qbkuSVsnQAq+q/VX19W75B8C9wPHA2cDObrOdwDnTCilJerZlnQNPsgk4BbgV2FBV+7tVDwMblthnW5K5JHPz8/NjRJUk9Ru5wJMcDVwLvLuqvt+/rqoKqEH7VdWOqpqtqtmZmZmxwkqSnjFSgSc5gl55f7KqPtMNP5JkY7d+I3BgOhElSYOMchVKgMuBe6vqQ32rdgFbu+WtwPWTjydJWsq6EbY5HXgbcFeS27ux9wGXAlcnOR/4FvCW6USUJA0ytMCr6stAllj96snGkSSNyndiSlKjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjhhZ4kiuSHEiyu2/skiT7ktzefb1hujElSYuNcgR+JXDWgPHLqmpz9/XZycaSJA0ztMCr6kvAo6uQRZK0DOOcA78oyZ3dKZb1S22UZFuSuSRz8/PzYzycJKnfSgv8I8CLgM3AfuCDS21YVTuqaraqZmdmZlb4cJKkxVZU4FX1SFU9VVVPAx8DTp1sLEnSMCsq8CQb+26+Gdi91LaSpOlYN2yDJFcBrwSOS/IQ8CfAK5NsBgp4ELhgihklSQMMLfCqOm/A8OVTyCJJWgbfiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRQws8yRVJDiTZ3Td2bJIbk+zpvq+fbkxJ0mKjHIFfCZy1aGw7cFNVnQTc1N2WJK2ioQVeVV8CHl00fDaws1veCZwz4VySpCFWeg58Q1Xt75YfBjYstWGSbUnmkszNz8+v8OEkSYuN/SJmVRVQB1m/o6pmq2p2ZmZm3IeTJHVWWuCPJNkI0H0/MLlIkqRRrLTAdwFbu+WtwPWTiSNJGtUolxFeBXwF+OUkDyU5H7gUeE2SPcCZ3W1J0ipaN2yDqjpviVWvnnAWSdIy+E5MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktSooR8ne6jYtP2GtY4wMQ9e+sa1jjAxP06/F6k1HoFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRjVzGeGPEy+9kzQJHoFLUqMscElqlAUuSY0a6xx4kgeBHwBPAU9W1ewkQkmShpvEi5ivqqpvT+B+JEnL4CkUSWrUuAVewL8luS3JtkEbJNmWZC7J3Pz8/JgPJ0laMG6B/0ZVvQx4PXBhkpcv3qCqdlTVbFXNzszMjPlwkqQFYxV4Ve3rvh8ArgNOnUQoSdJwKy7wJD+T5LkLy8Brgd2TCiZJOrhxrkLZAFyXZOF+/rGq/nUiqSRJQ624wKvqAeClE8wiSVoGLyOUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqLEKPMlZSe5LsjfJ9kmFkiQNt+ICT3I48GHg9cDJwHlJTp5UMEnSwY1zBH4qsLeqHqiq/wU+BZw9mViSpGHWjbHv8cB/9t1+CPjVxRsl2QZs624+nuS+MR5zseOAb0/w/qbJrNNh1ukw64TlA8DKs75w0OA4BT6SqtoB7JjGfSeZq6rZadz3pJl1Osw6HWadjklnHecUyj7g+X23n9eNSZJWwTgF/u/ASUlOSHIksAXYNZlYkqRhVnwKpaqeTHIR8HngcOCKqrp7YslGM5VTM1Ni1ukw63SYdTommjVVNcn7kyStEt+JKUmNssAlqVGHfIEnOTbJjUn2dN/XD9jmVUlu7/v6nyTndOuuTPLNvnWb1zJrt91TfXl29Y2fkOTW7qMJPt29OLxmWZNsTvKVJHcnuTPJb/Wtm/q8DvuohiRHdfO0t5u3TX3r3tuN35fkdZPOtsycf5jknm4Ob0rywr51A58La5j17Unm+zL9Xt+6rd3zZU+SrYdA1sv6cn4jyWN961Z7Xq9IciDJ7iXWJ8lfdz/LnUle1rdu5fNaVYf0F/CXwPZueTvwgSHbHws8Cvx0d/tK4NxDKSvw+BLjVwNbuuWPAu9cy6zALwEndcu/COwHjlmNeaX3wvj9wInAkcAdwMmLtvl94KPd8hbg093yyd32RwEndPdz+BrmfFXf8/GdCzkP9lxYw6xvB/5mwL7HAg9039d3y+vXMuui7f+A3oUUqz6v3eO9HHgZsHuJ9W8APgcEOA24dRLzesgfgdN7e/7ObnkncM6Q7c8FPldV/z3VVIMtN+uPJAlwBnDNSvZfgaFZq+obVbWnW/4v4AAwM8VM/Ub5qIb+n+Ea4NXdPJ4NfKqqnqiqbwJ7u/tbk5xVdUvf8/Gr9N4zsRbG+fiL1wE3VtWjVfVd4EbgrCnlhOVnPQ+4aop5DqqqvkTvwHEpZwN/Vz1fBY5JspEx57WFAt9QVfu75YeBDUO238Kzf5F/3v3ZclmSoyae8BmjZn1OkrkkX1041QP8PPBYVT3Z3X6I3scVrHVWAJKcSu9I6P6+4WnO66CPalg8Hz/appu379Gbx1H2Xc2c/c6ndyS2YNBzYVpGzfqb3e/1miQLb9ZbzTld1uN1p6ROAG7uG17NeR3FUj/PWPM69bfSjyLJF4BfGLDq4v4bVVVJlrzusfsX7VfoXZu+4L30CupIetdg/jHw/jXO+sKq2pfkRODmJHfRK5+JmvC8/j2wtaqe7oYnOq8/CZK8FZgFXtE3/KznQlXdP/geVsU/A1dV1RNJLqD3F84Za5hnFFuAa6rqqb6xQ21ep+KQKPCqOnOpdUkeSbKxqvZ3RXLgIHf1FuC6qvph330vHGU+keQTwB+tddaq2td9fyDJF4FTgGvp/Vm1rjuaHPujCSaRNcnPAjcAF3d/+i3c90TndYBRPqphYZuHkqwDfg74zoj7rmZOkpxJ7x/OV1TVEwvjSzwXplU0Q7NW1Xf6bn6c3mslC/u+ctG+X5x4wmcs53e4Bbiwf2CV53UUS/08Y81rC6dQdgELr8xuBa4/yLbPOg/WldPCOeZzgIGvEk/I0KxJ1i+cbkhyHHA6cE/1XtG4hd45/CX3X+WsRwLX0Tt3d82iddOe11E+qqH/ZzgXuLmbx13AlvSuUjkBOAn42oTzjZwzySnA3wJvqqoDfeMDnwtTyjlq1o19N98E3Nstfx54bZd5PfBa/v9fuquetcv7Enov/n2lb2y153UUu4Df7q5GOQ34XncQNN68ruYrtSv5ondO8yZgD/AF4NhufBb4eN92m+j9a3bYov1vBu6iVzD/ABy9llmBX+/y3NF9P79v/xPpFc1e4J+Ao9Y461uBHwK3931tXq15pffK/TfoHTld3I29n14RAjynm6e93byd2Lfvxd1+9wGvn/JzdFjOLwCP9M3hrmHPhTXM+hfA3V2mW4CX9O37u91c7wV+Z62zdrcvAS5dtN9azOtV9K7S+iG989jnA+8A3tGtD73/AOf+LtPsJObVt9JLUqNaOIUiSRrAApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmN+j/w530xsb5twAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 21==== Step 2 Train Loss 0.7177225947380066 ======  0.41509433962264153\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4766,  0.5652, -0.4130,  ...,  0.2610, -0.6927, -0.4606],\n",
            "        [ 0.1945,  0.7061, -0.2769,  ..., -0.0486, -0.4817, -0.3923],\n",
            "        [-1.0002,  1.1573,  0.3372,  ..., -0.1511, -0.1675, -0.6355],\n",
            "        ...,\n",
            "        [-1.2630,  0.9828,  0.4450,  ...,  0.0366, -0.3558, -0.4377],\n",
            "        [ 0.8080, -0.2619, -0.4361,  ...,  0.2211, -0.4679,  0.1823],\n",
            "        [-1.2882,  1.1016,  0.5764,  ..., -0.0596, -0.4065, -0.4521]],\n",
            "       device='cuda:0')\n",
            "tensor([-3.0456e-01,  8.1597e-01,  9.7673e-01,  9.9060e-01, -1.7519e-01,\n",
            "         9.8222e-01, -5.1163e-01, -7.1309e-01,  6.2714e-02, -7.7380e-01,\n",
            "         9.7967e-01,  9.2121e-01,  3.9362e-01, -7.4667e-03,  9.2761e-01,\n",
            "        -6.7524e-01, -4.9899e-01,  6.9285e-01, -7.9044e-01,  9.8346e-01,\n",
            "         9.6354e-01,  1.0888e-01,  9.6533e-01, -4.7237e-01,  2.2617e-01,\n",
            "         9.4240e-01, -2.4167e-04,  4.5558e-01, -4.4242e-01,  9.5309e-01,\n",
            "         9.1280e-01, -7.5482e-01,  8.4742e-01,  5.8791e-01,  7.4767e-01,\n",
            "         9.4005e-01,  1.0922e-01,  9.2926e-01,  4.5060e-02,  6.1952e-01,\n",
            "         8.4963e-01,  9.8692e-01, -4.6883e-01,  9.3935e-01, -5.4614e-01,\n",
            "         9.1891e-01,  7.9470e-01,  9.6361e-01,  6.1352e-01,  9.8296e-01,\n",
            "         9.8182e-01,  3.7077e-01,  7.3116e-01,  9.9120e-01, -7.7568e-01,\n",
            "         9.6963e-01,  2.3437e-01,  9.9537e-01,  9.8874e-01, -1.5568e-01,\n",
            "        -2.3069e-01,  9.8231e-01,  2.7636e-01, -9.3039e-02], device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 22==== Step 2 Train Loss 0.7041245698928833 ======  0.4444444444444444\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [ 0.0228,  0.9145, -0.0177,  ..., -0.1188, -0.0558, -0.4685],\n",
            "        [-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        ...,\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        [-1.4107,  1.1037,  0.4881,  ...,  0.0159, -0.7426, -0.2042],\n",
            "        [-0.6442,  1.1782,  0.2434,  ...,  0.0233, -0.4369, -0.5772]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9545,  0.6686,  0.9927,  0.4458,  0.9716,  0.3868, -0.6500,  0.5280,\n",
            "         0.9858,  0.9951,  0.4493,  0.6158,  0.6258,  0.8357,  0.9855,  0.9629,\n",
            "         0.4982, -0.8031,  0.9944,  0.9317, -0.3725,  0.3719,  0.9605,  0.0273,\n",
            "         0.8829,  0.9178,  0.9652,  0.4426,  0.8763,  0.9005, -0.0746,  0.8076,\n",
            "        -0.3008,  0.9851,  0.7296,  0.9007,  0.0454,  0.6368,  0.1067,  0.9721,\n",
            "         0.6361,  0.1329,  0.9146, -0.2652, -0.7720,  0.0328,  0.9875, -0.4402,\n",
            "         0.8765, -0.3407,  0.9128, -0.0451,  0.9814,  0.9766,  0.5531, -0.5477,\n",
            "         0.5274,  0.4069,  0.6093,  0.9924,  0.7800,  0.4531, -0.7840,  0.9416],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 23==== Step 2 Train Loss 0.6874130964279175 ======  0.4583333333333333\n",
            "torch.Size([64, 48])\n",
            "tensor([[-9.7601e-01,  1.3536e+00,  6.3612e-01,  ...,  6.8190e-03,\n",
            "         -2.3077e-01, -3.8898e-01],\n",
            "        [-1.2708e+00,  1.0498e+00,  3.9554e-01,  ..., -9.8638e-04,\n",
            "         -6.9632e-01, -4.4897e-01],\n",
            "        [-9.7765e-01,  1.3314e+00,  3.7630e-01,  ..., -1.1343e-01,\n",
            "         -3.2506e-01, -4.5926e-01],\n",
            "        ...,\n",
            "        [ 6.5899e-01, -2.9577e-01,  3.0366e-02,  ...,  1.3116e-01,\n",
            "          6.0647e-01, -3.7955e-01],\n",
            "        [-3.4436e-01,  1.1132e+00,  9.6339e-02,  ..., -1.7127e-01,\n",
            "         -2.2060e-01, -5.8124e-01],\n",
            "        [ 5.4288e-01,  2.9889e-01, -1.8059e-03,  ..., -8.3883e-02,\n",
            "          2.8741e-01, -3.3828e-01]], device='cuda:0')\n",
            "tensor([ 0.3069,  0.9869,  0.7392,  0.9748,  0.9223,  0.2255,  0.4685,  0.9410,\n",
            "         1.0000,  0.9058,  0.1860,  0.9590,  0.9877,  0.9892,  0.9876, -0.5120,\n",
            "         0.8784,  0.9696,  0.9227,  0.9410,  0.9755, -0.0849, -0.2579, -0.3082,\n",
            "        -0.6445,  0.3568,  0.9421,  0.9016,  0.2848,  0.7833,  0.9184,  0.8496,\n",
            "         0.9946,  0.9870, -0.8506,  0.7279,  0.8323, -0.6709, -0.6099, -0.3793,\n",
            "         0.3212,  0.8696,  0.9717,  0.8766,  0.9455,  0.9395,  0.9293, -0.4356,\n",
            "        -0.1061,  0.8201,  0.9866,  0.9102,  0.9836,  0.9424,  0.9962,  0.9829,\n",
            "        -0.2687,  0.4932,  0.9587,  0.2025,  0.9687,  0.9587,  0.9244,  0.7222],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 24==== Step 2 Train Loss 0.7263009548187256 ======  0.3773584905660377\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3104,  1.0613,  0.6367,  ..., -0.0392, -0.2553, -0.4147],\n",
            "        [ 0.7873, -0.5681, -0.1221,  ...,  0.2029,  0.3178,  0.0037],\n",
            "        [-0.8584,  1.0800,  0.1536,  ..., -0.0567, -0.2992, -0.5394],\n",
            "        ...,\n",
            "        [-0.8337,  1.1159,  0.1479,  ...,  0.0218, -0.1356, -0.4791],\n",
            "        [-1.1813,  1.1511,  0.4591,  ...,  0.0195, -0.2204, -0.5355],\n",
            "        [-0.2617,  1.0294,  0.1573,  ...,  0.1007, -0.2208, -0.4087]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9918,  0.3206,  0.9683, -0.6688, -0.2082,  0.9141,  0.5314,  0.9952,\n",
            "        -0.7632,  0.9955, -0.5000,  0.9418,  0.9828,  0.7782,  0.9420,  0.7140,\n",
            "        -0.1906,  0.0138,  0.9716,  0.9947,  0.9222,  0.7719, -0.8509,  0.7097,\n",
            "         0.8938,  0.9860, -0.5737,  0.5004, -0.3698,  0.9919,  0.4066,  0.9048,\n",
            "        -0.6801,  0.9914, -0.0832,  0.9867,  0.5977,  0.6296,  0.9882,  0.9791,\n",
            "        -0.6795, -0.7866, -0.7543, -0.0374,  0.9072,  0.6350, -0.4720,  0.9124,\n",
            "         0.4094,  0.9926,  0.9770,  0.7314,  0.9671, -0.3310, -0.0399, -0.1259,\n",
            "         0.9208,  0.6286, -0.2299,  0.4290,  0.9238,  0.9384,  0.9914, -0.2140],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 25==== Step 2 Train Loss 0.7588914036750793 ======  0.3636363636363636\n",
            "torch.Size([64, 48])\n",
            "tensor([[-6.8197e-01,  9.8863e-01,  2.8513e-01,  ..., -7.5350e-02,\n",
            "         -3.1892e-04, -6.9913e-01],\n",
            "        [-8.3858e-01,  1.1379e+00,  2.8216e-01,  ..., -1.2843e-01,\n",
            "         -2.1993e-01, -5.0951e-01],\n",
            "        [-1.0635e+00,  1.0076e+00,  6.4998e-01,  ...,  5.7955e-02,\n",
            "         -1.9653e-01, -4.0289e-01],\n",
            "        ...,\n",
            "        [-1.4876e+00,  1.0979e+00,  5.1740e-01,  ...,  8.6918e-02,\n",
            "         -6.3262e-01, -4.6162e-01],\n",
            "        [-1.2133e+00,  1.2402e+00,  5.2971e-01,  ..., -1.1618e-01,\n",
            "         -1.5118e-01, -4.7657e-01],\n",
            "        [ 4.9239e-01,  1.1442e-01, -4.6210e-01,  ...,  1.3173e-01,\n",
            "         -8.5904e-01,  5.5259e-02]], device='cuda:0')\n",
            "tensor([-0.1071,  0.9825,  0.9918,  0.7625,  0.9857, -0.5122,  0.9588, -0.3266,\n",
            "         0.9498,  0.2409,  0.8614,  0.7748, -0.6924,  0.9829,  0.3757, -0.0928,\n",
            "        -0.7225, -0.6505,  0.8310, -0.1061,  0.0559,  0.9944,  0.1239,  0.8053,\n",
            "         0.7950, -0.3166,  0.8325, -0.6029,  0.6885,  0.9895,  0.9783,  0.7689,\n",
            "        -0.2280, -0.8632,  0.9762,  0.9907,  0.8605,  0.8888, -0.4790,  0.8457,\n",
            "         0.3153,  0.7328,  0.3047,  0.9394,  0.9770,  0.5758,  0.9896, -0.2019,\n",
            "         0.1940,  0.3170,  0.7584,  0.9541,  0.9919,  0.9542,  0.5859, -0.4154,\n",
            "         0.9938,  0.2189,  0.9843,  0.9921,  0.6962, -0.6572,  0.5388,  0.9568],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQNElEQVR4nO3dbYxcZ32G8evGzgstlNjNynUTwAlNG0WtcNDWTUvFS3gLIBGjRtSRoKZNZaBQgUorAvlQQEUNVSFS1QowJMRtaSA1RHF5KTWJEUKC0A11HDtpsBOCGtfECyFAVNUl5t8PcxaGza5nvDuz68dcP2m0Z55zzsztZ6zbZ8+cGaeqkCS153HLHUCStDAWuCQ1ygKXpEZZ4JLUKAtckhq1cimf7Mwzz6x169Yt5VNKUvNuv/32b1XVxOzxJS3wdevWMTU1tZRPKUnNS/KNucY9hSJJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY1a0k9iStLxWHflp5Y7wsjcf/VLR/6YHoFLUqMGFniS05N8JckdSfYleUc3fn2SryfZ3d3Wjz+uJGnGMKdQjgAXV9UjSU4BvpjkM926P6uq7eOLJ0maz8ACr97/evxId/eU7ub/hCxJy2yoc+BJViTZDRwGdlbVbd2qdyXZk+SaJKfNs++WJFNJpqanp0cUW5I0VIFX1dGqWg+cDWxI8qvAW4HzgV8HVgNvmWffrVU1WVWTExOP+T5ySdICHddVKFX1MLALuKSqDlXPEeDDwIZxBJQkzW2Yq1AmkpzRLT8eeAHwn0nWdmMBNgJ7xxlUkvSThrkKZS2wLckKeoV/Y1V9MsmtSSaAALuB144xpyRplmGuQtkDXDjH+MVjSSRJGoqfxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMGFniS05N8JckdSfYleUc3fk6S25IcSPKxJKeOP64kacYwR+BHgIur6unAeuCSJBcB7wauqapfAr4DXDG+mJKk2QYWePU80t09pbsVcDGwvRvfBmwcS0JJ0pyGOgeeZEWS3cBhYCdwL/BwVT3abfIAcNY8+25JMpVkanp6ehSZJUkMWeBVdbSq1gNnAxuA84d9gqraWlWTVTU5MTGxwJiSpNmO6yqUqnoY2AX8JnBGkpXdqrOBgyPOJkk6hmGuQplIcka3/HjgBcDd9Ir8sm6zzcDN4wopSXqslYM3YS2wLckKeoV/Y1V9MsldwEeT/AXwH8C1Y8wpSZplYIFX1R7gwjnG76N3PlyStAz8JKYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1sMCTPDnJriR3JdmX5I3d+NuTHEyyu7u9ZPxxJUkzBv6v9MCjwJur6qtJngjcnmRnt+6aqvrr8cWTJM1nYIFX1SHgULf8/SR3A2eNO5gk6diO6xx4knXAhcBt3dAbkuxJcl2SVfPssyXJVJKp6enpRYWVJP3Y0AWe5AnAx4E3VdX3gPcBTwPW0ztCf89c+1XV1qqarKrJiYmJEUSWJMGQBZ7kFHrl/ZGq+gRAVT1YVUer6ofAB4EN44spSZptmKtQAlwL3F1V7+0bX9u32cuBvaOPJ0mazzBXoTwTeBVwZ5Ld3djbgMuTrAcKuB94zVgSSpLmNMxVKF8EMseqT48+jiRpWH4SU5IaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRpY4EmenGRXkruS7Evyxm58dZKdSfZ3P1eNP64kacYwR+CPAm+uqguAi4DXJ7kAuBK4parOA27p7kuSlsjAAq+qQ1X11W75+8DdwFnApcC2brNtwMZxhZQkPdZxnQNPsg64ELgNWFNVh7pV3wTWzLPPliRTSaamp6cXEVWS1G/oAk/yBODjwJuq6nv966qqgJprv6raWlWTVTU5MTGxqLCSpB8bqsCTnEKvvD9SVZ/ohh9MsrZbvxY4PJ6IkqS5DHMVSoBrgbur6r19q3YAm7vlzcDNo48nSZrPyiG2eSbwKuDOJLu7sbcBVwM3JrkC+AbwivFElCTNZWCBV9UXgcyz+nmjjSNJGpafxJSkRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNcz/Sn9dksNJ9vaNvT3JwSS7u9tLxhtTkjTbMEfg1wOXzDF+TVWt726fHm0sSdIgAwu8qr4APLQEWSRJx2Ex58DfkGRPd4pl1cgSSZKGstACfx/wNGA9cAh4z3wbJtmSZCrJ1PT09AKfTpI024IKvKoerKqjVfVD4IPAhmNsu7WqJqtqcmJiYqE5JUmzLKjAk6ztu/tyYO9820qSxmPloA2S3AA8BzgzyQPAnwPPSbIeKOB+4DVjzChJmsPAAq+qy+cYvnYMWSRJx8FPYkpSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEDCzzJdUkOJ9nbN7Y6yc4k+7ufq8YbU5I02zBH4NcDl8wauxK4parOA27p7kuSltDAAq+qLwAPzRq+FNjWLW8DNo44lyRpgIWeA19TVYe65W8Ca+bbMMmWJFNJpqanpxf4dJKk2Rb9JmZVFVDHWL+1qiaranJiYmKxTydJ6iy0wB9Mshag+3l4dJEkScNYaIHvADZ3y5uBm0cTR5I0rGEuI7wB+BLwK0keSHIFcDXwgiT7ged39yVJS2jloA2q6vJ5Vj1vxFkkScfBT2JKUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEDv8xK+mmw7spPLXeEkbn/6pcudwQtEY/AJalRFrgkNcoCl6RGWeCS1Khm3sT0TaYT08n0ukit8QhckhplgUtSoxZ1CiXJ/cD3gaPAo1U1OYpQkqTBRnEO/LlV9a0RPI4k6Th4CkWSGrXYI/AC/i1JAR+oqq2zN0iyBdgC8JSnPGWRT3dy8MoNSaOw2CPw366qZwAvBl6f5FmzN6iqrVU1WVWTExMTi3w6SdKMRRV4VR3sfh4GbgI2jCKUJGmwBRd4kp9N8sSZZeCFwN5RBZMkHdtizoGvAW5KMvM4/1RV/zqSVJKkgRZc4FV1H/D0EWaRJB2HZr4LRdJwvMrpp4fXgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNWlSBJ7kkyT1JDiS5clShJEmDLbjAk6wA/g54MXABcHmSC0YVTJJ0bIs5At8AHKiq+6rq/4CPApeOJpYkaZCVi9j3LOC/+u4/APzG7I2SbAG2dHcfSXLPIp5zVM4EvrXcIQYw42iYcTRO9Iwnej7y7kVlfOpcg4sp8KFU1VZg67if53gkmaqqyeXOcSxmHA0zjsaJnvFEzwfjybiYUygHgSf33T+7G5MkLYHFFPi/A+clOSfJqcAmYMdoYkmSBlnwKZSqejTJG4DPAiuA66pq38iSjdcJdUpnHmYcDTOOxome8UTPB2PImKoa9WNKkpaAn8SUpEZZ4JLUqJO2wJOsTrIzyf7u56o5tnlukt19t/9NsrFbd32Sr/etW78cGbvtjvbl2NE3fk6S27qvMvhY92bykuZLsj7Jl5LsS7Inye/2rRvbHA76Gockp3VzcqCbo3V9697ajd+T5EWjyrSAjH+S5K5u3m5J8tS+dXO+5suQ8dVJpvuy/GHfus3d3439STYvY8Zr+vJ9LcnDfeuWah6vS3I4yd551ifJ33R/hj1JntG3buHzWFUn5Q34K+DKbvlK4N0Dtl8NPAT8THf/euCyEyEj8Mg84zcCm7rl9wOvW+p8wC8D53XLvwgcAs4Y5xzSe9P8XuBc4FTgDuCCWdv8EfD+bnkT8LFu+YJu+9OAc7rHWbFMGZ/b9/ftdTMZj/WaL0PGVwN/O8e+q4H7up+ruuVVy5Fx1vZ/TO+CiiWbx+55ngU8A9g7z/qXAJ8BAlwE3DaKeTxpj8Dpfax/W7e8Ddg4YPvLgM9U1f+MNdVPOt6MP5IkwMXA9oXsP6SB+arqa1W1v1v+b+AwMDHiHLMN8zUO/dm3A8/r5uxS4KNVdaSqvg4c6B5vyTNW1a6+v29fpvdZiqW0mK/DeBGws6oeqqrvADuBS06AjJcDN4whxzFV1RfoHQDO51Lg76vny8AZSdayyHk8mQt8TVUd6pa/CawZsP0mHvvCv6v7deeaJKeNPOHwGU9PMpXkyzOneICfBx6uqke7+w/Q+3qD5cgHQJIN9I6S7u0bHscczvU1DrP/7D/appuj79Kbs2H2XaqM/a6gd4Q2Y67XfNSGzfg73Wu4PcnMh/dOuHnsTkGdA9zaN7wU8ziM+f4ci5rHsX+UfpySfA74hTlWXdV/p6oqybzXS3b/Ev4avWvaZ7yVXmmdSu/6zbcA71ymjE+tqoNJzgVuTXInvUJatBHP4T8Am6vqh93wSObwZJfklcAk8Oy+4ce85lV179yPMFb/AtxQVUeSvIbebzUXL0OOYWwCtlfV0b6xE2Uex6LpAq+q58+3LsmDSdZW1aGuXA4f46FeAdxUVT/oe+yZI88jST4M/OlyZayqg93P+5J8HrgQ+Di9X8NWdkeYC/oqg1HkS/JzwKeAq7pfD2ceeyRzOIdhvsZhZpsHkqwEngR8e8h9lyojSZ5P7x/LZ1fVkZnxeV7zURfPwIxV9e2+ux+i977IzL7PmbXv50ecb+Z5hn29NgGv7x9Yonkcxnx/jkXN48l8CmUHMPOO7mbg5mNs+5jzZl1hzZxr3gjM+e7yuDMmWTVz6iHJmcAzgbuq9w7ILnrn7ufdfwnynQrcRO/83vZZ68Y1h8N8jUN/9suAW7s52wFsSu8qlXOA84CvjCjXcWVMciHwAeBlVXW4b3zO13yZMq7tu/sy4O5u+bPAC7usq4AX8pO/wS5Zxi7n+fTeBPxS39hSzeMwdgC/112NchHw3e4AZ3HzuBTv0C7Hjd75zluA/cDngNXd+CTwob7t1tH7V/Bxs/a/FbiTXun8I/CE5cgI/FaX447u5xV9+59Lr3wOAP8MnLYM+V4J/ADY3XdbP+45pPeu/tfoHU1d1Y29k14ZApzezcmBbo7O7dv3qm6/e4AXj/Hv4KCMnwMe7Ju3HYNe82XI+JfAvi7LLuD8vn3/oJvfA8DvL1fG7v7bgatn7beU83gDvSuwfkDvPPYVwGuB13brQ+8/wLm3yzI5inn0o/SS1KiT+RSKJJ3ULHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqP8HxE3gimZLM50AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 26==== Step 2 Train Loss 0.6818059086799622 ======  0.4166666666666667\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.5526,  1.0429,  0.2054,  ..., -0.0217, -0.3591, -0.5348],\n",
            "        [-1.2760,  1.1292,  0.6219,  ..., -0.0763, -0.3256, -0.4322],\n",
            "        [-1.0414,  1.3739,  0.6016,  ...,  0.0296, -0.2668, -0.3415],\n",
            "        ...,\n",
            "        [-0.8919,  1.1891,  0.4681,  ...,  0.0017, -0.0401, -0.5482],\n",
            "        [ 0.4131,  0.3238, -0.2471,  ...,  0.2179, -0.3592, -0.2757],\n",
            "        [ 0.5304, -0.5254,  0.0628,  ...,  0.1543,  0.7044, -0.1635]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1076, -0.0052,  0.9774, -0.8184,  0.9391,  0.9601,  0.4337,  0.9223,\n",
            "         0.9395, -0.0870, -0.1443,  0.9821,  0.9226,  0.9962,  0.9905,  0.0905,\n",
            "         0.8775,  0.9595, -0.6109,  0.2500,  0.9170,  0.6140, -0.0724,  0.9680,\n",
            "        -0.7446,  0.0977,  0.9564, -0.7370,  0.9913,  0.9837,  0.9846,  0.9859,\n",
            "         0.9812,  0.8906,  0.8795,  0.7150, -0.1470,  0.9917,  0.9550, -0.2916,\n",
            "         0.7797,  0.4386,  0.9695,  0.9324, -0.2255, -0.4686,  0.9411,  0.7037,\n",
            "        -0.2158,  0.9803,  0.9825,  0.7466,  0.9766, -0.7206, -0.0365,  0.9867,\n",
            "         0.9395,  0.6733,  0.9808,  0.9874, -0.8071,  0.8036,  0.1812,  0.8896],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARBklEQVR4nO3dfaxkdX3H8ffHXR5stWWRG7oFdUFpCWnjYm63tDQ+4BNiI5gSu6TataVZtdpotK0gf1RNTaGp0jZt1FWQbWsRihK2PtSusMSYKPaiy3OVBTFdurJXEZU0pQLf/jHnyni5d2funZl7+cH7lUzuOb9zzsyHc4fPnnvmzEyqCklSe5602gEkSctjgUtSoyxwSWqUBS5JjbLAJalRa1fywY444ojasGHDSj6kJDXv+uuv/05VTc0fX9EC37BhAzMzMyv5kJLUvCTfWmh86FMoSdYk+VqST3XzxyS5LsmeJJclOXhcYSVJgy3lHPhbgNv65i8ALqyqZwPfA84eZzBJ0oENVeBJjgZeAXykmw9wCnBFt8p24IxJBJQkLWzYI/C/Bv4UeLibfxpwX1U92M3vBY5aaMMkW5PMJJmZnZ0dKawk6REDCzzJbwL7q+r65TxAVW2rqumqmp6aetSLqJKkZRrmKpSTgVcmOQ04FPgZ4G+Aw5Ks7Y7CjwbunlxMSdJ8A4/Aq+rcqjq6qjYAm4Frqup3gF3Amd1qW4CrJpZSkvQoo7wT8x3A25LsoXdO/KLxRJIkDWNJb+SpqmuBa7vpO4FN448kSRrGir4TU5KWYsM5n17tCGNz1/mvGPt9+mFWktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KiBBZ7k0CRfSXJDkluSvLsbvyTJN5Ps7m4bJx9XkjRnmK9UewA4paruT3IQ8MUkn+2W/UlVXTG5eJKkxQws8Koq4P5u9qDuVpMMJUkabKhz4EnWJNkN7Ad2VtV13aL3JrkxyYVJDllk261JZpLMzM7Ojim2JGmoAq+qh6pqI3A0sCnJLwHnAscDvwIcDrxjkW23VdV0VU1PTU2NKbYkaUlXoVTVfcAu4NSq2lc9DwAfBTZNIqAkaWHDXIUyleSwbvrJwEuA/0yyvhsLcAZw8ySDSpJ+0jBXoawHtidZQ6/wL6+qTyW5JskUEGA38IYJ5pQkzTPMVSg3AicuMH7KRBJJkobiOzElqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUcN8J+ahSb6S5IYktyR5dzd+TJLrkuxJclmSgycfV5I0Z5gj8AeAU6rqOcBG4NQkJwEXABdW1bOB7wFnTy6mJGm+gQVePfd3swd1twJOAa7oxrfT+2Z6SdIKGeoceJI1SXYD+4GdwB3AfVX1YLfKXuCoyUSUJC1kqAKvqoeqaiNwNLAJOH7YB0iyNclMkpnZ2dllxpQkzbekq1Cq6j5gF/BrwGFJ1naLjgbuXmSbbVU1XVXTU1NTI4WVJD1imKtQppIc1k0/GXgJcBu9Ij+zW20LcNWkQkqSHm3t4FVYD2xPsoZe4V9eVZ9Kcivw8SR/DnwNuGiCOSVJ8wws8Kq6EThxgfE76Z0PlyStAt+JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUcN8qfHTk+xKcmuSW5K8pRt/V5K7k+zubqdNPq4kac4wX2r8IPD2qvpqkqcC1yfZ2S27sKr+anLxJEmLGeZLjfcB+7rpHya5DThq0sEkSQe2pHPgSTbQ+4b667qhNye5McnFSdYtss3WJDNJZmZnZ0cKK0l6xNAFnuQpwCeAt1bVD4APAM8CNtI7Qn/fQttV1baqmq6q6ampqTFEliTBkAWe5CB65f2xqvokQFXdU1UPVdXDwIeBTZOLKUmab5irUAJcBNxWVe/vG1/ft9qrgJvHH0+StJhhrkI5GXgtcFOS3d3YO4GzkmwECrgLeP1EEkqSFjTMVShfBLLAos+MP44kaVi+E1OSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNcx3Yj49ya4ktya5JclbuvHDk+xMcnv3c93k40qS5gxzBP4g8PaqOgE4CXhTkhOAc4Crq+o44OpuXpK0QgYWeFXtq6qvdtM/BG4DjgJOB7Z3q20HzphUSEnSoy3pHHiSDcCJwHXAkVW1r1v0beDIsSaTJB3Q0AWe5CnAJ4C3VtUP+pdVVQG1yHZbk8wkmZmdnR0prCTpEUMVeJKD6JX3x6rqk93wPUnWd8vXA/sX2raqtlXVdFVNT01NjSOzJInhrkIJcBFwW1W9v2/RDmBLN70FuGr88SRJi1k7xDonA68Fbkqyuxt7J3A+cHmSs4FvAa+eTERJ0kIGFnhVfRHIIotfNN44kqRh+U5MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNGuZLjS9Osj/JzX1j70pyd5Ld3e20ycaUJM03zBH4JcCpC4xfWFUbu9tnxhtLkjTIwAKvqi8A965AFknSEoxyDvzNSW7sTrGsW2ylJFuTzCSZmZ2dHeHhJEn9llvgHwCeBWwE9gHvW2zFqtpWVdNVNT01NbXMh5MkzbesAq+qe6rqoap6GPgwsGm8sSRJgyyrwJOs75t9FXDzYutKkiZj7aAVklwKvAA4Isle4M+AFyTZCBRwF/D6CWaUJC1gYIFX1VkLDF80gSySpCXwnZiS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1sMCTXJxkf5Kb+8YOT7Izye3dz3WTjSlJmm+YI/BLgFPnjZ0DXF1VxwFXd/OSpBU0sMCr6gvAvfOGTwe2d9PbgTPGnEuSNMByz4EfWVX7uulvA0cutmKSrUlmkszMzs4u8+EkSfON/CJmVRVQB1i+raqmq2p6ampq1IeTJHWWW+D3JFkP0P3cP75IkqRhLLfAdwBbuuktwFXjiSNJGtYwlxFeCnwJ+MUke5OcDZwPvCTJ7cCLu3lJ0gpaO2iFqjprkUUvGnMWSdIS+E5MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGjXw88AfKzac8+nVjjA2d53/itWOMDaPl9/L4+l3oicOj8AlqVEjHYEnuQv4IfAQ8GBVTY8jlCRpsHGcQnlhVX1nDPcjSVoCT6FIUqNGPQIv4N+TFPChqto2f4UkW4GtAM94xjNGfDhpMh4vL8aCL8g+kYx6BP4bVfVc4OXAm5I8b/4KVbWtqqaranpqamrEh5MkzRmpwKvq7u7nfuBKYNM4QkmSBlt2gSf56SRPnZsGXgrcPK5gkqQDG+Uc+JHAlUnm7uefq+rfxpJKkjTQsgu8qu4EnjPGLJKkJWjmrfSPJ4+nKx702OPz64nD68AlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpUSMVeJJTk3w9yZ4k54wrlCRpsFG+lX4N8PfAy4ETgLOSnDCuYJKkAxvlCHwTsKeq7qyq/wM+Dpw+nliSpEFG+VLjo4D/6pvfC/zq/JWSbAW2drP3J/n6CI+5FEcA31mhxxqFOcenhYxgznFrImcuGCnnMxcanPi30lfVNmDbpB9nviQzVTW90o+7VOYcnxYygjnH7Ymcc5RTKHcDT++bP7obkyStgFEK/D+A45Ick+RgYDOwYzyxJEmDLPsUSlU9mOTNwOeANcDFVXXL2JKNbsVP2yyTOcenhYxgznF7wuZMVY37PiVJK8B3YkpSoyxwSWpUswWe5PAkO5Pc3v1ct8A6L0yyu+/2v0nO6JZdkuSbfcs2rlbObr2H+rLs6Bs/Jsl13ccVXNa9YLwqOZNsTPKlJLckuTHJb/ctm+j+HPSxDUkO6fbPnm5/behbdm43/vUkLxtnrmXkfFuSW7v9d3WSZ/YtW/A5sEo5X5dkti/PH/Qt29I9T25PsmWVc17Yl/EbSe7rW7Yi+zPJxUn2J7l5keVJ8rfdf8ONSZ7bt2y0fVlVTd6AvwTO6abPAS4YsP7hwL3AT3XzlwBnPlZyAvcvMn45sLmb/iDwxtXKCfwCcFw3/fPAPuCwSe9Pei+S3wEcCxwM3ACcMG+dPwQ+2E1vBi7rpk/o1j8EOKa7nzWrmPOFfc/BN87lPNBzYJVyvg74uwW2PRy4s/u5rptet1o5563/R/Qupljp/fk84LnAzYssPw34LBDgJOC6ce3LZo/A6b1tf3s3vR04Y8D6ZwKfrar/mWiqR1tqzh9LEuAU4IrlbL9EA3NW1Teq6vZu+r+B/cDUhPL0G+ZjG/rzXwG8qNt/pwMfr6oHquqbwJ7u/lYlZ1Xt6nsOfpne+ydW2igfg/EyYGdV3VtV3wN2Aqc+RnKeBVw6oSyLqqov0Ds4XMzpwD9Uz5eBw5KsZwz7suUCP7Kq9nXT3waOHLD+Zh79y31v9yfNhUkOGXvCnmFzHppkJsmX507zAE8D7quqB7v5vfQ+wmA1cwKQZBO9o6I7+oYntT8X+tiG+fvhx+t0++v79PbfMNuuZM5+Z9M7Mpuz0HNgEobN+Vvd7/OKJHNv2ntM7s/uVNQxwDV9wyu1PwdZ7L9j5H058bfSjyLJ54GfW2DRef0zVVVJFr0esvvX7pfpXbM+51x6RXUwvesz3wG8ZxVzPrOq7k5yLHBNkpvoldDYjHl//iOwpaoe7obHtj+fCJK8BpgGnt83/KjnQFXdsfA9TNy/ApdW1QNJXk/vr5tTVinLMDYDV1TVQ31jj6X9ORGP6QKvqhcvtizJPUnWV9W+rlD2H+CuXg1cWVU/6rvvuaPNB5J8FPjj1cxZVXd3P+9Mci1wIvAJen9ure2OKkf6uIJx5EzyM8CngfO6Pwfn7nts+3MBw3xsw9w6e5OsBX4W+O6Q265kTpK8mN4/ms+vqgfmxhd5DkyicAbmrKrv9s1+hN5rJHPbvmDetteOPeEjjzXs724z8Kb+gRXcn4Ms9t8x8r5s+RTKDmDuVdstwFUHWPdR58a6kpo7z3wGsOAryGMwMGeSdXOnHJIcAZwM3Fq9Vzp20Tt/v+j2K5jzYOBKeufzrpi3bJL7c5iPbejPfyZwTbf/dgCb07tK5RjgOOArY8y2pJxJTgQ+BLyyqvb3jS/4HFjFnOv7Zl8J3NZNfw54aZd3HfBSfvIv2xXN2WU9nt6LgF/qG1vJ/TnIDuB3u6tRTgK+3x3wjL4vV+JV2knc6J3fvBq4Hfg8cHg3Pg18pG+9DfT+pXvSvO2vAW6iVzT/BDxltXICv95luaH7eXbf9sfSK5w9wL8Ah6xiztcAPwJ29902rsT+pPdK/jfoHUGd1429h14RAhza7Z893f46tm/b87rtvg68fMLPy0E5Pw/c07f/dgx6DqxSzr8Abuny7AKO79v297v9vAf4vdXM2c2/Czh/3nYrtj/pHRzu6/7f2EvvtY03AG/olofel9/c0WWZHte+9K30ktSolk+hSNITmgUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGvX/7wst7srhZMsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 27==== Step 2 Train Loss 0.744556725025177 ======  0.26666666666666666\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.9794,  1.1208,  0.3896,  ...,  0.2332, -0.3649, -0.4267],\n",
            "        [ 0.7323, -0.4314,  0.0396,  ...,  0.1146,  0.7083, -0.0817],\n",
            "        [-1.1499,  1.1676,  0.5367,  ..., -0.1130, -0.2179, -0.4193],\n",
            "        ...,\n",
            "        [-0.2035,  1.0347,  0.1565,  ..., -0.0972,  0.1179, -0.6036],\n",
            "        [ 0.6043,  0.2412, -0.4276,  ...,  0.1543, -0.6282, -0.2268],\n",
            "        [ 0.8068, -0.1543, -0.3726,  ...,  0.1219, -0.2660, -0.0454]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.7333,  0.8296,  0.9836,  0.8594, -0.8616,  0.9915,  0.8367, -0.2831,\n",
            "         0.9909,  0.8776,  0.9574,  0.9903, -0.5441, -0.2581, -0.7206,  0.8160,\n",
            "         0.9755,  0.9539,  0.9652,  0.9855,  0.9916,  0.9425,  0.2249,  0.9958,\n",
            "         0.9567,  0.9540, -0.0957,  0.8051, -0.1823,  0.9485,  0.9335, -0.1734,\n",
            "         0.9658, -0.4344,  0.9951,  0.3755,  0.9776,  0.9095,  0.3756,  0.9862,\n",
            "         0.9880,  0.9457,  0.9918, -0.3646,  0.8349, -0.0718,  0.0097,  0.9899,\n",
            "        -0.8239,  0.9636,  0.7735, -0.6593,  0.9885,  0.1152,  0.9089,  0.0166,\n",
            "        -0.1729,  0.8380,  0.5423, -0.0344,  0.7155,  0.6841,  0.8723,  0.9301],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 28==== Step 2 Train Loss 0.7333834171295166 ======  0.3137254901960785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5195, -0.6425,  0.0930,  ..., -0.0053,  0.5773,  0.0143],\n",
            "        [-0.3584,  0.9323,  0.2754,  ...,  0.1028, -0.1456, -0.4018],\n",
            "        [ 0.6314, -0.8390,  0.0364,  ...,  0.2888,  0.6167, -0.0752],\n",
            "        ...,\n",
            "        [ 0.7775,  0.2745, -0.4390,  ...,  0.3652, -0.7770, -0.2891],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930],\n",
            "        [-1.3226,  1.0878,  0.5782,  ..., -0.0493, -0.2830, -0.4893]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.6113,  0.1763,  0.9690,  0.4033,  0.5995,  0.9323,  0.9306,  0.9931,\n",
            "         0.6010,  0.7906,  0.5129,  0.9895,  0.9912,  0.9066,  0.8787,  0.9386,\n",
            "         0.8995,  0.9711, -0.5565,  0.9540,  0.8374,  0.8665,  0.8653,  0.8093,\n",
            "         0.9166,  0.8809,  0.8657,  0.9923,  0.9807,  0.9621,  0.6714,  0.7296,\n",
            "        -0.0726, -0.3594,  0.9825,  0.0854, -0.2192,  0.9856,  0.7475,  0.1700,\n",
            "         0.8686,  0.2578,  0.3166,  0.7962,  0.9812,  0.9872, -0.2026,  0.9876,\n",
            "         0.9539,  0.8215,  0.9809, -0.6739,  0.8195,  0.9809,  0.6812,  0.6169,\n",
            "        -0.6542, -0.3808,  0.8676,  0.8876,  0.9657,  0.3161,  0.8727,  0.9913],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 29==== Step 2 Train Loss 0.7592346668243408 ======  0.24000000000000002\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.2334,  1.0349,  0.1818,  ...,  0.0226,  0.3519, -0.5501],\n",
            "        [ 0.1102,  1.0545,  0.1324,  ..., -0.0222, -0.1060, -0.4995],\n",
            "        [-0.8209,  1.2372,  0.3226,  ..., -0.0980, -0.2596, -0.6085],\n",
            "        ...,\n",
            "        [ 0.3910,  0.3610, -0.2484,  ...,  0.2830, -0.5612, -0.3109],\n",
            "        [ 0.6907, -0.7153,  0.0214,  ...,  0.2138,  0.5468, -0.1133],\n",
            "        [ 0.5569,  0.2474, -0.4614,  ...,  0.1131, -0.8210, -0.1078]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.3670,  0.5297, -0.2435,  0.8148, -0.1489,  0.6432,  0.9189, -0.3440,\n",
            "        -0.5687, -0.1441, -0.1415,  0.9812,  0.8215,  0.9649,  0.5487,  0.9783,\n",
            "        -0.1264,  0.5792, -0.4552, -0.1871,  0.8933, -0.3894,  0.9537,  0.9506,\n",
            "         0.6592,  0.5784, -0.3906,  0.0651,  0.6712,  0.9103,  0.9859,  0.9803,\n",
            "         0.5950, -0.6402,  0.6968,  0.2546,  0.0254,  0.9280, -0.1212, -0.7019,\n",
            "         0.9692,  0.9716,  0.9391,  0.1652,  0.9188,  0.9711, -0.4219,  0.7990,\n",
            "        -0.1523,  0.9930,  0.1254,  0.9962,  0.8003,  0.9824,  0.9577,  0.2529,\n",
            "        -0.3881,  0.9727,  0.9424, -0.8509, -0.2898,  0.2833,  0.9829,  0.9324],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 30==== Step 2 Train Loss 0.6679248213768005 ======  0.40816326530612246\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 4.2797e-01,  1.4269e-02,  5.6385e-02,  ..., -1.0010e-01,\n",
            "          5.6009e-01, -4.0528e-01],\n",
            "        [-3.3404e-01,  7.7429e-01,  2.7858e-01,  ..., -1.4572e-01,\n",
            "          2.0112e-01, -6.8407e-01],\n",
            "        [ 5.5828e-01, -2.9561e-01, -5.2195e-02,  ...,  2.3638e-01,\n",
            "          2.2236e-01, -2.4535e-01],\n",
            "        ...,\n",
            "        [ 5.3074e-01,  5.9583e-01, -3.4955e-01,  ...,  1.9063e-01,\n",
            "         -6.9459e-01, -2.3508e-01],\n",
            "        [-1.3078e+00,  9.3616e-01,  5.6553e-01,  ..., -9.5568e-04,\n",
            "         -3.3426e-01, -3.9002e-01],\n",
            "        [-1.4824e+00,  1.0727e+00,  6.0131e-01,  ...,  2.5642e-02,\n",
            "         -6.8301e-01, -4.1993e-01]], device='cuda:0')\n",
            "tensor([ 0.9541,  0.7865,  0.7263,  0.0446,  0.0137,  0.7820,  0.4342,  0.9780,\n",
            "         0.1823,  0.8649, -0.2297, -0.8077,  0.2294, -0.6415, -0.4475,  0.9098,\n",
            "         0.9772,  0.2387,  0.9844,  0.9869,  0.9490,  0.9767,  0.0599, -0.2176,\n",
            "        -0.0502,  0.9853,  0.9863,  0.9888,  0.9667,  0.8704,  0.9861,  0.9741,\n",
            "        -0.3699,  0.1730,  0.4649,  0.9577,  0.7058,  0.9015, -0.6181,  0.9404,\n",
            "         0.9624,  0.5245, -0.7537,  0.9418, -0.4958, -0.4597,  0.9469,  0.9940,\n",
            "         0.9832,  0.9353,  0.9721,  0.6632,  0.9917, -0.8171,  0.9734, -0.6731,\n",
            "         0.9889,  0.8900,  0.7545, -0.5674,  0.9156,  0.9577,  0.9894,  0.9791],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 31==== Step 2 Train Loss 0.7724631428718567 ======  0.3076923076923077\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1857e+00,  1.2949e+00,  6.3564e-01,  ...,  1.3141e-02,\n",
            "         -3.8132e-01, -3.2770e-01],\n",
            "        [-1.1925e+00,  1.0694e+00,  5.8523e-01,  ...,  1.0899e-02,\n",
            "         -2.7463e-01, -4.1628e-01],\n",
            "        [ 2.5864e-01,  9.1326e-01,  6.2614e-02,  ..., -3.0893e-02,\n",
            "         -5.8252e-02, -3.1970e-01],\n",
            "        ...,\n",
            "        [-1.3731e+00,  1.0866e+00,  5.0358e-01,  ...,  1.0678e-02,\n",
            "         -6.2422e-01, -3.1449e-01],\n",
            "        [-1.3078e+00,  9.3616e-01,  5.6553e-01,  ..., -9.5568e-04,\n",
            "         -3.3426e-01, -3.9002e-01],\n",
            "        [-1.0388e-02,  4.6811e-01,  6.7217e-02,  ..., -1.9371e-01,\n",
            "          5.0538e-01, -6.5036e-01]], device='cuda:0')\n",
            "tensor([ 0.9618,  0.9916,  0.4897,  0.9530,  0.8253,  0.8700,  0.7780,  0.7329,\n",
            "        -0.5415,  0.2132,  0.1952,  0.9757,  0.7375,  0.9945,  0.9889,  0.8046,\n",
            "         0.9872,  0.9589, -0.1796,  0.9499,  0.2026,  0.7777,  0.8220,  0.9106,\n",
            "         0.9887,  0.9757,  0.9894,  0.7944, -0.6151,  0.6937, -0.1298,  0.9823,\n",
            "         0.9872, -0.3126,  0.9406,  0.8877,  0.9404, -0.5198, -0.7476,  0.9482,\n",
            "         0.9966,  0.6773,  0.9731,  0.3537,  0.9861,  0.0721,  0.9078,  0.8560,\n",
            "         0.7568,  0.7642, -0.5035,  0.9902,  0.9798,  0.9685,  0.9688,  0.9898,\n",
            "         0.9934, -0.4993, -0.5049,  0.9928,  0.9936, -0.1478,  0.9837,  0.4010],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 32==== Step 2 Train Loss 0.7552103996276855 ======  0.3333333333333333\n",
            "torch.Size([64, 48])\n",
            "tensor([[-9.1474e-01,  1.3524e+00,  3.8300e-01,  ..., -6.8030e-02,\n",
            "         -1.9902e-01, -5.2560e-01],\n",
            "        [ 4.7274e-01, -1.4379e-01,  1.9012e-02,  ...,  9.4718e-04,\n",
            "          6.3774e-01, -3.3736e-01],\n",
            "        [ 5.9406e-01, -4.1089e-01,  4.6624e-02,  ...,  3.6400e-02,\n",
            "          6.1483e-01, -9.6298e-02],\n",
            "        ...,\n",
            "        [ 3.8262e-01, -1.5463e-01, -3.9036e-01,  ...,  2.5116e-01,\n",
            "         -9.7916e-01, -2.4113e-02],\n",
            "        [ 4.4201e-01, -5.9489e-01,  9.6121e-02,  ...,  9.5700e-02,\n",
            "          6.9554e-01, -7.0354e-02],\n",
            "        [ 4.5483e-01,  4.2875e-01, -2.8329e-01,  ...,  2.8345e-01,\n",
            "         -3.9228e-01, -2.3205e-01]], device='cuda:0')\n",
            "tensor([-0.1830,  0.7997, -0.0505,  0.9622,  0.9852, -0.0889,  0.6732,  0.8814,\n",
            "         0.9334,  0.8035,  0.5458, -0.5123,  0.9890,  0.0405,  0.6596,  0.9964,\n",
            "         0.9808, -0.6564,  0.9284,  0.0559,  0.9806,  0.7981,  0.5213, -0.5320,\n",
            "         0.9102,  0.9137, -0.0716, -0.7482,  0.8580,  0.9823,  0.6579,  0.9704,\n",
            "         0.9907,  0.9102,  0.9102, -0.1579, -0.0168, -0.5256,  0.9700,  0.4911,\n",
            "         0.6841,  0.9871, -0.2357, -0.5538,  0.6053,  0.7737,  0.9954,  0.9022,\n",
            "        -0.5565, -0.4695,  0.3251,  0.9883, -0.0537,  0.0114,  0.7045, -0.5532,\n",
            "        -0.7393,  0.3656, -0.2693, -0.7693,  0.9503, -0.0248,  0.9537,  0.9274],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 33==== Step 2 Train Loss 0.7029683589935303 ======  0.4528301886792453\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.2157,  0.1355, -0.0691,  ...,  0.2422, -0.2970, -0.3497],\n",
            "        [ 0.6393, -0.6124,  0.0274,  ...,  0.1480,  0.6559, -0.0971],\n",
            "        [ 0.5041,  0.2477, -0.3837,  ...,  0.1762, -0.4469, -0.2920],\n",
            "        ...,\n",
            "        [-0.7776,  1.3167,  0.3063,  ..., -0.0636, -0.4296, -0.3785],\n",
            "        [ 0.4742,  0.1476, -0.5290,  ...,  0.2490, -0.9195, -0.1385],\n",
            "        [-1.1658,  1.1070,  0.5041,  ...,  0.0690, -0.5541, -0.4117]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1491,  0.8793,  0.9178,  0.9868,  0.9826,  0.4568,  0.9940,  0.8083,\n",
            "        -0.8905,  0.8703,  0.6349,  0.9704,  0.8223,  0.6968,  0.5494,  0.7596,\n",
            "         0.4598,  0.9820,  0.9907,  0.9915,  0.8174, -0.2383, -0.5186,  0.2107,\n",
            "         0.8826,  0.9738,  0.7354,  0.2159, -0.6798, -0.7158, -0.0329,  0.9667,\n",
            "         0.9589, -0.4197,  0.9668,  0.9543,  0.9681,  0.7165,  0.9926,  0.8615,\n",
            "         0.9024, -0.1103,  0.6531,  0.9715,  0.4355,  0.9609,  0.9767,  0.9725,\n",
            "        -0.6427, -0.5427,  0.9872,  0.8756,  0.9331, -0.7134, -0.1962,  0.8467,\n",
            "        -0.1764,  0.9845, -0.5461,  0.5413,  0.9916, -0.1714,  0.3639, -0.2191],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 34==== Step 2 Train Loss 0.7360268831253052 ======  0.3846153846153846\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1646,  1.1628,  0.4900,  ..., -0.0530, -0.1701, -0.4644],\n",
            "        [ 0.5098,  0.4887, -0.4310,  ...,  0.1977, -0.8386, -0.2799],\n",
            "        [-1.3364,  1.1315,  0.5472,  ...,  0.0434, -0.6048, -0.3285],\n",
            "        ...,\n",
            "        [-0.8306,  1.0393,  0.3829,  ..., -0.0665,  0.0087, -0.5656],\n",
            "        [ 0.5310,  0.0678, -0.4880,  ...,  0.3716, -0.5317, -0.3916],\n",
            "        [-1.3364,  1.1315,  0.5472,  ...,  0.0434, -0.6048, -0.3285]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8791, -0.0424,  0.5250,  0.1389,  0.9761,  0.4504,  0.5214,  0.9833,\n",
            "        -0.8132,  0.6214,  0.6085,  0.8832,  0.0809,  0.7030,  0.8827, -0.5570,\n",
            "         0.1469, -0.0313,  0.9412,  0.9919,  0.7300, -0.1081, -0.8704,  0.9001,\n",
            "         0.7628,  0.9932,  0.9957, -0.3267,  0.8315, -0.2383,  0.9328, -0.5800,\n",
            "         0.9882, -0.0541,  0.9904,  0.4553,  0.8924,  0.9541, -0.1756,  0.9744,\n",
            "        -0.3725,  0.9844, -0.0497,  0.9826, -0.3291, -0.4070,  0.9626,  0.9003,\n",
            "         0.8763,  0.8763,  0.8035, -0.2564,  0.9668,  0.9889,  0.9725,  0.9956,\n",
            "        -0.2748,  0.9810,  0.3220, -0.4842,  0.7991,  0.8346,  0.7837,  0.9910],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 35==== Step 2 Train Loss 0.7246994376182556 ======  0.30188679245283023\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4798,  0.2126, -0.4592,  ...,  0.2435, -0.5062, -0.2580],\n",
            "        [-1.3104,  1.0687,  0.4741,  ..., -0.0561, -0.6286, -0.2293],\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        ...,\n",
            "        [ 0.6035, -0.2898,  0.1126,  ...,  0.1062,  0.5783, -0.2909],\n",
            "        [-0.0825,  1.1024,  0.0033,  ..., -0.0323, -0.4410, -0.3963],\n",
            "        [-0.4319,  1.2429,  0.1302,  ..., -0.0160, -0.3629, -0.5111]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.2424,  0.9945,  0.1297,  0.0112, -0.0421, -0.3342,  0.5511, -0.6132,\n",
            "         0.7545,  0.9843,  0.9898, -0.0826, -0.1393,  0.7839,  0.7379,  0.9502,\n",
            "         0.3653,  0.9850,  0.8187,  0.9825,  0.6563, -0.7566,  0.4255,  0.9615,\n",
            "        -0.7136,  0.5227, -0.1798, -0.0036,  0.9354,  0.7628,  0.9748,  0.9104,\n",
            "         0.9661,  0.0799,  0.2923,  0.3619,  0.8653,  0.3090, -0.5126,  0.9492,\n",
            "         0.9255,  0.9711,  0.8919,  0.2452,  0.5586,  0.9913,  0.2217,  0.9740,\n",
            "         0.9032,  0.9780, -0.0834,  0.9493,  0.9692, -0.5272,  0.7032,  0.9821,\n",
            "         0.8440,  0.9570,  0.6028,  0.9055, -0.0401, -0.2326,  0.7286,  0.8332],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 36==== Step 2 Train Loss 0.6920762658119202 ======  0.42857142857142855\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8242,  0.8144,  0.2183,  ..., -0.0336, -0.1667, -0.6377],\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        [ 0.9299,  0.0810, -0.2730,  ...,  0.2157, -0.0020, -0.1180],\n",
            "        ...,\n",
            "        [-1.5234,  1.0478,  0.6344,  ...,  0.0575, -0.6737, -0.2081],\n",
            "        [-0.6043,  0.9728,  0.1522,  ...,  0.0780, -0.3033, -0.5098],\n",
            "        [-0.3575,  0.9949,  0.0407,  ..., -0.1381, -0.5036, -0.3321]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9541,  0.9887,  0.8368,  0.2572,  0.1930,  0.9276, -0.1669, -0.2184,\n",
            "         0.9872, -0.6506,  0.2246,  0.8275,  0.8400,  0.9850,  0.2827,  0.7540,\n",
            "         0.9153, -0.4101,  0.6467,  0.0439, -0.1367, -0.6508,  0.9764,  0.8774,\n",
            "        -0.2397,  0.9869,  0.9881,  0.9561, -0.7841, -0.3869,  0.9889,  0.9836,\n",
            "         0.5055,  0.9605,  0.8814,  0.8646,  0.7352,  0.9510,  0.9899,  0.7844,\n",
            "        -0.6448,  0.8714,  0.8207, -0.1001, -0.6951,  0.9494,  0.8623, -0.7830,\n",
            "        -0.6942, -0.1461, -0.1553,  0.9958, -0.1285,  0.9738,  0.1018,  0.9781,\n",
            "         0.9060,  0.6924,  0.4032,  0.9830,  0.9932,  0.9929,  0.8675, -0.8826],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 37==== Step 2 Train Loss 0.7023418545722961 ======  0.4912280701754386\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0906,  1.0925,  0.4708,  ...,  0.1214, -0.4169, -0.3713],\n",
            "        [-0.9841,  1.1294,  0.2784,  ..., -0.1160, -0.2417, -0.6029],\n",
            "        [-1.3567,  1.1212,  0.5071,  ...,  0.0908, -0.4530, -0.4381],\n",
            "        ...,\n",
            "        [ 0.1766,  0.9311, -0.0326,  ...,  0.1531, -0.5511, -0.3917],\n",
            "        [-0.6706,  1.1306,  0.1335,  ..., -0.0167, -0.5591, -0.3684],\n",
            "        [-0.8265,  1.1467,  0.2594,  ..., -0.1064, -0.2028, -0.5914]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9695, -0.6142,  0.7803,  0.9650, -0.7798,  0.9897,  0.9723,  0.9793,\n",
            "         0.9619,  0.3894,  0.9830,  0.8969, -0.1727,  0.1134, -0.7485,  0.9258,\n",
            "         0.9891, -0.1648,  0.9700,  0.9355, -0.1577,  0.7631,  0.9951,  0.8198,\n",
            "         0.8619,  0.9700,  0.9523,  0.9503,  0.9371,  0.9754, -0.1135,  0.8548,\n",
            "         0.9766,  0.9717,  0.8124,  0.9847,  0.9607,  0.3587,  0.9891,  0.3084,\n",
            "        -0.6409,  0.9858,  0.7522,  0.6550,  0.8694,  0.9852,  0.4189, -0.7571,\n",
            "         0.0543,  0.9794,  0.9868,  0.5752,  0.9959,  0.8900, -0.6523,  0.9898,\n",
            "         0.9853,  0.7647,  0.9910, -0.2980,  0.9512,  0.8516, -0.5433, -0.6490],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 38==== Step 2 Train Loss 0.7353240251541138 ======  0.3137254901960784\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4305,  1.1246,  0.6082,  ...,  0.0213, -0.5281, -0.2977],\n",
            "        [ 0.5694,  0.2653, -0.1678,  ..., -0.0353,  0.2107, -0.5137],\n",
            "        [ 0.6239,  0.4491, -0.3787,  ...,  0.1498, -0.6670, -0.2056],\n",
            "        ...,\n",
            "        [ 0.4909, -0.3995, -0.3639,  ...,  0.4401, -0.7187,  0.0113],\n",
            "        [ 0.7897, -0.4585, -0.3521,  ...,  0.1796, -0.3563,  0.1268],\n",
            "        [-0.7278,  1.1143,  0.3110,  ...,  0.0565, -0.4296, -0.5378]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.4230,  0.5092, -0.1357,  0.9794,  0.9931,  0.5593,  0.1736,  0.9735,\n",
            "        -0.0126,  0.7198, -0.5210,  0.7470,  0.9606,  0.9925,  0.9964,  0.6548,\n",
            "         0.8058,  0.9578,  0.8746,  0.8915, -0.4806,  0.8926,  0.9809,  0.9773,\n",
            "         0.9934, -0.1193,  0.9465,  0.9823, -0.2986, -0.1757, -0.3128, -0.7950,\n",
            "        -0.2098,  0.7646,  0.9371,  0.9584,  0.9668, -0.6383,  0.9813, -0.3878,\n",
            "        -0.1752,  0.6929, -0.6219, -0.8240,  0.9891,  0.0652,  0.3432, -0.1940,\n",
            "         0.9939,  0.7480,  0.5722,  0.4105,  0.8982, -0.3631,  0.9728,  0.1016,\n",
            "         0.2661,  0.9926,  0.9911,  0.8850,  0.7121,  0.6804,  0.4662,  0.3297],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 39==== Step 2 Train Loss 0.7355985045433044 ======  0.2553191489361702\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3731,  1.0866,  0.5036,  ...,  0.0107, -0.6242, -0.3145],\n",
            "        [ 0.4568,  0.5208, -0.3628,  ...,  0.0662, -0.7030, -0.2066],\n",
            "        [-1.2644,  1.0952,  0.5467,  ...,  0.0036, -0.1489, -0.5242],\n",
            "        ...,\n",
            "        [ 0.3782,  0.1249, -0.2557,  ...,  0.3521, -0.8663, -0.2478],\n",
            "        [-1.2558,  1.1673,  0.3576,  ..., -0.0415, -0.4248, -0.5587],\n",
            "        [-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.7819, -0.1413, -0.7339,  0.9899,  0.6122,  0.9060, -0.8250,  0.2278,\n",
            "         0.9703,  0.9900,  0.9555,  0.9338,  0.1505, -0.8184,  0.6681,  0.8687,\n",
            "         0.6562, -0.7379, -0.6564, -0.7793,  0.6186,  0.8754,  0.9335, -0.5068,\n",
            "         0.8983,  0.9784,  0.9849,  0.9602,  0.1893,  0.0797,  0.9874,  0.2081,\n",
            "         0.9441,  0.4573,  0.0846,  0.9349,  0.9524,  0.9192, -0.3112,  0.8890,\n",
            "         0.9782,  0.9631, -0.4123, -0.0662,  0.2174,  0.1688, -0.3879,  0.0775,\n",
            "         0.9600,  0.9797,  0.9811,  0.9806,  0.5070,  0.5324,  0.9681,  0.3067,\n",
            "        -0.3315,  0.8405,  0.9832,  0.5735, -0.6210,  0.7625,  0.4328,  0.0568],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 40==== Step 2 Train Loss 0.6928179264068604 ======  0.2978723404255319\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.9727,  1.0120,  0.2522,  ..., -0.0505, -0.2880, -0.5602],\n",
            "        [ 0.4232, -0.8927,  0.0313,  ...,  0.1150,  0.5829,  0.0558],\n",
            "        [-0.6651,  1.1630,  0.2637,  ...,  0.1229, -0.3259, -0.4469],\n",
            "        ...,\n",
            "        [ 0.3782,  0.1249, -0.2557,  ...,  0.3521, -0.8663, -0.2478],\n",
            "        [-1.3162,  1.0528,  0.7049,  ..., -0.0031, -0.3018, -0.3868],\n",
            "        [-0.0272,  0.4727,  0.0856,  ...,  0.0871,  0.0867, -0.5415]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9634,  0.0824,  0.8773,  0.7881,  0.8821,  0.3868,  0.9801,  0.9823,\n",
            "         0.8584, -0.2252,  0.8505,  0.0988,  0.9894,  0.9875,  0.6588,  0.1915,\n",
            "        -0.7053, -0.1040,  0.8327, -0.2080,  0.9617, -0.3658,  0.7857,  0.9850,\n",
            "        -0.5969,  0.6672,  0.9193,  0.8988,  0.9871, -0.1501, -0.6917,  0.2864,\n",
            "         0.4577,  0.9241,  0.9869,  0.3666,  0.9948,  0.6054,  0.9888,  0.9479,\n",
            "         0.9222,  0.9880, -0.6099,  0.9660,  0.9896, -0.6185,  0.9830,  0.6536,\n",
            "         0.9025,  0.5354, -0.6840,  0.4493,  0.9228,  0.8385,  0.9496,  0.6490,\n",
            "         0.9865,  0.2455,  0.1766,  0.9930,  0.9713, -0.3181,  0.9912,  0.7907],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 41==== Step 2 Train Loss 0.7065871953964233 ======  0.36000000000000004\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1857,  1.2949,  0.6356,  ...,  0.0131, -0.3813, -0.3277],\n",
            "        [ 0.6835, -0.4718, -0.0862,  ...,  0.0549,  0.5785, -0.1074],\n",
            "        [-1.3104,  1.0613,  0.6367,  ..., -0.0392, -0.2553, -0.4147],\n",
            "        ...,\n",
            "        [-1.0739,  1.0833,  0.5214,  ..., -0.0788, -0.2597, -0.4735],\n",
            "        [-1.2911,  1.0738,  0.5832,  ..., -0.1055, -0.3588, -0.3891],\n",
            "        [ 0.5821, -0.5007,  0.0814,  ...,  0.1920,  0.6956, -0.2509]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9856,  0.9002,  0.9889, -0.5111,  0.9339,  0.9887,  0.2582,  0.9338,\n",
            "         0.9758,  0.9819,  0.3522, -0.3852,  0.7641, -0.6746, -0.6576, -0.2939,\n",
            "         0.8931,  0.7492,  0.6428, -0.4011,  0.6279,  0.9916,  0.3661, -0.2599,\n",
            "         0.9701,  0.8124,  0.9354,  0.9905,  0.6329,  0.9759,  0.2081,  0.8857,\n",
            "         0.7491,  0.6158,  0.9793, -0.6215,  0.2766, -0.4386,  0.9826,  0.6176,\n",
            "        -0.0149,  0.9912,  0.9803, -0.0611,  0.4208,  0.9948,  0.8872,  0.9803,\n",
            "         0.9007,  0.3784,  0.3415,  0.9667,  0.8708,  0.9889,  0.9509, -0.0686,\n",
            "         0.9811,  0.6231,  0.9911,  0.3675,  0.9210,  0.9499,  0.9913,  0.8202],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 42==== Step 2 Train Loss 0.8025758266448975 ======  0.14814814814814814\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.6617, -0.7728, -0.0079,  ..., -0.0395,  0.4374,  0.1676],\n",
            "        [-1.2804,  1.0736,  0.4059,  ...,  0.0111, -0.6980, -0.3929],\n",
            "        [ 0.5887, -0.7102,  0.0697,  ...,  0.1691,  0.7192, -0.1345],\n",
            "        ...,\n",
            "        [-1.2749,  1.1009,  0.5427,  ..., -0.0261, -0.1424, -0.4852],\n",
            "        [-1.3712,  1.0347,  0.5497,  ..., -0.0186, -0.2164, -0.4524],\n",
            "        [ 0.2621,  0.8358, -0.1861,  ...,  0.0366, -0.5389, -0.2166]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9469,  0.9919,  0.9783, -0.2319,  0.9823,  0.9788,  0.4233, -0.7804,\n",
            "         0.0707,  0.9326, -0.6896, -0.3672, -0.1535, -0.2171,  0.7318,  0.9623,\n",
            "         0.1389, -0.6999,  0.9953,  0.9937,  0.1701,  0.7526,  0.1655,  0.8789,\n",
            "         0.9332,  0.9145,  0.9945,  0.9885,  0.9461,  0.9812,  0.5603,  0.5959,\n",
            "         0.4911, -0.4904, -0.2590,  0.9917,  0.3297,  0.9700,  0.6529,  0.9936,\n",
            "         0.7095,  0.9948, -0.7166,  0.9925,  0.9298, -0.5919, -0.2488,  0.2409,\n",
            "         0.6976,  1.0000,  0.8395,  0.9504,  0.9526,  0.9957,  0.9772,  0.9900,\n",
            "         0.9601, -0.6462, -0.3106,  0.9745,  0.3170, -0.7983,  0.9907,  0.4192],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 43==== Step 2 Train Loss 0.7533140778541565 ======  0.2127659574468085\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8860,  1.1328,  0.2878,  ..., -0.0463, -0.2738, -0.6006],\n",
            "        [ 0.5209, -0.9021,  0.0292,  ...,  0.1750,  0.6252,  0.0852],\n",
            "        [-0.9531,  1.3404,  0.3884,  ..., -0.0310, -0.1275, -0.5755],\n",
            "        ...,\n",
            "        [-0.7979,  1.0310,  0.2928,  ..., -0.1073, -0.2584, -0.4477],\n",
            "        [-0.7817,  1.0717,  0.3966,  ..., -0.0083, -0.0664, -0.6679],\n",
            "        [ 0.3818,  0.2530, -0.2665,  ...,  0.0493, -0.0393, -0.5041]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9919,  0.9779,  0.8430,  0.9877,  0.9865,  0.6176,  0.0299,  0.9794,\n",
            "         0.9758,  0.5933, -0.6132,  0.8338,  0.9949, -0.1443, -0.1553,  0.3462,\n",
            "         0.2858, -0.0691,  0.9676,  0.9360,  0.6049,  0.8629, -0.1521,  0.8956,\n",
            "         0.9824,  0.6934,  0.9741,  0.1139,  0.8820,  0.6955,  0.6069, -0.0940,\n",
            "        -0.3436,  0.9954, -0.8644,  0.9145, -0.0905, -0.8792, -0.5034, -0.6203,\n",
            "        -0.3999,  0.2881,  0.4443,  0.6140,  0.9706,  0.2737, -0.7883,  0.8437,\n",
            "        -0.6512,  0.0064,  0.7758, -0.0558,  0.2531, -0.6157, -0.0526,  0.9710,\n",
            "         0.9718,  0.0249,  0.9119,  0.7592, -0.4590,  0.3957,  0.1146,  0.6402],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 44==== Step 2 Train Loss 0.690985918045044 ======  0.39999999999999997\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.8575, -0.2708, -0.3633,  ...,  0.1699, -0.3490,  0.0358],\n",
            "        [ 0.7364, -0.2269, -0.1143,  ...,  0.0178,  0.5294, -0.1515],\n",
            "        [-1.2618,  0.9934,  0.5118,  ...,  0.0750, -0.2198, -0.4760],\n",
            "        ...,\n",
            "        [-1.2995,  1.0537,  0.6157,  ..., -0.0884, -0.3123, -0.4504],\n",
            "        [-1.1386,  1.1936,  0.4138,  ...,  0.0390, -0.4786, -0.3436],\n",
            "        [-1.3170,  1.0739,  0.5085,  ...,  0.0322, -0.6405, -0.3679]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9492,  0.8122,  0.9870,  0.9810,  0.9423,  0.8223, -0.6237,  0.6980,\n",
            "         0.8766,  0.6654,  0.7733,  0.9714,  0.8856,  0.8764,  0.9605, -0.3535,\n",
            "         0.9897,  0.9910,  0.8579,  0.9612, -0.7558,  0.9323,  0.5782,  0.9853,\n",
            "         0.8709, -0.6387,  0.2986,  0.2280,  0.9603,  0.9883, -0.1171,  0.9899,\n",
            "         0.9839,  0.9880,  0.8474,  0.4812,  0.6325, -0.3879,  0.9606,  0.0842,\n",
            "         0.9527,  0.9918,  0.7372, -0.0954,  0.9301,  0.9832, -0.1175, -0.6470,\n",
            "         0.3946, -0.3526,  0.7913, -0.1183,  0.9829,  0.9606, -0.7480, -0.5249,\n",
            "        -0.0427,  0.0459,  0.9247,  0.9544,  0.9948,  0.9851,  0.9599,  0.9929],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 45==== Step 2 Train Loss 0.7025796175003052 ======  0.4067796610169491\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3582,  1.1746,  0.4468,  ..., -0.0675, -0.3501, -0.4429],\n",
            "        [-1.4038,  1.0228,  0.5959,  ..., -0.0419, -0.2282, -0.3501],\n",
            "        [-0.6857,  1.0146,  0.2045,  ..., -0.1150,  0.0984, -0.6723],\n",
            "        ...,\n",
            "        [-1.4348,  1.0779,  0.6346,  ...,  0.0551, -0.4850, -0.3412],\n",
            "        [-0.1154,  0.7405, -0.0016,  ..., -0.0758,  0.0360, -0.5243],\n",
            "        [-1.0398,  0.8727,  0.3665,  ..., -0.0694, -0.0544, -0.5357]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9865, -0.5388,  0.6235,  0.9548,  0.8852,  0.0568,  0.9694,  0.7483,\n",
            "         0.7876, -0.4099,  0.9716,  0.1302,  0.5554,  0.2076,  0.9040, -0.0687,\n",
            "         0.9684,  0.8227,  0.9755,  0.9841,  0.6704, -0.8087,  0.9736,  0.6086,\n",
            "         0.9272,  0.9888,  0.8402,  0.9746,  0.9683,  0.7883,  0.9925,  0.9929,\n",
            "         0.9160,  0.2025,  0.9561,  0.7697,  0.8438,  0.5174,  0.9913,  0.9750,\n",
            "         0.9950, -0.2068,  0.4969,  0.9883,  0.0193,  0.0944,  0.6432, -0.6039,\n",
            "        -0.1800, -0.1934,  0.8183,  0.5775,  0.9853,  0.3055, -0.0913,  0.3568,\n",
            "         0.0853,  0.9565,  0.2209,  0.9430,  0.9935, -0.0450,  0.7179,  0.8555],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 46==== Step 2 Train Loss 0.6997793912887573 ======  0.3913043478260869\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2366,  1.0443,  0.3502,  ...,  0.0460, -0.6261, -0.3316],\n",
            "        [ 0.6826, -0.3700, -0.0073,  ...,  0.0644,  0.4212, -0.0949],\n",
            "        [-0.5296,  1.1117,  0.1358,  ...,  0.0310, -0.7473, -0.3664],\n",
            "        ...,\n",
            "        [ 0.6122, -0.0304, -0.4642,  ...,  0.3203, -0.7254, -0.3153],\n",
            "        [ 0.8113, -0.2609, -0.0339,  ..., -0.1562,  0.6521, -0.1348],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930]],\n",
            "       device='cuda:0')\n",
            "tensor([-1.8224e-01,  9.5563e-01, -4.7524e-01,  3.9715e-01,  8.5991e-01,\n",
            "         4.8831e-01,  1.1309e-01, -8.7584e-01,  4.7980e-01,  9.4217e-01,\n",
            "        -3.0697e-01,  9.1854e-01,  4.9691e-01,  8.5350e-01,  9.1454e-01,\n",
            "        -6.2785e-01,  9.6079e-01,  8.3172e-01,  9.6550e-01, -1.8482e-01,\n",
            "         2.6607e-01,  9.6849e-01, -6.1173e-01,  7.6932e-01, -2.9800e-01,\n",
            "         3.8818e-01,  9.7743e-01,  9.8378e-01, -1.2529e-01, -2.7278e-01,\n",
            "         9.8887e-01,  9.8288e-01,  4.8600e-01,  8.2009e-01,  6.4593e-01,\n",
            "         9.5278e-01,  9.6678e-01,  9.8694e-01, -3.9859e-01, -4.6862e-01,\n",
            "        -2.1584e-01, -2.9679e-01,  9.9398e-01,  4.1382e-01, -1.3573e-01,\n",
            "        -1.4773e-01,  9.8799e-01,  9.4187e-01,  9.5201e-01,  8.0037e-01,\n",
            "         6.4969e-01, -7.1694e-01,  9.5479e-01,  6.8651e-01,  8.2852e-01,\n",
            "         9.7537e-01,  7.0044e-04, -7.5661e-01,  9.8170e-01,  5.8213e-01,\n",
            "         4.7116e-01,  9.4670e-01, -3.9393e-02,  9.9034e-01], device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 47==== Step 2 Train Loss 0.7203590869903564 ======  0.3846153846153846\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8989,  1.3933,  0.4126,  ...,  0.0661, -0.3699, -0.3615],\n",
            "        [-1.0183,  1.3367,  0.4970,  ...,  0.0245, -0.0845, -0.4584],\n",
            "        [ 0.2157,  0.1355, -0.0691,  ...,  0.2422, -0.2970, -0.3497],\n",
            "        ...,\n",
            "        [ 0.5821, -0.5007,  0.0814,  ...,  0.1920,  0.6956, -0.2509],\n",
            "        [ 0.5037,  0.3129, -0.4136,  ...,  0.0964, -0.8207, -0.0311],\n",
            "        [-1.4090,  1.1067,  0.5068,  ...,  0.0768, -0.5922, -0.3176]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6720,  0.9716,  0.1185,  0.5975,  0.9197,  0.9744, -0.3117,  0.9739,\n",
            "        -0.4661,  0.9911,  0.9847, -0.1399,  0.9447,  0.9844, -0.1503,  0.9728,\n",
            "         0.9933,  0.6108,  0.2432,  0.0677,  0.5946, -0.5461,  0.9041,  0.9714,\n",
            "         0.5939,  0.9548,  0.9874,  0.9905,  0.7608,  0.8633,  0.9523, -0.4212,\n",
            "         0.9776,  0.9557,  0.9828,  0.9349,  0.2740,  0.3987,  0.9857,  0.7356,\n",
            "         0.8464,  0.9753,  0.9875,  0.9407,  0.9529,  0.9727,  0.4682, -0.4323,\n",
            "         0.8646,  0.9577,  0.7820, -0.8628,  0.9339,  0.8224,  0.9162,  0.9208,\n",
            "         0.1234,  0.9596,  0.4123, -0.4943,  0.9853,  0.6682,  0.9274,  0.9459],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 48==== Step 2 Train Loss 0.7717744708061218 ======  0.35714285714285715\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2960,  1.0792,  0.5453,  ..., -0.0307, -0.3771, -0.4643],\n",
            "        [-1.2760,  1.1292,  0.6219,  ..., -0.0763, -0.3256, -0.4322],\n",
            "        [-1.2087,  1.2789,  0.3959,  ..., -0.1254, -0.4096, -0.3585],\n",
            "        ...,\n",
            "        [-1.3057,  1.1706,  0.6448,  ..., -0.0530, -0.3760, -0.4322],\n",
            "        [-1.3849,  1.0794,  0.5900,  ...,  0.0777, -0.5834, -0.3529],\n",
            "        [-0.8933,  1.2544,  0.2316,  ..., -0.0379, -0.3523, -0.5660]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9964,  0.9930, -0.6989,  0.9805,  0.7491,  0.8744,  0.9574,  0.5674,\n",
            "        -0.5777, -0.2702,  0.9282,  0.2342,  0.0050, -0.0915,  0.3890,  0.9300,\n",
            "         0.4903,  0.7982,  0.9740,  0.9130,  0.5422,  0.9612,  0.9328,  0.9633,\n",
            "         0.7500,  0.6980,  0.9894,  0.8055,  0.9793,  0.8874, -0.4337,  0.6748,\n",
            "         0.9606,  0.3193, -0.5103,  0.9062,  0.0058,  0.9404, -0.7355,  0.9925,\n",
            "        -0.1728,  0.9690,  0.9806,  0.3945,  0.1564,  0.9895,  0.5243,  0.8696,\n",
            "         0.9934,  0.9735,  0.9898,  0.6524, -0.0329,  0.9431, -0.7318,  0.9857,\n",
            "         0.9427,  0.6500, -0.6739,  0.9584, -0.1026, -0.2040,  0.9863,  0.9243],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 49==== Step 2 Train Loss 0.6910907030105591 ======  0.4489795918367347\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.2586,  0.9133,  0.0626,  ..., -0.0309, -0.0583, -0.3197],\n",
            "        [-0.8676,  0.8973,  0.3587,  ..., -0.0530,  0.1908, -0.6784],\n",
            "        [-1.1309,  1.1055,  0.3674,  ...,  0.0069, -0.2851, -0.5010],\n",
            "        ...,\n",
            "        [ 0.3719, -0.4709, -0.3351,  ...,  0.4447, -0.4224, -0.1388],\n",
            "        [-0.9100,  0.9146,  0.2761,  ...,  0.0049, -0.0912, -0.7227],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2211,  0.7862, -0.0943,  0.9095,  0.9587, -0.4118,  0.8995, -0.4076,\n",
            "         0.9956,  0.9576, -0.7007,  0.5156, -0.6087,  0.9686,  0.6532,  0.9425,\n",
            "         0.8292,  0.9837,  0.9119, -0.6423,  0.4086,  0.9924,  0.9482,  0.7407,\n",
            "         0.9360,  0.6173,  0.9215,  0.4504,  0.9586,  0.9082,  0.7153,  0.9786,\n",
            "         0.1207,  0.9208,  0.8242,  0.9896,  0.9858,  0.9803,  0.9751,  0.7165,\n",
            "        -0.0394,  0.8108, -0.4690,  0.6889,  0.9696,  0.6508,  0.9672,  0.9643,\n",
            "         0.1299,  0.9954,  0.9789,  0.7807,  0.6008,  0.8436,  0.9427, -0.8207,\n",
            "         0.6321,  0.0118,  0.9339, -0.0501, -0.0986,  0.3220,  0.9684,  0.9958],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 50==== Step 2 Train Loss 0.7128228545188904 ======  0.29166666666666663\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2982,  1.1451,  0.5844,  ...,  0.0090, -0.2458, -0.4737],\n",
            "        [-1.4919,  0.9920,  0.5243,  ...,  0.0598, -0.6825, -0.3526],\n",
            "        [ 0.0228,  0.9145, -0.0177,  ..., -0.1188, -0.0558, -0.4685],\n",
            "        ...,\n",
            "        [-1.0593,  1.2020,  0.4404,  ..., -0.0745, -0.2726, -0.5630],\n",
            "        [-1.5528,  0.9868,  0.6148,  ...,  0.0784, -0.6554, -0.2778],\n",
            "        [-1.4107,  1.1037,  0.4881,  ...,  0.0159, -0.7426, -0.2042]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.2008, -0.7020,  0.4901, -0.7891, -0.2700,  0.6677,  0.5295, -0.6909,\n",
            "         0.9931, -0.4999,  0.8969,  0.9170,  0.9778,  0.9342,  0.9617,  0.9502,\n",
            "         0.9597,  0.9680,  0.3153,  0.7675,  0.7751,  0.9911,  0.4257,  0.9811,\n",
            "         0.9896,  0.1938,  0.9811,  0.9707,  0.9919,  0.7396,  0.9047,  0.3773,\n",
            "         0.9913,  0.9329,  0.6160,  0.8580,  0.7946, -0.5998, -0.7318, -0.0394,\n",
            "         0.9605,  0.9662, -0.0597, -0.0915,  0.1845,  0.5324, -0.6149,  0.9529,\n",
            "         0.9648,  0.9193,  0.9805,  0.9616, -0.6824,  0.9719,  0.0758,  0.9742,\n",
            "        -0.5839,  0.7892,  0.9489, -0.1179,  0.9913,  0.8572,  0.9781,  0.9923],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 51==== Step 2 Train Loss 0.7161099314689636 ======  0.4074074074074075\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.5098,  1.2666,  0.2094,  ..., -0.1714, -0.3442, -0.5363],\n",
            "        [-0.9531,  1.3404,  0.3884,  ..., -0.0310, -0.1275, -0.5755],\n",
            "        [-0.2035,  1.0347,  0.1565,  ..., -0.0972,  0.1179, -0.6036],\n",
            "        ...,\n",
            "        [-1.3466,  1.0309,  0.5970,  ..., -0.0847, -0.3619, -0.3244],\n",
            "        [ 0.5130, -0.6971,  0.0717,  ...,  0.1526,  0.6546, -0.0350],\n",
            "        [-1.2879,  1.0512,  0.4925,  ...,  0.0162, -0.6509, -0.2583]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2319,  0.9746,  0.9030, -0.2972, -0.0273, -0.4336,  0.4084,  0.8415,\n",
            "         0.9669, -0.2439,  0.9900,  0.8384, -0.6020,  0.9910,  0.7983,  0.0084,\n",
            "         0.9870,  0.8696, -0.0580, -0.7282,  0.8847,  0.6409,  0.9145,  0.9296,\n",
            "        -0.0594,  0.9830,  0.9883,  0.8916,  0.9912, -0.0056,  0.7466,  0.9503,\n",
            "        -0.2329,  0.9904,  0.1092, -0.5424,  0.9645,  0.8465,  0.1802,  0.9824,\n",
            "         0.3515,  0.9885,  0.9861,  0.9178,  0.9171, -0.1670, -0.1833, -0.1098,\n",
            "         0.2138,  0.3895,  0.9905, -0.4084,  0.9844,  0.9883,  0.0873,  0.5164,\n",
            "        -0.0594,  0.4200,  0.5920,  0.8603, -0.0384,  0.9636, -0.0749, -0.6838],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 52==== Step 2 Train Loss 0.6768016219139099 ======  0.4897959183673469\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2547,  1.1978,  0.5102,  ...,  0.0179, -0.3000, -0.4958],\n",
            "        [ 0.1790,  0.6889, -0.1979,  ..., -0.0559, -0.5116, -0.2769],\n",
            "        [ 0.5685, -0.7785,  0.0699,  ...,  0.0567,  0.6170,  0.0695],\n",
            "        ...,\n",
            "        [-0.6621,  1.2225,  0.1562,  ..., -0.0177, -0.5613, -0.4450],\n",
            "        [-1.4209,  0.9922,  0.4254,  ...,  0.0422, -0.6627, -0.4782],\n",
            "        [-0.9986,  1.2936,  0.4480,  ..., -0.0631, -0.3644, -0.5373]],\n",
            "       device='cuda:0')\n",
            "tensor([ 9.9063e-01,  1.9948e-01,  6.6865e-01,  4.5294e-01,  9.2549e-01,\n",
            "         2.4090e-01, -3.2425e-01,  9.3267e-01,  5.8699e-01, -4.8908e-01,\n",
            "         7.2898e-01, -1.0965e-01,  9.6584e-01,  7.9767e-04, -2.2059e-01,\n",
            "         9.3034e-01,  9.8105e-01,  1.6878e-01,  9.8655e-01,  8.2009e-01,\n",
            "         7.8712e-01,  7.6867e-01,  8.9017e-01,  8.7976e-01,  8.2015e-01,\n",
            "         7.8574e-02,  9.7689e-01,  8.9107e-01,  9.4646e-01,  7.8131e-01,\n",
            "         9.6334e-01,  9.0829e-01,  9.1156e-01,  9.8681e-01,  6.0104e-01,\n",
            "         9.7619e-01, -8.1957e-01,  8.6346e-01,  9.9052e-01,  9.8666e-01,\n",
            "         8.0769e-01,  7.8630e-01,  9.6127e-01,  1.9243e-01,  9.7337e-01,\n",
            "         6.6786e-01,  9.6485e-01, -2.4050e-01, -8.3038e-02,  9.9371e-01,\n",
            "         9.7350e-01, -4.8569e-01,  7.3467e-01,  3.0289e-02,  7.4320e-01,\n",
            "         9.9574e-01,  9.3786e-01, -3.5751e-01,  9.1768e-01,  9.6312e-01,\n",
            "         9.6885e-01,  5.8586e-01,  9.8238e-01,  7.3771e-01], device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 53==== Step 2 Train Loss 0.7403871417045593 ======  0.3571428571428571\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8933,  1.2544,  0.2316,  ..., -0.0379, -0.3523, -0.5660],\n",
            "        [ 0.4187,  0.3984, -0.3849,  ...,  0.2015, -0.5779, -0.3197],\n",
            "        [ 0.2219,  0.1535, -0.0320,  ...,  0.1341,  0.2579, -0.4211],\n",
            "        ...,\n",
            "        [-1.1925,  1.0694,  0.5852,  ...,  0.0109, -0.2746, -0.4163],\n",
            "        [ 0.4856, -0.3292, -0.3875,  ...,  0.2724, -0.7811,  0.0058],\n",
            "        [ 0.5417,  0.5957, -0.3669,  ...,  0.1204, -0.5667, -0.2311]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5871,  0.4509,  0.8171, -0.6017, -0.3947, -0.3597,  0.9881,  0.0868,\n",
            "         0.7442,  0.6100, -0.0928,  0.9732,  0.4255,  0.9254, -0.2029,  0.5338,\n",
            "         0.9705,  0.9696,  0.9685,  0.5325,  0.9129,  0.6598, -0.3321, -0.5341,\n",
            "         0.3633, -0.0516,  0.9879,  0.9918, -0.7078,  0.8780,  0.9193,  0.9766,\n",
            "         0.2209,  0.2525,  0.5628, -0.0441, -0.3879,  0.9614, -0.0489, -0.3838,\n",
            "        -0.6183,  0.8761,  0.5770,  0.9881,  0.9683,  0.5799,  0.6468, -0.6171,\n",
            "         0.9961,  0.8664,  0.9089,  0.9557,  0.9715,  0.9850,  0.0747, -0.8868,\n",
            "        -0.6109,  0.9911,  0.7012,  0.9601,  0.9110,  0.9916, -0.0488,  0.7973],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 54==== Step 2 Train Loss 0.7052297592163086 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.6398, -0.7083,  0.1087,  ...,  0.2305,  0.7000, -0.1119],\n",
            "        [-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [ 0.5941, -0.4109,  0.0466,  ...,  0.0364,  0.6148, -0.0963],\n",
            "        ...,\n",
            "        [-1.3364,  1.1315,  0.5472,  ...,  0.0434, -0.6048, -0.3285],\n",
            "        [-0.0932,  0.9244, -0.0622,  ...,  0.0248, -0.3655, -0.5096],\n",
            "        [ 0.2078,  0.7243, -0.0922,  ...,  0.0840, -0.2580, -0.2437]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.4451, -0.7485, -0.4402,  0.9869,  0.9800,  0.8618,  0.7545, -0.0921,\n",
            "         0.9266, -0.1807,  0.9767,  0.9538,  0.0775, -0.0566,  0.5259,  0.6321,\n",
            "         0.6530, -0.1850, -0.7307,  0.9451, -0.3408,  0.8490,  0.3686,  0.9792,\n",
            "         0.9955, -0.2619,  0.9175,  0.7133,  0.7665, -0.5892,  0.9846,  0.9298,\n",
            "         0.6773,  0.8530,  0.9825,  0.3015,  0.9453,  0.2787,  0.9107,  0.9425,\n",
            "         0.1111, -0.3557,  0.9633,  0.0757,  0.8979,  0.8956, -0.7155,  0.5151,\n",
            "         0.9531, -0.0541, -0.4737,  0.9692, -0.0807,  0.9617,  0.2077,  0.9830,\n",
            "        -0.2972,  0.7510,  0.9597,  0.2858,  0.7563,  0.9858,  0.9170,  0.9433],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
            "        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 55==== Step 2 Train Loss 0.6719694137573242 ======  0.47826086956521735\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1239,  1.2301,  0.4535,  ...,  0.0948, -0.4713, -0.4432],\n",
            "        [-0.1552,  1.0605, -0.1490,  ...,  0.0434, -0.4132, -0.6567],\n",
            "        [-1.4291,  1.0356,  0.5421,  ...,  0.0283, -0.6316, -0.2754],\n",
            "        ...,\n",
            "        [ 0.0087,  0.3115, -0.3894,  ...,  0.0163, -0.5916, -0.3513],\n",
            "        [-1.0031,  0.9289,  0.3501,  ..., -0.0564, -0.2396, -0.4708],\n",
            "        [-1.3162,  1.0528,  0.7049,  ..., -0.0031, -0.3018, -0.3868]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9629,  0.9553,  0.9834,  0.9354,  0.8813,  0.9925, -0.3879,  0.2974,\n",
            "        -0.2735,  0.8216,  0.8403, -0.2391,  0.9888,  0.9197,  0.9859,  0.9183,\n",
            "         0.9869,  0.9846, -0.6560,  0.5425,  0.9957,  0.9607,  0.8556,  0.9912,\n",
            "         0.0706,  0.9713,  0.9913,  0.9917,  0.8933,  0.9881,  0.9783,  0.9963,\n",
            "        -0.7251, -0.3631,  0.9735, -0.3724,  0.9913,  0.9562,  0.9834,  0.9903,\n",
            "         0.9701,  0.0294,  0.9899,  0.9283,  0.9113,  0.6925, -0.1585, -0.6745,\n",
            "        -0.3091,  0.4713,  0.8367,  0.8347,  0.0701, -0.5731,  0.3176,  0.9881,\n",
            "         0.9199,  0.9914, -0.8551,  0.9887,  0.8434,  0.1149,  0.9861, -0.1034],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 56==== Step 2 Train Loss 0.74184650182724 ======  0.33333333333333337\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2331,  1.2515,  0.5476,  ..., -0.1216, -0.3072, -0.4213],\n",
            "        [-1.0122,  1.4354,  0.4911,  ...,  0.1031, -0.2712, -0.4033],\n",
            "        [-0.5240,  1.1403,  0.2868,  ..., -0.1296, -0.2523, -0.5639],\n",
            "        ...,\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        [-0.8285,  1.1976,  0.2564,  ..., -0.1177, -0.1841, -0.5822],\n",
            "        [ 0.7252, -0.4601, -0.0061,  ...,  0.0468,  0.6158, -0.0361]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9714,  0.8917,  0.9605, -0.4428,  0.8005,  0.9349, -0.4612,  0.9439,\n",
            "         0.8866, -0.1474,  0.9909,  0.3756,  0.9575,  0.2875,  0.9903,  0.2351,\n",
            "         0.6661,  0.9799,  0.7975,  0.9321,  0.9811,  0.9682,  0.4275,  0.9913,\n",
            "        -0.0992,  0.8859,  0.9669,  0.9880,  0.9441,  0.6540,  0.9916,  0.7314,\n",
            "         0.9478, -0.5890,  0.2482,  0.9706,  0.9591,  0.8854,  0.5779,  0.0513,\n",
            "         0.9096,  0.9764,  0.9652, -0.6269,  0.9676,  0.9709, -0.6662,  0.9703,\n",
            "         0.9554,  0.9872,  0.5477,  0.9877, -0.7830,  0.7997,  0.9932,  0.9151,\n",
            "         0.9877,  0.7166,  0.9176,  0.9846, -0.0324,  0.9932,  0.9672,  0.9763],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 57==== Step 2 Train Loss 0.7457030415534973 ======  0.459016393442623\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4941, -0.5949,  0.0408,  ...,  0.1187,  0.6551, -0.1850],\n",
            "        [ 0.4791, -1.0049,  0.0772,  ...,  0.0858,  0.4971,  0.2117],\n",
            "        [ 0.4073,  0.5618, -0.3719,  ...,  0.1893, -0.3844, -0.2963],\n",
            "        ...,\n",
            "        [-1.3162,  1.0528,  0.7049,  ..., -0.0031, -0.3018, -0.3868],\n",
            "        [-0.7979,  1.0310,  0.2928,  ..., -0.1073, -0.2584, -0.4477],\n",
            "        [ 0.9208, -0.3942, -0.2488,  ...,  0.1091,  0.4275, -0.0849]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5700, -0.7384,  0.8097,  0.7170,  0.1138,  0.9000,  0.9824,  0.9952,\n",
            "         0.9927,  0.3510, -0.7658,  0.0721, -0.6562, -0.1091,  0.9942,  0.9911,\n",
            "        -0.0594,  0.9150,  0.9120,  0.9224,  0.9814,  0.9886,  0.8944,  0.9695,\n",
            "        -0.1226,  0.9604, -0.3360,  0.8967,  0.9100,  0.9900,  0.9262,  0.5187,\n",
            "         0.8714,  0.9224, -0.8184,  0.9808, -0.0827,  0.6182,  0.8995,  0.7809,\n",
            "         0.9548,  0.5494,  0.1018,  0.5269, -0.1830,  0.9918,  0.9926,  0.9970,\n",
            "        -0.7111, -0.0667,  0.1631,  0.5924,  0.9299,  0.1761,  0.9931,  0.2596,\n",
            "         0.8127, -0.0281,  0.9477,  0.9768,  0.9070,  0.5098,  0.2546,  0.8270],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 58==== Step 2 Train Loss 0.7300602197647095 ======  0.32653061224489793\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 4.0729e-01,  5.6181e-01, -3.7187e-01,  ...,  1.8925e-01,\n",
            "         -3.8437e-01, -2.9629e-01],\n",
            "        [-1.2395e-01,  1.1540e+00,  3.0269e-01,  ..., -4.3615e-02,\n",
            "          2.2714e-02, -6.1691e-01],\n",
            "        [ 3.8262e-01, -1.5463e-01, -3.9036e-01,  ...,  2.5116e-01,\n",
            "         -9.7916e-01, -2.4113e-02],\n",
            "        ...,\n",
            "        [-4.3190e-01,  1.2429e+00,  1.3020e-01,  ..., -1.5986e-02,\n",
            "         -3.6288e-01, -5.1111e-01],\n",
            "        [-1.3078e+00,  9.3616e-01,  5.6553e-01,  ..., -9.5568e-04,\n",
            "         -3.3426e-01, -3.9002e-01],\n",
            "        [ 5.4155e-01, -4.3922e-01, -6.0035e-02,  ...,  1.1735e-02,\n",
            "          4.5574e-01, -1.9255e-02]], device='cuda:0')\n",
            "tensor([ 0.9369,  0.7212, -0.0248,  0.8374,  0.9824,  0.7725,  0.9750,  0.9860,\n",
            "         0.9439, -0.6631,  0.9533,  0.9561,  0.9881,  0.4754,  0.9905,  0.9303,\n",
            "         0.9934,  0.4271,  0.9858,  0.9943, -0.3132, -0.2697,  0.9551,  0.9865,\n",
            "         0.9826, -0.2651,  0.9581, -0.8702,  0.0084, -0.8493, -0.1590,  0.9783,\n",
            "         0.9645,  0.9867,  0.9187,  0.9560,  0.9429,  0.9546,  0.9252, -0.3821,\n",
            "         0.9407, -0.0757,  0.8150,  0.9931,  0.9714,  0.9638,  0.9694, -0.1599,\n",
            "         0.9142,  0.8719,  0.9647,  0.7925,  0.8566,  0.9853, -0.1531,  0.9957,\n",
            "         0.2624,  0.7631, -0.4837,  0.9130,  0.9539, -0.7657, -0.6627,  0.8194],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 59==== Step 2 Train Loss 0.7339347004890442 ======  0.4000000000000001\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0064,  1.1612,  0.2788,  ..., -0.0927, -0.3348, -0.5654],\n",
            "        [ 0.0549,  1.0062, -0.2714,  ..., -0.0853, -0.6683, -0.5103],\n",
            "        [-1.1309,  1.1055,  0.3674,  ...,  0.0069, -0.2851, -0.5010],\n",
            "        ...,\n",
            "        [-1.2911,  1.0738,  0.5832,  ..., -0.1055, -0.3588, -0.3891],\n",
            "        [ 0.5810, -0.2976,  0.0161,  ...,  0.0887,  0.6549, -0.3665],\n",
            "        [ 0.4280,  0.0143,  0.0564,  ..., -0.1001,  0.5601, -0.4053]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.0721,  0.9758,  0.9680,  0.0292,  0.9293,  0.9888,  0.8491, -0.5018,\n",
            "         0.4362,  0.7780,  0.8818,  0.9885,  0.8804, -0.5035,  0.6215,  0.9966,\n",
            "         0.6333,  0.2694,  0.3329,  0.0059, -0.1337,  0.9407,  0.9944, -0.3982,\n",
            "         0.9283,  0.7043,  0.4233,  0.2654,  0.9874, -0.1709,  0.9151,  0.2587,\n",
            "         0.9776,  0.9297,  0.5576,  0.9746,  0.9195,  0.7013,  0.9789, -0.6237,\n",
            "         0.7253,  0.8669, -0.2930,  0.9955,  0.8543,  0.9765, -0.1706,  0.9530,\n",
            "         0.2077,  0.9899,  0.9134,  0.9728, -0.6127,  0.1349,  0.5664,  0.3243,\n",
            "         0.9729,  0.9855,  0.9874,  0.9816,  0.6576,  0.9927,  0.4030,  0.9541],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 60==== Step 2 Train Loss 0.7089842557907104 ======  0.4482758620689655\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4073,  0.5618, -0.3719,  ...,  0.1893, -0.3844, -0.2963],\n",
            "        [-0.8988,  1.0355,  0.2791,  ..., -0.0644, -0.4630, -0.6298],\n",
            "        [-1.2689,  1.2059,  0.5439,  ..., -0.0313, -0.6080, -0.3506],\n",
            "        ...,\n",
            "        [-1.3104,  1.0613,  0.6367,  ..., -0.0392, -0.2553, -0.4147],\n",
            "        [-0.8337,  1.1159,  0.1479,  ...,  0.0218, -0.1356, -0.4791],\n",
            "        [-0.8972,  1.2112,  0.3385,  ..., -0.2034, -0.2981, -0.5731]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9369, -0.1766,  0.9897,  0.9756,  0.9558,  0.7334, -0.4914,  0.9420,\n",
            "         0.8165,  0.9838,  0.3676,  0.9826,  0.8091,  0.3801, -0.0669,  0.3038,\n",
            "         0.8162,  0.4022,  0.9935,  0.4816,  0.9777,  0.1834,  0.9779,  0.7346,\n",
            "        -0.0377,  0.1639, -0.3582, -0.1127,  0.5964,  0.9121,  0.9918,  0.9058,\n",
            "         0.9945, -0.7657,  0.3017,  0.9293,  0.6137, -0.5437, -0.1446,  0.9027,\n",
            "         0.1722,  0.0647, -0.6203,  0.9852, -0.1357,  0.6208,  0.1358,  0.7833,\n",
            "         0.9504,  0.9832, -0.4944, -0.4835,  0.8920,  0.9646, -0.7286, -0.5366,\n",
            "         0.9793,  0.9740,  0.9136, -0.4735,  0.8087,  0.9843,  0.2146,  0.2609],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 61==== Step 2 Train Loss 0.704382061958313 ======  0.41509433962264153\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3731,  1.0866,  0.5036,  ...,  0.0107, -0.6242, -0.3145],\n",
            "        [-1.1707,  1.3597,  0.4744,  ...,  0.0482, -0.1981, -0.3794],\n",
            "        [ 0.0093,  1.0110, -0.1024,  ...,  0.0707, -0.3269, -0.5262],\n",
            "        ...,\n",
            "        [-1.2644,  1.0952,  0.5467,  ...,  0.0036, -0.1489, -0.5242],\n",
            "        [-0.3705,  1.2712,  0.2074,  ..., -0.0291, -0.3014, -0.5383],\n",
            "        [ 0.5425, -0.3278,  0.0979,  ...,  0.1151,  0.8034, -0.3171]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9505,  0.9843,  0.6115,  0.9497, -0.4313,  0.9948,  0.4652,  0.6651,\n",
            "         0.9851,  0.9218,  0.7261,  0.9883, -0.6834,  0.9748, -0.2008,  0.0906,\n",
            "         0.8578,  0.9751, -0.3526,  0.9055,  0.1860,  0.9921,  0.9442, -0.0399,\n",
            "        -0.5263,  0.8722, -0.6415,  0.9794, -0.4063,  0.9624,  0.9782,  0.0632,\n",
            "         0.9662,  0.4221,  0.9844, -0.0779,  0.9453,  0.5377,  0.9466,  0.8629,\n",
            "         0.9724,  0.9875, -0.1656, -0.0509,  0.8224,  0.9867,  0.8390, -0.5372,\n",
            "        -0.3500,  0.9913,  0.9863, -0.3349, -0.1620,  0.9957,  0.6156,  0.3759,\n",
            "         0.1712,  0.9885,  0.8016,  0.4816,  0.9622, -0.4475,  0.9646,  0.9192],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 62==== Step 2 Train Loss 0.750665009021759 ======  0.32142857142857145\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.1117,  1.0713, -0.0091,  ..., -0.0382, -0.5868, -0.3688],\n",
            "        [ 0.3703,  0.5603,  0.0423,  ...,  0.1453,  0.0414, -0.3923],\n",
            "        [-1.2804,  1.0736,  0.4059,  ...,  0.0111, -0.6980, -0.3929],\n",
            "        ...,\n",
            "        [-0.8356,  1.3510,  0.4815,  ...,  0.0966, -0.1293, -0.4623],\n",
            "        [ 0.3651, -0.8368, -0.0228,  ...,  0.1812,  0.3445,  0.1345],\n",
            "        [-1.4077,  1.1398,  0.5148,  ...,  0.0621, -0.6013, -0.3745]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7766,  0.7306, -0.7536, -0.6635,  0.3412,  0.1645,  0.8754, -0.5636,\n",
            "         0.8050,  0.9185,  0.9934,  0.8994, -0.7400,  0.6204,  0.8861,  0.7912,\n",
            "         0.2269, -0.5012,  0.9836,  0.9795,  0.9540, -0.0648,  0.6651,  0.9730,\n",
            "        -0.0011,  0.9932,  0.9631,  0.9290,  0.9272,  0.9877, -0.5197,  0.9833,\n",
            "         0.1989,  0.9918,  0.7405,  0.9787,  0.9242,  0.9879,  0.9834,  0.9944,\n",
            "        -0.6917,  0.9937,  0.9912,  0.9245, -0.3859,  0.9883, -0.8782,  0.9891,\n",
            "         0.9775,  0.8296,  0.6516,  0.9208,  0.5023,  0.7199,  0.9455, -0.2954,\n",
            "         0.5359,  0.9911,  0.6813,  0.9540, -0.2358,  0.9639,  0.6596,  0.9827],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 63==== Step 2 Train Loss 0.7336326241493225 ======  0.37037037037037035\n",
            "torch.Size([64, 48])\n",
            "tensor([[-7.2781e-01,  1.1143e+00,  3.1105e-01,  ...,  5.6457e-02,\n",
            "         -4.2965e-01, -5.3779e-01],\n",
            "        [ 5.3800e-01, -2.8343e-01, -7.6518e-02,  ..., -1.7476e-01,\n",
            "          4.1635e-01, -8.4727e-02],\n",
            "        [-9.1407e-01,  1.2611e+00,  5.4519e-01,  ..., -1.0462e-01,\n",
            "         -2.3632e-01, -4.8253e-01],\n",
            "        ...,\n",
            "        [ 4.6149e-01, -8.1123e-01,  9.6725e-02,  ...,  1.9848e-01,\n",
            "          6.8729e-01, -1.2190e-03],\n",
            "        [-8.9376e-01,  1.3843e+00,  4.9887e-01,  ..., -1.9164e-02,\n",
            "         -2.7202e-01, -4.6650e-01],\n",
            "        [-9.8413e-01,  1.1294e+00,  2.7837e-01,  ..., -1.1596e-01,\n",
            "         -2.4175e-01, -6.0290e-01]], device='cuda:0')\n",
            "tensor([ 0.2093, -0.7002,  0.9832, -0.2266,  0.3614,  0.9259, -0.7084,  0.6828,\n",
            "         0.8916,  0.9717,  0.9794, -0.1284,  0.9746,  0.9527,  0.9765,  0.6375,\n",
            "         0.0383,  0.8253,  0.1569, -0.2855,  0.9222, -0.8839,  0.0953,  0.9863,\n",
            "        -0.6632,  0.9878, -0.3010, -0.2849,  0.9503,  0.9885,  0.3450,  0.9255,\n",
            "        -0.7813,  0.8407,  0.9914, -0.0886, -0.6728,  0.5462,  0.8345, -0.4980,\n",
            "        -0.3587, -0.2007,  0.4060, -0.7378,  0.9882,  0.9412,  0.8779,  0.9788,\n",
            "         0.9281,  0.8116,  0.9427, -0.1751,  0.9938,  0.8775,  0.1384,  0.3523,\n",
            "         0.5664,  0.9679,  0.8639,  0.9873,  0.9870, -0.6996,  0.9575,  0.9806],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 64==== Step 2 Train Loss 0.7218093276023865 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743],\n",
            "        [ 0.2905,  0.3283,  0.1226,  ..., -0.1208,  0.5098, -0.5255],\n",
            "        [ 0.3946,  0.1347, -0.4562,  ...,  0.2042, -0.6463, -0.1608],\n",
            "        ...,\n",
            "        [-1.0156,  1.2767,  0.3918,  ..., -0.0145, -0.2219, -0.4080],\n",
            "        [-1.2982,  1.1451,  0.5844,  ...,  0.0090, -0.2458, -0.4737],\n",
            "        [-1.2441,  1.1606,  0.5669,  ..., -0.0108, -0.2069, -0.3485]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9927, -0.1415, -0.3034,  0.9800, -0.8039,  0.9243, -0.7091,  0.1926,\n",
            "         0.9146,  0.9596, -0.7481,  0.9847,  0.8119, -0.0016,  0.1278,  0.9341,\n",
            "         0.9917,  0.9772,  0.9783,  0.9935,  0.8877,  0.9854,  0.9813, -0.0724,\n",
            "         0.6443, -0.7851,  0.9873, -0.2841,  0.8729,  0.9208,  0.8344, -0.4245,\n",
            "         0.2554,  0.8793,  0.9492,  0.2426,  0.9744, -0.7752, -0.2096,  0.9914,\n",
            "        -0.9285,  0.9597,  0.2406, -0.0958,  0.9859,  0.1879,  0.9878,  0.5921,\n",
            "        -0.0505, -0.1926,  0.9835,  0.9571, -0.2821, -0.3599,  0.9333,  0.8781,\n",
            "        -0.5866,  0.9853,  0.9884,  0.9841, -0.3315, -0.5624, -0.7133,  0.9640],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQpElEQVR4nO3df4zkdX3H8efLO37YasshG3qC8cBSKWnjYbaU1sYf+As1EUyphUR7tjSnVhtNbeOpf2ibmmJTJWnaaE9Brq1F6SnhKlp7AoaYCHaxJxxQ5EBMoSe3iqik6VXg3T/mu+24zN7M7szs7Qeej2Sz3/l8vzPzus/svfZ73/l+51JVSJLa86TDHUCStDIWuCQ1ygKXpEZZ4JLUKAtckhq1fjWf7LjjjqtNmzat5lNKUvNuuumm71TVzOLxoQWe5GjgeuCobvudVfXeJJcBLwC+3236hqrac6jH2rRpE3Nzc8vNLklPaEm+NWh8lD3wg8BZVfVQkiOALyf5fLfuj6pq56RCSpJGN7TAq3elz0PdzSO6L6/+kaTDbKQ3MZOsS7IHOADsrqobu1XvT3JzkouTHDW1lJKkxxipwKvqkaraDJwInJHkF4B3AacCvwQcC7xz0H2TbE0yl2Rufn5+QrElScs6jbCqHgSuA86uqv3VcxD4OHDGEvfZXlWzVTU7M/OYN1ElSSs0tMCTzCQ5plt+MvBS4N+TbOzGApwL7J1mUEnSjxvlLJSNwI4k6+gV/hVV9dkk1yaZAQLsAd40xZySpEVGOQvlZuD0AeNnTSWRJGkkXkovSY1a1UvpJWk5Nm27+nBHmJh7LnrVxB/TPXBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVqaIEnOTrJV5N8PcmtSf64Gz8pyY1J9iX5VJIjpx9XkrRglD3wg8BZVfUcYDNwdpIzgQ8AF1fVzwLfAy6cXkxJ0mJDC7x6HupuHtF9FXAWsLMb3wGcO5WEkqSBRjoGnmRdkj3AAWA3cBfwYFU93G1yL3DCEvfdmmQuydz8/PwkMkuSGLHAq+qRqtoMnAicAZw66hNU1faqmq2q2ZmZmRXGlCQttqyzUKrqQeA64FeAY5Ks71adCNw34WySpEMY5SyUmSTHdMtPBl4K3E6vyM/rNtsCXDWtkJKkx1o/fBM2AjuSrKNX+FdU1WeT3AZ8MsmfAv8GXDLFnJKkRYYWeFXdDJw+YPxuesfDJUmHgVdiSlKjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVqaIEneUaS65LcluTWJG/rxt+X5L4ke7qvV04/riRpwfoRtnkYeEdVfS3JU4Gbkuzu1l1cVX8xvXiSpKUMLfCq2g/s75Z/mOR24IRpB5MkHdqyjoEn2QScDtzYDb01yc1JLk2yYcLZJEmHMHKBJ3kK8Gng7VX1A+DDwLOAzfT20D+4xP22JplLMjc/Pz+ByJIkGLHAkxxBr7w/UVWfAaiq+6vqkap6FPgocMag+1bV9qqararZmZmZSeWWpCe8Uc5CCXAJcHtVfahvfGPfZq8B9k4+niRpKaOchfI84PXALUn2dGPvBi5Ishko4B7gjVNJKEkaaJSzUL4MZMCqz00+jiRpVF6JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGjW0wJM8I8l1SW5LcmuSt3XjxybZneTO7vuG6ceVJC0YZQ/8YeAdVXUacCbwliSnAduAa6rqFOCa7rYkaZUMLfCq2l9VX+uWfwjcDpwAnAPs6DbbAZw7rZCSpMda1jHwJJuA04EbgeOran+36tvA8UvcZ2uSuSRz8/PzY0SVJPUbucCTPAX4NPD2qvpB/7qqKqAG3a+qtlfVbFXNzszMjBVWkvT/RirwJEfQK+9PVNVnuuH7k2zs1m8EDkwnoiRpkFHOQglwCXB7VX2ob9UuYEu3vAW4avLxJElLWT/CNs8DXg/ckmRPN/Zu4CLgiiQXAt8CXjudiJKkQYYWeFV9GcgSq1882TiSpFF5JaYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSo4YWeJJLkxxIsrdv7H1J7kuyp/t65XRjSpIWG2UP/DLg7AHjF1fV5u7rc5ONJUkaZmiBV9X1wAOrkEWStAzjHAN/a5Kbu0MsG5baKMnWJHNJ5ubn58d4OklSv5UW+IeBZwGbgf3AB5fasKq2V9VsVc3OzMys8OkkSYutqMCr6v6qeqSqHgU+Cpwx2ViSpGFWVOBJNvbdfA2wd6ltJUnTsX7YBkkuB14IHJfkXuC9wAuTbAYKuAd44xQzSpIGGFrgVXXBgOFLppBFkrQMXokpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNbTAk1ya5ECSvX1jxybZneTO7vuG6caUJC02yh74ZcDZi8a2AddU1SnANd1tSdIqGlrgVXU98MCi4XOAHd3yDuDcCeeSJA2x0mPgx1fV/m7528DxS22YZGuSuSRz8/PzK3w6SdJiY7+JWVUF1CHWb6+q2aqanZmZGffpJEmdlRb4/Uk2AnTfD0wukiRpFCst8F3Alm55C3DVZOJIkkY1ymmElwNfAZ6d5N4kFwIXAS9Ncifwku62JGkVrR+2QVVdsMSqF084iyRpGYYWuHQom7ZdfbgjTMQ9F73qcEeQls1L6SWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEY181koj5fP3AA/d0PSZLgHLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqGbOQnk8eTydUfN48Xh6TTzL6YnDPXBJapQFLkmNGusQSpJ7gB8CjwAPV9XsJEJJkoabxDHwF1XVdybwOJKkZfAQiiQ1atwCL+BfktyUZOugDZJsTTKXZG5+fn7Mp5MkLRi3wH+tqp4LvAJ4S5LnL96gqrZX1WxVzc7MzIz5dJKkBWMVeFXd130/AFwJnDGJUJKk4VZc4El+MslTF5aBlwF7JxVMknRo45yFcjxwZZKFx/mHqvrniaSSJA214gKvqruB50wwiyRpGTyNUJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVGT+E+NJa0hm7ZdfbgjaJW4By5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1FgFnuTsJHck2Zdk26RCSZKGW3GBJ1kH/DXwCuA04IIkp00qmCTp0MbZAz8D2FdVd1fV/wCfBM6ZTCxJ0jDjfBbKCcB/9N2+F/jlxRsl2Qps7W4+lOSOvtXHAd8ZI8M0reVsYL5xrOVssLbzreVssIbz5QPAyvM9c9Dg1D/Mqqq2A9sHrUsyV1Wz086wEms5G5hvHGs5G6ztfGs5Gzzx8o1zCOU+4Bl9t0/sxiRJq2CcAv9X4JQkJyU5Ejgf2DWZWJKkYVZ8CKWqHk7yVuALwDrg0qq6dZkPM/DQyhqxlrOB+caxlrPB2s63lrPBEyxfqmqSjydJWiVeiSlJjbLAJalRUy/wJL+R5NYkjyZZ8vSZpS7L794kvbEb/1T3humksh2bZHeSO7vvGwZs86Ike/q+/jvJud26y5J8s2/d5kllGzVft90jfRl29Y0f7rnbnOQr3et/c5Lf7Fs3lbkb9vEOSY7q5mJfNzeb+ta9qxu/I8nLJ5Fnmdn+IMlt3Vxdk+SZfesGvsarnO8NSeb7cvxu37ot3c/CnUm2HIZsF/fl+kaSB/vWrcbcXZrkQJK9S6xPkr/s8t+c5Ll961Y+d1U11S/g54FnA18CZpfYZh1wF3AycCTwdeC0bt0VwPnd8keAN08w258D27rlbcAHhmx/LPAA8BPd7cuA86Y4dyPlAx5aYvywzh3wc8Ap3fLTgf3AMdOau0P9HPVt83vAR7rl84FPdcunddsfBZzUPc66Vc72or6frTcvZDvUa7zK+d4A/NWA+x4L3N1939Atb1jNbIu2/316J1Wsytx1z/F84LnA3iXWvxL4PBDgTODGSczd1PfAq+r2qrpjyGYDL8tPEuAsYGe33Q7g3AnGO6d7zFEf+zzg81X1XxPMcCjLzfd/1sLcVdU3qurObvk/gQPAzAQzLDbKxzv0594JvLibq3OAT1bVwar6JrCve7xVy1ZV1/X9bN1A79qK1TLOR2O8HNhdVQ9U1feA3cDZhzHbBcDlE3z+oarqeno7d0s5B/jb6rkBOCbJRsacu7VyDHzQZfknAE8DHqyqhxeNT8rxVbW/W/42cPyQ7c/nsT8Y7+/+SXRxkqMmmG05+Y5OMpfkhoXDO6yxuUtyBr29p7v6hic9d0v9HA3cppub79Obq1HuO+1s/S6kt8e2YNBrPEmj5vv17jXbmWThQr41M3fdYaeTgGv7hqc9d6NY6s8w1txN5FL6JF8EfmbAqvdU1VWTeI6VOlS2/htVVUmWPKey+235i/TOe1/wLnrldSS98zvfCfzJYcj3zKq6L8nJwLVJbqFXTGOZ8Nz9HbClqh7thseeu8erJK8DZoEX9A0/5jWuqrsGP8LU/BNweVUdTPJGev+SOWuVMwxzPrCzqh7pG1sLczcVEynwqnrJmA+x1GX536X3T4313d7Ssi/XP1S2JPcn2VhV+7uSOXCIh3otcGVV/ajvsRf2QA8m+Tjwh8vJNql8VXVf9/3uJF8CTgc+zRqYuyQ/BVxN75f5DX2PPfbcDTDKxzssbHNvkvXAT9P7OZv2R0OM9PhJXkLvF+QLqurgwvgSr/EkS2hovqr6bt/Nj9F7H2Thvi9cdN8vrWa2PucDb+kfWIW5G8VSf4ax5m6tHEIZeFl+9Y7yX0fv2DPAFmCSe/S7uscc5bEfc1ytK66F483nAgPfgZ5mviQbFg4/JDkOeB5w21qYu+61vJLesb+di9ZNY+5G+XiH/tznAdd2c7ULOD+9s1ROAk4BvjqBTCNnS3I68DfAq6vqQN/4wNd4gtlGzbex7+argdu75S8AL+tybgBexo//S3Xq2bp8p9J7I/ArfWOrMXej2AX8Vnc2ypnA97udmPHmbhXenX0NveM6B4H7gS90408HPrfoXdpv0PvN+J6+8ZPp/UXaB/wjcNQEsz0NuAa4E/gicGw3Pgt8rG+7TfR+Uz5p0f2vBW6hVz5/DzxlwnM3NB/wq12Gr3ffL1wrcwe8DvgRsKfva/M0527QzxG9QzOv7paP7uZiXzc3J/fd9z3d/e4AXjGFvwvDsn2x+zuyMFe7hr3Gq5zvz4BbuxzXAaf23fd3ujndB/z2amfrbr8PuGjR/VZr7i6nd5bVj+j13YXAm4A3detD7z/AuavLMdt33xXPnZfSS1Kj1sohFEnSMlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVH/C19r3KZVYebqAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 65==== Step 2 Train Loss 0.734336793422699 ======  0.2916666666666667\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [-1.2366,  1.0443,  0.3502,  ...,  0.0460, -0.6261, -0.3316],\n",
            "        [-0.9159,  1.4036,  0.5151,  ..., -0.0381, -0.2807, -0.3692],\n",
            "        ...,\n",
            "        [-1.3342,  1.0919,  0.5945,  ...,  0.0202, -0.5763, -0.2895],\n",
            "        [-0.4662,  1.0617, -0.0678,  ...,  0.1706, -0.6424, -0.4735],\n",
            "        [-1.2591,  1.1094,  0.4071,  ...,  0.0706, -0.4204, -0.4626]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8644,  0.9862, -0.6011,  0.7165, -0.5998,  0.8924,  0.9851,  0.9819,\n",
            "         0.6053,  0.8777,  0.9882, -0.6779,  0.9942,  0.7719,  0.9716,  0.2628,\n",
            "        -0.0361,  0.6506,  0.9812,  0.5709,  0.9830, -0.5764, -0.0041,  0.9898,\n",
            "         0.9921,  0.5227,  0.9888,  0.2154,  0.2250,  0.5336,  0.7475,  0.8239,\n",
            "         0.9955, -0.6373, -0.1282,  0.9540,  0.9628,  0.8147,  0.3381,  0.5212,\n",
            "         0.6834,  0.9630,  0.9805, -0.0016,  0.7339,  0.9895,  0.2215, -0.8237,\n",
            "         0.9797,  0.9606,  0.6161,  0.7510,  0.8424,  0.7926, -0.0543,  0.2707,\n",
            "         0.9925,  0.9911,  0.9342,  0.9021,  0.8500,  0.8639,  0.7013,  0.2505],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 66==== Step 2 Train Loss 0.7679869532585144 ======  0.3272727272727273\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1644,  1.1991,  0.5249,  ..., -0.0393, -0.2487, -0.5677],\n",
            "        [-0.0365,  0.8051, -0.1119,  ...,  0.0983, -0.2337, -0.3486],\n",
            "        [-0.1240,  1.1540,  0.3027,  ..., -0.0436,  0.0227, -0.6169],\n",
            "        ...,\n",
            "        [-1.1503,  1.3181,  0.4642,  ...,  0.0609, -0.4010, -0.4159],\n",
            "        [-1.5234,  1.0478,  0.6344,  ...,  0.0575, -0.6737, -0.2081],\n",
            "        [-1.2788,  1.0906,  0.5091,  ..., -0.0722, -0.2141, -0.4788]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9579,  0.7951,  0.7980,  0.4086,  0.9065,  0.9909, -0.2411,  0.8669,\n",
            "         0.6357,  0.3390,  0.9739,  0.9831,  0.7926,  0.9369,  0.2042,  0.9570,\n",
            "         0.9434,  0.4645, -0.4487,  0.9925,  0.9293, -0.3941,  0.0184,  0.9917,\n",
            "        -0.7400,  0.3516,  0.5389,  0.9788,  0.6910,  0.9670,  0.9917, -0.3504,\n",
            "         0.6020,  0.9933,  0.8046,  0.6718,  0.9833,  0.8506,  0.5880,  0.5995,\n",
            "        -0.3492,  0.9033,  0.2941,  0.8277, -0.1784,  0.2482,  0.7473,  0.0310,\n",
            "         0.9662,  0.8249, -0.2616,  0.6477,  0.8624,  0.9857, -0.3027,  0.9016,\n",
            "         0.9434,  0.8156,  0.9861,  0.9744, -0.0463, -0.0463, -0.0838,  0.6666],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 67==== Step 2 Train Loss 0.7170304656028748 ======  0.3928571428571429\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.0104,  0.4681,  0.0672,  ..., -0.1937,  0.5054, -0.6504],\n",
            "        [-1.0635,  1.0076,  0.6500,  ...,  0.0580, -0.1965, -0.4029],\n",
            "        [-1.0265,  1.3387,  0.6677,  ...,  0.0159, -0.1478, -0.3517],\n",
            "        ...,\n",
            "        [ 0.7304,  0.3282, -0.0898,  ...,  0.0151,  0.3074, -0.2675],\n",
            "        [ 0.5980,  0.8060, -0.0940,  ..., -0.0178, -0.1036, -0.3299],\n",
            "        [ 0.2333,  0.5223, -0.2132,  ...,  0.1557, -0.4239, -0.5639]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8640, -0.3303, -0.0091,  0.9880, -0.1182, -0.5543,  0.9828,  0.9941,\n",
            "         0.9042,  0.9855,  0.8006, -0.2577,  0.4028,  0.4988, -0.5104, -0.8042,\n",
            "        -0.5122,  0.2985,  0.9890,  0.2707,  0.0488,  0.8574,  0.9856,  0.1134,\n",
            "         0.9644, -0.3732, -0.6047,  0.9877, -0.0597,  0.8976,  0.9930,  0.6715,\n",
            "        -0.7453,  0.9792,  0.6396,  0.9966,  0.7488, -0.3793,  0.9322, -0.5738,\n",
            "         0.9423, -0.1784,  0.5557,  0.9275,  0.2680,  0.9393,  0.9921,  0.9725,\n",
            "        -0.7763,  0.9728,  0.9811,  0.9302,  0.9888, -0.1943, -0.3559,  0.6065,\n",
            "        -0.5385,  0.9706,  0.1413, -0.6179,  0.8542,  0.7007,  0.5493,  0.2383],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 68==== Step 2 Train Loss 0.7417861223220825 ======  0.2916666666666667\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5503, -0.0166, -0.4326,  ...,  0.1222, -0.8772, -0.0227],\n",
            "        [ 0.7131,  0.3678,  0.1303,  ..., -0.0653,  0.3235,  0.0799],\n",
            "        [ 0.5694,  0.2653, -0.1678,  ..., -0.0353,  0.2107, -0.5137],\n",
            "        ...,\n",
            "        [ 0.4791, -1.0049,  0.0772,  ...,  0.0858,  0.4971,  0.2117],\n",
            "        [ 0.7252, -0.4601, -0.0061,  ...,  0.0468,  0.6158, -0.0361],\n",
            "        [ 0.6560,  0.3316, -0.3128,  ...,  0.1620, -0.4369, -0.2091]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2596,  0.3695,  0.4763, -0.3807,  0.9091, -0.5477, -0.1729,  0.0514,\n",
            "         0.7078,  0.6204, -0.1992, -0.5054,  0.9875, -0.4102,  0.9287,  0.9948,\n",
            "         0.9578, -0.2536,  0.9544,  0.7037,  0.7728,  0.9439, -0.6870,  0.9623,\n",
            "         0.9646,  0.8967,  0.9478,  0.9738,  0.9518,  0.8706, -0.3979,  0.5177,\n",
            "         0.9137,  0.1352,  0.1167,  0.8730,  0.8234,  0.9827,  0.9800, -0.6948,\n",
            "        -0.6756,  0.9837,  0.2345, -0.6921,  0.9641,  0.9947,  0.9931,  0.9874,\n",
            "         0.8864,  0.1065,  0.9361, -0.0849,  0.7134,  0.8900, -0.0693,  0.9949,\n",
            "         0.9931, -0.0723,  0.5581,  0.9906, -0.4528, -0.6910,  0.9697,  0.9270],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 69==== Step 2 Train Loss 0.6908203959465027 ======  0.49180327868852464\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.6004, -0.4798, -0.1127,  ...,  0.2081,  0.5327, -0.1817],\n",
            "        [-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [-0.8356,  1.3510,  0.4815,  ...,  0.0966, -0.1293, -0.4623],\n",
            "        ...,\n",
            "        [-0.7239,  1.1399,  0.3868,  ..., -0.0072, -0.2037, -0.4712],\n",
            "        [-0.4908,  1.0151,  0.3486,  ..., -0.1349,  0.1698, -0.5773],\n",
            "        [ 0.4548,  0.4287, -0.2833,  ...,  0.2834, -0.3923, -0.2320]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9570,  0.9941,  0.9631,  0.9880,  0.9737,  0.4133,  0.2233, -0.7671,\n",
            "         0.9612,  0.9058,  0.8857, -0.3211,  0.6169, -0.1040, -0.1564, -0.4547,\n",
            "         0.9356,  0.3769,  0.9784,  0.9938,  0.9720, -0.2976, -0.2441,  0.8772,\n",
            "        -0.2743, -0.0423,  0.9154,  0.7183,  0.9914,  0.0102,  0.9677, -0.5100,\n",
            "         0.9847, -0.8196,  0.9801, -0.5733,  0.4527,  0.2339,  0.1315,  0.9428,\n",
            "         0.9657,  0.9885,  0.8473, -0.0390,  0.9951, -0.2424, -0.7064,  0.9798,\n",
            "         0.9646,  0.9943,  0.9823,  0.9852,  0.0159, -0.2548, -0.8436, -0.5791,\n",
            "         0.0643,  0.9162, -0.1793,  0.7387,  0.9110,  0.8308,  0.8885, -0.1228],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 70==== Step 2 Train Loss 0.7030298709869385 ======  0.4727272727272727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 48])\n",
            "tensor([[-1.2788,  1.0906,  0.5091,  ..., -0.0722, -0.2141, -0.4788],\n",
            "        [ 0.1696, -0.2963, -0.0257,  ...,  0.2114, -0.1839, -0.1407],\n",
            "        [ 0.1945,  0.7061, -0.2769,  ..., -0.0486, -0.4817, -0.3923],\n",
            "        ...,\n",
            "        [-1.1540,  1.0509,  0.3702,  ...,  0.0580, -0.5099, -0.3729],\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [-1.2760,  1.1292,  0.6219,  ..., -0.0763, -0.3256, -0.4322]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9823,  0.7466, -0.0211,  0.0615,  0.6258,  0.5935,  0.9612,  0.4167,\n",
            "         0.9095,  0.2395,  0.7850, -0.2698,  0.9362,  0.4207,  0.4507, -0.2192,\n",
            "         0.9685,  0.9500,  0.9958,  0.9356,  0.9217, -0.6631,  0.9888,  0.9658,\n",
            "         0.7003,  0.1111,  0.9636,  0.9862, -0.6841,  0.9441,  0.3884,  0.9888,\n",
            "         0.9868,  0.9412,  0.9222,  0.9570,  0.8806,  0.0952,  0.9770,  0.9935,\n",
            "         0.4476, -0.5084,  0.6583, -0.1797, -0.4435,  0.9416, -0.0825, -0.5945,\n",
            "         0.9924,  0.9939,  0.9647,  0.9361,  0.9860,  0.9765,  0.9757, -0.1503,\n",
            "         0.9792,  0.8878,  0.7767,  0.9893,  0.9559,  0.9813,  0.9876,  0.9937],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 71==== Step 2 Train Loss 0.72098308801651 ======  0.37037037037037035\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4977, -0.1591, -0.4705,  ...,  0.2923, -0.7666, -0.1391],\n",
            "        [ 0.6317,  0.0311,  0.1004,  ..., -0.0261,  0.3219,  0.0641],\n",
            "        [-0.9143,  1.3219,  0.3223,  ..., -0.0114, -0.2028, -0.5850],\n",
            "        ...,\n",
            "        [-1.1386,  1.1936,  0.4138,  ...,  0.0390, -0.4786, -0.3436],\n",
            "        [ 0.6914, -0.6622, -0.0116,  ...,  0.1920,  0.6327, -0.1508],\n",
            "        [-0.4701,  1.0007,  0.1265,  ..., -0.0161, -0.4456, -0.6164]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9714, -0.5148,  0.8644,  0.4789, -0.5821, -0.1434, -0.4273,  0.7536,\n",
            "         0.9737,  0.0120,  0.9883, -0.8168,  0.9711,  0.8357, -0.8138,  0.2282,\n",
            "        -0.4482,  0.9856, -0.7385,  0.9497,  0.9603,  0.1705,  0.9862,  0.8451,\n",
            "         0.9922, -0.3999,  0.7337,  0.9565,  0.0826,  0.8477,  0.7490,  0.9548,\n",
            "         0.9792,  0.5471,  0.9942, -0.5086, -0.7787,  0.9859,  0.2159,  0.7812,\n",
            "         0.1995, -0.0054,  0.9893,  0.9425,  0.9299,  0.3640,  0.8738,  0.9551,\n",
            "        -0.6026,  0.9588,  0.8958,  0.9753, -0.7374,  0.9665,  0.5281,  0.8897,\n",
            "         0.9914, -0.6056,  0.0281,  0.9880,  0.5876, -0.7242,  0.5792,  0.4501],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 72==== Step 2 Train Loss 0.7152840495109558 ======  0.27272727272727276\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3364,  1.1315,  0.5472,  ...,  0.0434, -0.6048, -0.3285],\n",
            "        [ 0.5348, -0.0662, -0.4665,  ...,  0.2833, -0.9400, -0.1275],\n",
            "        [-0.8242,  0.8144,  0.2183,  ..., -0.0336, -0.1667, -0.6377],\n",
            "        ...,\n",
            "        [-0.8956,  0.9140,  0.2645,  ..., -0.1659, -0.1927, -0.5044],\n",
            "        [ 0.1682,  0.7976, -0.0246,  ...,  0.2027, -0.2853, -0.3937],\n",
            "        [-0.7509,  1.2571,  0.1315,  ...,  0.0354, -0.5684, -0.4213]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1336,  0.8782,  0.9610,  0.9920,  0.0514,  0.9331,  0.8850, -0.1766,\n",
            "        -0.1457, -0.6771,  0.8398,  0.8743,  0.8188, -0.7107,  0.8931, -0.3432,\n",
            "         0.8542, -0.6373,  0.9112,  0.5372, -0.3168,  0.6269,  0.9429,  0.9875,\n",
            "         0.9055,  0.9264,  0.9933, -0.4776,  0.2985,  0.2155, -0.0749, -0.3126,\n",
            "        -0.8509,  0.6054,  0.7417,  0.9899, -0.8269, -0.7713, -0.1830,  0.8958,\n",
            "        -0.3097,  0.7840,  0.9811,  0.2586,  0.9917,  0.7839,  0.9696,  0.8029,\n",
            "        -0.6578,  0.9317,  0.9471,  0.4224,  0.9899,  0.7793,  0.9763, -0.6999,\n",
            "        -0.7660, -0.7368,  0.9799, -0.8628,  0.9408,  0.9103,  0.7407,  0.9212],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 73==== Step 2 Train Loss 0.6530769467353821 ======  0.3902439024390244\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.4487,  0.9212, -0.0880,  ..., -0.2330, -0.5489, -0.4732],\n",
            "        [-0.8943,  1.2696,  0.5990,  ...,  0.0763, -0.3132, -0.4385],\n",
            "        [-0.5215,  1.0631, -0.0059,  ..., -0.0762, -0.4281, -0.4321],\n",
            "        ...,\n",
            "        [-0.0272,  0.4727,  0.0856,  ...,  0.0871,  0.0867, -0.5415],\n",
            "        [-1.2356,  1.0466,  0.4485,  ...,  0.0270, -0.6474, -0.3582],\n",
            "        [-1.3567,  1.1212,  0.5071,  ...,  0.0908, -0.4530, -0.4381]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7228,  0.9786,  0.9700, -0.6275,  0.9935, -0.3658, -0.3815,  0.0028,\n",
            "         0.9587,  0.9471, -0.2801,  0.3257, -0.6047,  0.7413,  0.9002,  0.9634,\n",
            "         0.5197, -0.5384,  0.9553,  0.9574,  0.9841, -0.0642,  0.6350, -0.5181,\n",
            "         0.8687,  0.9680,  0.9932,  0.7296,  0.9472,  0.9829, -0.0460,  0.4219,\n",
            "         0.9800,  0.5340, -0.2262,  0.9851,  0.1979,  0.9818,  0.1884,  0.9657,\n",
            "         0.9948, -0.0209,  0.0015,  0.9911,  0.0748,  0.6673,  0.9774,  0.8799,\n",
            "         0.8148,  0.7054,  0.8215,  0.8320,  0.9911,  0.9944,  0.1613,  0.0036,\n",
            "        -0.5975,  0.9162,  0.9727,  0.9791, -0.2090,  0.2379,  0.9894,  0.9136],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 74==== Step 2 Train Loss 0.7548077702522278 ======  0.3214285714285714\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.5234,  1.0478,  0.6344,  ...,  0.0575, -0.6737, -0.2081],\n",
            "        [-0.5308,  1.2628,  0.2311,  ...,  0.0910, -0.2654, -0.4449],\n",
            "        [-0.8486,  1.0985,  0.3665,  ..., -0.1437,  0.0706, -0.4745],\n",
            "        ...,\n",
            "        [ 0.7912,  0.2414, -0.0117,  ...,  0.0406,  0.3236, -0.0156],\n",
            "        [ 0.5651, -0.3555, -0.1069,  ...,  0.1122,  0.6507, -0.3559],\n",
            "        [-0.6651,  1.1630,  0.2637,  ...,  0.1229, -0.3259, -0.4469]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9683,  0.9162,  0.9286, -0.7187, -0.5059,  0.9852,  0.9772,  0.5535,\n",
            "         0.9909,  0.9170,  0.9818,  0.9943,  0.2355,  0.1309,  0.8956,  0.9821,\n",
            "         0.3158, -0.5563,  0.6269,  0.7394,  0.9829,  0.5430,  0.9702, -0.3768,\n",
            "         0.8403,  0.8901,  0.2459,  0.8944,  0.4126,  0.3176,  0.8365,  0.9773,\n",
            "         0.5932,  0.7813,  0.9218,  0.3533,  0.9844, -0.0392,  0.9856, -0.1098,\n",
            "         0.9676,  0.9882,  0.9909,  0.9766,  0.9959,  0.9916, -0.0292,  0.9664,\n",
            "         0.9899,  0.9734,  0.8415,  0.9306,  0.9809,  0.5593,  0.2054, -0.1496,\n",
            "         0.7687,  0.9814,  0.7108,  0.4909,  0.9508,  0.6889,  0.9593, -0.5864],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARBUlEQVR4nO3dfYxldX3H8ffHXR5s1bLIhG5B3UVpCWnjYqZbWhof8AmxEUyJXVLt2tKsWm002laQP6qmptBUaZs22lWQbWsRukrY+lC7whJjothBF1igyIKYsl3ZUUQlTangt3/cM3qdndl7Z+bemf3J+5XczDm/c869nzl789kz5557b6oKSVJ7nrDSASRJi2OBS1KjLHBJapQFLkmNssAlqVGrl/PBjjvuuFq3bt1yPqQkNe/mm2/+ZlVNzB5f1gJft24dU1NTy/mQktS8JF+fa9xTKJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRQxd4klVJvpLkE938+iQ3Jdmb5OokR44vpiRptoUcgb8ZuLNv/lLgsqp6FvBt4IJRBpMkHdpQBZ7kRODlwIe6+QBnAtu7VbYB544joCRpbsO+E/OvgD8BntzNPxV4qKoe7ebvB06Ya8MkW4AtAE9/+tMXn1TS4866Cz+50hFG5r5LXj7y+xx4BJ7kN4ADVXXzYh6gqrZW1WRVTU5MHPRWfknSIg1zBH4G8IokZwNHA08B/ho4Jsnq7ij8RGDf+GJKkmYbeAReVRdV1YlVtQ7YBNxQVb8N7ALO61bbDFw3tpSSpIMs5TrwtwNvTbKX3jnxy0cTSZI0jAV9nGxV3Qjc2E3fC2wcfSRJ0jB8J6YkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHDfKnx0Um+lOSWJLcneVc3fmWSryXZ3d02jD+uJGnGMN/I8whwZlU9nOQI4PNJPt0t++Oq2j6+eJKk+Qws8Koq4OFu9ojuVuMMJUkabKhz4ElWJdkNHAB2VtVN3aL3JLk1yWVJjhpbSknSQYYq8Kp6rKo2ACcCG5P8InARcArwy8Cx9L6l/iBJtiSZSjI1PT09otiSpAVdhVJVDwG7gLOqan/1PAJ8mHm+ob6qtlbVZFVNTkxMLD2xJAkY7iqUiSTHdNNPBF4M/GeStd1YgHOBPeMMKkn6ccNchbIW2JZkFb3Cv6aqPpHkhiQTQIDdwOvHmFOSNMswV6HcCpw2x/iZY0kkSRqK78SUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRg3znZhHJ/lSkluS3J7kXd34+iQ3Jdmb5OokR44/riRpxjBH4I8AZ1bVs4ENwFlJTgcuBS6rqmcB3wYuGF9MSdJsAwu8eh7uZo/obgWcCWzvxrfR+2Z6SdIyGeoceJJVSXYDB4CdwD3AQ1X1aLfK/cAJ82y7JclUkqnp6elRZJYkMWSBV9VjVbUBOBHYCJwy7ANU1daqmqyqyYmJiUXGlCTNtqCrUKrqIWAX8KvAMUlWd4tOBPaNOJsk6RCGuQplIskx3fQTgRcDd9Ir8vO61TYD140rpCTpYKsHr8JaYFuSVfQK/5qq+kSSO4CPJvkz4CvA5WPMKUmaZWCBV9WtwGlzjN9L73y4JGkF+E5MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatQw34n5tCS7ktyR5PYkb+7G35lkX5Ld3e3s8ceVJM0Y5jsxHwXeVlVfTvJk4OYkO7tll1XVX44vniRpPsN8J+Z+YH83/b0kdwInjDuYJOnQFnQOPMk6el9wfFM39KYktya5IsmaebbZkmQqydT09PSSwkqSfmToAk/yJOBjwFuq6rvA+4FnAhvoHaG/d67tqmprVU1W1eTExMQIIkuSYMgCT3IEvfL+SFV9HKCqHqiqx6rqB8AHgY3jiylJmm2Yq1ACXA7cWVXv6xtf27faK4E9o48nSZrPMFehnAG8Brgtye5u7B3A+Uk2AAXcB7xuLAklSXMa5iqUzwOZY9GnRh9HkjQs34kpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRrmOzGflmRXkjuS3J7kzd34sUl2Jrm7+7lm/HElSTOGOQJ/FHhbVZ0KnA68McmpwIXA9VV1MnB9Ny9JWiYDC7yq9lfVl7vp7wF3AicA5wDbutW2AeeOK6Qk6WALOgeeZB1wGnATcHxV7e8WfQM4fp5ttiSZSjI1PT29hKiSpH5DF3iSJwEfA95SVd/tX1ZVBdRc21XV1qqarKrJiYmJJYWVJP3IUAWe5Ah65f2Rqvp4N/xAkrXd8rXAgfFElCTNZZirUAJcDtxZVe/rW7QD2NxNbwauG308SdJ8Vg+xzhnAa4Dbkuzuxt4BXAJck+QC4OvAq8YTUZI0l4EFXlWfBzLP4heONo4kaVi+E1OSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNcx3Yl6R5ECSPX1j70yyL8nu7nb2eGNKkmYb5gj8SuCsOcYvq6oN3e1To40lSRpkYIFX1eeAB5chiyRpAZZyDvxNSW7tTrGsmW+lJFuSTCWZmp6eXsLDSZL6LbbA3w88E9gA7AfeO9+KVbW1qiaranJiYmKRDydJmm1RBV5VD1TVY1X1A+CDwMbRxpIkDbKoAk+ytm/2lcCe+daVJI3H6kErJLkKeD5wXJL7gT8Fnp9kA1DAfcDrxphRkjSHgQVeVefPMXz5GLJIkhbAd2JKUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSowYWeJIrkhxIsqdv7NgkO5Pc3f1cM96YkqTZhjkCvxI4a9bYhcD1VXUycH03L0laRgMLvKo+Bzw4a/gcYFs3vQ04d8S5JEkDLPYc+PFVtb+b/gZw/HwrJtmSZCrJ1PT09CIfTpI025JfxKyqAuoQy7dW1WRVTU5MTCz14SRJncUW+ANJ1gJ0Pw+MLpIkaRiLLfAdwOZuejNw3WjiSJKGNcxlhFcBXwB+Icn9SS4ALgFenORu4EXdvCRpGa0etEJVnT/PoheOOIskaQF8J6YkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRg18K730eLDuwk+udARpwTwCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY1a0mWESe4Dvgc8BjxaVZOjCCVJGmwU14G/oKq+OYL7kSQtgKdQJKlRSy3wAv49yc1Jtsy1QpItSaaSTE1PTy/x4SRJM5Za4L9eVc8BXga8MclzZ69QVVurarKqJicmJpb4cJKkGUsq8Kra1/08AFwLbBxFKEnSYIsu8CQ/neTJM9PAS4A9owomSTq0pVyFcjxwbZKZ+/nnqvq3kaSSJA206AKvqnuBZ48wyyH9JH3c532XvHylI4zMT9K/i9QaLyOUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGjeILHbRAvntR0ih4BC5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqOWVOBJzkpyV5K9SS4cVShJ0mBL+VLjVcDfAS8DTgXOT3LqqIJJkg5tKUfgG4G9VXVvVf0f8FHgnNHEkiQNspS30p8A/Fff/P3Ar8xeKckWYEs3+3CSu4a47+OAby4h23Iz7/i1lrm1vNBe5qby5tIl5X3GXINj/yyUqtoKbF3INkmmqmpyTJFGzrzj11rm1vJCe5nNu7RTKPuAp/XNn9iNSZKWwVIK/D+Ak5OsT3IksAnYMZpYkqRBFn0KpaoeTfIm4DPAKuCKqrp9RLkWdMrlMGDe8Wstc2t5ob3Mj/u8qapR36ckaRn4TkxJapQFLkmNWpECT3Jskp1J7u5+rpljnRck2d13+98k53bLrkzytb5lGw6HzN16j/Xl2tE3vj7JTd3HDlzdvfC7onmTbEjyhSS3J7k1yW/1LVuWfTzo4xiSHNXtr73d/lvXt+yibvyuJC8dR75FZn5rkju6fXp9kmf0LZvz+bHCeV+bZLov1+/3LdvcPYfuTrL5MMl7WV/WryZ5qG/ZSuzfK5IcSLJnnuVJ8jfd73Nrkuf0LVva/q2qZb8BfwFc2E1fCFw6YP1jgQeBn+rmrwTOOxwzAw/PM34NsKmb/gDwhpXOC/w8cHI3/XPAfuCY5drH9F78vgc4CTgSuAU4ddY6fwB8oJveBFzdTZ/arX8UsL67n1XL8DwYJvML+p6rb5jJfKjnxwrnfS3wt3Nseyxwb/dzTTe9ZqXzzlr/D+ldQLEi+7d7zOcCzwH2zLP8bODTQIDTgZtGtX9X6hTKOcC2bnobcO6A9c8DPl1V/zPWVIe20Mw/lCTAmcD2xWy/SAPzVtVXq+rubvq/gQPAxJhz9Rvm4xj6f4/twAu7/XkO8NGqeqSqvgbs7e5vxTNX1a6+5+oX6b1HYqUs5SMvXgrsrKoHq+rbwE7grDHlnLHQvOcDV4050yFV1efoHWDO5xzgH6rni8AxSdYygv27UgV+fFXt76a/ARw/YP1NHPyP9J7uz5HLkhw18oQHGzbz0Ummknxx5pQP8FTgoap6tJu/n95HEYzTgvZxko30jnju6Rse9z6e6+MYZu+XH67T7b/v0Nufw2w7Dgt93AvoHX3NmOv5MU7D5v3N7t96e5KZN+itxD4e+jG7U1PrgRv6hpd7/w5jvt9pyft3bG+lT/JZ4GfnWHRx/0xVVZJ5r2Xs/qf6JXrXm8+4iF4pHUnv2sq3A+8+TDI/o6r2JTkJuCHJbfRKZ+RGvI//EdhcVT/ohseyjx9PkrwamASe1zd80POjqu6Z+x6Wzb8CV1XVI0leR+8vnjNXONMwNgHbq+qxvrHDcf+OzdgKvKpeNN+yJA8kWVtV+7vyOHCIu3oVcG1Vfb/vvmeOLB9J8mHgjw6XzFW1r/t5b5IbgdOAj9H7s2l1dxQ5ko8dGEXeJE8BPglc3P15N3PfY9nHswzzcQwz69yfZDXwM8C3htx2HIZ63CQvovcf6fOq6pGZ8XmeH+MsmIF5q+pbfbMfovf6ycy2z5+17Y0jT/jjFvLvugl4Y//ACuzfYcz3Oy15/67UKZQdwMwrrpuB6w6x7kHnuLpCmjm3fC4w56u/IzYwc5I1M6cakhwHnAHcUb1XLHbRO5c/7/YrkPdI4Fp65+e2z1q2HPt4mI9j6P89zgNu6PbnDmBTeleprAdOBr40howLzpzkNODvgVdU1YG+8TmfH4dB3rV9s68A7uymPwO8pMu9BngJP/6X8Irk7TKfQu+Fvy/0ja3E/h3GDuB3uqtRTge+0x0gLX3/Lvcrtt2rr08FrgfuBj4LHNuNTwIf6ltvHb3/pZ4wa/sbgNvolco/AU86HDIDv9bluqX7eUHf9ifRK5i9wL8ARx0GeV8NfB/Y3XfbsJz7mN4r9F+ld5R0cTf2bnrlB3B0t7/2dvvvpL5tL+62uwt42TI+fwdl/izwQN8+3THo+bHCef8cuL3LtQs4pW/b3+v2/V7gdw+HvN38O4FLZm23Uvv3KnpXcH2f3nnsC4DXA6/vlofel9/c0+WaHNX+9a30ktQo34kpSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1Kj/h+DCCRPfLvIsgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 75==== Step 2 Train Loss 0.7560520768165588 ======  0.3018867924528302\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0377,  1.1186,  0.3893,  ...,  0.0325, -0.1161, -0.5927],\n",
            "        [-1.4348,  1.0779,  0.6346,  ...,  0.0551, -0.4850, -0.3412],\n",
            "        [-1.0377,  1.1186,  0.3893,  ...,  0.0325, -0.1161, -0.5927],\n",
            "        ...,\n",
            "        [-1.2995,  1.0325,  0.5813,  ...,  0.0100, -0.4038, -0.3478],\n",
            "        [-1.1584,  1.1140,  0.3739,  ..., -0.0214, -0.3133, -0.4230],\n",
            "        [-0.7266,  1.2504,  0.4474,  ...,  0.0523, -0.2305, -0.5874]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9694,  0.9813, -0.6325,  0.8675,  0.4184,  0.0105,  0.5621,  0.8324,\n",
            "         0.9930, -0.0982,  0.9835, -0.3291,  0.3083,  0.6923,  0.9707, -0.3865,\n",
            "        -0.6553,  0.9170,  0.0911,  0.5250,  0.9614,  0.9857,  0.6812,  0.5834,\n",
            "         0.9859,  0.0593, -0.4285,  0.8605,  0.6800,  0.2310,  0.6362,  0.7174,\n",
            "        -0.5667,  0.9931,  0.4712,  0.9704,  0.9434,  0.5072,  0.5601,  0.6950,\n",
            "        -0.3545,  0.0864,  0.9941,  0.9777,  0.9832,  0.9891, -0.8083,  0.9515,\n",
            "         0.9221,  0.9193,  0.1234,  0.9784,  0.9830,  0.9917, -0.3500,  0.9839,\n",
            "         0.7235,  0.9658, -0.6329, -0.5733, -0.1031, -0.7695,  0.9427,  0.9815],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPYElEQVR4nO3dfYxldX3H8fdHEGyrLUuZbLf4MGBpDUnjYiaU1sYHfAJNBFNil0S7tjSrVhtNbdJV/qg1bYpNlaRpo65C2bYWpSBhW7R2BYwxUexgV1gguAtiutuVHUV8SFMq+O0f94zcDjN778592h++X8lkzv2dc+797G/vfvbMuefeSVUhSWrPk2YdQJK0Pha4JDXKApekRlngktQoC1ySGnX8NB/slFNOqfn5+Wk+pCQ177bbbvtmVc2tHJ9qgc/Pz7O4uDjNh5Sk5iX5+mrjnkKRpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGTfWdmJJ0NOa33zjrCGNz/2WvGvt9egQuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatTAAk/ylCRfSvKVJHcm+ZNu/LQktybZn+TjSU6YfFxJ0rJhjsAfBs6tqucCm4HzkpwDvBe4vKp+Afg2cMnkYkqSVhpY4NXz/e7mk7uvAs4Fru3GdwIXTiShJGlVQ50DT3Jckj3AYWA3cC/wUFU90m1yADh1MhElSasZqsCr6tGq2gw8HTgbeM6wD5BkW5LFJItLS0vrjClJWumorkKpqoeAW4BfBU5Ksvxphk8HDq6xz46qWqiqhbm5uZHCSpIeM8xVKHNJTuqWfwJ4GXA3vSK/qNtsK3DDpEJKkh5vmM8D3wTsTHIcvcK/pqr+JcldwMeS/CnwH8AVE8wpSVphYIFX1e3AWauM30fvfLgkaQZ8J6YkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRAws8yTOS3JLkriR3JnlbN/7uJAeT7Om+Xjn5uJKkZccPsc0jwDuq6stJngbclmR3t+7yqvrLycWTJK1lYIFX1SHgULf8vSR3A6dOOpgk6ciO6hx4knngLODWbuitSW5PcmWSDWvssy3JYpLFpaWlkcJKkh4zdIEneSpwHfD2qvou8AHg2cBmekfo71ttv6raUVULVbUwNzc3hsiSJBiywJM8mV55f7SqPgFQVQ9U1aNV9UPgw8DZk4spSVppmKtQAlwB3F1V7+8b39S32WuAveOPJ0layzBXoTwfeD1wR5I93di7gIuTbAYKuB9440QSSpJWNcxVKJ8HssqqT44/jiRpWL4TU5IaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDSzwJM9IckuSu5LcmeRt3fjJSXYn2dd93zD5uJKkZcMcgT8CvKOqzgTOAd6S5ExgO3BTVZ0B3NTdliRNycACr6pDVfXlbvl7wN3AqcAFwM5us53AhZMKKUl6vKM6B55kHjgLuBXYWFWHulXfADausc+2JItJFpeWlkaIKknqN3SBJ3kqcB3w9qr6bv+6qiqgVtuvqnZU1UJVLczNzY0UVpL0mKEKPMmT6ZX3R6vqE93wA0k2des3AYcnE1GStJphrkIJcAVwd1W9v2/VLmBrt7wVuGH88SRJazl+iG2eD7weuCPJnm7sXcBlwDVJLgG+Drx2MhElSasZWOBV9Xkga6x+yXjjSJKG5TsxJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRpY4EmuTHI4yd6+sXcnOZhkT/f1ysnGlCStNMwR+FXAeauMX15Vm7uvT443liRpkIEFXlWfAx6cQhZJ0lEY5Rz4W5Pc3p1i2bDWRkm2JVlMsri0tDTCw0mS+q23wD8APBvYDBwC3rfWhlW1o6oWqmphbm5unQ8nSVppXQVeVQ9U1aNV9UPgw8DZ440lSRpkXQWeZFPfzdcAe9faVpI0GccP2iDJ1cCLgFOSHAD+GHhRks1AAfcDb5xgRknSKgYWeFVdvMrwFRPIIkk6Cr4TU5IaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRr4S40ltWV++42zjqAp8QhckhplgUtSowYWeJIrkxxOsrdv7OQku5Ps675vmGxMSdJKwxyBXwWct2JsO3BTVZ0B3NTdliRN0cACr6rPAQ+uGL4A2Nkt7wQuHHMuSdIA6z0HvrGqDnXL3wA2rrVhkm1JFpMsLi0trfPhJEkrjfwiZlUVUEdYv6OqFqpqYW5ubtSHkyR11lvgDyTZBNB9Pzy+SJKkYay3wHcBW7vlrcAN44kjSRrWMJcRXg18AfilJAeSXAJcBrwsyT7gpd1tSdIUDXwrfVVdvMaql4w5iyTpKPhOTElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqIEfJ3usmN9+46wjjM39l71q1hEkPQF4BC5JjbLAJalRFrgkNcoCl6RGWeCS1KhmrkLRsemJcnWQVwapRR6BS1KjLHBJatRIp1CS3A98D3gUeKSqFsYRSpI02DjOgb+4qr45hvuRJB0FT6FIUqNGPQIv4N+SFPChqtqxcoMk24BtAM985jNHfLgnhifKlRuSZmvUI/Bfr6rnAecDb0nygpUbVNWOqlqoqoW5ubkRH06StGykAq+qg933w8D1wNnjCCVJGmzdBZ7kp5I8bXkZeDmwd1zBJElHNso58I3A9UmW7+cfq+pfx5JKkjTQugu8qu4DnjvGLJKko+BlhJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcpfqSbhB4ypTR6BS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNVKBJzkvyT1J9ifZPq5QkqTB1l3gSY4D/gY4HzgTuDjJmeMKJkk6slGOwM8G9lfVfVX1v8DHgAvGE0uSNMgovxPzVOA/+24fAH5l5UZJtgHbupvfT3LPCI85jFOAb074McbBnOPXSlZzjlcTOfNeYP1Zn7Xa4MR/qXFV7QB2TPpxliVZrKqFaT3eeplz/FrJas7xaiUnjD/rKKdQDgLP6Lv99G5MkjQFoxT4vwNnJDktyQnAFmDXeGJJkgZZ9ymUqnokyVuBTwPHAVdW1Z1jS7Z+UztdMyJzjl8rWc05Xq3khDFnTVWN8/4kSVPiOzElqVEWuCQ1qskCT3Jykt1J9nXfN6yyzYuT7On7+p8kF3brrkrytb51m2eVs9vu0b4su/rGT0tya/dRBR/vXiyeSc4km5N8IcmdSW5P8pt96yY6n4M+siHJid387O/ma75v3Tu78XuSvGKcudaR8w+S3NXN301JntW3btXnwAyzviHJUl+m3+1bt7V7ruxLsnXGOS/vy/jVJA/1rZvanCa5MsnhJHvXWJ8kf9X9OW5P8ry+deufz6pq7gv4C2B7t7wdeO+A7U8GHgR+srt9FXDRsZIT+P4a49cAW7rlDwJvnlVO4BeBM7rlnwcOASdNej7pvUB+L3A6cALwFeDMFdv8HvDBbnkL8PFu+cxu+xOB07r7OW6GOV/c9xx883LOIz0HZpj1DcBfr7LvycB93fcN3fKGWeVcsf3v07uYYhZz+gLgecDeNda/EvgUEOAc4NZxzGeTR+D03rK/s1veCVw4YPuLgE9V1X9PNNXjHW3OH0kS4Fzg2vXsf5QG5qyqr1bVvm75v4DDwNyE8vQb5iMb+vNfC7ykm78LgI9V1cNV9TVgf3d/M8lZVbf0PQe/SO+9E7MwysdgvALYXVUPVtW3gd3AecdIzouBqyeU5Yiq6nP0DhLXcgHwd9XzReCkJJsYcT5bLfCNVXWoW/4GsHHA9lt4/F/sn3U/ylye5MSxJ+wZNudTkiwm+eLyaR7gZ4GHquqR7vYBeh9fMMucACQ5m94R0b19w5Oaz9U+smHlPPxom26+vkNv/obZd5o5+11C74hs2WrPgUkZNutvdH+n1yZZftPeMTmn3emo04Cb+4anOaeDrPVnGWk+J/5W+vVK8hng51ZZdWn/jaqqJGteC9n9L/fL9K5XX/ZOekV1Ar3rMv8IeM8Mcz6rqg4mOR24Ockd9EpobMY8n38PbK2qH3bDY5vPHwdJXgcsAC/sG37cc6Cq7l39Hqbin4Grq+rhJG+k9xPOuTPMM8gW4NqqerRv7Fib07E7Zgu8ql661rokDyTZVFWHukI5fIS7ei1wfVX9oO++l482H07yt8AfzjJnVR3svt+X5LPAWcB19H7MOr47qhzpowrGkTPJTwM3Apd2PwYu3/fY5nMVw3xkw/I2B5IcD/wM8K0h951mTpK8lN5/mi+sqoeXx9d4DkyqbAZmrapv9d38CL3XSZb3fdGKfT879oSPPdawf39bgLf0D0x5TgdZ688y0ny2egplF7D8au1W4IYjbPu482JdSS2fZ74QWPWV4zEYmDPJhuVTDklOAZ4P3FW9VzhuoXf+fs39p5jzBOB6eufxrl2xbpLzOcxHNvTnvwi4uZu/XcCW9K5SOQ04A/jSGLMdVc4kZwEfAl5dVYf7xld9Dkwo57BZN/XdfDVwd7f8aeDlXeYNwMv5/z/dTjVnl/U59F4A/ELf2LTndJBdwG91V6OcA3ynO/AZbT6n9SrtOL/ond+8CdgHfAY4uRtfAD7St908vf/hnrRi/5uBO+gVzT8AT51VTuDXuixf6b5f0rf/6fQKZz/wT8CJM8z5OuAHwJ6+r83TmE96r+B/ld7R06Xd2HvoFSHAU7r52d/N1+l9+17a7XcPcP6En5eDcn4GeKBv/nYNeg7MMOufA3d2mW4BntO37+90c70f+O1Z5uxuvxu4bMV+U51TegeJh7p/IwfovcbxJuBN3frQ+wU493Z5FsYxn76VXpIa1eopFEn6sWeBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEb9H5vdhm+QdXR6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 76==== Step 2 Train Loss 0.7711712121963501 ======  0.22641509433962265\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.6327, -0.0543, -0.5367,  ...,  0.2751, -0.8200, -0.1399],\n",
            "        [-0.3374,  0.9135,  0.0524,  ..., -0.1916,  0.0949, -0.6658],\n",
            "        [ 0.5415, -0.4392, -0.0600,  ...,  0.0117,  0.4557, -0.0193],\n",
            "        ...,\n",
            "        [-1.3531,  1.1383,  0.5271,  ...,  0.0971, -0.6268, -0.3693],\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [ 0.8080, -0.2619, -0.4361,  ...,  0.2211, -0.4679,  0.1823]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8335,  0.4822,  0.9601,  0.9924,  0.7114, -0.4977,  0.9901,  0.9515,\n",
            "         0.9524,  0.8607, -0.0943, -0.7915, -0.5777,  0.9449,  0.8140,  0.0188,\n",
            "         0.9797,  0.3885,  0.0072,  0.5369,  0.2344,  0.8988,  0.5757, -0.8494,\n",
            "         0.9808, -0.0385,  0.9595,  0.9838,  0.9731,  0.9155,  0.9590,  0.9716,\n",
            "         0.8797,  0.9370,  0.9784,  0.9178,  0.3890,  0.9533,  0.9360,  0.7446,\n",
            "         0.7859,  0.9953,  0.9675,  0.6661,  0.9302,  0.9542,  0.3669, -0.8109,\n",
            "        -0.1341, -0.0572,  0.6529,  0.1171,  0.9856,  0.4606,  0.0404,  0.5477,\n",
            "        -0.3067, -0.8780,  0.0777,  0.8859,  0.9849,  0.9899,  0.9963, -0.4720],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 77==== Step 2 Train Loss 0.7290927171707153 ======  0.39999999999999997\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0122,  1.4354,  0.4911,  ...,  0.1031, -0.2712, -0.4033],\n",
            "        [ 0.2725,  0.7180, -0.2659,  ...,  0.1861, -0.5643, -0.1336],\n",
            "        [-0.8168,  1.1994,  0.2125,  ..., -0.0018, -0.4168, -0.4847],\n",
            "        ...,\n",
            "        [ 0.2905,  0.4028, -0.2786,  ...,  0.2503, -0.5889, -0.4088],\n",
            "        [-0.8306,  1.0393,  0.3829,  ..., -0.0665,  0.0087, -0.5656],\n",
            "        [ 0.5195, -0.6425,  0.0930,  ..., -0.0053,  0.5773,  0.0143]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.5795,  0.8274,  0.9554,  0.8069,  0.3582,  0.9908,  0.9868,  0.9824,\n",
            "         0.7545,  0.2984,  0.6704,  0.5665,  0.7147,  0.9161, -0.7179,  0.9675,\n",
            "        -0.3465,  0.8528,  0.8849,  0.2923,  0.7719, -0.5072,  0.6335,  0.9593,\n",
            "         0.3702,  0.6199,  0.9719, -0.1643,  0.9339, -0.4761, -0.3211,  0.7418,\n",
            "        -0.0645,  0.8037,  0.8868,  0.6750,  0.8923,  0.9540,  0.9500,  0.8506,\n",
            "         0.2029,  0.9964,  0.8082,  0.2042,  0.4246,  0.3572,  0.9292,  0.9872,\n",
            "         0.9905,  0.9819,  0.8050,  0.9909, -0.1564,  0.9756, -0.0757,  0.4507,\n",
            "        -0.5325,  0.9932, -0.5287,  0.5672, -0.1853,  0.8496,  0.8387,  0.9622],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 78==== Step 2 Train Loss 0.7240477800369263 ======  0.18181818181818182\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.7509,  1.2571,  0.1315,  ...,  0.0354, -0.5684, -0.4213],\n",
            "        [-0.9383,  1.2930,  0.3140,  ...,  0.0346, -0.1589, -0.5606],\n",
            "        [-1.3162,  1.0528,  0.7049,  ..., -0.0031, -0.3018, -0.3868],\n",
            "        ...,\n",
            "        [-1.4209,  0.9922,  0.4254,  ...,  0.0422, -0.6627, -0.4782],\n",
            "        [-0.4971,  1.2594,  0.2807,  ..., -0.0947, -0.0316, -0.5818],\n",
            "        [ 0.3719, -0.4709, -0.3351,  ...,  0.4447, -0.4224, -0.1388]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7365, -0.5550,  0.9912, -0.3776,  0.9498,  0.9933,  0.7884, -0.2397,\n",
            "         0.9910,  0.9885,  0.0233,  0.9936,  0.0652,  0.9720, -0.4124, -0.3919,\n",
            "         0.9894,  0.9737,  0.9773,  0.4629,  0.8456,  0.1241,  0.9673,  0.9895,\n",
            "         0.9948,  0.7428, -0.0016,  0.5742,  0.9897,  0.9918,  0.9573,  0.1749,\n",
            "        -0.0478,  0.9299,  0.8702,  0.8037,  0.3757,  0.9931,  0.6968,  0.1995,\n",
            "         0.9571,  0.9633,  0.8349,  0.9515, -0.3298,  0.4934, -0.8509,  0.4373,\n",
            "        -0.7638,  0.9434,  0.8426, -0.1176,  0.5469,  0.2249,  0.8484, -0.4123,\n",
            "         0.9298, -0.1591,  0.9892, -0.2697,  0.3461,  0.9839,  0.9509,  0.3591],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 79==== Step 2 Train Loss 0.7031583786010742 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0363,  1.2024,  0.3165,  ..., -0.0372, -0.1416, -0.5221],\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        [ 0.1460,  0.7597, -0.1854,  ..., -0.0323, -0.3095, -0.4144],\n",
            "        ...,\n",
            "        [-0.9852,  1.1821,  0.2202,  ..., -0.0543, -0.3842, -0.6609],\n",
            "        [ 0.4178, -0.0588, -0.3935,  ...,  0.1752, -0.8744,  0.1370],\n",
            "        [ 0.1602,  0.8499, -0.0942,  ...,  0.2114, -0.5358, -0.4550]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9859,  0.9936,  0.5107,  0.3704,  0.0367,  0.7996, -0.6492,  0.8063,\n",
            "        -0.1157,  0.7799,  0.9439,  0.9909,  0.1239,  0.7389,  0.8183,  0.9829,\n",
            "         0.9818,  0.9534,  0.5434, -0.3069, -0.5787,  0.9865,  0.9862,  0.9829,\n",
            "         0.8946,  0.9506, -0.7074,  0.9049,  0.9877, -0.1458,  0.4566,  0.4501,\n",
            "        -0.4345,  0.9485,  0.0399,  0.9902, -0.3567,  0.9857,  0.9736,  0.9662,\n",
            "         0.9099,  0.9806,  0.5105,  0.9958,  0.9953,  0.9893, -0.2648,  0.9570,\n",
            "         0.9119,  0.8533,  0.9293,  0.9864,  0.9053,  0.5707, -0.4701,  0.4667,\n",
            "        -0.4409,  0.9128, -0.0552, -0.2173,  0.3427,  0.9817, -0.2195,  0.7237],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 80==== Step 2 Train Loss 0.7450069785118103 ======  0.35714285714285715\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4876e+00,  1.0979e+00,  5.1740e-01,  ...,  8.6918e-02,\n",
            "         -6.3262e-01, -4.6162e-01],\n",
            "        [-1.1165e+00,  1.0646e+00,  5.7593e-01,  ...,  4.9428e-04,\n",
            "         -2.0331e-01, -3.4027e-01],\n",
            "        [ 5.7772e-01, -5.8791e-01,  1.0713e-01,  ...,  1.1626e-01,\n",
            "          7.3783e-01, -1.1353e-01],\n",
            "        ...,\n",
            "        [-1.2630e+00,  9.8283e-01,  4.4500e-01,  ...,  3.6629e-02,\n",
            "         -3.5585e-01, -4.3771e-01],\n",
            "        [-1.4109e+00,  1.0001e+00,  5.5658e-01,  ...,  5.6658e-03,\n",
            "         -4.2908e-01, -5.9147e-01],\n",
            "        [-1.1644e+00,  1.1991e+00,  5.2486e-01,  ..., -3.9258e-02,\n",
            "         -2.4873e-01, -5.6768e-01]], device='cuda:0')\n",
            "tensor([ 0.9921,  0.9782,  0.9161,  0.9894,  0.9724, -0.1301,  0.2029,  0.9517,\n",
            "         0.8029, -0.3506, -0.1342,  0.8621,  0.9929,  0.9914, -0.2737,  0.9891,\n",
            "         0.7469,  0.9127, -0.5527, -0.0825, -0.2880,  0.9492, -0.1054, -0.1859,\n",
            "         0.9696, -0.2059, -0.5065, -0.5815, -0.1960,  0.9811,  0.4069,  0.8356,\n",
            "        -0.0313, -0.5449, -0.0209,  0.8960,  0.9231,  0.8789,  0.9899,  0.7751,\n",
            "        -0.6189,  0.0662,  0.9846,  0.7909,  0.7858, -0.5464,  0.9958,  0.3932,\n",
            "         0.9720,  0.0940,  0.9672,  0.9723,  0.8151,  0.8014,  0.8987, -0.5966,\n",
            "        -0.6448,  0.9821, -0.3943, -0.7928,  0.9002, -0.7128,  0.9751, -0.4421],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 81==== Step 2 Train Loss 0.7127370238304138 ======  0.4482758620689655\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1660,  1.2503,  0.2727,  ..., -0.0540, -0.2665, -0.6168],\n",
            "        [-1.0593,  1.2020,  0.4404,  ..., -0.0745, -0.2726, -0.5630],\n",
            "        [-1.2710,  1.0923,  0.4392,  ...,  0.0119, -0.5537, -0.3883],\n",
            "        ...,\n",
            "        [-1.4348,  1.0779,  0.6346,  ...,  0.0551, -0.4850, -0.3412],\n",
            "        [-0.8247,  1.2377,  0.2788,  ..., -0.1285, -0.1999, -0.5552],\n",
            "        [-0.8285,  1.1976,  0.2564,  ..., -0.1177, -0.1841, -0.5822]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.3369,  0.6766, -0.7575,  0.9925,  0.2119,  0.8400,  0.9894,  0.3880,\n",
            "        -0.7883,  0.6718,  0.9384,  0.6295,  0.0548,  0.9889,  0.9436,  0.9625,\n",
            "         0.9506,  0.4362,  0.1950,  0.9656, -0.2945,  0.6937,  0.9761,  0.9795,\n",
            "        -0.2067,  0.8895,  0.9541,  0.2409,  0.9436,  0.6540,  0.9546,  0.7870,\n",
            "         0.8903, -0.5210,  0.9895,  0.9329,  0.9860,  0.9886,  0.9858,  0.8350,\n",
            "         0.9119,  0.7282, -0.1211, -0.4587,  0.9894,  0.2262,  0.8387,  0.9844,\n",
            "         0.9031,  0.9873, -0.7544,  0.5539, -0.0930, -0.8184, -0.1934,  0.9909,\n",
            "        -0.6700,  0.9224,  0.4196, -0.3741,  0.0197,  0.8358,  0.2367,  0.3538],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 82==== Step 2 Train Loss 0.7202576994895935 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [-1.4844,  1.0692,  0.5513,  ...,  0.0655, -0.6020, -0.2896],\n",
            "        [-1.0635,  1.0076,  0.6500,  ...,  0.0580, -0.1965, -0.4029],\n",
            "        ...,\n",
            "        [ 0.5307,  0.5958, -0.3495,  ...,  0.1906, -0.6946, -0.2351],\n",
            "        [-0.6442,  1.1782,  0.2434,  ...,  0.0233, -0.4369, -0.5772],\n",
            "        [ 0.3983, -0.3045,  0.0470,  ...,  0.0660,  0.5105, -0.3029]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9927,  0.9842,  0.9918,  0.0149, -0.7438,  0.9844, -0.8401,  0.8447,\n",
            "         0.9922,  0.6828,  0.8875,  0.0236, -0.0980,  0.9692, -0.1107,  0.3257,\n",
            "         0.5243,  0.9495,  0.8392,  0.9128, -0.0367,  0.5369,  0.9459,  0.9264,\n",
            "         0.9712,  0.6506,  0.8875,  0.9905,  0.8449,  0.9622, -0.2659,  0.5743,\n",
            "         0.0414,  0.9851,  0.8906,  0.6999,  0.9829, -0.6831,  0.5670,  0.9731,\n",
            "         0.9316,  0.0372,  0.8620,  0.3727,  0.8326,  0.8882,  0.4244,  0.9820,\n",
            "         0.9479,  0.9796,  0.8460,  0.9911,  0.2864,  0.9603,  0.9452, -0.7266,\n",
            "         0.9719,  0.1573,  0.6228,  0.8181,  0.9939,  0.9361,  0.9405, -0.4264],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 83==== Step 2 Train Loss 0.7522386908531189 ======  0.2857142857142857\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.0198,  0.8874, -0.1421,  ...,  0.1852, -0.2916, -0.4161],\n",
            "        [ 0.5209, -0.9021,  0.0292,  ...,  0.1750,  0.6252,  0.0852],\n",
            "        [ 0.7952,  0.1868,  0.1524,  ...,  0.1220,  0.4865, -0.1601],\n",
            "        ...,\n",
            "        [-1.4291,  1.0356,  0.5421,  ...,  0.0283, -0.6316, -0.2754],\n",
            "        [ 0.2157,  0.1355, -0.0691,  ...,  0.2422, -0.2970, -0.3497],\n",
            "        [-0.9159,  1.4036,  0.5151,  ..., -0.0381, -0.2807, -0.3692]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6546, -0.0209,  0.0089,  0.9712,  0.9695,  0.3624,  0.8160,  0.7439,\n",
            "         0.9880,  0.9735,  0.9846,  0.1067,  0.9808, -0.4578,  0.9524,  0.8694,\n",
            "         0.9932,  0.0058,  0.8310,  0.7800,  0.9729,  0.9325, -0.7289, -0.8493,\n",
            "         0.9857,  0.9693,  0.1599, -0.1112,  0.9861,  0.3532,  0.8886,  0.9913,\n",
            "         0.2461, -0.0042,  0.9272,  0.9932,  0.9910, -0.7870, -0.4909, -0.3127,\n",
            "         0.9878,  0.2370,  0.8539,  0.9677,  0.7299,  0.0030,  0.9017,  0.9644,\n",
            "         0.9690,  0.9883,  0.7584,  0.9422, -0.1025, -0.6179,  0.9808,  0.9925,\n",
            "         0.1069,  0.3456,  0.9911,  0.8060,  0.7791,  0.9734,  0.8861, -0.3207],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 84==== Step 2 Train Loss 0.7048099040985107 ======  0.36\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3170,  1.0739,  0.5085,  ...,  0.0322, -0.6405, -0.3679],\n",
            "        [-0.8265,  1.1467,  0.2594,  ..., -0.1064, -0.2028, -0.5914],\n",
            "        [ 0.7249,  0.1164, -0.0711,  ...,  0.0174,  0.0868,  0.1031],\n",
            "        ...,\n",
            "        [-1.4772,  1.0713,  0.5109,  ...,  0.0568, -0.6489, -0.3798],\n",
            "        [ 0.5862, -0.3358, -0.0459,  ...,  0.0875,  0.7939, -0.3583],\n",
            "        [ 0.3486,  0.8046, -0.3333,  ...,  0.0380, -0.6201, -0.4998]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9450,  0.9583, -0.3988,  0.9091,  0.8808, -0.1584,  0.8728,  0.6258,\n",
            "         0.8547,  0.0326,  0.6396,  0.4232,  0.5799,  0.9185,  0.9197,  0.5426,\n",
            "         0.1108,  0.7923,  0.5435,  0.9832,  0.9746,  0.2758,  0.7028,  0.9826,\n",
            "         0.5459,  0.2877,  0.9206,  0.9692,  0.6053,  0.9909, -0.7107,  0.5752,\n",
            "         0.9668, -0.1633,  0.2999,  0.7963, -0.0526,  0.9238,  0.9071,  0.9858,\n",
            "         0.9838,  0.9525, -0.1934,  0.9808,  0.8746,  0.8921, -0.1284,  0.2132,\n",
            "         0.9661,  0.9239,  0.8739,  0.9614, -0.4006,  0.7290,  0.0606,  0.9708,\n",
            "         0.1763,  0.5459, -0.1800, -0.1605,  0.5832,  0.2389,  0.9165,  0.2959],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 85==== Step 2 Train Loss 0.7337267398834229 ======  0.4210526315789474\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3104,  1.0613,  0.6367,  ..., -0.0392, -0.2553, -0.4147],\n",
            "        [ 0.8234,  0.0159, -0.4278,  ...,  0.0972, -0.1774, -0.2801],\n",
            "        [-1.4348,  1.0779,  0.6346,  ...,  0.0551, -0.4850, -0.3412],\n",
            "        ...,\n",
            "        [-0.4487,  0.9212, -0.0880,  ..., -0.2330, -0.5489, -0.4732],\n",
            "        [-0.3819,  0.9011,  0.2103,  ..., -0.0637,  0.0047, -0.6330],\n",
            "        [ 0.5829, -0.0029,  0.0511,  ..., -0.1490,  0.6723, -0.2594]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9872,  0.7650,  0.9824,  0.0241,  0.9778,  0.5779,  0.7982,  0.0923,\n",
            "         0.3359,  0.9310,  0.9646,  0.9886,  0.9935,  0.9690, -0.7724,  0.9931,\n",
            "         0.9425,  0.5335,  0.6208, -0.0142, -0.4243,  0.1301,  0.9762, -0.0552,\n",
            "         0.7565,  0.9907, -0.2416,  0.9187, -0.8644,  0.9791,  0.7960,  0.7983,\n",
            "         0.9909,  0.9605,  0.8058,  0.9429,  0.7928,  0.4522,  0.3614,  0.9093,\n",
            "         0.7267,  0.8266,  0.9124,  0.6090,  0.5971, -0.7738,  0.9710, -0.4981,\n",
            "         0.9951,  0.9657,  0.3868,  0.6994,  0.5406,  0.9540,  0.8384,  0.9227,\n",
            "         0.2629, -0.1602, -0.2590, -0.0111,  0.3270,  0.9467,  0.6477,  0.9494],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 86==== Step 2 Train Loss 0.7149463891983032 ======  0.48387096774193544\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5840, -0.5251,  0.0705,  ...,  0.0889,  0.6569, -0.1970],\n",
            "        [-0.2249,  0.9508, -0.1857,  ...,  0.0933, -0.5021, -0.6406],\n",
            "        [ 0.3703,  0.5603,  0.0423,  ...,  0.1453,  0.0414, -0.3923],\n",
            "        ...,\n",
            "        [-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052],\n",
            "        [ 0.8113, -0.2609, -0.0339,  ..., -0.1562,  0.6521, -0.1348],\n",
            "        [-0.1248,  0.8705, -0.0216,  ...,  0.0385, -0.2608, -0.6340]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9791,  0.9498,  0.5111,  0.9126,  0.7805,  0.7993,  0.9868, -0.0445,\n",
            "        -0.2091,  0.7832,  0.9898, -0.5515, -0.6329,  0.8950,  0.9685,  0.9178,\n",
            "         0.9755,  0.7661,  0.3404,  0.8996,  0.9764,  0.9934, -0.2810,  0.9521,\n",
            "         0.9030,  0.7444,  0.3443,  0.9662, -0.5094,  0.1664,  0.9737,  0.7618,\n",
            "         0.9921, -0.3109, -0.7073,  0.9356,  0.9295,  0.3964,  0.3403,  0.2074,\n",
            "         0.9683,  0.8538,  0.6531,  0.9917,  0.3007,  0.8488,  0.2142,  0.2903,\n",
            "         0.3974,  0.8465,  0.6315, -0.6838,  0.9577,  0.2367,  0.4028, -0.6321,\n",
            "         0.9897, -0.0448,  0.9539,  0.8645, -0.6342,  0.9941,  0.6530,  0.7777],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 87==== Step 2 Train Loss 0.7124118208885193 ======  0.46428571428571425\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4107,  1.1037,  0.4881,  ...,  0.0159, -0.7426, -0.2042],\n",
            "        [-1.4498,  1.1389,  0.5289,  ...,  0.1342, -0.5719, -0.2795],\n",
            "        [-1.3356,  1.0630,  0.6518,  ...,  0.0380, -0.3062, -0.3769],\n",
            "        ...,\n",
            "        [-0.6917,  0.9628,  0.2721,  ..., -0.0549, -0.0926, -0.5986],\n",
            "        [-1.3582,  1.1746,  0.4468,  ..., -0.0675, -0.3501, -0.4429],\n",
            "        [-1.0414,  1.3739,  0.6016,  ...,  0.0296, -0.2668, -0.3415]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1054, -0.5042, -0.8897, -0.0311,  0.0076,  0.8992,  0.5589,  0.9885,\n",
            "         0.9395, -0.5517,  0.9200,  0.5354,  0.9686,  0.9652,  0.8757, -0.4339,\n",
            "         0.9561,  0.9019,  0.9853,  0.6798, -0.5728,  0.9425,  0.8104,  0.8656,\n",
            "         0.7419,  0.8738,  0.9679,  0.9192,  0.4322,  0.9441, -0.7603,  0.9242,\n",
            "         0.7817,  0.9898,  0.0439, -0.1547,  0.9823, -0.7223, -0.4025, -0.2829,\n",
            "         0.9327,  0.9915,  0.9190,  0.1290,  0.7872,  0.8998,  0.8412, -0.1073,\n",
            "         0.1469,  0.7845,  0.8545,  0.9774,  0.2242, -0.4761,  0.7236,  0.9886,\n",
            "         0.9019, -0.6042,  0.6968,  0.4268, -0.6655,  0.8368, -0.0075, -0.1706],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 88==== Step 2 Train Loss 0.7140953540802002 ======  0.4814814814814815\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.5099,  0.9884,  0.2797,  ...,  0.0155, -0.1420, -0.4038],\n",
            "        [-0.9531,  1.3404,  0.3884,  ..., -0.0310, -0.1275, -0.5755],\n",
            "        [-0.8257,  1.3505,  0.1982,  ..., -0.0217, -0.3555, -0.5510],\n",
            "        ...,\n",
            "        [-1.1503,  1.3181,  0.4642,  ...,  0.0609, -0.4010, -0.4159],\n",
            "        [ 0.4815,  0.2858, -0.4466,  ...,  0.1091, -0.7968,  0.0141],\n",
            "        [-1.5528,  0.9868,  0.6148,  ...,  0.0784, -0.6554, -0.2778]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8115,  0.9734,  0.9130,  0.9076, -0.7390,  0.2778,  0.9843,  0.1452,\n",
            "        -0.5120,  0.6377,  0.9869,  0.9266,  0.9335,  0.7253,  0.8767,  0.9757,\n",
            "        -0.3176,  0.9551,  0.4805,  0.7897,  0.9913,  0.9805,  0.9832,  0.8870,\n",
            "        -0.4547, -0.1889,  0.2500, -0.3002,  0.9439,  0.9909,  0.8916,  0.9183,\n",
            "         0.8746,  0.1492, -0.8419, -0.4189,  0.9677, -0.1797,  0.9807,  0.9690,\n",
            "         0.2568,  0.7818,  0.1920,  0.2253,  0.4258,  0.9943, -0.6615,  0.4199,\n",
            "         0.6053,  0.9690, -0.1052,  0.9917,  0.9902,  0.2882,  0.4922, -0.6210,\n",
            "         0.9800, -0.3489,  0.9431,  0.3514, -0.8081, -0.0463, -0.0342,  0.9739],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 89==== Step 2 Train Loss 0.7136375904083252 ======  0.31111111111111117\n",
            "torch.Size([64, 48])\n",
            "tensor([[-9.9244e-01,  1.2704e+00,  4.8349e-01,  ..., -1.5161e-01,\n",
            "         -3.5789e-01, -4.5589e-01],\n",
            "        [-1.2882e+00,  1.1016e+00,  5.7636e-01,  ..., -5.9641e-02,\n",
            "         -4.0654e-01, -4.5207e-01],\n",
            "        [-1.2708e+00,  1.0498e+00,  3.9554e-01,  ..., -9.8638e-04,\n",
            "         -6.9632e-01, -4.4897e-01],\n",
            "        ...,\n",
            "        [-4.1394e-01,  1.2744e+00,  3.7555e-01,  ...,  1.2386e-02,\n",
            "         -1.4310e-01, -5.8040e-01],\n",
            "        [-8.9876e-01,  1.0355e+00,  2.7910e-01,  ..., -6.4379e-02,\n",
            "         -4.6296e-01, -6.2980e-01],\n",
            "        [ 1.3679e-01,  5.5502e-01,  1.0061e-01,  ...,  5.3668e-02,\n",
            "          5.7972e-02, -4.3195e-01]], device='cuda:0')\n",
            "tensor([ 0.9815,  0.9939,  0.0663,  0.9695,  0.9485,  0.9531, -0.6438,  0.9114,\n",
            "        -0.5474,  0.9206, -0.0439,  0.8436, -0.3935,  0.6102,  0.9840,  0.7887,\n",
            "         0.9911, -0.4770, -0.7209,  0.9843,  0.8569,  0.9561,  0.9924,  0.9237,\n",
            "         0.8451,  0.9771, -0.2876,  0.6137,  0.8866,  0.2300,  0.9729,  0.9581,\n",
            "         0.9695,  0.7033,  0.1932,  0.8588, -0.8577,  0.1914,  0.1123, -0.4603,\n",
            "        -0.3179,  0.9913,  0.8938,  0.6413, -0.4327,  0.9604, -0.4595,  0.9118,\n",
            "         0.9846, -0.3935,  0.9565,  0.6296,  0.0617,  0.6035,  0.9841, -0.5405,\n",
            "         0.9944,  0.9763,  0.7762,  0.8153, -0.7092,  0.8029,  0.9781,  0.5655],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQJ0lEQVR4nO3df6zdd13H8eeLdj9Q0LXuptYN6IbTZdHQkWudYvgxfg1IWIkLdglYdKaAYCCiobA/BCJxGGGJ0QCFjVXFwSwsqwzEspUQEhjeYde1m6PdGHG1rBfGgMVYWXn7x/leONzd23N67zn37lOfj+Tkfs/n+/2e8+rn3Lx67vd8zzmpKiRJ7XnCcgeQJC2MBS5JjbLAJalRFrgkNcoCl6RGrVzKOzvzzDNr3bp1S3mXktS822+//VtVNTF7fEkLfN26dUxNTS3lXUpS85J8Y65xD6FIUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjlvSdmJJ0ItZtvXm5I4zM/Ve9bOS36TNwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaNbDAk5ye5CtJ7kiyP8k7u/Hrknw9yZ7usn78cSVJM4Z5I89R4OKqeiTJKcAXk3ymW/enVbVjfPEkSfMZWOBVVcAj3dVTukuNM5QkabChjoEnWZFkD3AE2FVVt3Wr3p1kb5Krk5w2z75bkkwlmZqenh5RbEnSUAVeVceqaj1wNrAhya8AbwPOB34NWA28dZ59t1XVZFVNTkxMjCi2JOmEzkKpqoeB3cAlVXW4eo4CHwE2jCOgJGluw5yFMpHkjG75icALgf9IsrYbC7AR2DfOoJKknzTMWShrge1JVtAr/Buq6lNJbk0yAQTYA7xujDklSbMMcxbKXuDCOcYvHksiSdJQfCemJDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KhhvpX+9CRfSXJHkv1J3tmNn5PktiQHk3w8yanjjytJmjHMM/CjwMVV9QxgPXBJkouA9wBXV9UvAt8BrhhfTEnSbAMLvHoe6a6e0l0KuBjY0Y1vBzaOJaEkaU5DHQNPsiLJHuAIsAu4F3i4qh7tNnkAOGuefbckmUoyNT09PYrMkiSGLPCqOlZV64GzgQ3A+cPeQVVtq6rJqpqcmJhYYExJ0mwndBZKVT0M7AZ+Azgjycpu1dnAoRFnkyQdxzBnoUwkOaNbfiLwQuBuekV+WbfZZuCmcYWUJD3WysGbsBbYnmQFvcK/oao+leQu4GNJ/hz4d+CaMeaUJM0ysMCrai9w4Rzj99E7Hi5JWga+E1OSGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1DDfSv+UJLuT3JVkf5I3dePvSHIoyZ7u8tLxx5UkzRjmW+kfBd5SVV9N8mTg9iS7unVXV9VfjS+eJGk+w3wr/WHgcLf8/SR3A2eNO5gk6fhO6Bh4knXAhcBt3dAbk+xNcm2SVfPssyXJVJKp6enpRYWVJP3Y0AWe5EnAJ4A3V9X3gPcDTwfW03uG/t659quqbVU1WVWTExMTI4gsSYIhCzzJKfTK+6NV9UmAqnqwqo5V1Q+BDwEbxhdTkjTbMGehBLgGuLuq3tc3vrZvs1cA+0YfT5I0n2HOQnkW8GrgziR7urG3A5cnWQ8UcD/w2rEklCTNaZizUL4IZI5Vnx59HEnSsHwnpiQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktSoYb6V/ilJdie5K8n+JG/qxlcn2ZXkQPdz1fjjSpJmDPMM/FHgLVV1AXAR8IYkFwBbgVuq6jzglu66JGmJDCzwqjpcVV/tlr8P3A2cBVwKbO822w5sHFdISdJjndAx8CTrgAuB24A1VXW4W/VNYM08+2xJMpVkanp6ehFRJUn9hi7wJE8CPgG8uaq+17+uqgqoufarqm1VNVlVkxMTE4sKK0n6saEKPMkp9Mr7o1X1yW74wSRru/VrgSPjiShJmsswZ6EEuAa4u6re17dqJ7C5W94M3DT6eJKk+awcYptnAa8G7kyypxt7O3AVcEOSK4BvAK8cT0RJ0lwGFnhVfRHIPKufP9o4kqRh+U5MSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHDfCv9tUmOJNnXN/aOJIeS7OkuLx1vTEnSbMM8A78OuGSO8auran13+fRoY0mSBhlY4FX1BeChJcgiSToBizkG/sYke7tDLKvm2yjJliRTSaamp6cXcXeSpH4LLfD3A08H1gOHgffOt2FVbauqyaqanJiYWODdSZJmW1CBV9WDVXWsqn4IfAjYMNpYkqRBFlTgSdb2XX0FsG++bSVJ47Fy0AZJrgeeC5yZ5AHgz4DnJlkPFHA/8NoxZpQkzWFggVfV5XMMXzOGLJKkE+A7MSWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGDSzwJNcmOZJkX9/Y6iS7khzofq4ab0xJ0mzDPAO/Drhk1thW4JaqOg+4pbsuSVpCAwu8qr4APDRr+FJge7e8Hdg44lySpAFWLnC/NVV1uFv+JrBmvg2TbAG2ADz1qU9d4N2dXNZtvXm5I4zM/Ve9bLkjSP9vLfpFzKoqoI6zfltVTVbV5MTExGLvTpLUWWiBP5hkLUD388joIkmShrHQAt8JbO6WNwM3jSaOJGlYw5xGeD3wJeCXkzyQ5ArgKuCFSQ4AL+iuS5KW0MAXMavq8nlWPX/EWSRJJ8B3YkpSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGrXQb6WX9Di1buvNyx1BS8Rn4JLUKAtckhq1qEMoSe4Hvg8cAx6tqslRhJIkDTaKY+DPq6pvjeB2JEknwEMoktSoxRZ4Af+a5PYkW+baIMmWJFNJpqanpxd5d5KkGYst8N+qqmcCLwHekOTZszeoqm1VNVlVkxMTE4u8O0nSjEUVeFUd6n4eAW4ENowilCRpsAUXeJKfTvLkmWXgRcC+UQWTJB3fYs5CWQPcmGTmdv6xqv5lJKkkSQMtuMCr6j7gGSPMcly+PVjj5O+XWuRphJLUKAtckhplgUtSoyxwSWqUBS5JjfILHbQonr0hLR+fgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVqUQWe5JIk9yQ5mGTrqEJJkgZbcIEnWQH8LfAS4ALg8iQXjCqYJOn4FvMMfANwsKruq6r/BT4GXDqaWJKkQRbzhQ5nAf/Zd/0B4Ndnb5RkC7Clu/pIknsWcZ+jcCbwrWXOMIgZR8OMo2HGEch7FpXxaXMNjv0beapqG7Bt3PczrCRTVTW53DmOx4yjYcbRMONojCPjYg6hHAKe0nf97G5MkrQEFlPg/wacl+ScJKcCm4Cdo4klSRpkwYdQqurRJG8EPgusAK6tqv0jSzY+j5vDOcdhxtEw42iYcTRGnjFVNerblCQtAd+JKUmNssAlqVEnZYEnWZ1kV5ID3c9Vc2zzvCR7+i7/k2Rjt+66JF/vW7d+OTJ22x3ry7Gzb/ycJLd1H2Pw8e6F5CXPmGR9ki8l2Z9kb5Lf6Vs3tnkc9DEOSU7r5uVgN0/r+ta9rRu/J8mLR5VpARn/OMld3bzdkuRpfevmfNyXIeNrkkz3ZfmDvnWbu9+NA0k2L2PGq/vyfS3Jw33rlmoer01yJMm+edYnyV93/4a9SZ7Zt27h81hVJ90F+Etga7e8FXjPgO1XAw8BP9Vdvw647PGQEXhknvEbgE3d8geA1y9HRuCXgPO65V8ADgNnjHMe6b1ofi9wLnAqcAdwwaxt/hD4QLe8Cfh4t3xBt/1pwDnd7axYpozP6/ude/1MxuM97suQ8TXA38yx72rgvu7nqm551XJknLX9H9E7oWLJ5rG7n2cDzwT2zbP+pcBngAAXAbeNYh5Pymfg9N7Sv71b3g5sHLD9ZcBnquq/x5rqJ51oxh9JEuBiYMdC9j8BAzNW1deq6kC3/F/AEWBiDFn6DfMxDv3ZdwDP7+btUuBjVXW0qr4OHOxub8kzVtXuvt+5L9N7L8VSWszHYbwY2FVVD1XVd4BdwCWPg4yXA9ePIcdxVdUX6D0JnM+lwN9Vz5eBM5KsZZHzeLIW+JqqOtwtfxNYM2D7TTz2QX9396fO1UlOG3nC4TOenmQqyZdnDvEAPwc8XFWPdtcfoPfRBsuVEYAkG+g9S7q3b3gc8zjXxzjM/vf/aJtunr5Lb96G2XepMva7gt4ztBlzPe6jNmzG3+4ewx1JZt6897ibx+4Q1DnArX3DSzGPw5jv37GoeRz7W+nHJcnngJ+fY9WV/VeqqpLMe65k97/gr9I7n33G2+gV1qn0zt18K/CuZcr4tKo6lORc4NYkd9Iro5EY8Tz+PbC5qn7YDY9kHk92SV4FTALP6Rt+zONeVffOfQtj9c/A9VV1NMlr6f1Vc/Ey5BjGJmBHVR3rG3u8zONYNFvgVfWC+dYleTDJ2qo63BXLkePc1CuBG6vqB323PfOs82iSjwB/slwZq+pQ9/O+JJ8HLgQ+Qe9PsJXds8sFf4zBKDIm+RngZuDK7s/DmdseyTzOYZiPcZjZ5oEkK4GfBb495L5LlZEkL6D3n+VzqurozPg8j/uoi2dgxqr6dt/VD9N7XWRm3+fO2vfzI843cz/DPl6bgDf0DyzRPA5jvn/HoubxZD2EshOYeTV3M3DTcbZ9zDGzrqxmjjVvBOZ8ZXncGZOsmjnskORM4FnAXdV79WM3vWP38+6/RBlPBW6kd3xvx6x145rHYT7GoT/7ZcCt3bztBDald5bKOcB5wFdGlOuEMia5EPgg8PKqOtI3PufjvkwZ1/ZdfTlwd7f8WeBFXdZVwIv4yb9ilyxjl/N8ei8CfqlvbKnmcRg7gd/tzka5CPhu9wRncfO4FK/QLvWF3rHOW4ADwOeA1d34JPDhvu3W0fsf8Amz9r8VuJNe4fwD8KTlyAj8Zpfjju7nFX37n0uveA4C/wSctkwZXwX8ANjTd1k/7nmk96r+1+g9m7qyG3sXvTIEOL2bl4PdPJ3bt++V3X73AC8Z4+/hoIyfAx7sm7edgx73Zcj4F8D+Lstu4Py+fX+/m9+DwO8tV8bu+juAq2btt5TzeD29M7B+QO849hXA64DXdetD7wtw7u2yTI5iHn0rvSQ16mQ9hCJJJz0LXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXq/wBBrtrcYK/+SAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 90==== Step 2 Train Loss 0.7550629377365112 ======  0.16666666666666666\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3073,  1.0413,  0.5789,  ..., -0.0993, -0.3444, -0.4910],\n",
            "        [-0.5873,  1.2090,  0.0806,  ..., -0.1192, -0.3044, -0.6091],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930],\n",
            "        ...,\n",
            "        [-0.1393,  1.2867,  0.0997,  ..., -0.0567, -0.3639, -0.4851],\n",
            "        [ 0.5195, -0.6425,  0.0930,  ..., -0.0053,  0.5773,  0.0143],\n",
            "        [ 0.5503, -0.0166, -0.4326,  ...,  0.1222, -0.8772, -0.0227]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9446, -0.7288,  0.7654,  0.9887, -0.0659, -0.3107,  0.4173,  0.9901,\n",
            "         0.8673, -0.2050, -0.1656, -0.3027,  0.9656, -0.1869,  0.9861,  0.9946,\n",
            "        -0.6015,  0.9146,  0.9665,  0.9881,  0.1403,  0.9865,  0.8743,  0.9440,\n",
            "         0.9935, -0.2477,  0.9868,  0.9380,  0.7748,  0.9851,  0.9767, -0.7148,\n",
            "         0.9541,  0.8737, -0.7196,  0.9000,  0.9118, -0.5073,  0.7571,  0.9592,\n",
            "         0.9465,  0.9748,  0.7782,  0.5521,  0.8153,  0.9880,  0.8826, -0.6325,\n",
            "         0.4608,  0.9909,  0.3725,  0.9363,  0.9588,  0.9940,  0.9872,  0.8961,\n",
            "         0.9941,  0.3343, -0.2890,  0.9935,  0.7913,  0.8588,  0.0036, -0.1179],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 91==== Step 2 Train Loss 0.7486357092857361 ======  0.25531914893617014\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.7451, -0.2130,  0.0425,  ..., -0.0595,  0.4277,  0.1291],\n",
            "        [ 0.9208, -0.3942, -0.2488,  ...,  0.1091,  0.4275, -0.0849],\n",
            "        [ 0.3703,  0.5603,  0.0423,  ...,  0.1453,  0.0414, -0.3923],\n",
            "        ...,\n",
            "        [-0.9952,  1.1996,  0.3800,  ..., -0.0753, -0.3678, -0.5027],\n",
            "        [ 0.0228,  0.9145, -0.0177,  ..., -0.1188, -0.0558, -0.4685],\n",
            "        [-1.2882,  1.1016,  0.5764,  ..., -0.0596, -0.4065, -0.4521]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7449,  0.9255,  0.2955,  0.9420,  0.4173, -0.3253, -0.6354,  0.9171,\n",
            "        -0.1055,  0.9717,  0.7417, -0.3446,  0.8326,  0.6051, -0.3128, -0.8041,\n",
            "         0.9162,  0.7760,  0.6433,  0.9325,  0.9558,  0.9855,  0.9761, -0.1945,\n",
            "         0.9037, -0.6665,  0.9559, -0.6755, -0.1734, -0.5424,  0.9407, -0.6290,\n",
            "         0.9214,  0.9553,  0.9914, -0.2192,  0.8943,  0.3716,  0.9544,  0.6860,\n",
            "         0.9588,  0.9802, -0.6325,  0.9943, -0.4294, -0.1865,  0.9851,  0.5600,\n",
            "        -0.0394,  0.3948,  0.9799,  0.6279, -0.3532,  0.9306,  0.9802,  0.9547,\n",
            "         0.9955, -0.1940,  0.0974, -0.4500,  0.9597,  0.9555,  0.6831,  0.9682],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 92==== Step 2 Train Loss 0.6785169839859009 ======  0.5357142857142857\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4919e+00,  9.9196e-01,  5.2432e-01,  ...,  5.9810e-02,\n",
            "         -6.8246e-01, -3.5261e-01],\n",
            "        [ 1.6564e-01,  7.8193e-01, -2.2008e-01,  ...,  1.5986e-04,\n",
            "         -6.8573e-01, -3.0539e-01],\n",
            "        [-1.2279e+00,  1.1347e+00,  5.3740e-01,  ..., -3.8596e-02,\n",
            "         -3.0396e-01, -5.1748e-01],\n",
            "        ...,\n",
            "        [ 5.0854e-01,  5.0212e-01, -2.8084e-01,  ...,  1.6881e-01,\n",
            "         -5.5637e-01, -3.5176e-01],\n",
            "        [-3.7234e-01,  8.0262e-01, -5.3231e-02,  ...,  7.0549e-02,\n",
            "         -4.7371e-02, -6.2707e-01],\n",
            "        [ 5.0825e-01,  4.0675e-01, -3.8018e-01,  ...,  3.8402e-01,\n",
            "         -7.0926e-01, -3.5356e-01]], device='cuda:0')\n",
            "tensor([-0.3323,  0.8255, -0.8549,  0.9580,  0.8392,  0.9976,  0.9821,  0.9779,\n",
            "         0.8067,  0.9857, -0.6754,  0.8909, -0.7226, -0.5601,  0.9907,  0.8225,\n",
            "         0.6486, -0.0136,  0.9862, -0.6612,  0.7530,  0.8235,  0.6885, -0.7356,\n",
            "        -0.3181,  0.9393,  0.6480,  0.6477,  0.9899,  0.2457,  0.9875,  0.8142,\n",
            "         0.9610,  0.7727,  0.9333,  0.7863,  0.8945, -0.2288,  0.0242, -0.7236,\n",
            "         0.5791, -0.3027,  0.9124,  0.9871,  0.9954, -0.1217,  0.8119, -0.6884,\n",
            "         0.6027,  0.4981,  0.7780,  0.9856,  0.9855,  0.9639,  0.9886, -0.2918,\n",
            "         0.8117,  0.1675,  0.9848,  0.7947, -0.0667,  0.5248,  0.4920,  0.1990],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQO0lEQVR4nO3df6zdd13H8eeLdT9QkLXuptaN0Q2my6KhI9c6xfBj/BqQsBIX7BKw6EwBwUBEQ2GJApE4jLDESIDCxqriYBaWVX6IpSshJDC8w25rN0e7MeJqWS+MAYuxsvL2j/O9cLi7t+f03nPu3ac+H8nJ/Z7P9/s959XPaV793u/5ntNUFZKk9jxuuQNIkhbGApekRlngktQoC1ySGmWBS1KjVizlk51xxhm1du3apXxKSWrerbfe+u2qmpg9vqQFvnbtWqamppbyKSWpeUm+Ode4p1AkqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSowYWeJLTknw1yW1J9iV5Rzd+XZJvJNnT3daNP64kacYw14EfAS6uqoeTnAx8Kclnu3V/WlXbxxdPkjSfgQVevS8Mf7i7e3J380vEJWmZDfVJzCQnAbcCTwPeV1W3JHkd8K4kfwbsArZU1ZE59t0MbAY4++yzRxZc0olv7ZZPL3eEkbnvqpeO/DGHehOzqo5W1TrgLGB9kl8B3gqcD/wasAp4yzz7bq2qyaqanJh41Ef5JUkLdFxXoVTVQ8Bu4JKqOlQ9R4CPAOvHEVCSNLdhrkKZSHJ6t/x44AXAfyRZ040F2ADsHWdQSdJPG+Yc+BpgW3ce/HHADVX1qSQ3J5kAAuwBXjvGnJKkWYa5CuV24MI5xi8eSyJJ0lD8JKYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho1sMCTnJbkq0luS7IvyTu68XOS3JLkQJKPJzll/HElSTOGOQI/AlxcVU8H1gGXJLkIeDdwdVU9DfgucMX4YkqSZhtY4NXzcHf35O5WwMXA9m58G7BhLAklSXMa6hx4kpOS7AEOAzuBe4CHquqRbpP7gTPn2XdzkqkkU9PT06PILEliyAKvqqNVtQ44C1gPnD/sE1TV1qqarKrJiYmJBcaUJM12XFehVNVDwG7gN4DTk6zoVp0FHBxxNknSMQxzFcpEktO75ccDLwDuolfkl3WbbQJuGldISdKjrRi8CWuAbUlOolf4N1TVp5LcCXwsyV8A/w5cM8ackqRZBhZ4Vd0OXDjH+L30zodLkpaBn8SUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatTAAk/y5CS7k9yZZF+SN3bjb09yMMme7vaS8ceVJM1YMcQ2jwBvrqqvJXkicGuSnd26q6vqr8cXT5I0n4EFXlWHgEPd8g+S3AWcOe5gkqRjO65z4EnWAhcCt3RDb0hye5Jrk6ycZ5/NSaaSTE1PTy8qrCTpJ4Yu8CRPAD4BvKmqvg+8H3gqsI7eEfp75tqvqrZW1WRVTU5MTIwgsiQJhizwJCfTK++PVtUnAarqgao6WlU/Aj4ErB9fTEnSbMNchRLgGuCuqnpv3/iavs1eDuwdfTxJ0nyGuQrlmcCrgDuS7OnG3gZcnmQdUMB9wGvGklCSNKdhrkL5EpA5Vn1m9HEkScPyk5iS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWrUwAJP8uQku5PcmWRfkjd246uS7Eyyv/u5cvxxJUkzhjkCfwR4c1VdAFwEvD7JBcAWYFdVnQfs6u5LkpbIwAKvqkNV9bVu+QfAXcCZwKXAtm6zbcCGcYWUJD3acZ0DT7IWuBC4BVhdVYe6Vd8CVs+zz+YkU0mmpqenFxFVktRv6AJP8gTgE8Cbqur7/euqqoCaa7+q2lpVk1U1OTExsaiwkqSfGKrAk5xMr7w/WlWf7IYfSLKmW78GODyeiJKkuQxzFUqAa4C7quq9fat2AJu65U3ATaOPJ0maz4ohtnkm8CrgjiR7urG3AVcBNyS5Avgm8IrxRJQkzWVggVfVl4DMs/p5o40jSRqWn8SUpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatTAAk9ybZLDSfb2jb09ycEke7rbS8YbU5I02zBH4NcBl8wxfnVVretunxltLEnSIAMLvKq+CDy4BFkkScdhMefA35Dk9u4Uy8r5NkqyOclUkqnp6elFPJ0kqd9CC/z9wFOBdcAh4D3zbVhVW6tqsqomJyYmFvh0kqTZFlTgVfVAVR2tqh8BHwLWjzaWJGmQBRV4kjV9d18O7J1vW0nSeKwYtEGS64HnAGckuR/4c+A5SdYBBdwHvGaMGSVJcxhY4FV1+RzD14whiyTpOPhJTElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNWpggSe5NsnhJHv7xlYl2Zlkf/dz5XhjSpJmG+YI/DrgklljW4BdVXUesKu7L0laQgMLvKq+CDw4a/hSYFu3vA3YMOJckqQBFnoOfHVVHeqWvwWsHlEeSdKQFv0mZlUVUPOtT7I5yVSSqenp6cU+nSSps9ACfyDJGoDu5+H5NqyqrVU1WVWTExMTC3w6SdJsCy3wHcCmbnkTcNNo4kiShjXMZYTXA18GfjnJ/UmuAK4CXpBkP/D87r4kaQmtGLRBVV0+z6rnjTiLJOk4+ElMSWrUwCNwSW1Zu+XTyx1BS8QjcElqlAUuSY2ywCWpURa4JDWqmTcxT6Q3Zu676qXLHUHSCcAjcElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqEV9nWyS+4AfAEeBR6pqchShJEmDjeL7wJ9bVd8eweNIko6Dp1AkqVGLPQIv4F+TFPDBqto6e4Mkm4HNAGefffYin06PNSfK/5Tk/5KkFi32CPy3quoZwIuB1yd51uwNqmprVU1W1eTExMQin06SNGNRBV5VB7ufh4EbgfWjCCVJGmzBBZ7kZ5M8cWYZeCGwd1TBJEnHtphz4KuBG5PMPM4/VtW/jCSVJGmgBRd4Vd0LPH2EWSRJx2EU14HrOJ0oV25IWl5eBy5JjbLAJalRFrgkNcoCl6RGWeCS1CivQpHwyiC1ySNwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktSoRRV4kkuS3J3kQJItowolSRpswQWe5CTgfcCLgQuAy5NcMKpgkqRjW8wR+HrgQFXdW1X/C3wMuHQ0sSRJgyzmf+Q5E/jPvvv3A78+e6Mkm4HN3d2Hk9y9iOdcrDOAby/j8w+jhYzQRk4zjoYZRyDvXlTGp8w1OPb/Uq2qtgJbx/08w0gyVVWTy53jWFrICG3kNONomHE0xpFxMadQDgJP7rt/VjcmSVoCiynwfwPOS3JOklOAjcCO0cSSJA2y4FMoVfVIkjcAnwNOAq6tqn0jSzYej4lTOQO0kBHayGnG0TDjaIw8Y6pq1I8pSVoCfhJTkhplgUtSo064Ak+yKsnOJPu7nyvn2Oa5Sfb03f4nyYZu3XVJvtG3bt1yZOy2O9qXY0ff+DlJbum+wuDj3ZvIS54xybokX06yL8ntSX6nb93Y5nHQVzgkObWblwPdPK3tW/fWbvzuJC8aVaYFZPzjJHd287YryVP61s35ui9Dxlcnme7L8gd96zZ1fzf2J9m0jBmv7sv39SQP9a1bqnm8NsnhJHvnWZ8kf9P9GW5P8oy+dYubx6o6oW7AXwFbuuUtwLsHbL8KeBD4me7+dcBlj4WMwMPzjN8AbOyWPwC8bjkyAr8EnNct/yJwCDh9nPNI7w3ze4BzgVOA24ALZm3zh8AHuuWNwMe75Qu67U8Fzuke56Rlyvjcvr9zr5vJeKzXfRkyvhr42zn2XQXc2/1c2S2vXI6Ms7b/I3oXUyzZPHbP8yzgGcDeeda/BPgsEOAi4JZRzeMJdwRO7+P827rlbcCGAdtfBny2qv57rKl+2vFm/LEkAS4Gti9k/+MwMGNVfb2q9nfL/wUcBibGkKXfMF/h0J99O/C8bt4uBT5WVUeq6hvAge7xljxjVe3u+zv3FXqfo1hKi/kqjBcBO6vqwar6LrATuOQxkPFy4Pox5DimqvoivYPA+VwK/F31fAU4PckaRjCPJ2KBr66qQ93yt4DVA7bfyKNf9Hd1v+pcneTUkSccPuNpSaaSfGXmFA/w88BDVfVId/9+el9rsFwZAUiynt5R0j19w+OYx7m+wmH2n//H23Tz9D168zbMvkuVsd8V9I7QZsz1uo/asBl/u3sNtyeZ+eDeY24eu1NQ5wA39w0vxTwOY74/x6LncewfpR+HJJ8HfmGOVVf236mqSjLvdZLdv4K/Su9a9hlvpVdYp9C7bvMtwDuXKeNTqupgknOBm5PcQa+MRmLE8/j3wKaq+lE3PJJ5PNEleSUwCTy7b/hRr3tV3TP3I4zVPwPXV9WRJK+h91vNxcuQYxgbge1VdbRv7LEyj2PTZIFX1fPnW5fkgSRrqupQVyyHj/FQrwBurKof9j32zFHnkSQfAf5kuTJW1cHu571JvgBcCHyC3q9gK7qjywV/hcEoMib5OeDTwJXdr4czjz2SeZzDMF/hMLPN/UlWAE8CvjPkvkuVkSTPp/eP5bOr6sjM+Dyv+6iLZ2DGqvpO390P03tfZGbf58za9wsjzjfzPMO+XhuB1/cPLNE8DmO+P8ei5/FEPIWyA5h5N3cTcNMxtn3UObOurGbONW8A5nxnedwZk6ycOe2Q5AzgmcCd1Xv3Yze9c/fz7r9EGU8BbqR3fm/7rHXjmsdhvsKhP/tlwM3dvO0ANqZ3lco5wHnAV0eU67gyJrkQ+CDwsqo63Dc+5+u+TBnX9N19GXBXt/w54IVd1pXAC/np32KXLGOX83x6bwJ+uW9sqeZxGDuA3+2uRrkI+F53gLP4eVyKd2mX8kbvXOcuYD/weWBVNz4JfLhvu7X0/gV83Kz9bwbuoFc4/wA8YTkyAr/Z5bit+3lF3/7n0iueA8A/AacuU8ZXAj8E9vTd1o17Hum9q/91ekdTV3Zj76RXhgCndfNyoJunc/v2vbLb727gxWP8ezgo4+eBB/rmbceg130ZMv4lsK/Lshs4v2/f3+/m9wDwe8uVsbv/duCqWfst5TxeT+8KrB/SO499BfBa4LXd+tD7z2/u6bJMjmoe/Si9JDXqRDyFIkn/L1jgktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVH/B5dk4KIMnm2cAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 93==== Step 2 Train Loss 0.6830852031707764 ======  0.5172413793103449\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8242,  0.8144,  0.2183,  ..., -0.0336, -0.1667, -0.6377],\n",
            "        [-1.0635,  1.0076,  0.6500,  ...,  0.0580, -0.1965, -0.4029],\n",
            "        [-1.4251,  1.0237,  0.5623,  ...,  0.0974, -0.6975, -0.3793],\n",
            "        ...,\n",
            "        [-0.7817,  1.0717,  0.3966,  ..., -0.0083, -0.0664, -0.6679],\n",
            "        [ 0.6242, -0.5360, -0.0182,  ...,  0.1521,  0.6371, -0.2428],\n",
            "        [-0.0048,  0.5871, -0.1415,  ...,  0.1645, -0.6381, -0.5023]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9541, -0.6324,  0.9816, -0.5997,  0.9596,  0.9685, -0.3186,  0.9859,\n",
            "         0.9890,  0.7508,  0.9929,  0.2167,  0.6945, -0.0156,  0.8698,  0.9897,\n",
            "        -0.5430,  0.6481, -0.3802,  0.9913,  0.2864, -0.4358,  0.9910, -0.3502,\n",
            "         0.8990,  0.9755,  0.0605,  0.7983,  0.5333,  0.9886,  0.9611,  0.9090,\n",
            "         0.9570,  0.5046, -0.1992, -0.1003,  0.9079,  0.7183,  0.9862,  0.3873,\n",
            "         0.9266,  0.6980,  0.9868,  0.5978, -0.4897, -0.7284,  0.9822,  0.9781,\n",
            "         0.9520,  0.7799,  0.2073, -0.8633,  0.2189, -0.4670, -0.5350,  0.9899,\n",
            "         0.5910,  0.9815, -0.8206, -0.6891,  0.9284, -0.5010,  0.9738,  0.8203],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 94==== Step 2 Train Loss 0.7028999328613281 ======  0.2857142857142857\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930],\n",
            "        [-0.8972,  0.9919,  0.3185,  ..., -0.0945, -0.4443, -0.4936],\n",
            "        [ 0.3638,  0.4704, -0.1656,  ...,  0.1800, -0.3423, -0.1010],\n",
            "        ...,\n",
            "        [-0.0231,  0.6213,  0.0063,  ..., -0.0407, -0.3479, -0.2021],\n",
            "        [-1.2982,  1.1451,  0.5844,  ...,  0.0090, -0.2458, -0.4737],\n",
            "        [ 0.3981,  0.3872, -0.3250,  ...,  0.1264, -0.5630, -0.2440]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9193,  0.1445,  0.2022,  0.8960,  0.9584, -0.3427,  0.5435,  0.9281,\n",
            "         0.3478,  0.5200, -0.6215,  0.9672, -0.5895,  0.9862, -0.4994,  0.7686,\n",
            "        -0.4663,  0.9927,  0.3941,  0.9706,  0.1039,  0.9771,  0.9515,  0.9723,\n",
            "        -0.3652,  0.9839,  0.1860,  0.8004,  0.0239, -0.3051,  0.9787, -0.6239,\n",
            "         0.7675,  0.6296,  0.9721,  0.5295,  0.1369,  0.7997, -0.7172,  0.9937,\n",
            "         0.9833, -0.1308,  0.2614, -0.7048,  0.9957,  0.9846,  0.9938,  0.5586,\n",
            "         0.9796, -0.7019,  0.9914,  0.9924,  0.7421,  0.9429,  0.9325,  0.7636,\n",
            "         0.9704,  0.9960,  0.8631,  0.9739,  0.9645,  0.9212, -0.0488,  0.5847],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 95==== Step 2 Train Loss 0.7400079369544983 ======  0.2692307692307692\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2879,  1.0512,  0.4925,  ...,  0.0162, -0.6509, -0.2583],\n",
            "        [-0.6658,  1.0653, -0.0021,  ..., -0.1911, -0.4968, -0.5012],\n",
            "        [-1.5155,  1.0566,  0.6494,  ...,  0.0953, -0.3893, -0.4249],\n",
            "        ...,\n",
            "        [-1.1873,  1.2165,  0.4781,  ..., -0.0027, -0.6671, -0.3672],\n",
            "        [ 0.5840, -0.5251,  0.0705,  ...,  0.0889,  0.6569, -0.1970],\n",
            "        [-0.5526,  1.0429,  0.2054,  ..., -0.0217, -0.3591, -0.5348]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9926,  0.8140,  0.9846,  0.9001,  0.9397,  0.8387,  0.8643,  0.9792,\n",
            "        -0.6831,  0.9824,  0.6882, -0.0607,  0.8116, -0.2122,  0.8335, -0.6728,\n",
            "         0.4606,  0.2009,  0.8724, -0.0394,  0.9876,  0.4341,  0.9759,  0.9769,\n",
            "         0.9629,  0.9685,  0.4748,  0.9882,  0.9573, -0.8156,  0.9489,  0.9896,\n",
            "         0.2527,  0.0391,  0.9641, -0.5607,  0.3892, -0.2132,  0.8738,  0.1584,\n",
            "        -0.1218,  0.9859,  0.6342,  0.9596,  0.0045,  0.2379,  0.2910,  0.6335,\n",
            "         0.1422,  0.9497,  0.9925,  0.5471,  0.1234,  0.9903,  0.9859,  0.9876,\n",
            "         0.9866, -0.3608, -0.0666,  0.7105,  0.5415,  0.9859,  0.9724, -0.7032],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 96==== Step 2 Train Loss 0.7152103185653687 ======  0.4642857142857143\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.9531,  1.3404,  0.3884,  ..., -0.0310, -0.1275, -0.5755],\n",
            "        [-1.2986,  1.0662,  0.3883,  ...,  0.1176, -0.5177, -0.2900],\n",
            "        [ 0.2905,  0.4028, -0.2786,  ...,  0.2503, -0.5889, -0.4088],\n",
            "        ...,\n",
            "        [-0.2302,  0.6647, -0.2794,  ...,  0.1683, -0.5227, -0.6067],\n",
            "        [-1.1098,  1.1550,  0.2467,  ..., -0.0540, -0.2313, -0.5849],\n",
            "        [-1.5155,  1.0566,  0.6494,  ...,  0.0953, -0.3893, -0.4249]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9801,  0.9883, -0.4741, -0.1351,  0.4816, -0.1044,  0.8726, -0.5525,\n",
            "         0.9846,  0.9745,  0.9570,  0.9945,  0.8309, -0.2778,  0.7262,  0.3367,\n",
            "         0.9912,  0.9961,  0.9787,  0.9905,  0.9955,  0.9813,  0.6500, -0.6265,\n",
            "         0.9914,  0.2540,  0.9965,  0.8965,  0.9171, -0.6947, -0.5759, -0.0922,\n",
            "         0.8598,  0.9587,  0.9882,  0.9184,  0.1110,  0.9788,  0.8508, -0.6614,\n",
            "        -0.0900, -0.7780, -0.5053,  0.9781,  0.8850,  0.9534, -0.7594, -0.6880,\n",
            "        -0.2797, -0.7199, -0.0668,  0.9888,  0.9943,  0.9359,  0.9580,  0.8056,\n",
            "        -0.2796,  0.9921, -0.0491, -0.0054,  0.9927,  0.5667,  0.9360,  0.9879],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 97==== Step 2 Train Loss 0.7421560287475586 ======  0.33962264150943394\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.7144,  1.3005,  0.3446,  ..., -0.0782, -0.3563, -0.5837],\n",
            "        [-0.5064,  0.5918, -0.1084,  ..., -0.0378, -0.5567, -0.4954],\n",
            "        [ 0.7364, -0.2269, -0.1143,  ...,  0.0178,  0.5294, -0.1515],\n",
            "        ...,\n",
            "        [-0.9991,  1.3487,  0.4281,  ...,  0.0453, -0.1767, -0.3482],\n",
            "        [-1.4291,  1.0356,  0.5421,  ...,  0.0283, -0.6316, -0.2754],\n",
            "        [-1.2283,  1.2088,  0.4402,  ...,  0.0137, -0.2309, -0.4243]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9640,  0.9066,  0.9398,  0.9932,  0.5581,  0.7526,  0.3595, -0.2126,\n",
            "         0.9083,  0.9407,  0.9043,  0.6741, -0.6409,  0.9551,  0.9704,  0.9423,\n",
            "         0.9150, -0.3544,  0.9001, -0.7208, -0.6685,  0.9851, -0.4435, -0.1564,\n",
            "         0.9720,  0.8691,  0.9849,  0.4312,  0.0941,  0.0564, -0.7473, -0.2446,\n",
            "        -0.1888,  0.9437,  0.9797,  0.4157, -0.8561, -0.2169,  0.9524, -0.6931,\n",
            "        -0.1507,  0.8388,  0.9150,  0.9392,  0.8770,  0.9291,  0.6350,  0.7894,\n",
            "        -0.6426,  0.4536,  0.2409,  0.9916,  0.9961, -0.2090,  0.5655,  0.3515,\n",
            "        -0.1194,  0.9320,  0.2251,  0.6098,  0.8277,  0.3050,  0.9862, -0.0377],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 98==== Step 2 Train Loss 0.6629538536071777 ======  0.5283018867924528\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0999,  1.2697,  0.5187,  ..., -0.0409, -0.3708, -0.4182],\n",
            "        [-0.9159,  1.4036,  0.5151,  ..., -0.0381, -0.2807, -0.3692],\n",
            "        [ 0.5209, -0.9021,  0.0292,  ...,  0.1750,  0.6252,  0.0852],\n",
            "        ...,\n",
            "        [-1.4251,  1.0237,  0.5623,  ...,  0.0974, -0.6975, -0.3793],\n",
            "        [-1.2441,  1.1606,  0.5669,  ..., -0.0108, -0.2069, -0.3485],\n",
            "        [-0.9370,  1.4225,  0.4471,  ...,  0.1484, -0.2408, -0.4696]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8316, -0.1735,  0.8776, -0.7443,  0.9426,  0.9702,  0.9442,  0.7730,\n",
            "         0.4530,  0.1621,  0.3182,  0.9365,  0.9497,  0.9160,  0.9002,  0.8632,\n",
            "         0.5056,  0.9910,  0.8732,  0.8225, -0.0199,  0.9183,  0.8550, -0.1361,\n",
            "         0.9861,  0.9925,  0.4467,  0.8932,  0.3756,  0.9395,  0.9728,  0.5519,\n",
            "         0.6934,  0.9792,  0.2250,  0.9895,  0.7783, -0.2521,  0.8790, -0.0433,\n",
            "        -0.4301,  0.9808,  0.9859,  0.8084,  0.8448, -0.8510,  0.9683,  0.8425,\n",
            "        -0.1305,  0.7069,  0.8876,  0.9283,  0.8597,  0.8325, -0.2513,  0.9801,\n",
            "         0.9897,  0.9784,  0.9042,  0.9503,  0.5775, -0.0976,  0.9914,  0.2294],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 99==== Step 2 Train Loss 0.6950429081916809 ======  0.5\n",
            "  Batch   100  of    508.    Elapsed: 0:00:59.\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5535, -0.7609,  0.0229,  ...,  0.1984,  0.7142, -0.0787],\n",
            "        [-1.2087,  1.2789,  0.3959,  ..., -0.1254, -0.4096, -0.3585],\n",
            "        [ 0.1902,  0.5673, -0.2032,  ...,  0.0815, -0.5705, -0.2686],\n",
            "        ...,\n",
            "        [ 0.3782,  0.1249, -0.2557,  ...,  0.3521, -0.8663, -0.2478],\n",
            "        [ 0.6122, -0.0304, -0.4642,  ...,  0.3203, -0.7254, -0.3153],\n",
            "        [-0.5873,  1.2090,  0.0806,  ..., -0.1192, -0.3044, -0.6091]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9711,  0.9628,  0.7982,  0.2242,  0.9942, -0.2073,  0.6647, -0.6358,\n",
            "         0.8697,  0.6989,  0.9811, -0.1276,  0.9473,  0.9055,  0.9275,  0.5760,\n",
            "         0.9917, -0.3802, -0.2702,  0.6777,  0.9609,  0.9928, -0.7793,  0.9355,\n",
            "        -0.1904,  0.8588,  0.9629,  0.9723,  0.9862, -0.7205,  0.9629,  0.9914,\n",
            "        -0.3539, -0.2319,  0.8703,  0.9909,  0.8036,  0.2498,  0.3069,  0.9758,\n",
            "        -0.4602, -0.3550, -0.3775, -0.3281,  0.7466, -0.4024,  0.9176,  0.9646,\n",
            "         0.8925,  0.2678,  0.0208, -0.7914,  0.9847, -0.6287,  0.9926,  0.9823,\n",
            "         0.2104,  0.4437, -0.1901,  0.9691,  0.8053,  0.9124,  0.8217,  0.1610],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 100==== Step 2 Train Loss 0.6931896805763245 ======  0.43137254901960786\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4209,  0.9922,  0.4254,  ...,  0.0422, -0.6627, -0.4782],\n",
            "        [ 0.6363, -0.0309,  0.0473,  ..., -0.0788,  0.5509, -0.2580],\n",
            "        [-0.0111,  1.0306,  0.0998,  ...,  0.0105,  0.0282, -0.4315],\n",
            "        ...,\n",
            "        [ 0.9208, -0.3942, -0.2488,  ...,  0.1091,  0.4275, -0.0849],\n",
            "        [ 0.4858, -0.2691, -0.5149,  ...,  0.3341, -0.7034, -0.1211],\n",
            "        [ 0.5968,  0.6069, -0.4477,  ...,  0.1455, -0.5736, -0.1389]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.3885,  0.0892,  0.8807, -0.4835,  0.9633,  0.6705,  0.6431,  0.9176,\n",
            "         0.9557, -0.2984,  0.6176,  0.8150, -0.2470,  0.9745,  0.8966,  0.8853,\n",
            "         0.9823, -0.8003, -0.8075,  0.0956,  0.5689,  0.9895,  0.8570,  0.9917,\n",
            "         0.3290,  0.8873,  0.4275, -0.8184,  0.7677, -0.2871, -0.5028,  0.8981,\n",
            "         0.9311,  0.9615,  0.2082, -0.6166,  0.2727,  0.9361,  0.9486,  0.7642,\n",
            "         0.4765,  0.9735,  0.7627,  0.9621,  0.3894,  0.2649,  0.0498,  0.9710,\n",
            "         0.9249,  0.4308, -0.2490, -0.0797,  0.9867,  0.9251,  0.9734,  0.8987,\n",
            "        -0.8868,  0.9466,  0.9780,  0.9890,  0.9298,  0.8718,  0.8876,  0.8882],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 101==== Step 2 Train Loss 0.6911488771438599 ======  0.41509433962264153\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5887, -0.7102,  0.0697,  ...,  0.1691,  0.7192, -0.1345],\n",
            "        [-1.2879,  1.0512,  0.4925,  ...,  0.0162, -0.6509, -0.2583],\n",
            "        [ 0.3703,  0.5603,  0.0423,  ...,  0.1453,  0.0414, -0.3923],\n",
            "        ...,\n",
            "        [ 0.5417,  0.5957, -0.3669,  ...,  0.1204, -0.5667, -0.2311],\n",
            "        [ 0.6177, -0.4473,  0.0371,  ...,  0.0116,  0.7406, -0.1746],\n",
            "        [ 0.5829, -0.0029,  0.0511,  ..., -0.1490,  0.6723, -0.2594]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9785,  0.9928,  0.2955,  0.9559,  0.7135,  0.9357,  0.8399,  0.2478,\n",
            "         0.6848, -0.7111,  0.0780, -0.1401, -0.2804,  0.3704,  0.4437,  0.9841,\n",
            "         0.7638,  0.9430,  0.0704,  0.9508,  0.9535,  0.9604,  0.8506,  0.8909,\n",
            "         0.9921, -0.1281,  0.9182,  0.6529,  0.7956,  0.9785, -0.5514,  0.5117,\n",
            "         0.7820,  0.9872,  0.9882,  0.8983,  0.6980,  0.9857,  0.7612,  0.9397,\n",
            "         0.9911,  0.1846,  0.3147, -0.2788,  0.9321, -0.4466,  0.9943, -0.4673,\n",
            "         0.2588, -0.3275,  0.2698,  0.9805, -0.3464,  0.8151, -0.6303,  0.9201,\n",
            "         0.7712,  0.9037,  0.2282,  0.9617,  0.6280,  0.2367,  0.8298, -0.0764],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 102==== Step 2 Train Loss 0.723476231098175 ======  0.423076923076923\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.1682,  0.7976, -0.0246,  ...,  0.2027, -0.2853, -0.3937],\n",
            "        [ 0.5941, -0.4109,  0.0466,  ...,  0.0364,  0.6148, -0.0963],\n",
            "        [-0.9986,  1.2936,  0.4480,  ..., -0.0631, -0.3644, -0.5373],\n",
            "        ...,\n",
            "        [-0.8492,  0.9457,  0.2521,  ..., -0.1311, -0.0841, -0.5844],\n",
            "        [ 0.6638,  0.2282, -0.3845,  ...,  0.1277, -0.3795, -0.2432],\n",
            "        [-1.2799,  1.2140,  0.3725,  ..., -0.0098, -0.6778, -0.3219]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7407, -0.6334,  0.9868, -0.0042,  0.8278,  0.9707,  0.9245,  0.9915,\n",
            "        -0.4271, -0.1348,  0.9481,  0.2614,  0.9902, -0.7917, -0.1546,  0.6674,\n",
            "         0.9876,  0.9512,  0.9492, -0.6881,  0.8639,  0.6695,  0.9890,  0.7057,\n",
            "        -0.5827,  0.9832,  0.8358,  0.7232,  0.9657,  0.0868,  0.9466,  0.0488,\n",
            "         0.9930,  0.9571,  0.8737, -0.7373,  0.9647,  0.2150,  0.9532,  0.8051,\n",
            "         0.9356,  0.9206,  0.9538,  0.9321,  0.9736, -0.8376,  0.7388, -0.1869,\n",
            "        -0.2286,  0.9734,  0.8029, -0.0329,  0.0430, -0.8873, -0.7214,  0.9712,\n",
            "        -0.0342,  0.9842,  0.9241,  0.6788,  0.8746, -0.6243,  0.9552,  0.9866],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 103==== Step 2 Train Loss 0.6704649329185486 ======  0.4999999999999999\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.9538, -0.0024, -0.4087,  ...,  0.1048, -0.1753, -0.0655],\n",
            "        [ 0.4232, -0.8927,  0.0313,  ...,  0.1150,  0.5829,  0.0558],\n",
            "        [-0.0162,  0.6320, -0.1741,  ...,  0.1140, -0.5041, -0.4461],\n",
            "        ...,\n",
            "        [-1.0122,  1.4354,  0.4911,  ...,  0.1031, -0.2712, -0.4033],\n",
            "        [ 0.1460,  0.7597, -0.1854,  ..., -0.0323, -0.3095, -0.4144],\n",
            "        [-1.2856,  1.1997,  0.3838,  ..., -0.0056, -0.6806, -0.2989]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7779,  0.4504, -0.0830,  0.9972,  0.6137,  0.9323, -0.0233,  0.9290,\n",
            "         0.9109,  0.1004,  0.9516, -0.7855,  0.7425,  0.9955,  0.3577,  0.6596,\n",
            "        -0.7282,  0.6206, -0.2869, -0.5388,  0.9584,  0.7876,  0.9249,  0.6965,\n",
            "         0.7708,  0.7848, -0.5655, -0.6833,  0.9361,  0.8509,  0.9257, -0.6483,\n",
            "         0.6746,  0.9778, -0.6748,  0.9347,  0.6618,  0.9736,  0.9703,  0.9849,\n",
            "         0.7343, -0.3541, -0.3712,  0.9669,  0.8299, -0.8237,  0.9866,  0.9731,\n",
            "         0.2708,  0.0192,  0.9147,  0.4192,  0.9823,  0.9664,  0.8573,  0.8327,\n",
            "         0.9377,  0.1797, -0.0019, -0.0847,  0.9242,  0.9501,  0.7575,  0.9908],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 104==== Step 2 Train Loss 0.6743225455284119 ======  0.37499999999999994\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1584,  1.1140,  0.3739,  ..., -0.0214, -0.3133, -0.4230],\n",
            "        [-1.4209,  0.9922,  0.4254,  ...,  0.0422, -0.6627, -0.4782],\n",
            "        [-0.9519,  1.3078,  0.3642,  ...,  0.0275, -0.2816, -0.4355],\n",
            "        ...,\n",
            "        [-0.8313,  0.9267,  0.2940,  ...,  0.0815,  0.0583, -0.4111],\n",
            "        [ 0.7125, -0.3681,  0.0648,  ...,  0.0952,  0.7561,  0.0172],\n",
            "        [-1.3555,  1.0751,  0.5338,  ...,  0.0804, -0.6597, -0.2875]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9616,  0.9861,  0.9738,  0.2748, -0.7581,  0.9721,  0.9918,  0.1348,\n",
            "         0.5682,  0.2893,  0.9459, -0.4644,  0.8486,  0.7465,  0.8964,  0.1973,\n",
            "         0.9925,  0.7973,  0.8811, -0.0630,  0.9234,  0.8896,  0.3634, -0.7722,\n",
            "         0.9245,  0.9870, -0.2543,  0.9843,  0.1069,  0.8083,  0.9482,  0.8927,\n",
            "         0.0661,  0.7094,  0.4927,  0.9703, -0.2089, -0.7574,  0.0810,  0.6089,\n",
            "         0.6008,  0.9877,  0.3661,  0.9899, -0.6712, -0.2567, -0.1766,  0.9910,\n",
            "         0.9254,  0.7628,  0.3057,  0.3587, -0.4608,  0.6693,  0.9602,  0.9760,\n",
            "        -0.0273, -0.4584,  0.9041,  0.9841,  0.3515,  0.9436,  0.9806,  0.9937],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 105==== Step 2 Train Loss 0.6924645900726318 ======  0.4150943396226416\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.6358, -0.2854, -0.1328,  ..., -0.1055,  0.5652, -0.2010],\n",
            "        [-1.2957,  0.9894,  0.4829,  ..., -0.0125, -0.2878, -0.3933],\n",
            "        [-1.1925,  1.0694,  0.5852,  ...,  0.0109, -0.2746, -0.4163],\n",
            "        ...,\n",
            "        [ 0.5810, -0.2976,  0.0161,  ...,  0.0887,  0.6549, -0.3665],\n",
            "        [-0.9147,  1.3524,  0.3830,  ..., -0.0680, -0.1990, -0.5256],\n",
            "        [ 0.4848, -0.0429, -0.4726,  ...,  0.1856, -0.8957,  0.0595]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1591,  0.9950, -0.1026,  0.9816,  0.5738,  0.9915,  0.9898,  0.9835,\n",
            "        -0.1614, -0.2581,  0.9883,  0.9760,  0.9560,  0.9966,  0.9938,  0.2405,\n",
            "         0.5304,  0.9873,  0.8947, -0.0736,  0.6154,  0.9901,  0.1896,  0.7834,\n",
            "         0.9797, -0.4402,  0.8780, -0.1150, -0.6644,  0.8222,  0.9928,  0.9713,\n",
            "         0.9873,  0.6258,  0.9346,  0.9951, -0.2655, -0.7739,  0.8384,  0.9664,\n",
            "         0.9556,  0.5959,  0.9656,  0.9851, -0.6030,  0.9473,  0.6368,  0.6053,\n",
            "        -0.6428,  0.8304,  0.9671,  0.9108,  0.0521,  0.9876,  0.2250,  0.9912,\n",
            "         0.7736,  0.8020,  0.6081,  0.9528,  0.9438,  0.9564,  0.3005,  0.7348],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 106==== Step 2 Train Loss 0.7218427658081055 ======  0.3272727272727273\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.0404,  0.9925, -0.1200,  ...,  0.0925, -0.4536, -0.4438],\n",
            "        [ 0.7597, -0.6939, -0.1312,  ...,  0.2384,  0.4092, -0.1514],\n",
            "        [-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743],\n",
            "        ...,\n",
            "        [-1.0593,  1.2020,  0.4404,  ..., -0.0745, -0.2726, -0.5630],\n",
            "        [-1.3104,  1.1130,  0.5366,  ..., -0.0132, -0.2718, -0.5188],\n",
            "        [-0.7095,  1.3701,  0.3176,  ..., -0.0603, -0.1213, -0.3374]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.5333,  0.8532, -0.5340,  0.3001, -0.1622,  0.6167,  0.9829,  0.0899,\n",
            "         0.9900,  0.9407,  0.9355,  0.9775, -0.7826,  0.8856,  0.0399,  0.3615,\n",
            "         0.9849,  0.7689,  0.1077,  0.7443,  0.8614,  0.4308, -0.2451,  0.4443,\n",
            "         0.9909,  0.9913, -0.3211,  0.8819,  0.9327,  0.9818,  0.9811,  0.9654,\n",
            "         0.7786, -0.4122,  0.1679,  0.4452,  0.9838,  0.7812, -0.7004,  0.7987,\n",
            "         0.9737,  0.5868,  0.9370,  0.8220, -0.7917,  0.9928,  0.6917,  0.9234,\n",
            "         0.9728,  0.1631,  0.0012,  0.9682,  0.9788,  0.8322,  0.8242, -0.7076,\n",
            "        -0.1054,  0.9791,  0.9261,  0.8124, -0.3007,  0.9751,  0.8671,  0.9532],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 107==== Step 2 Train Loss 0.6921772360801697 ======  0.48275862068965525\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 7.6589e-01, -2.1255e-03, -4.5797e-01,  ...,  1.8850e-01,\n",
            "         -3.4473e-01, -1.7893e-01],\n",
            "        [-9.8484e-01,  1.1233e+00,  3.1504e-01,  ..., -3.2594e-03,\n",
            "         -6.8400e-01, -2.7411e-01],\n",
            "        [-1.3078e+00,  9.3616e-01,  5.6553e-01,  ..., -9.5568e-04,\n",
            "         -3.3426e-01, -3.9002e-01],\n",
            "        ...,\n",
            "        [-1.2986e+00,  1.0662e+00,  3.8829e-01,  ...,  1.1758e-01,\n",
            "         -5.1766e-01, -2.8996e-01],\n",
            "        [-8.9891e-01,  1.3933e+00,  4.1263e-01,  ...,  6.6071e-02,\n",
            "         -3.6994e-01, -3.6149e-01],\n",
            "        [-7.8171e-01,  1.0717e+00,  3.9662e-01,  ..., -8.2513e-03,\n",
            "         -6.6436e-02, -6.6786e-01]], device='cuda:0')\n",
            "tensor([ 0.7562,  0.9613,  0.9892,  0.5989,  0.9664, -0.4187,  0.9635,  0.7856,\n",
            "        -0.2341,  0.9348, -0.2299, -0.3755,  0.9914,  0.3243, -0.3441,  0.9672,\n",
            "         0.6189,  0.9194,  0.9935,  0.9716, -0.6427,  0.8284,  0.9031,  0.9877,\n",
            "         0.0998,  0.7751,  0.8496,  0.9749,  0.8787,  0.9809, -0.5648,  0.9690,\n",
            "        -0.3960,  0.9212,  0.5891,  0.9852,  0.4084, -0.8491,  0.7834,  0.7472,\n",
            "         0.8327,  0.4802,  0.9825, -0.5962, -0.4698,  0.9896,  0.9655,  0.9468,\n",
            "        -0.1333,  0.9908,  0.8565,  0.9111,  0.1340,  0.8858,  0.9966,  0.6244,\n",
            "         0.8900,  0.9811,  0.5985,  0.1039,  0.8937,  0.9910,  0.6350, -0.3933],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 108==== Step 2 Train Loss 0.733481228351593 ======  0.43333333333333335\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1405,  1.1992,  0.5828,  ...,  0.0036, -0.3449, -0.4362],\n",
            "        [-0.4104,  0.9800,  0.1111,  ..., -0.0826, -0.0869, -0.6566],\n",
            "        [-0.6917,  0.9628,  0.2721,  ..., -0.0549, -0.0926, -0.5986],\n",
            "        ...,\n",
            "        [-0.1552,  1.0605, -0.1490,  ...,  0.0434, -0.4132, -0.6567],\n",
            "        [-1.1813,  1.1511,  0.4591,  ...,  0.0195, -0.2204, -0.5355],\n",
            "        [-1.0927,  1.0805,  0.4936,  ..., -0.1843, -0.1848, -0.3815]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1870,  0.3882,  0.8733,  0.9857,  0.4802,  0.8882,  0.8776,  0.9804,\n",
            "        -0.5564,  0.9925,  0.9777,  0.9691, -0.1325,  0.6238,  0.9882, -0.4227,\n",
            "         0.9778,  0.9304,  0.9293,  0.9753,  0.9915,  0.9776, -0.2333,  0.9940,\n",
            "         0.9364, -0.7631, -0.0526,  0.9604, -0.5936, -0.4173,  0.0192,  0.9799,\n",
            "        -0.8168,  0.5950,  0.9928,  0.9888,  0.5014,  0.9882, -0.7130,  0.9287,\n",
            "         0.9894,  0.9843, -0.2890, -0.1670, -0.4701,  0.9557, -0.6647,  0.9508,\n",
            "         0.6069,  0.7145,  0.4444, -0.7985,  0.9859,  0.9726,  0.2056,  0.8612,\n",
            "         0.7863,  0.9910,  0.9516, -0.7537,  0.9698,  0.9095,  0.5054,  0.9651],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 109==== Step 2 Train Loss 0.7432745695114136 ======  0.3333333333333333\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 7.6550e-01, -7.7023e-01, -1.2975e-02,  ...,  4.4169e-02,\n",
            "          3.5075e-01,  8.3771e-02],\n",
            "        [-1.2879e+00,  1.0512e+00,  4.9250e-01,  ...,  1.6238e-02,\n",
            "         -6.5090e-01, -2.5829e-01],\n",
            "        [-1.2986e+00,  1.0662e+00,  3.8829e-01,  ...,  1.1758e-01,\n",
            "         -5.1766e-01, -2.8996e-01],\n",
            "        ...,\n",
            "        [-2.6541e-01,  1.1335e+00, -2.4064e-02,  ...,  9.7321e-04,\n",
            "         -5.6005e-01, -3.9480e-01],\n",
            "        [-1.1098e+00,  1.1550e+00,  2.4665e-01,  ..., -5.4031e-02,\n",
            "         -2.3132e-01, -5.8489e-01],\n",
            "        [-7.9786e-01,  1.0310e+00,  2.9281e-01,  ..., -1.0729e-01,\n",
            "         -2.5838e-01, -4.4771e-01]], device='cuda:0')\n",
            "tensor([-0.7421, -0.2767, -0.6795,  0.9889,  0.9943,  0.2627,  0.2864, -0.4434,\n",
            "        -0.7025,  0.9774,  0.9523,  0.9613,  0.9905,  0.9639, -0.6906,  0.9961,\n",
            "         0.9147,  0.9873,  0.1767,  0.8581,  0.8981, -0.1893,  0.9942,  0.5844,\n",
            "         0.9880,  0.1851,  0.6386, -0.7829,  0.9072,  0.7949,  0.3948,  0.9839,\n",
            "         0.9862,  0.9441,  0.9836, -0.3323,  0.9891,  0.9905, -0.6344,  0.0513,\n",
            "         0.7595,  0.5262,  0.9721,  0.7773,  0.8424, -0.1681,  0.0977, -0.1421,\n",
            "         0.4303,  0.9913,  0.9637, -0.7193,  0.9881,  0.9639, -0.7290,  0.8212,\n",
            "        -0.4780, -0.5263,  0.8120,  0.9250,  0.5695,  0.4575,  0.9085,  0.8657],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 110==== Step 2 Train Loss 0.721304178237915 ======  0.3673469387755102\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.1742,  0.6754, -0.2077,  ...,  0.0399, -0.6339, -0.2681],\n",
            "        [-1.0255,  0.9810,  0.3453,  ..., -0.0034, -0.3100, -0.4295],\n",
            "        [ 0.7665, -0.2334, -0.0928,  ..., -0.1098,  0.4316, -0.0112],\n",
            "        ...,\n",
            "        [ 0.7249, -0.2902, -0.1187,  ...,  0.1176,  0.3453, -0.3477],\n",
            "        [ 0.6224, -0.5825,  0.0156,  ...,  0.1775,  0.7018, -0.1850],\n",
            "        [ 0.7775,  0.2745, -0.4390,  ...,  0.3652, -0.7770, -0.2891]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2856,  0.9799, -0.8620,  0.8985,  0.9453,  0.9846, -0.8387,  0.9854,\n",
            "         0.7725,  0.7943,  0.5407,  0.5406,  0.9951,  0.7809,  0.9438,  0.7466,\n",
            "        -0.4569,  0.2026,  0.7114, -0.5433,  0.5295,  0.7382, -0.1569,  0.8825,\n",
            "         0.9865,  0.8929,  0.8747,  0.9197,  0.9934,  0.9792,  0.9934,  0.9962,\n",
            "        -0.1404,  0.7950,  0.0027,  0.9852, -0.0174, -0.6681, -0.7174, -0.4078,\n",
            "         0.5055, -0.8653,  0.3257,  0.9846, -0.4200,  0.9951, -0.4187, -0.5263,\n",
            "        -0.1107,  0.9919,  0.8083,  0.0639,  0.1625,  0.8847,  0.9846,  0.9558,\n",
            "         0.6787,  0.8923,  0.8990,  0.6976,  0.9942,  0.8714,  0.5775,  0.7105],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 111==== Step 2 Train Loss 0.7480244636535645 ======  0.3272727272727273\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.6043,  0.9013,  0.1998,  ..., -0.0540, -0.6052, -0.3171],\n",
            "        [ 0.5821, -0.5007,  0.0814,  ...,  0.1920,  0.6956, -0.2509],\n",
            "        [-0.3031,  1.2016,  0.1742,  ...,  0.1093, -0.3025, -0.4364],\n",
            "        ...,\n",
            "        [-0.3705,  1.2712,  0.2074,  ..., -0.0291, -0.3014, -0.5383],\n",
            "        [-1.4090,  1.1067,  0.5068,  ...,  0.0768, -0.5922, -0.3176],\n",
            "        [ 0.5583, -0.2956, -0.0522,  ...,  0.2364,  0.2224, -0.2454]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9385,  0.9764,  0.8729,  0.7909, -0.4223,  0.3957,  0.9154,  0.8900,\n",
            "         0.9448,  0.9860, -0.5165, -0.2752, -0.5737,  0.9854,  0.7732, -0.6581,\n",
            "        -0.6071,  0.9675,  0.9573, -0.5388,  0.7318,  0.1511,  0.9562,  0.9243,\n",
            "         0.9811,  0.9672,  0.7910,  0.9539, -0.7111,  0.9917,  0.9792,  0.7298,\n",
            "         0.1192,  0.9532, -0.0052,  0.9682,  0.9934,  0.9838,  0.1398,  0.7807,\n",
            "        -0.7321, -0.5545, -0.5287,  0.9428,  0.9927,  0.9047,  0.7610,  0.9838,\n",
            "        -0.6800, -0.0957,  0.7601,  0.7106,  0.8809,  0.7538,  0.9753, -0.6065,\n",
            "         0.9778,  0.9275, -0.6462, -0.0653,  0.3604,  0.9110, -0.0203,  0.5425],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 112==== Step 2 Train Loss 0.6805140376091003 ======  0.4615384615384615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 48])\n",
            "tensor([[ 0.7143,  0.1348, -0.0088,  ...,  0.0441,  0.2799, -0.2412],\n",
            "        [-0.7928,  1.3466,  0.4217,  ...,  0.0201, -0.0535, -0.4203],\n",
            "        [ 0.6204,  0.0745, -0.1706,  ..., -0.0700,  0.4327, -0.4120],\n",
            "        ...,\n",
            "        [-1.0179,  1.1543,  0.3259,  ..., -0.0318, -0.3608, -0.5335],\n",
            "        [-1.2087,  1.2789,  0.3959,  ..., -0.1254, -0.4096, -0.3585],\n",
            "        [-1.2804,  1.0736,  0.4059,  ...,  0.0111, -0.6980, -0.3929]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6520,  0.9661,  0.0690,  0.2141,  0.9872,  0.7997,  0.9297,  0.8500,\n",
            "        -0.2375, -0.7099,  0.9898, -0.0541,  0.7314,  0.9926, -0.7287,  0.7127,\n",
            "         0.9863, -0.2926,  0.1683,  0.9165,  0.8434, -0.0011, -0.3074,  0.9507,\n",
            "         0.6299,  0.8412,  0.3776, -0.2158,  0.9888, -0.8701,  0.3873, -0.7160,\n",
            "         0.9935,  0.9517,  0.9007,  0.5544,  0.0720,  0.9897,  0.8518,  0.9918,\n",
            "         0.9183, -0.4652,  0.9162,  0.9442,  0.8886, -0.1491,  0.9635, -0.7156,\n",
            "        -0.2497,  0.7657,  0.0521, -0.1799,  0.9670,  0.9228,  0.9829,  0.9758,\n",
            "         0.1542, -0.6519, -0.7205,  0.7362, -0.1444,  0.9337,  0.9540,  0.9968],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 113==== Step 2 Train Loss 0.7220497131347656 ======  0.4\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.5308,  1.2628,  0.2311,  ...,  0.0910, -0.2654, -0.4449],\n",
            "        [-1.4077,  1.1398,  0.5148,  ...,  0.0621, -0.6013, -0.3745],\n",
            "        [-1.0771,  1.0361,  0.2324,  ..., -0.0354, -0.3807, -0.4206],\n",
            "        ...,\n",
            "        [ 0.3638,  0.4704, -0.1656,  ...,  0.1800, -0.3423, -0.1010],\n",
            "        [-0.5471,  1.3423,  0.2826,  ...,  0.0357, -0.1621, -0.6077],\n",
            "        [-1.3392,  1.0580,  0.5910,  ..., -0.0844, -0.4738, -0.3501]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9448, -0.4841,  0.9344,  0.5989, -0.0068,  0.8744,  0.0479,  0.6695,\n",
            "         0.4232,  0.9752,  0.8948, -0.6161,  0.9924,  0.9149, -0.1645, -0.4540,\n",
            "         0.9339,  0.5782,  0.3084,  0.9409,  0.9770, -0.0186,  0.9826,  0.6543,\n",
            "         0.8491,  0.9752, -0.6688,  0.8640,  0.8538,  0.5458,  0.6523,  0.6979,\n",
            "         0.6249,  0.4494,  0.9500,  0.7014,  0.9911,  0.9700,  0.5544,  0.0748,\n",
            "         0.2795,  0.8032,  0.9887, -0.2442,  0.5712,  0.9227,  0.8937,  0.9855,\n",
            "        -0.7496,  0.5187,  0.9898,  0.5853, -0.0223,  0.5935,  0.5896,  0.9925,\n",
            "         0.6788, -0.2424,  0.3027,  0.9912,  0.7053,  0.8646,  0.9356, -0.2357],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 114==== Step 2 Train Loss 0.6837748289108276 ======  0.5333333333333333\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0771,  1.0361,  0.2324,  ..., -0.0354, -0.3807, -0.4206],\n",
            "        [ 0.8185, -0.4541, -0.2030,  ..., -0.0215,  0.3811,  0.0581],\n",
            "        [ 0.3719, -0.4709, -0.3351,  ...,  0.4447, -0.4224, -0.1388],\n",
            "        ...,\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [-1.1098,  1.1550,  0.2467,  ..., -0.0540, -0.2313, -0.5849],\n",
            "        [-0.8991,  1.2932,  0.4628,  ...,  0.0361, -0.2071, -0.6166]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9323,  0.3515,  0.7428, -0.8580,  0.3144,  0.9669,  0.9912, -0.6554,\n",
            "         0.7902, -0.4891, -0.1859,  0.9138,  0.6516,  0.8029,  0.2110,  0.9735,\n",
            "         0.9331,  0.3516,  0.9821,  0.5917,  0.9768,  0.6675,  0.7136, -0.1941,\n",
            "         0.9940, -0.1828,  0.0959,  0.9639,  0.4811,  0.8755,  0.9962,  0.4907,\n",
            "         0.9763,  0.9130,  0.9185, -0.3181,  0.1878, -0.1362,  0.9824,  0.8488,\n",
            "         0.9898,  0.9282, -0.1443,  0.9598, -0.1013,  0.9861,  0.0700,  0.9911,\n",
            "         0.7826,  0.9357,  0.1246,  0.8225, -0.3105,  0.8776,  0.9778,  0.9650,\n",
            "         0.3124, -0.5103,  0.9556, -0.0049,  0.1315, -0.7409,  0.9327, -0.8036],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 115==== Step 2 Train Loss 0.6897289752960205 ======  0.5172413793103448\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5310,  0.0678, -0.4880,  ...,  0.3716, -0.5317, -0.3916],\n",
            "        [-1.4876,  1.0979,  0.5174,  ...,  0.0869, -0.6326, -0.4616],\n",
            "        [-1.3731,  1.0866,  0.5036,  ...,  0.0107, -0.6242, -0.3145],\n",
            "        ...,\n",
            "        [ 0.4243,  0.2453, -0.4365,  ...,  0.2270, -0.7709, -0.2980],\n",
            "        [ 0.4315, -0.2380, -0.0620,  ...,  0.0605,  0.4145, -0.2347],\n",
            "        [-1.4077,  1.1398,  0.5148,  ...,  0.0621, -0.6013, -0.3745]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.0603,  0.9946, -0.8168,  0.4559,  0.5055,  0.9689,  0.9037,  0.9644,\n",
            "         0.8416, -0.1791,  0.9718,  0.8772,  0.7997,  0.8025,  0.9716,  0.9854,\n",
            "         0.6722,  0.8729,  0.1473,  0.9884,  0.9860,  0.3290, -0.2599,  0.4332,\n",
            "        -0.5793,  0.5716,  0.9586,  0.9726,  0.9581,  0.9802, -0.2383,  0.5847,\n",
            "         0.4094,  0.7088,  0.9647,  0.7653,  0.9244, -0.3346,  0.2800,  0.6420,\n",
            "         0.9803, -0.1931, -0.4133,  0.9885,  0.3862,  0.9752, -0.2657,  0.4308,\n",
            "         0.9787, -0.2761,  0.6899,  0.9776,  0.7312, -0.7333,  0.9748, -0.0940,\n",
            "         0.9852,  0.9883,  0.9887, -0.2262,  0.7738,  0.4203,  0.9055,  0.9945],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 116==== Step 2 Train Loss 0.7578356266021729 ======  0.3157894736842105\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8541,  1.0261,  0.2606,  ..., -0.1620, -0.1384, -0.5400],\n",
            "        [-1.0280,  1.0625,  0.3169,  ..., -0.0407, -0.2847, -0.5186],\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        ...,\n",
            "        [ 0.5821, -0.5007,  0.0814,  ...,  0.1920,  0.6956, -0.2509],\n",
            "        [ 0.1602,  0.8499, -0.0942,  ...,  0.2114, -0.5358, -0.4550],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9784,  0.9794,  0.9907,  0.9882, -0.1829, -0.4737,  0.9859,  0.9932,\n",
            "         0.9122,  0.9603,  0.9935, -0.3091,  0.8632,  0.9329,  0.9715,  0.9148,\n",
            "         0.8304,  0.9708,  0.8944, -0.2298,  0.9954, -0.3148,  0.1277,  0.4404,\n",
            "         0.9665,  0.9833, -0.6346,  0.9892,  0.2729, -0.2622,  0.8714, -0.7624,\n",
            "         0.3875,  0.8342,  0.9828,  0.9857,  0.9736,  0.5494,  0.8488,  0.9026,\n",
            "         0.9184,  0.9170,  0.3887,  0.9457, -0.8873,  0.8593, -0.6118,  0.6104,\n",
            "        -0.2156,  0.1224,  0.9923,  0.9931, -0.6161,  0.9653,  0.2823,  0.8947,\n",
            "         0.9329,  0.7484,  0.9871,  0.9797,  0.9750, -0.5802,  0.9047, -0.7979],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 117==== Step 2 Train Loss 0.7194597125053406 ======  0.35294117647058826\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.8843,  1.2498,  0.4276,  ...,  0.0083, -0.1354, -0.4991],\n",
            "        [-0.0198,  0.8874, -0.1421,  ...,  0.1852, -0.2916, -0.4161],\n",
            "        [-1.2330,  1.0996,  0.4440,  ..., -0.0840, -0.5587, -0.4490],\n",
            "        ...,\n",
            "        [-1.2760,  1.1292,  0.6219,  ..., -0.0763, -0.3256, -0.4322],\n",
            "        [ 0.3764,  0.2099,  0.0508,  ...,  0.1711,  0.1765, -0.5730],\n",
            "        [-0.1248,  0.8705, -0.0216,  ...,  0.0385, -0.2608, -0.6340]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.7328,  0.7598,  0.9813,  0.9809,  0.9940,  0.2461,  0.6968,  0.9861,\n",
            "         0.3320, -0.3767,  0.9911,  0.6491,  0.7470, -0.3217,  0.9906,  0.8075,\n",
            "         0.0778, -0.2211,  0.8165,  0.7393,  0.5515, -0.6324,  0.2383, -0.0014,\n",
            "         0.9441,  0.8519, -0.5867,  0.9136,  0.9889,  0.9486, -0.5543,  0.7773,\n",
            "         0.8852, -0.6229, -0.5799,  0.1492,  0.9803,  0.8838, -0.8071,  0.0700,\n",
            "         0.1455,  0.7701,  0.7982,  0.9689,  0.9857, -0.0320, -0.1408, -0.7160,\n",
            "         0.8918,  0.9870,  0.9948,  0.9867,  0.7388,  0.7348, -0.1007,  0.0487,\n",
            "         0.9825,  0.7113,  0.9108,  0.9868,  0.3432, -0.4213,  0.7679,  0.9041],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 118==== Step 2 Train Loss 0.6929712891578674 ======  0.4313725490196078\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2688,  1.0855,  0.6017,  ..., -0.0183, -0.2832, -0.3602],\n",
            "        [ 0.2898,  0.5771, -0.0424,  ...,  0.0460, -0.0290, -0.4949],\n",
            "        [ 0.5569,  0.2474, -0.4614,  ...,  0.1131, -0.8210, -0.1078],\n",
            "        ...,\n",
            "        [-0.5098,  1.2666,  0.2094,  ..., -0.1714, -0.3442, -0.5363],\n",
            "        [-1.0265,  1.3387,  0.6677,  ...,  0.0159, -0.1478, -0.3517],\n",
            "        [-0.3723,  0.8026, -0.0532,  ...,  0.0705, -0.0474, -0.6271]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.5213,  0.8235, -0.1337,  0.9500,  0.9936,  0.2210, -0.4659, -0.1792,\n",
            "        -0.5215,  0.9177,  0.8065,  0.8923,  0.9238,  0.9818,  0.9445,  0.9622,\n",
            "        -0.1206,  0.7595,  0.9876,  0.9744,  0.9864, -0.1464,  0.9824,  0.9698,\n",
            "        -0.3859,  0.9853, -0.2018,  0.9937,  0.8644,  0.9629,  0.6937,  0.9646,\n",
            "        -0.7535, -0.7730,  0.9848,  0.6017, -0.6592,  0.9881,  0.8428,  0.0631,\n",
            "         0.0365, -0.7002,  0.9792, -0.0555,  0.9064,  0.9360,  0.1332,  0.9849,\n",
            "         0.9381,  0.9695, -0.1248, -0.4799,  0.9550, -0.1593,  0.9308,  0.9153,\n",
            "         0.8529,  0.9914,  0.9944,  0.8379,  0.1294,  0.8728,  0.9773,  0.8821],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 119==== Step 2 Train Loss 0.7294964790344238 ======  0.37499999999999994\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.1441,  1.3307,  0.6555,  ..., -0.0181, -0.3380, -0.2226],\n",
            "        [-0.0048,  0.5871, -0.1415,  ...,  0.1645, -0.6381, -0.5023],\n",
            "        [-1.1499,  1.1676,  0.5367,  ..., -0.1130, -0.2179, -0.4193],\n",
            "        ...,\n",
            "        [-1.2986,  1.0662,  0.3883,  ...,  0.1176, -0.5177, -0.2900],\n",
            "        [ 0.2973,  0.9606, -0.0914,  ..., -0.1031, -0.2041, -0.4218],\n",
            "        [ 0.7120, -0.5674, -0.0526,  ...,  0.1234,  0.5407, -0.1935]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9805,  0.7863,  0.9822,  0.9763,  0.9413,  0.9309,  0.7109,  0.8530,\n",
            "        -0.6048, -0.7174, -0.0830,  0.9814, -0.7179, -0.8616,  0.9874,  0.7095,\n",
            "         0.6259,  0.6325, -0.4417,  0.9908,  0.9865, -0.8087,  0.9944,  0.7649,\n",
            "         0.9438,  0.9687,  0.9871,  0.9756,  0.9866, -0.1457,  0.2524,  0.9559,\n",
            "         0.5626, -0.5998,  0.9736,  0.9930,  0.9662, -0.3748,  0.5646,  0.6480,\n",
            "         0.1573, -0.2800,  0.9459,  0.8238,  0.9725, -0.7177,  0.5912, -0.1585,\n",
            "         0.4615,  0.8909, -0.3444, -0.5461,  0.9281,  0.9711,  0.8350,  0.9577,\n",
            "         0.6987,  0.3013,  0.0829,  0.4482,  0.0430, -0.7257,  0.5328,  0.9608],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQKUlEQVR4nO3df6zddX3H8edLyg833WjHDetALTg2QrZYzF3H5uIPFEVNBDPiINHVjaXqdNHMLVb5Q2dmhsuUZNmiVkG6zaGsSuj8MVcBQ0wUd3G1FBi2IGawSq8iKlnWCb73x/led7zc23N67/fcy6c+H8nJ/Z7P9/s959XPaV793u/5ntNUFZKk9jxhtQNIkpbGApekRlngktQoC1ySGmWBS1Kj1qzkk5100km1YcOGlXxKSWrerbfe+q2qmpo/PrLAk5wA3Awc322/o6renuRq4DnAd7tNX11Vuw/3WBs2bGBmZuZIs0vST7Qk31hofJwj8EPAuVX1cJJjgS8k+Uy37k+rakdfISVJ4xtZ4DX4pM/D3d1ju5uf/pGkVTbWm5hJjkmyGzgI7KqqW7pV70qyJ8kVSY6fWEpJ0mOMVeBV9WhVbQROBTYl+RXgrcCZwK8B64C3LLRvki1JZpLMzM7O9hRbknRElxFW1UPATcD5VXWgBg4BHwY2LbLPtqqarqrpqanHvIkqSVqikQWeZCrJid3yE4HzgP9Isr4bC3AhsHeSQSVJP26cq1DWA9uTHMOg8K+tqk8muTHJFBBgN/DaCeaUJM0zzlUoe4CzFxg/dyKJJElj8aP0ktSoFf0ovSQdiQ1bP7XaEXpz7+Uv7f0xPQKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjRhZ4khOSfDnJV5PcnuTPuvHTktySZH+SjyU5bvJxJUlzxjkCPwScW1XPADYC5yc5B3g3cEVV/SLwHeDSycWUJM03ssBr4OHu7rHdrYBzgR3d+HbgwokklCQtaKxz4EmOSbIbOAjsAu4GHqqqR7pN7gNOWWTfLUlmkszMzs72kVmSxJgFXlWPVtVG4FRgE3DmuE9QVduqarqqpqemppYYU5I03xFdhVJVDwE3Ab8BnJhkTbfqVOD+nrNJkg5jnKtQppKc2C0/ETgPuJNBkV/UbbYZuH5SISVJj7Vm9CasB7YnOYZB4V9bVZ9Mcgfw0SR/Dvw7cOUEc0qS5hlZ4FW1Bzh7gfF7GJwPlyStAj+JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktSokQWe5ClJbkpyR5Lbk7yxG39HkvuT7O5uL5l8XEnSnDVjbPMI8Oaq+kqSJwO3JtnVrbuiqv5qcvEkSYsZWeBVdQA40C1/P8mdwCmTDiZJOrwjOgeeZANwNnBLN/SGJHuSXJVkbc/ZJEmHMXaBJ3kS8HHgTVX1PeB9wNOBjQyO0N+zyH5bkswkmZmdne0hsiQJxizwJMcyKO+PVNUnAKrqgap6tKp+CHwQ2LTQvlW1raqmq2p6amqqr9yS9BNvnKtQAlwJ3FlV7x0aXz+02cuBvf3HkyQtZpyrUJ4FvAq4LcnubuxtwCVJNgIF3Au8ZiIJJUkLGucqlC8AWWDVp/uPI0kal5/ElKRGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjRpZ4EmekuSmJHckuT3JG7vxdUl2JdnX/Vw7+biSpDnjHIE/Ary5qs4CzgFen+QsYCtwQ1WdAdzQ3ZckrZCRBV5VB6rqK93y94E7gVOAC4Dt3WbbgQsnFVKS9FhHdA48yQbgbOAW4OSqOtCt+iZw8iL7bEkyk2RmdnZ2GVElScPGLvAkTwI+Drypqr43vK6qCqiF9quqbVU1XVXTU1NTyworSfp/YxV4kmMZlPdHquoT3fADSdZ369cDBycTUZK0kHGuQglwJXBnVb13aNVOYHO3vBm4vv94kqTFrBljm2cBrwJuS7K7G3sbcDlwbZJLgW8Ar5hMREnSQkYWeFV9Acgiq5/fbxxJ0rj8JKYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSo0YWeJKrkhxMsndo7B1J7k+yu7u9ZLIxJUnzjXMEfjVw/gLjV1TVxu726X5jSZJGGVngVXUz8OAKZJEkHYHlnAN/Q5I93SmWtYttlGRLkpkkM7Ozs8t4OknSsKUW+PuApwMbgQPAexbbsKq2VdV0VU1PTU0t8ekkSfMtqcCr6oGqerSqfgh8ENjUbyxJ0ihLKvAk64fuvhzYu9i2kqTJWDNqgyTXAM8FTkpyH/B24LlJNgIF3Au8ZoIZJUkLGFngVXXJAsNXTiCLJOkI+ElMSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUqJEFnuSqJAeT7B0aW5dkV5J93c+1k40pSZpvnCPwq4Hz541tBW6oqjOAG7r7kqQVNLLAq+pm4MF5wxcA27vl7cCFPeeSJI2w1HPgJ1fVgW75m8DJi22YZEuSmSQzs7OzS3w6SdJ8y34Ts6oKqMOs31ZV01U1PTU1tdynkyR1llrgDyRZD9D9PNhfJEnSOJZa4DuBzd3yZuD6fuJIksY1zmWE1wBfBH45yX1JLgUuB85Lsg94QXdfkrSC1ozaoKouWWTV83vOIkk6AiMLXFJbNmz91GpH0Arxo/SS1CgLXJIaZYFLUqMscElqlAUuSY1q5iqUo+md9Xsvf+lqR5B0FPAIXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1allfJ5vkXuD7wKPAI1U13UcoSdJofXwf+POq6ls9PI4k6Qh4CkWSGrXcI/AC/jVJAR+oqm3zN0iyBdgC8NSnPnWZT3d08H8XktSH5R6B/1ZVPRN4MfD6JM+ev0FVbauq6aqanpqaWubTSZLmLKvAq+r+7udB4DpgUx+hJEmjLbnAk/x0kifPLQMvBPb2FUySdHjLOQd+MnBdkrnH+ceq+pdeUkmSRlpygVfVPcAzeswiSToCfVwHrp9gR9MVNVJrvA5ckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVHLKvAk5ye5K8n+JFv7CiVJGm3JBZ7kGOBvgRcDZwGXJDmrr2CSpMNbzhH4JmB/Vd1TVf8LfBS4oJ9YkqRR1ixj31OA/xy6fx/w6/M3SrIF2NLdfTjJXct4zj6cBHxrlTOMYsZ+mLEfZuxB3r2sjE9baHA5BT6WqtoGbJv084wryUxVTa92jsMxYz/M2A8z9mMSGZdzCuV+4ClD90/txiRJK2A5Bf5vwBlJTktyHHAxsLOfWJKkUZZ8CqWqHknyBuCzwDHAVVV1e2/JJudxczrnMMzYDzP2w4z96D1jqqrvx5QkrQA/iSlJjbLAJalRR2WBJ1mXZFeSfd3PtQts87wku4du/5Pkwm7d1Um+PrRu42pk7LZ7dCjHzqHx05Lc0n2Nwce6N5JXPGOSjUm+mOT2JHuS/M7QuonN46ivcUhyfDcv+7t52jC07q3d+F1JXtRXpiVk/OMkd3TzdkOSpw2tW/B1X4WMr04yO5TlD4bWbe7+buxLsnkVM14xlO9rSR4aWrdS83hVkoNJ9i6yPkn+uvsz7EnyzKF1S5/HqjrqbsBfAlu75a3Au0dsvw54EPip7v7VwEWPh4zAw4uMXwtc3C2/H3jdamQEfgk4o1v+BeAAcOIk55HBm+Z3A6cDxwFfBc6at80fAu/vli8GPtYtn9VtfzxwWvc4x6xSxucN/Z173VzGw73uq5Dx1cDfLLDvOuCe7ufabnntamSct/0fMbigYsXmsXueZwPPBPYusv4lwGeAAOcAt/Qxj0flETiDj/Rv75a3AxeO2P4i4DNV9d8TTfXjjjTjjyQJcC6wYyn7H4GRGavqa1W1r1v+L+AgMDWBLMPG+RqH4ew7gOd383YB8NGqOlRVXwf2d4+34hmr6qahv3NfYvBZipW0nK/DeBGwq6oerKrvALuA8x8HGS8BrplAjsOqqpsZHAQu5gLg72rgS8CJSdazzHk8Wgv85Ko60C1/Ezh5xPYX89gX/V3drzpXJDm+94TjZzwhyUySL82d4gF+Dnioqh7p7t/H4KsNVisjAEk2MThKuntoeBLzuNDXOMz/8/9om26evstg3sbZd6UyDruUwRHanIVe976Nm/G3u9dwR5K5D+897uaxOwV1GnDj0PBKzOM4FvtzLGseJ/5R+klJ8jng5xdYddnwnaqqJIteK9n9K/irDK5nn/NWBoV1HINrN98CvHOVMj6tqu5PcjpwY5LbGJRRL3qex78HNlfVD7vhXubxaJfklcA08Jyh4ce87lV198KPMFH/DFxTVYeSvIbBbzXnrkKOcVwM7KiqR4fGHi/zOBHNFnhVvWCxdUkeSLK+qg50xXLwMA/1CuC6qvrB0GPPHXUeSvJh4E9WK2NV3d/9vCfJ54GzgY8z+BVsTXd0ueSvMegjY5KfAT4FXNb9ejj32L3M4wLG+RqHuW3uS7IG+Fng22Puu1IZSfICBv9YPqeqDs2NL/K69108IzNW1beH7n6Iwfsic/s+d96+n+8539zzjPt6XQy8fnhgheZxHIv9OZY1j0frKZSdwNy7uZuB6w+z7WPOmXVlNXeu+UJgwXeWJ50xydq50w5JTgKeBdxRg3c/bmJw7n7R/Vco43HAdQzO7+2Yt25S8zjO1zgMZ78IuLGbt53AxRlcpXIacAbw5Z5yHVHGJGcDHwBeVlUHh8YXfN1XKeP6obsvA+7slj8LvLDLuhZ4IT/+W+yKZexynsngTcAvDo2t1DyOYyfwu93VKOcA3+0OcJY3jyvxDu1K3xic67wB2Ad8DljXjU8DHxrabgODfwGfMG//G4HbGBTOPwBPWo2MwG92Ob7a/bx0aP/TGRTPfuCfgONXKeMrgR8Au4duGyc9jwze1f8ag6Opy7qxdzIoQ4ATunnZ383T6UP7Xtbtdxfw4gn+PRyV8XPAA0PztnPU674KGf8CuL3LchNw5tC+v9/N737g91YrY3f/HcDl8/ZbyXm8hsEVWD9gcB77UuC1wGu79WHwH+Dc3WWZ7mMe/Si9JDXqaD2FIklHPQtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNer/AIcN49cwBP8cAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 120==== Step 2 Train Loss 0.723661482334137 ======  0.32653061224489793\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5460, -0.2487,  0.0796,  ...,  0.0712,  0.5809, -0.2903],\n",
            "        [-0.8919,  1.1891,  0.4681,  ...,  0.0017, -0.0401, -0.5482],\n",
            "        [ 0.3677,  1.0110, -0.0076,  ..., -0.1495,  0.3437, -0.5003],\n",
            "        ...,\n",
            "        [ 0.5621,  0.0019, -0.5127,  ...,  0.2555, -0.8495, -0.0778],\n",
            "        [-1.1584,  1.1140,  0.3739,  ..., -0.0214, -0.3133, -0.4230],\n",
            "        [ 0.0228,  0.9145, -0.0177,  ..., -0.1188, -0.0558, -0.4685]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7487,  0.9737,  0.0858,  0.9966,  0.5765, -0.5679,  0.2174,  0.8964,\n",
            "         0.9809,  0.7280,  0.9129,  0.6592,  0.9496,  0.7739, -0.7273,  0.2156,\n",
            "         0.9871,  0.1183,  0.8440,  0.9114,  0.7151, -0.7955,  0.6775,  0.9711,\n",
            "         0.9962,  0.7754, -0.5410,  0.7201,  0.9853, -0.4597,  0.5085,  0.9671,\n",
            "         0.9534,  0.9802,  0.8804, -0.5907,  0.9941,  0.5212,  0.5377, -0.5098,\n",
            "        -0.0779, -0.7881,  0.8868,  0.6455, -0.6419,  0.8631,  0.9718,  0.8789,\n",
            "        -0.1765, -0.6447,  0.0944,  0.7212,  0.9525,  0.9363,  0.9623,  0.2100,\n",
            "         0.1952,  0.9842,  0.8766,  0.9929,  0.4960, -0.2930,  0.9739,  0.4901],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 121==== Step 2 Train Loss 0.7312758564949036 ======  0.3703703703703704\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4742,  0.1476, -0.5290,  ...,  0.2490, -0.9195, -0.1385],\n",
            "        [ 0.3946,  0.1347, -0.4562,  ...,  0.2042, -0.6463, -0.1608],\n",
            "        [ 0.5425, -0.3278,  0.0979,  ...,  0.1151,  0.8034, -0.3171],\n",
            "        ...,\n",
            "        [-1.1503,  1.3181,  0.4642,  ...,  0.0609, -0.4010, -0.4159],\n",
            "        [ 0.6835, -0.4718, -0.0862,  ...,  0.0549,  0.5785, -0.1074],\n",
            "        [ 0.5348, -0.0662, -0.4665,  ...,  0.2833, -0.9400, -0.1275]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.3639,  0.4575,  0.9506, -0.1222,  0.7449, -0.5871,  0.9425,  0.7812,\n",
            "         0.9918,  0.9932,  0.6016,  0.7241,  0.8691, -0.1644, -0.1833,  0.9207,\n",
            "         0.5084,  0.9782,  0.9648,  0.9365,  0.9757,  0.4638,  0.8302,  0.8569,\n",
            "         0.9858,  0.3619,  0.9659, -0.1586,  0.9559,  0.9949,  0.9675,  0.9300,\n",
            "         0.9601, -0.1163, -0.6340, -0.1884,  0.9905,  0.9880, -0.6375,  0.2587,\n",
            "         0.1677,  0.9791,  0.9323, -0.3097,  0.9356, -0.2027,  0.7140,  0.9896,\n",
            "         0.4289,  0.1187,  0.9393,  0.9876, -0.7582,  0.9617,  0.9699,  0.9865,\n",
            "         0.8424,  0.9923, -0.4709,  0.9850,  0.9525,  0.9797,  0.9593,  0.8589],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 122==== Step 2 Train Loss 0.760334312915802 ======  0.25\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3731,  1.0866,  0.5036,  ...,  0.0107, -0.6242, -0.3145],\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [ 0.5829, -0.0029,  0.0511,  ..., -0.1490,  0.6723, -0.2594],\n",
            "        ...,\n",
            "        [ 0.6315,  0.2634, -0.4012,  ...,  0.1298, -0.4018, -0.4272],\n",
            "        [-1.3104,  1.1130,  0.5366,  ..., -0.0132, -0.2718, -0.5188],\n",
            "        [-1.0593,  1.2020,  0.4404,  ..., -0.0745, -0.2726, -0.5630]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9890,  0.9961,  0.9505,  0.9151,  0.9622,  0.7477,  0.9697,  0.8689,\n",
            "         0.9928, -0.6968,  0.6891, -0.0454,  0.9273,  0.9919,  0.9928, -0.8868,\n",
            "         0.9337,  0.9098,  0.9389,  0.9762,  0.8434,  0.9927,  0.9879,  0.9481,\n",
            "         0.9887, -0.7074,  0.9198,  0.9654, -0.5404,  0.5345,  0.9911, -0.2515,\n",
            "         0.9210,  0.9150,  0.9921,  0.9885, -0.7187, -0.5298,  0.9348,  0.9465,\n",
            "         0.9924,  0.9925,  0.9743,  0.4254,  0.0843,  0.9363,  0.9852,  0.9906,\n",
            "         0.9940,  0.9909,  0.9837,  0.6860,  0.9624,  0.9429,  0.9421, -0.3079,\n",
            "         0.9870, -0.5175,  0.8509,  0.9802,  0.8104,  0.9620,  0.3725,  0.9819],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 123==== Step 2 Train Loss 0.7896329760551453 ======  0.24561403508771928\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5444, -0.9329,  0.0776,  ...,  0.0885,  0.5517,  0.2694],\n",
            "        [-1.2957,  0.9894,  0.4829,  ..., -0.0125, -0.2878, -0.3933],\n",
            "        [ 0.0997,  0.9889, -0.0663,  ..., -0.0116, -0.2888, -0.1708],\n",
            "        ...,\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [-1.2087,  1.2789,  0.3959,  ..., -0.1254, -0.4096, -0.3585],\n",
            "        [-1.2366,  1.0443,  0.3502,  ...,  0.0460, -0.6261, -0.3316]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9812,  0.9556, -0.3678, -0.7708, -0.2067,  0.8665, -0.6479,  0.9803,\n",
            "         0.9831,  0.9817, -0.2631,  0.8344,  0.8023, -0.6648,  0.6705, -0.1312,\n",
            "         0.9876,  0.9834,  0.0218, -0.6397, -0.4963,  0.9841,  0.9612,  0.8815,\n",
            "        -0.6547,  0.4199,  0.8869,  0.0266,  0.9383,  0.9865,  0.9487, -0.1267,\n",
            "         0.9851, -0.2800,  0.9391,  0.2885,  0.9727,  0.9718, -0.7739,  0.3329,\n",
            "        -0.4098,  0.9744,  0.9888,  0.9465, -0.3197,  0.7946, -0.1670,  0.8401,\n",
            "         0.3468, -0.1575,  0.8933,  0.8684, -0.5351,  0.6727,  0.9701, -0.6183,\n",
            "         0.9089,  0.9554,  0.6386, -0.0834, -0.4063,  0.9961,  0.9701,  0.9807],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 124==== Step 2 Train Loss 0.6917229890823364 ======  0.43636363636363634\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 5.2090e-01, -9.0209e-01,  2.9167e-02,  ...,  1.7499e-01,\n",
            "          6.2518e-01,  8.5153e-02],\n",
            "        [-1.2788e+00,  1.0906e+00,  5.0907e-01,  ..., -7.2157e-02,\n",
            "         -2.1405e-01, -4.7880e-01],\n",
            "        [ 8.7103e-03,  3.1153e-01, -3.8937e-01,  ...,  1.6266e-02,\n",
            "         -5.9158e-01, -3.5131e-01],\n",
            "        ...,\n",
            "        [-1.2688e+00,  1.0855e+00,  6.0170e-01,  ..., -1.8337e-02,\n",
            "         -2.8322e-01, -3.6021e-01],\n",
            "        [-1.0283e+00,  1.1775e+00,  3.0125e-01,  ...,  2.6897e-02,\n",
            "         -1.2090e-01, -5.9619e-01],\n",
            "        [-1.1165e+00,  1.0646e+00,  5.7593e-01,  ...,  4.9428e-04,\n",
            "         -2.0331e-01, -3.4027e-01]], device='cuda:0')\n",
            "tensor([-0.6274,  0.1361,  0.8060,  0.4032,  0.8381, -0.3677, -0.2001,  0.9532,\n",
            "         0.0033,  0.8137,  0.4638, -0.8077,  0.9704,  0.6533,  0.8816,  0.9627,\n",
            "         0.0033,  0.9862, -0.7421, -0.7724,  0.9764, -0.0541,  0.9394,  0.9938,\n",
            "         0.4871,  0.9539,  0.5339,  0.9863,  0.8380,  0.8217,  0.8728,  0.9846,\n",
            "         0.7770, -0.7402,  0.3634,  0.1473,  0.9824, -0.8035,  0.0406,  0.9488,\n",
            "        -0.2697, -0.1793, -0.2536,  0.4700,  0.9497,  0.8432,  0.9634,  0.8936,\n",
            "         0.9897, -0.8780,  0.6732,  0.5203,  0.9624, -0.1097, -0.4190,  0.9880,\n",
            "         0.9544,  0.6569,  0.9862,  0.5847,  0.6176,  0.9935,  0.7153,  0.9633],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAPYElEQVR4nO3dfYxldX3H8fdHEGyrLUuZbLdoHLC0hqRxMRNKa+MDPoEmgimxS6JdW5pVq42mNukqf9SaNsWmStK00a6CbFuLUJSwLVq7AsaYKHawKywQ3AUxZbuyo4gPaUoFv/3jnpHbYWbvnbkPM7/l/Uomc+7vnHPvZ3938tkz5557J1WFJKk9T1nvAJKktbHAJalRFrgkNcoCl6RGWeCS1Kjjp/lgp5xySs3Ozk7zISWpebfddtu3qmpm6fhUC3x2dpb5+flpPqQkNS/JN5Yb9xSKJDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1aqrvxJSk1ZjdeeN6Rxib+y979djv0yNwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMGFniSpyX5cpKvJrkzyZ9046cluTXJwSTXJDlh8nElSYuGOQJ/BDi3qp4HbAXOS3IO8D7g8qr6BeA7wCWTiylJWmpggVfPD7qbT+2+CjgXuK4b3w1cOJGEkqRlDXUOPMlxSfYBR4C9wL3Aw1X1aLfJA8Cpk4koSVrOUAVeVY9V1VbgmcDZwHOHfYAkO5LMJ5lfWFhYY0xJ0lKrugqlqh4GbgF+FTgpyeKnGT4TOLTCPruqaq6q5mZmZkYKK0l63DBXocwkOalb/gng5cDd9Ir8om6z7cANkwopSXqiYT4PfAuwO8lx9Ar/2qr6lyR3AR9P8qfAfwBXTDCnJGmJgQVeVbcDZy0zfh+98+GSpHXgOzElqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNGljgSZ6V5JYkdyW5M8nbu/H3JDmUZF/39arJx5UkLTp+iG0eBd5ZVV9J8gzgtiR7u3WXV9VfTi6eJGklAwu8qg4Dh7vl7ye5Gzh10sEkSUe3qnPgSWaBs4Bbu6G3Jbk9yZVJNq2wz44k80nmFxYWRgorSXrc0AWe5OnAJ4B3VNX3gA8CzwG20jtCf/9y+1XVrqqaq6q5mZmZMUSWJMGQBZ7kqfTK+2NV9UmAqnqwqh6rqh8BHwbOnlxMSdJSw1yFEuAK4O6q+kDf+Ja+zV4L7B9/PEnSSoa5CuUFwBuAO5Ls68beDVycZCtQwP3AmyaSUJK0rGGuQvkCkGVWfWr8cSRJw/KdmJLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1amCBJ3lWkluS3JXkziRv78ZPTrI3yYHu+6bJx5UkLRrmCPxR4J1VdSZwDvDWJGcCO4GbquoM4KbutiRpSgYWeFUdrqqvdMvfB+4GTgUuAHZ3m+0GLpxUSEnSE63qHHiSWeAs4FZgc1Ud7lZ9E9i8wj47kswnmV9YWBghqiSp39AFnuTpwCeAd1TV9/rXVVUBtdx+VbWrquaqam5mZmaksJKkxw1V4EmeSq+8P1ZVn+yGH0yypVu/BTgymYiSpOUMcxVKgCuAu6vqA32r9gDbu+XtwA3jjydJWsnxQ2zzAuANwB1J9nVj7wYuA65NcgnwDeB1k4koSVrOwAKvqi8AWWH1S8cbR5I0LN+JKUmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatTAAk9yZZIjSfb3jb0nyaEk+7qvV002piRpqWGOwK8Czltm/PKq2tp9fWq8sSRJgwws8Kr6PPDQFLJIklZhlHPgb0tye3eKZdNKGyXZkWQ+yfzCwsIIDydJ6rfWAv8g8BxgK3AYeP9KG1bVrqqaq6q5mZmZNT6cJGmpNRV4VT1YVY9V1Y+ADwNnjzeWJGmQNRV4ki19N18L7F9pW0nSZBw/aIMkVwMvBk5J8gDwx8CLk2wFCrgfeNMEM0qSljGwwKvq4mWGr5hAFknSKvhOTElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNWrgHzWW1JbZnTeudwRNiUfgktQoC1ySGjWwwJNcmeRIkv19Yycn2ZvkQPd902RjSpKWGuYI/CrgvCVjO4GbquoM4KbutiRpigYWeFV9HnhoyfAFwO5ueTdw4ZhzSZIGWOs58M1Vdbhb/iaweaUNk+xIMp9kfmFhYY0PJ0laauQXMauqgDrK+l1VNVdVczMzM6M+nCSps9YCfzDJFoDu+5HxRZIkDWOtBb4H2N4tbwduGE8cSdKwhrmM8Grgi8AvJXkgySXAZcDLkxwAXtbdliRN0cC30lfVxSuseumYs0iSVsF3YkpSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1Cj/pJpGcqz8+a77L3v1ekeQVs0jcElqlAUuSY2ywCWpURa4JDXKApekRlngktSoZi4jPFYuVwMvWZM0Hh6BS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckho10ht5ktwPfB94DHi0qubGEUqSNNg43on5kqr61hjuR5K0Cp5CkaRGjVrgBfxbktuS7FhugyQ7kswnmV9YWBjx4SRJi0Yt8F+vqucD5wNvTfLCpRtU1a6qmququZmZmREfTpK0aKQCr6pD3fcjwPXA2eMIJUkabM0FnuSnkjxjcRl4BbB/XMEkSUc3ylUom4Hrkyzezz9W1b+OJZUkaaA1F3hV3Qc8b4xZJEmr4GWEktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ahwfJ6tVmt1543pH0BI+J2qRR+CS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEaNVOBJzktyT5KDSXaOK5QkabA1F3iS44C/Ac4HzgQuTnLmuIJJko5ulCPws4GDVXVfVf0v8HHggvHEkiQNMsrfxDwV+M++2w8Av7J0oyQ7gB3dzR8kuWeExxy3U4BvrXeIATZ6xo2eD8w4Dhs9H2zwjHkfsPaMz15ucOJ/1LiqdgG7Jv04a5Fkvqrm1jvH0Wz0jBs9H5hxHDZ6PnhyZhzlFMoh4Fl9t5/ZjUmSpmCUAv934IwkpyU5AdgG7BlPLEnSIGs+hVJVjyZ5G/AZ4Djgyqq6c2zJpmNDntpZYqNn3Oj5wIzjsNHzwZMwY6pqnPcnSZoS34kpSY2ywCWpUcd8gSc5OcneJAe675uW2eYlSfb1ff1Pkgu7dVcl+Xrfuq3Tztdt91hfhj1946clubX7OINruheUx2rIOdya5ItJ7kxye5Lf7Fs3sTkc9HEOSU7s5uVgN0+zfeve1Y3fk+SV48q0ynx/kOSubs5uSvLsvnXLPufrkPGNSRb6svxu37rt3c/FgSTb1ynf5X3Zvpbk4b5105rDK5McSbJ/hfVJ8lfdv+H2JM/vW7f2OayqY/oL+AtgZ7e8E3jfgO1PBh4CfrK7fRVw0XrnA36wwvi1wLZu+UPAW9YjI/CLwBnd8s8Dh4GTJjmH9F48vxc4HTgB+Cpw5pJtfg/4ULe8DbimWz6z2/5E4LTufo5bh3wv6ftZe8tivqM95+uQ8Y3AXy+z78nAfd33Td3ypmnnW7L979O7oGJqc9g9zguB5wP7V1j/KuDTQIBzgFvHMYfH/BE4vbf37+6WdwMXDtj+IuDTVfXfE031uNXm+7EkAc4FrlvL/qswMGNVfa2qDnTL/wUcAWYmkKXfMB/n0J/9OuCl3bxdAHy8qh6pqq8DB7v7m2q+qrql72ftS/TeTzFNo3wkxiuBvVX1UFV9B9gLnLfO+S4Grh5zhoGq6vP0DvxWcgHwd9XzJeCkJFsYcQ6fDAW+uaoOd8vfBDYP2H4bT/wB+LPu157Lk5y4TvmelmQ+yZcWT+8APws8XFWPdrcfoPcRB+O2qjlMcja9o6V7+4YnMYfLfZzD0n//j7fp5um79OZtmH2nka/fJfSO0hYt95yP27AZf6N7/q5LsvgGvg01h93pp9OAm/uGpzGHw1jp3zHSHE78rfTTkOSzwM8ts+rS/htVVUlWvG6y+x/xl+ld277oXfRK6wR613D+EfDedcj37Ko6lOR04OYkd9Aro7EY8xz+PbC9qn7UDY88h8e6JK8H5oAX9Q0/4TmvqnuXv4eJ+mfg6qp6JMmb6P1Gc+465BhkG3BdVT3WN7ZR5nAijokCr6qXrbQuyYNJtlTV4a5cjhzlrl4HXF9VP+y778Ujz0eSfBT4w/XIV1WHuu/3JfkccBbwCXq/ih3fHV2u+eMMxpExyU8DNwKXdr8mLt73yHO4gmE+zmFxmweSHA/8DPDtIfedRj6SvIzef5QvqqpHFsdXeM7HXT4DM1bVt/tufoTeayKL+754yb6fm3a+PtuAt/YPTGkOh7HSv2OkOXwynELZAyy+srsduOEo2z7h/FlXWIvnmy8Eln2VeZL5kmxaPO2Q5BTgBcBd1XsV5BZ65+1X3H9KGU8Arqd3nu+6JesmNYfDfJxDf/aLgJu7edsDbEvvKpXTgDOAL48p19D5kpwF/C3wmqo60je+7HM+5nzDZtzSd/M1wN3d8meAV3RZNwGv4P//9jqVfF3G59J7EfCLfWPTmsNh7AF+q7sa5Rzgu92BzWhzOI1XaNfzi975zpuAA8BngZO78TngI33bzdL73/ApS/a/GbiDXun8A/D0aecDfq3L8NXu+yV9+59Or3gOAv8EnLgecwi8HvghsK/va+uk55Deq/tfo3dUdWk39l56hQjwtG5eDnbzdHrfvpd2+90DnD+hn79B+T4LPNg3Z3sGPefrkPHPgTu7LLcAz+3b93e6uT0I/PZ65Otuvwe4bMl+05zDq+ldefVDeuexLwHeDLy5Wx96fwDn3i7L3Djm0LfSS1KjngynUCTpmGSBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEb9HwQ0hhNfulCpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 125==== Step 2 Train Loss 0.6800076365470886 ======  0.5245901639344263\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.7323, -0.4314,  0.0396,  ...,  0.1146,  0.7083, -0.0817],\n",
            "        [-1.4386,  1.1709,  0.5305,  ...,  0.0776, -0.5551, -0.2930],\n",
            "        [-0.4559,  0.8409,  0.1883,  ...,  0.1613, -0.1671, -0.7663],\n",
            "        ...,\n",
            "        [-1.3567,  1.1212,  0.5071,  ...,  0.0908, -0.4530, -0.4381],\n",
            "        [-1.1644,  1.1991,  0.5249,  ..., -0.0393, -0.2487, -0.5677],\n",
            "        [ 0.7688,  0.0416,  0.0851,  ..., -0.0717,  0.2484,  0.1197]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.1025,  0.9849,  0.9351,  0.0810,  0.9704,  0.9156, -0.6722,  0.9494,\n",
            "        -0.0258,  0.9912, -0.8493,  0.8372, -0.4429,  0.9788, -0.3702, -0.1588,\n",
            "         0.6672,  0.8922,  0.9184,  0.8243,  0.9839, -0.9129,  0.4595,  0.9506,\n",
            "         0.9533,  0.9571, -0.3762,  0.9829,  0.3497,  0.8585,  0.7835, -0.3475,\n",
            "         0.9281,  0.8986, -0.0227,  0.9027,  0.9811, -0.0630,  0.9229, -0.4101,\n",
            "         0.8823,  0.9842,  0.8397, -0.6511,  0.9627,  0.7414, -0.8625,  0.7137,\n",
            "         0.9867,  0.9806,  0.9208,  0.9779,  0.1758, -0.1289,  0.5316,  0.9814,\n",
            "         0.9958,  0.5408,  0.7399, -0.6269, -0.4732,  0.9910,  0.6078,  0.8345],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 126==== Step 2 Train Loss 0.676352858543396 ======  0.4705882352941177\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.3910,  0.3610, -0.2484,  ...,  0.2830, -0.5612, -0.3109],\n",
            "        [-1.3466,  1.0309,  0.5970,  ..., -0.0847, -0.3619, -0.3244],\n",
            "        [ 0.9284,  0.0091, -0.0275,  ...,  0.0936,  0.3994, -0.1019],\n",
            "        ...,\n",
            "        [-1.2911,  1.0738,  0.5832,  ..., -0.1055, -0.3588, -0.3891],\n",
            "        [ 0.5063, -0.1391, -0.1948,  ...,  0.2391, -0.0964, -0.3514],\n",
            "        [-0.9357,  1.0441,  0.3356,  ..., -0.0637, -0.3658, -0.4985]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8492,  0.9816,  0.8605,  0.9925,  0.9209,  0.2951,  0.9809,  0.9880,\n",
            "        -0.7101, -0.7537, -0.6665,  0.6801, -0.6188,  0.9364,  0.9904,  0.9553,\n",
            "         0.9849,  0.9853,  0.9748,  0.7136, -0.1282, -0.3809,  0.2892,  0.9657,\n",
            "         0.2571,  0.0977, -0.1632,  0.9675, -0.4652,  0.9745,  0.6901,  0.3492,\n",
            "         0.9825,  0.9716,  0.9885, -0.7713,  0.7907,  0.9542,  0.9866,  0.9578,\n",
            "        -0.6546, -0.0439,  0.7990,  0.9695, -0.6722, -0.8707,  0.9881,  0.9454,\n",
            "        -0.5802, -0.2673,  0.7565,  0.9876, -0.7927,  0.9615,  0.9889,  0.9189,\n",
            "         0.9814, -0.4993,  0.9923, -0.6939,  0.9821,  0.9925,  0.9184,  0.9632],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 127==== Step 2 Train Loss 0.7055736184120178 ======  0.40816326530612246\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3200,  1.0510,  0.6014,  ...,  0.0297, -0.4826, -0.3896],\n",
            "        [-1.0263,  1.2552,  0.3542,  ...,  0.0186, -0.0880, -0.5781],\n",
            "        [-0.8313,  0.9267,  0.2940,  ...,  0.0815,  0.0583, -0.4111],\n",
            "        ...,\n",
            "        [-1.2982,  1.1451,  0.5844,  ...,  0.0090, -0.2458, -0.4737],\n",
            "        [ 0.8838, -0.5037, -0.2511,  ...,  0.0599,  0.3533, -0.0494],\n",
            "        [-1.4107,  1.1037,  0.4881,  ...,  0.0159, -0.7426, -0.2042]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1500,  0.9841, -0.7717,  0.9687,  0.3240,  0.0559,  0.7812,  0.9716,\n",
            "         0.9912,  0.9601,  0.3389,  0.9782,  0.9226, -0.1195,  0.8972,  0.9507,\n",
            "        -0.4327,  0.7817,  0.9889,  0.2386,  0.5556,  0.9393,  0.8115,  0.1425,\n",
            "         0.9896,  0.0491,  0.9441,  0.8762, -0.6143,  0.9847,  0.8964, -0.4584,\n",
            "         0.0041,  0.9596,  0.8824,  0.9923,  0.9887,  0.8075,  0.9359,  0.6214,\n",
            "         0.8420,  0.7302, -0.0541,  0.9098,  0.9806,  0.7371, -0.2690,  0.9220,\n",
            "         0.9814,  0.0632,  0.9585,  0.9857,  0.9610,  0.9837,  0.0848, -0.7448,\n",
            "         0.9914,  0.9882,  0.8398, -0.8580,  0.9802,  0.9945,  0.7159,  0.9923],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOFklEQVR4nO3df6zd9V3H8edr7QqaZdKOm1rpQkuGkiZGWG4QJXEO2MaGgSaSWeK005q6Oc3MNK7IP7poBP8QNZrMBpBODT/sXKgjy1JKyWICzIvjN4EWtsVioXcDpsSIg73943zvdna5t+f03nPu4VOfj+TmfH+e8+rn3L7u937P+Z6bqkKS1J43TTqAJGlpLHBJapQFLkmNssAlqVEWuCQ1avVKPtjpp59emzZtWsmHlKTmPfDAA9+oqqn5y1e0wDdt2sTMzMxKPqQkNS/J1xda7ikUSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElq1IpeiSlJJ2LTrjsnHWEkvnbtZWO5X4/AJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVq6AJPsirJV5J8vpvfnOT+JIeT3JZkzfhiSpLmO5Ej8I8DT/TNXwdcX1XvAF4EdowymCTp+IYq8CQbgcuAG7r5ABcBe7tN9gBbxxFQkrSwYY/A/xz4PeA73fzbgJeq6tVu/ghwxkI7JtmZZCbJzOzs7LLCSpK+Z2CBJ/k54FhVPbCUB6iq3VU1XVXTU1NTS7kLSdIChvk88AuBy5N8ADgVeCvwF8BpSVZ3R+EbgWfHF1OSNN/AI/CqurqqNlbVJmAbcHdV/SJwELiy22w7cMfYUkqSXmc57wP/JPCJJIfpnRO/cTSRJEnDOKE/qVZV9wD3dNPPAOePPpIkaRheiSlJjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjBhZ4klOTfDnJQ0keS/KH3fLNSe5PcjjJbUnWjD+uJGnOMEfgrwAXVdVPAOcClya5ALgOuL6q3gG8COwYX0xJ0nwDC7x6Xu5m39x9FXARsLdbvgfYOpaEkqQFDXUOPMmqJA8Cx4D9wNPAS1X1arfJEeCMRfbdmWQmyczs7OwoMkuSGLLAq+q1qjoX2AicD5wz7ANU1e6qmq6q6ampqSXGlCTNd0LvQqmql4CDwE8BpyVZ3a3aCDw74mySpOMY5l0oU0lO66Z/AHgP8AS9Ir+y22w7cMe4QkqSXm/14E3YAOxJsope4d9eVZ9P8jhwa5I/Ar4C3DjGnJKkeQYWeFU9DJy3wPJn6J0PlyRNgFdiSlKjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1KiBBZ7k7UkOJnk8yWNJPt4tX5dkf5JD3e3a8ceVJM0Z5gj8VeB3qmoLcAHwsSRbgF3Agao6GzjQzUuSVsjAAq+qo1X1b930fwFPAGcAVwB7us32AFvHFVKS9HondA48ySbgPOB+YH1VHe1WPQesX2SfnUlmkszMzs4uI6okqd/QBZ7kLcBngd+uqv/sX1dVBdRC+1XV7qqarqrpqampZYWVJH3PUAWe5M30yvsfquqfusXPJ9nQrd8AHBtPREnSQoZ5F0qAG4EnqurP+lbtA7Z309uBO0YfT5K0mNVDbHMh8EvAI0ke7Jb9PnAtcHuSHcDXgQ+OJ6IkaSEDC7yq/gXIIqsvHm0cSdKwvBJTkhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWqUBS5JjbLAJalRFrgkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRg0s8CQ3JTmW5NG+ZeuS7E9yqLtdO96YkqT5hjkCvxm4dN6yXcCBqjobONDNS5JW0MACr6ovAS/MW3wFsKeb3gNsHXEuSdIASz0Hvr6qjnbTzwHrR5RHkjSkZb+IWVUF1GLrk+xMMpNkZnZ2drkPJ0nqLLXAn0+yAaC7PbbYhlW1u6qmq2p6ampqiQ8nSZpvqQW+D9jeTW8H7hhNHEnSsIZ5G+EtwL3AjyU5kmQHcC3wniSHgEu6eUnSClo9aIOqumqRVRePOIsk6QR4JaYkNcoCl6RGWeCS1CgLXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDXKApekRlngktQoC1ySGmWBS1KjLHBJatTAj5OVFrNp152TjqAFfO3ayyYdQSvEI3BJapQFLkmNssAlqVEWuCQ1ygKXpEZZ4JLUKAtckhplgUtSoyxwSWpUM1dinkxX/XmlnKRR8AhckhplgUtSoyxwSWpUM+fATyYn0/l8SZPjEbgkNcoCl6RGWeCS1CgLXJIa5YuY0knGF8n///AIXJIaZYFLUqMscElqlAUuSY2ywCWpURa4JDVqWQWe5NIkTyY5nGTXqEJJkgZbcoEnWQX8NfB+YAtwVZItowomSTq+5RyBnw8crqpnqup/gVuBK0YTS5I0yHKuxDwD+Pe++SPAT87fKMlOYGc3+3KSJ5fxmKNwOvCNCWcYxIyjYcbRMOMy5TpgeRnPXGjh2C+lr6rdwO5xP86wksxU1fSkcxyPGUfDjKNhxtEYR8blnEJ5Fnh73/zGbpkkaQUsp8D/FTg7yeYka4BtwL7RxJIkDbLkUyhV9WqS3wS+CKwCbqqqx0aWbHzeMKdzjsOMo2HG0TDjaIw8Y6pq1PcpSVoBXokpSY2ywCWpUSdlgSdZl2R/kkPd7doFtnl3kgf7vv4nydZu3c1Jvtq37txJZOy2e60vx76+5ZuT3N99jMFt3QvJK54xyblJ7k3yWJKHk/xC37qxjeOgj3FIcko3Loe7cdrUt+7qbvmTSd43qkxLyPiJJI9343YgyZl96xZ83ieQ8cNJZvuy/Frfuu3d98ahJNsnmPH6vnxPJXmpb93YxzHJTUmOJXl0kfVJ8pdd/oeTvLNv3fLGsKpOui/gT4Fd3fQu4LoB268DXgB+sJu/GbjyjZAReHmR5bcD27rpTwMfnURG4EeBs7vpHwGOAqeNcxzpvWj+NHAWsAZ4CNgyb5vfAD7dTW8Dbuumt3TbnwJs7u5n1YQyvrvve+6jcxmP97xPIOOHgb9aYN91wDPd7dpueu0kMs7b/rfovaFiJcfxZ4B3Ao8usv4DwBeAABcA949qDE/KI3B6l/Tv6ab3AFsHbH8l8IWq+u+xpvp+J5rxu5IEuAjYu5T9T8DAjFX1VFUd6qb/AzgGTI0hS79hPsahP/te4OJu3K4Abq2qV6rqq8Dh7v5WPGNVHez7nruP3rUUK2k5H4fxPmB/Vb1QVS8C+4FL3wAZrwJuGUOORVXVl+gdAC7mCuAz1XMfcFqSDYxgDE/WAl9fVUe76eeA9QO238brn/Q/7n7duT7JKSNPOHzGU5PMJLlv7hQP8Dbgpap6tZs/Qu+jDSaVEYAk59M7Snq6b/E4xnGhj3GY/+//7jbdOH2L3rgNs+9KZey3g95R2pyFnvdRGzbjz3fP4d4kcxfvveHGsTsFtRm4u2/xSozjIIv9G5Y9hs3+VfokdwE/vMCqa/pnqqqSLPpeye4n4Y/Tez/7nKvpFdYaeu/d/CTwqQllPLOqnk1yFnB3kkfoldFIjHgc/w7YXlXf6RaPZBxPdkk+BEwD7+pb/LrnvaqeXvgexuqfgVuq6pUkv07vt5qLJpBjGNuAvVX1Wt+yN8o4jkWzBV5Vlyy2LsnzSTZU1dGuWI4d564+CHyuqr7dd99zR52vJPlb4HcnlbGqnu1un0lyD3Ae8Fl6v4at7o4ul/wxBqPImOStwJ3ANd2viHP3PZJxXMAwH+Mwt82RJKuBHwK+OeS+K5WRJJfQ+2H5rqp6ZW75Is/7qItnYMaq+mbf7A30XheZ2/dn5+17z4jzzT3OsM/XNuBj/QtWaBwHWezfsOwxPFlPoewD5l7R3Q7ccZxtX3fOrCuruXPNW4EFX10ed8Yka+dOOyQ5HbgQeLx6r4AcpHfuftH9VyjjGuBz9M7x7Z23blzjOMzHOPRnvxK4uxu3fcC29N6lshk4G/jyiHKdUMYk5wF/A1xeVcf6li/4vE8o44a+2cuBJ7rpLwLv7bKuBd7L9/8Wu2IZu5zn0Hsh8N6+ZSs1joPsA365ezfKBcC3uoOb5Y/huF+hncQXvXOdB4BDwF3Aum75NHBD33ab6P0UfNO8/e8GHqFXOH8PvGUSGYGf7nI81N3u6Nv/LHrFcxj4R+CUCWX8EPBt4MG+r3PHPY70Xtl/it7R1DXdsk/RK0OAU7txOdyN01l9+17T7fck8P4xfh8OyngX8HzfuO0b9LxPIOOfAI91WQ4C5/Tt+6vd+B4GfmVSGbv5PwCunbffiowjvQPAo93/gyP0Xs/4CPCRbn3o/fGbp7sc06MaQy+ll6RGnaynUCTppGeBS1KjLHBJapQFLkmNssAlqVEWuCQ1ygKXpEb9H77vBOxRtpmZAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 128==== Step 2 Train Loss 0.7193157076835632 ======  0.4482758620689656\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.6917,  0.9628,  0.2721,  ..., -0.0549, -0.0926, -0.5986],\n",
            "        [-1.5234,  1.0478,  0.6344,  ...,  0.0575, -0.6737, -0.2081],\n",
            "        [-1.1239,  1.2301,  0.4535,  ...,  0.0948, -0.4713, -0.4432],\n",
            "        ...,\n",
            "        [-0.1860,  0.9814, -0.0823,  ..., -0.0246, -0.5255, -0.4431],\n",
            "        [-0.8989,  1.3933,  0.4126,  ...,  0.0661, -0.3699, -0.3615],\n",
            "        [-1.2760,  1.1292,  0.6219,  ..., -0.0763, -0.3256, -0.4322]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6583,  0.9839,  0.9810,  0.9735,  0.1952,  0.9820, -0.8006,  0.9881,\n",
            "         0.9643,  0.8895,  0.5834,  0.8905, -0.3084,  0.9943,  0.9646,  0.7318,\n",
            "         0.9839,  0.7819,  0.9388, -0.1997,  0.9695,  0.7263,  0.8909, -0.7771,\n",
            "         0.9252,  0.1904,  0.9876,  0.9218,  0.7981,  0.8104,  0.9723, -0.3690,\n",
            "         0.8733,  0.2233,  0.9806,  0.9007,  0.1822,  0.8574,  0.9601,  0.9246,\n",
            "        -0.7078, -0.0151,  0.9895,  0.3781, -0.2245,  0.6829, -0.5286,  0.9476,\n",
            "         0.0706, -0.4794,  0.7799,  0.6546,  0.9326,  0.6758,  0.7183,  0.6437,\n",
            "         0.8080,  0.1473,  0.0422, -0.1889,  0.9955,  0.7358,  0.6591, -0.0052],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 129==== Step 2 Train Loss 0.6854904294013977 ======  0.36000000000000004\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 5.3820e-01,  9.4640e-01, -4.9709e-02,  ..., -4.4307e-02,\n",
            "         -2.0816e-01, -4.1490e-01],\n",
            "        [-3.6518e-02,  8.0515e-01, -1.1191e-01,  ...,  9.8303e-02,\n",
            "         -2.3371e-01, -3.4860e-01],\n",
            "        [ 4.2170e-01, -9.4411e-01,  6.7909e-02,  ...,  2.1883e-01,\n",
            "          6.2749e-01,  9.6894e-02],\n",
            "        ...,\n",
            "        [ 5.8982e-01,  8.8383e-01, -3.7515e-01,  ...,  1.5714e-01,\n",
            "         -4.4977e-01, -3.7483e-01],\n",
            "        [-9.1595e-01,  1.4036e+00,  5.1513e-01,  ..., -3.8131e-02,\n",
            "         -2.8072e-01, -3.6923e-01],\n",
            "        [-1.3078e+00,  9.3616e-01,  5.6553e-01,  ..., -9.5568e-04,\n",
            "         -3.3426e-01, -3.9002e-01]], device='cuda:0')\n",
            "tensor([ 0.7944,  0.6089, -0.5738,  0.4253,  0.8967,  0.8930, -0.3176,  0.9726,\n",
            "         0.7139,  0.9838,  0.9737,  0.4199,  0.6316,  0.7854,  0.9909,  0.7565,\n",
            "         0.9896,  0.0126, -0.6496,  0.3460, -0.2327,  0.9169,  0.9843,  0.9753,\n",
            "         0.9757, -0.7282,  0.3816,  0.8893,  0.5837, -0.3531,  0.9898,  0.9622,\n",
            "         0.9262, -0.6069, -0.2072,  0.5923,  0.9359,  0.9940, -0.2364,  0.5843,\n",
            "         0.1302,  0.1356,  0.9930,  0.3005,  0.2941,  0.9868, -0.5669,  0.9772,\n",
            "         0.9904,  0.9467,  0.9492,  0.7820,  0.2325, -0.1997,  0.9882,  0.6195,\n",
            "         0.9850, -0.7788,  0.8774,  0.9576, -0.7081,  0.4072, -0.2190,  0.9873],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
            "        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 130==== Step 2 Train Loss 0.7380703687667847 ======  0.2962962962962963\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.7252, -0.4601, -0.0061,  ...,  0.0468,  0.6158, -0.0361],\n",
            "        [-1.2558,  1.1673,  0.3576,  ..., -0.0415, -0.4248, -0.5587],\n",
            "        [-1.2986,  1.0662,  0.3883,  ...,  0.1176, -0.5177, -0.2900],\n",
            "        ...,\n",
            "        [ 0.6242, -0.5360, -0.0182,  ...,  0.1521,  0.6371, -0.2428],\n",
            "        [ 0.5075,  0.3922, -0.4368,  ...,  0.0763, -0.8062, -0.2098],\n",
            "        [ 0.1742,  0.6754, -0.2077,  ...,  0.0399, -0.6339, -0.2681]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9355,  0.7899,  0.9896,  0.8044,  0.9639,  0.5847,  0.9570, -0.3006,\n",
            "         0.4245,  0.9743,  0.7128, -0.4316, -0.8836,  0.9669, -0.8545,  0.9770,\n",
            "        -0.6562,  0.7833,  0.7538, -0.8973, -0.1589, -0.4233,  0.9207,  0.9813,\n",
            "        -0.5249,  0.8890,  0.9720,  0.6865,  0.6284, -0.1217,  0.8876,  0.9786,\n",
            "         0.4194, -0.0054,  0.9361,  0.1749,  0.9940,  0.7908,  0.9704,  0.9919,\n",
            "         0.9478,  0.8293,  0.9482,  0.9753,  0.7725,  0.0723,  0.9048, -0.1237,\n",
            "         0.3057,  0.9916,  0.9916,  0.8502,  0.7388,  0.5341,  0.8778,  0.9852,\n",
            "         0.8269,  0.9501,  0.8003,  0.7563,  0.4035,  0.9762, -0.2552, -0.0501],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 131==== Step 2 Train Loss 0.708624005317688 ======  0.5\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.7102,  1.3389,  0.5039,  ...,  0.0935, -0.0590, -0.3563],\n",
            "        [-1.3170,  1.0739,  0.5085,  ...,  0.0322, -0.6405, -0.3679],\n",
            "        [-1.3352,  1.1677,  0.5116,  ...,  0.0272, -0.5507, -0.3725],\n",
            "        ...,\n",
            "        [ 0.4691, -0.8044,  0.1886,  ...,  0.2271,  0.7785, -0.0239],\n",
            "        [ 0.3651, -0.8368, -0.0228,  ...,  0.1812,  0.3445,  0.1345],\n",
            "        [-1.1309,  1.1055,  0.3674,  ...,  0.0069, -0.2851, -0.5010]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9703,  0.9970, -0.2170,  0.9797,  0.9235,  0.9736, -0.7321,  0.7788,\n",
            "        -0.2706,  0.4537, -0.0739, -0.8810,  0.9293,  0.8533, -0.1380, -0.7124,\n",
            "         0.9851,  0.1554, -0.1598,  0.3912,  0.9923,  0.9879,  0.9271,  0.9576,\n",
            "         0.9942,  0.9521, -0.3132,  0.0351,  0.9923,  0.9274,  0.9810,  0.7881,\n",
            "         0.9746,  0.8394, -0.2488,  0.9699, -0.0708,  0.9261,  0.9871,  0.9363,\n",
            "         0.8549,  0.5433,  0.9540,  0.9762,  0.9865,  0.9874,  0.9756, -0.7370,\n",
            "         0.8466, -0.6934, -0.0501,  0.7139,  0.7864,  0.9904,  0.0854,  0.9575,\n",
            "         0.9716,  0.8128,  0.9225,  0.9432,  0.9930, -0.5733,  0.7932, -0.2222],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 132==== Step 2 Train Loss 0.6873115301132202 ======  0.4444444444444444\n",
            "torch.Size([64, 48])\n",
            "tensor([[-0.6857,  1.0146,  0.2045,  ..., -0.1150,  0.0984, -0.6723],\n",
            "        [ 0.5507,  0.3994, -0.3819,  ...,  0.0731, -0.7399, -0.0767],\n",
            "        [-0.0349,  0.7202, -0.0960,  ...,  0.0350, -0.2228, -0.3935],\n",
            "        ...,\n",
            "        [-1.3104,  1.0613,  0.6367,  ..., -0.0392, -0.2553, -0.4147],\n",
            "        [ 0.3782,  0.1249, -0.2557,  ...,  0.3521, -0.8663, -0.2478],\n",
            "        [-0.1552,  1.0605, -0.1490,  ...,  0.0434, -0.4132, -0.6567]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6235,  0.9588,  0.9121,  0.8824,  0.6808,  0.9824, -0.2670,  0.6583,\n",
            "         0.3463,  0.6243,  0.9740,  0.9524,  0.9586, -0.2753,  0.9264,  0.7547,\n",
            "         0.6710,  0.9585,  0.9880,  0.0384,  0.9681,  0.9209,  0.5151,  0.9792,\n",
            "         0.7656,  0.4260, -0.5601,  0.8986,  0.9585, -0.7053,  0.3050,  0.7390,\n",
            "         0.7987,  0.9949,  0.9538, -0.7231, -0.8674,  0.9573,  0.9751, -0.2364,\n",
            "         0.9695,  0.9697, -0.1759,  0.8201,  0.9776, -0.0502,  0.9730, -0.0469,\n",
            "         0.9654, -0.3515,  0.9698,  0.9940,  0.9815,  0.9175,  0.8355, -0.4779,\n",
            "         0.9927,  0.7127,  0.3697,  0.1989,  0.0948,  0.9918,  0.7625,  0.6626],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 133==== Step 2 Train Loss 0.7140077352523804 ======  0.339622641509434\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.3731,  1.0866,  0.5036,  ...,  0.0107, -0.6242, -0.3145],\n",
            "        [ 0.7361, -0.0419, -0.1151,  ...,  0.0074,  0.4744, -0.3698],\n",
            "        [-1.3170,  1.0739,  0.5085,  ...,  0.0322, -0.6405, -0.3679],\n",
            "        ...,\n",
            "        [ 0.6557, -0.6310, -0.0221,  ...,  0.2287,  0.6686, -0.1256],\n",
            "        [ 0.4704,  0.2733, -0.2495,  ...,  0.0974, -0.5209, -0.0523],\n",
            "        [ 0.8278,  0.0879, -0.1993,  ...,  0.0698,  0.2987, -0.3112]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.2233,  0.6523,  0.9929,  0.9925, -0.5687,  0.8915,  0.5834,  0.3473,\n",
            "         0.8299,  0.8320,  0.6401,  0.9774,  0.0319,  0.7446, -0.1670, -0.1653,\n",
            "         0.7141, -0.4745, -0.6848,  0.9888, -0.7613,  0.0646,  0.9342,  0.9954,\n",
            "         0.4820,  0.9185,  0.9932, -0.7092, -0.1260, -0.3812,  0.4057,  0.0906,\n",
            "         0.7637, -0.4909, -0.4429,  0.9555,  0.7357,  0.8762, -0.6506,  0.9444,\n",
            "         0.4255,  0.9900,  0.9940,  0.2250,  0.7770, -0.4205,  0.9415, -0.0258,\n",
            "         0.9651, -0.6775,  0.6613, -0.2211,  0.9355, -0.7575, -0.1564,  0.9743,\n",
            "         0.9521,  0.9698,  0.9783,  0.9029,  0.9945,  0.9662,  0.9078,  0.7432],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 134==== Step 2 Train Loss 0.6960758566856384 ======  0.42857142857142855\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0123,  1.2382,  0.3965,  ..., -0.0490, -0.3938, -0.3546],\n",
            "        [-0.7341,  1.1717,  0.4102,  ..., -0.0769, -0.3568, -0.4939],\n",
            "        [-0.5509,  1.2220,  0.2296,  ..., -0.0422, -0.0240, -0.7031],\n",
            "        ...,\n",
            "        [ 0.5348, -0.0662, -0.4665,  ...,  0.2833, -0.9400, -0.1275],\n",
            "        [ 0.3517,  1.0562, -0.2567,  ..., -0.0289, -0.4711, -0.3207],\n",
            "        [-1.0377,  1.1186,  0.3893,  ...,  0.0325, -0.1161, -0.5927]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.0953,  0.9367,  0.9599,  0.7982,  0.9938,  0.4602, -0.0380,  0.9469,\n",
            "         0.9911,  0.9705, -0.6891,  0.0574,  0.9294, -0.2663,  0.7981,  0.6952,\n",
            "         0.9809,  0.3515,  0.2006, -0.0113,  0.7924,  0.9471, -0.5700,  0.9457,\n",
            "         0.5290, -0.2403, -0.6157,  0.9887,  0.9661,  0.9687,  0.2135,  0.9007,\n",
            "        -0.4205, -0.6707,  0.9647,  0.7213, -0.5746,  0.8762,  0.9698, -0.1140,\n",
            "         1.0000,  0.8954,  0.9746,  0.9943,  0.0810,  0.9829,  0.1504,  0.9297,\n",
            "         0.7229,  0.7205,  0.9877,  0.9515,  0.9897, -0.7236, -0.2158,  0.9525,\n",
            "         0.9717,  0.8909,  0.9421,  0.9936, -0.2028,  0.9487,  0.8623,  0.9348],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 135==== Step 2 Train Loss 0.7199146747589111 ======  0.2857142857142857\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.5579,  0.2802, -0.4738,  ...,  0.1372, -0.7609, -0.1878],\n",
            "        [ 0.2078,  0.7243, -0.0922,  ...,  0.0840, -0.2580, -0.2437],\n",
            "        [-0.3224,  1.2290,  0.0317,  ..., -0.1354, -0.4267, -0.4288],\n",
            "        ...,\n",
            "        [-0.6855,  1.0726,  0.1944,  ...,  0.0835, -0.4580, -0.5436],\n",
            "        [-1.3462,  1.1147,  0.5766,  ..., -0.0501, -0.2399, -0.4743],\n",
            "        [-0.7564,  1.2050,  0.2498,  ..., -0.1190, -0.3366, -0.5061]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.1052,  0.5483,  0.8337,  0.9703,  0.6519,  0.6477,  0.9783,  0.9335,\n",
            "         0.8652,  0.9423,  0.9275,  0.8763,  0.9928,  0.5494,  0.9695,  0.8352,\n",
            "        -0.0706,  0.9836,  0.9898, -0.1247,  0.2462,  0.7946,  0.4953,  0.9693,\n",
            "        -0.0520, -0.8138, -0.0447,  0.8256,  0.9516,  0.9605, -0.5515,  0.9930,\n",
            "         0.9759,  0.7318,  0.9581,  0.9691,  0.9725,  0.9190,  0.1967,  0.9861,\n",
            "         0.9774, -0.0397,  0.9568, -0.7093,  0.3290,  0.6497,  0.5350,  0.8151,\n",
            "         0.4571,  0.9601,  0.9585, -0.3415,  0.3514,  0.7405,  0.7997,  0.9385,\n",
            "        -0.1985,  0.1062,  0.9880, -0.7635,  0.8768, -0.5468,  0.9868,  0.9841],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 136==== Step 2 Train Loss 0.7245652675628662 ======  0.4067796610169491\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4232, -0.8927,  0.0313,  ...,  0.1150,  0.5829,  0.0558],\n",
            "        [-1.2788,  1.0906,  0.5091,  ..., -0.0722, -0.2141, -0.4788],\n",
            "        [ 0.6224, -0.5825,  0.0156,  ...,  0.1775,  0.7018, -0.1850],\n",
            "        ...,\n",
            "        [ 0.4398,  0.2132, -0.4839,  ...,  0.1051, -0.9034,  0.0118],\n",
            "        [-0.9953,  1.3825,  0.5212,  ...,  0.0339, -0.1526, -0.4483],\n",
            "        [-0.9852,  1.1821,  0.2202,  ..., -0.0543, -0.3842, -0.6609]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.8369,  0.9733, -0.2395,  0.1706,  0.9791, -0.2508, -0.4855,  0.4620,\n",
            "        -0.5190,  0.9903,  0.9628, -0.1839, -0.4435,  0.3772,  0.9727,  0.9918,\n",
            "         0.7218, -0.8545,  0.9704,  0.4033,  0.9298, -0.7850,  0.9782,  0.8029,\n",
            "         0.8793,  0.9337,  0.7135,  0.9654,  0.9206,  0.9789,  0.9950, -0.6127,\n",
            "         0.9844,  0.5591,  0.8568, -0.0850, -0.5726,  0.8809,  0.9848,  0.9710,\n",
            "        -0.7274,  0.9511,  0.9931,  0.8586,  0.9668,  0.9876,  0.4369,  0.2426,\n",
            "         0.9861,  0.9614,  0.8956,  0.7334,  0.9163,  0.8869,  0.7631,  0.9440,\n",
            "         0.7285,  0.8866, -0.1752,  0.9611,  0.8447,  0.8313, -0.7235,  0.7454],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 137==== Step 2 Train Loss 0.6968441009521484 ======  0.4642857142857143\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.2710,  1.0923,  0.4392,  ...,  0.0119, -0.5537, -0.3883],\n",
            "        [ 0.9904,  0.0765,  0.0468,  ...,  0.0635,  0.4033, -0.1051],\n",
            "        [-1.5528,  0.9868,  0.6148,  ...,  0.0784, -0.6554, -0.2778],\n",
            "        ...,\n",
            "        [-1.3555,  1.0751,  0.5338,  ...,  0.0804, -0.6597, -0.2875],\n",
            "        [ 0.5082,  0.4068, -0.3802,  ...,  0.3840, -0.7093, -0.3536],\n",
            "        [ 0.5941, -0.4109,  0.0466,  ...,  0.0364,  0.6148, -0.0963]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9228,  0.8087,  0.9855,  0.9934,  0.6384,  0.9700, -0.1791,  0.8825,\n",
            "         0.7740,  0.9729, -0.2943,  0.9936,  0.9911,  0.9006,  0.9912,  0.8617,\n",
            "         0.9036,  0.9826,  0.9904, -0.2287,  0.9917, -0.2153, -0.7446,  0.9710,\n",
            "         0.0459,  0.9807,  0.9811,  0.9890,  0.4173, -0.6335,  0.9626,  0.9500,\n",
            "        -0.7004,  0.9432,  0.4820, -0.7977,  0.7899, -0.4732,  0.9859,  0.6599,\n",
            "         0.9930,  0.9746,  0.9720,  0.9570,  0.5918,  0.9898,  0.6421,  0.2012,\n",
            "         0.9787,  0.9861, -0.6212,  0.2499,  0.9928, -0.2187,  0.6054,  0.9647,\n",
            "        -0.0749,  0.7618, -0.3121,  0.8651,  0.7692,  0.9891,  0.8802,  0.8160],\n",
            "       device='cuda:0')\n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
            "        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 138==== Step 2 Train Loss 0.7020964622497559 ======  0.4074074074074074\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.0031e+00,  9.2893e-01,  3.5009e-01,  ..., -5.6363e-02,\n",
            "         -2.3959e-01, -4.7078e-01],\n",
            "        [-1.1165e+00,  1.0646e+00,  5.7593e-01,  ...,  4.9428e-04,\n",
            "         -2.0331e-01, -3.4027e-01],\n",
            "        [-1.4107e+00,  1.1037e+00,  4.8814e-01,  ...,  1.5879e-02,\n",
            "         -7.4265e-01, -2.0417e-01],\n",
            "        ...,\n",
            "        [-2.2486e-01,  9.5080e-01, -1.8573e-01,  ...,  9.3263e-02,\n",
            "         -5.0209e-01, -6.4058e-01],\n",
            "        [ 1.9257e-01,  1.8080e-01, -2.0716e-01,  ...,  1.9050e-01,\n",
            "         -7.9082e-01, -1.1393e-01],\n",
            "        [-1.2688e+00,  1.0855e+00,  6.0170e-01,  ..., -1.8337e-02,\n",
            "         -2.8322e-01, -3.6021e-01]], device='cuda:0')\n",
            "tensor([-0.6203, -0.0563, -0.4435,  0.6413, -0.8779,  0.8157, -0.8421,  0.6999,\n",
            "        -0.7445,  0.5896,  0.7625, -0.2677,  0.8960,  0.3558, -0.0062,  0.2957,\n",
            "         0.3847,  0.9357, -0.1149,  0.7237,  0.9896, -0.6479,  0.9559,  0.8986,\n",
            "         0.9704,  0.9150,  0.9301, -0.0374, -0.8628,  0.2629,  0.9603,  0.1258,\n",
            "         0.9397, -0.6373,  0.8791,  0.3015,  0.0102,  0.9883,  0.0701,  0.9874,\n",
            "        -0.7179,  0.8631,  0.6718,  0.9930, -0.1689,  0.9639,  0.9918,  0.4820,\n",
            "        -0.2606,  0.9463,  0.7928,  0.8775,  0.4455, -0.1752,  0.9726, -0.7788,\n",
            "         0.7526,  0.9429,  0.8784, -0.3709, -0.7204,  0.7840,  0.9468,  0.9881],\n",
            "       device='cuda:0')\n",
            "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 139==== Step 2 Train Loss 0.7081156969070435 ======  0.3529411764705882\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.3831,  0.7676, -0.3136,  ..., -0.0054, -0.5800, -0.3450],\n",
            "        [-0.3321,  0.8602, -0.1449,  ..., -0.1059, -0.3062, -0.6540],\n",
            "        [-1.4209,  0.9922,  0.4254,  ...,  0.0422, -0.6627, -0.4782],\n",
            "        ...,\n",
            "        [-1.0414,  1.3739,  0.6016,  ...,  0.0296, -0.2668, -0.3415],\n",
            "        [-1.4876,  1.0979,  0.5174,  ...,  0.0869, -0.6326, -0.4616],\n",
            "        [ 0.4774,  0.2584, -0.0378,  ..., -0.0413,  0.3429, -0.3216]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.9521,  0.7738,  0.9954,  0.9497,  0.9881, -0.4487,  0.1626,  0.0679,\n",
            "        -0.2606, -0.4595, -0.1023,  0.6932, -0.1009,  0.1573,  0.4903,  0.8330,\n",
            "         0.0726,  0.8808,  0.7955,  0.9834,  0.9598,  0.2117,  0.9275, -0.4972,\n",
            "         0.9605,  0.7314,  0.9430, -0.5793,  0.9293,  0.1029, -0.4209,  0.9783,\n",
            "         0.9779,  0.8290,  0.9696,  0.9803,  0.6935,  0.4726, -0.4684,  0.9665,\n",
            "         0.8774,  0.0319,  0.3442,  0.9672,  0.9768,  0.9093,  0.8988,  0.9341,\n",
            "         0.8180, -0.0451, -0.5122,  0.6852,  0.8124, -0.2232,  0.9640,  0.6447,\n",
            "         0.9756, -0.1790,  0.9329,  0.9020,  0.8795,  0.6521, -0.3557, -0.1504],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 140==== Step 2 Train Loss 0.7243386507034302 ======  0.2553191489361702\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4815,  0.2858, -0.4466,  ...,  0.1091, -0.7968,  0.0141],\n",
            "        [ 0.4131,  0.3238, -0.2471,  ...,  0.2179, -0.3592, -0.2757],\n",
            "        [ 0.6625, -0.3848,  0.0234,  ..., -0.0880,  0.3119,  0.1919],\n",
            "        ...,\n",
            "        [-1.1707,  1.3597,  0.4744,  ...,  0.0482, -0.1981, -0.3794],\n",
            "        [ 0.4073,  0.5618, -0.3719,  ...,  0.1893, -0.3844, -0.2963],\n",
            "        [ 0.3910,  0.3610, -0.2484,  ...,  0.2830, -0.5612, -0.3109]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.6703,  0.8722,  0.9243,  0.9654, -0.2028, -0.5295,  0.2286,  0.7497,\n",
            "        -0.8004,  0.6184,  0.9926, -0.0926,  0.9735,  0.9836,  0.9514,  0.4743,\n",
            "         0.9772,  0.9689,  0.7527,  0.9884, -0.1534,  0.9657,  0.0960,  0.1356,\n",
            "         0.9868,  0.2698,  0.7625,  0.3939, -0.2220, -0.0567,  0.1701,  0.9037,\n",
            "         0.9885,  0.9304,  0.9848, -0.8031,  0.9978,  0.4005,  0.3495, -0.3672,\n",
            "        -0.6358,  0.1062, -0.6585,  0.3874, -0.7331,  0.9392,  0.9419,  0.2534,\n",
            "         0.8304,  0.9177,  0.9913,  0.8975,  0.9269,  0.6195,  0.8432,  0.8689,\n",
            "         0.9929,  0.5193,  0.5200, -0.1119,  0.9855,  0.9623,  0.8905,  0.9506],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 141==== Step 2 Train Loss 0.7032139897346497 ======  0.4333333333333333\n",
            "torch.Size([64, 48])\n",
            "tensor([[-1.4876,  1.0979,  0.5174,  ...,  0.0869, -0.6326, -0.4616],\n",
            "        [ 0.5879,  0.6573,  0.1080,  ...,  0.0852,  0.3797, -0.3759],\n",
            "        [ 0.4015,  0.1139,  0.1180,  ..., -0.0792,  0.5543, -0.4007],\n",
            "        ...,\n",
            "        [-1.0031,  0.9289,  0.3501,  ..., -0.0564, -0.2396, -0.4708],\n",
            "        [ 0.3634,  0.9243,  0.0975,  ...,  0.0402,  0.2380, -0.5014],\n",
            "        [-1.0635,  1.0076,  0.6500,  ...,  0.0580, -0.1965, -0.4029]],\n",
            "       device='cuda:0')\n",
            "tensor([-0.7122,  0.8050, -0.2626,  0.9631,  0.9348,  0.4863,  0.9508,  0.7037,\n",
            "        -0.2363,  0.9000,  0.9273, -0.4570,  0.8663,  0.4602,  0.0194,  0.9868,\n",
            "         0.9043,  0.9710, -0.2005,  0.9452,  0.9414,  0.9732,  0.6637, -0.4188,\n",
            "        -0.0778,  0.9550, -0.4197, -0.6190,  0.9614,  0.9753, -0.7704, -0.3599,\n",
            "         0.8756,  0.9852,  0.9915,  0.8786,  0.8555,  0.6792,  0.8971, -0.0943,\n",
            "        -0.0885,  0.9864,  0.9640, -0.3873,  0.9824,  0.1519,  0.7183,  0.6961,\n",
            "         0.9935,  0.7865,  0.3849,  0.0771, -0.2925,  0.2959,  0.4399,  0.7892,\n",
            "         0.9719,  0.9335,  0.7947,  0.0757,  0.8856,  0.9861,  0.7135,  0.8794],\n",
            "       device='cuda:0')\n",
            "tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========== Epoch 0 Batch 142==== Step 2 Train Loss 0.69282066822052 ======  0.3529411764705882\n",
            "torch.Size([64, 48])\n",
            "tensor([[ 0.4073,  0.5618, -0.3719,  ...,  0.1893, -0.3844, -0.2963],\n",
            "        [-1.3962,  1.1124,  0.5396,  ...,  0.1115, -0.6507, -0.3063],\n",
            "        [-0.5568,  1.2106,  0.1859,  ..., -0.1740, -0.0663, -0.4090],\n",
            "        ...,\n",
            "        [-1.2986,  1.0662,  0.3883,  ...,  0.1176, -0.5177, -0.2900],\n",
            "        [-0.5568,  1.0923,  0.2305,  ...,  0.0321, -0.2401, -0.5229],\n",
            "        [-1.2786,  0.9488,  0.5772,  ..., -0.0457, -0.2610, -0.4052]],\n",
            "       device='cuda:0')\n",
            "tensor([ 0.7923,  0.3537,  0.9413,  0.8555,  0.8553,  0.8671,  0.1757, -0.4132,\n",
            "         0.9914,  0.9849, -0.1091,  0.9773,  0.9455,  0.2306,  0.5843,  0.9418,\n",
            "         0.9862,  0.9866,  0.9868, -0.0545,  0.9534,  0.5937, -0.1599,  0.0222,\n",
            "         0.8839,  0.9906,  0.9875,  0.9820,  0.9092,  0.8598,  0.5431, -0.2497,\n",
            "        -0.4196,  0.9476,  0.9031,  0.5881,  0.6980, -0.0124,  0.9936,  0.6392,\n",
            "        -0.5593, -0.0042,  0.9877,  0.4449, -0.7030,  0.9558, -0.0142, -0.8221,\n",
            "        -0.7576,  0.8973,  0.9927, -0.2670,  0.9790,  0.1460,  0.8890, -0.7321,\n",
            "        -0.3281,  0.9560,  0.8104,  0.8552,  0.9931, -0.8296,  0.9463,  0.9818],\n",
            "       device='cuda:0')\n",
            "tensor([1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-df6ed061ed57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodelCLS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyModelFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmytrainStep2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelCLS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodelEmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-cf1578988899>\u001b[0m in \u001b[0;36mmytrainStep2\u001b[0;34m(model, criterion1, criterion2, embModel)\u001b[0m\n\u001b[1;32m     85\u001b[0m               \u001b[0;31m#     p.requires_grad = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m               \u001b[0;31m#output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = embModel(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m               \u001b[0mFC11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFC22\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_input_ids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_input_mask2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m               \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFC11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFC22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-4684ce9212a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sent_id1, mask1, sent_id2, mask2, b_labels)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m       \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlogits1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \"\"\"\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfigure_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             display(\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-2>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2100\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m                     \u001b[0mbbox_artists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bbox_extra_artists\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2102\u001b[0;31m                     bbox_inches = self.figure.get_tightbbox(renderer,\n\u001b[0m\u001b[1;32m   2103\u001b[0m                             bbox_extra_artists=bbox_artists)\n\u001b[1;32m   2104\u001b[0m                     \u001b[0mpad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pad_inches\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   2392\u001b[0m                 \u001b[0;31m# need this conditional....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2394\u001b[0;31m                     bbox = ax.get_tightbbox(renderer,\n\u001b[0m\u001b[1;32m   2395\u001b[0m                             bbox_extra_artists=bbox_extra_artists)\n\u001b[1;32m   2396\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   4321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxison\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4323\u001b[0;31m             \u001b[0mbb_xaxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbb_xaxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4325\u001b[0m                 \u001b[0mbb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbb_xaxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_label_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;31m# go back to just this axis's tick labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_label_position\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;31m# get bounding boxes for this axis and any siblings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;31m# that have been set by `fig.align_xlabels()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2021\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_boxes_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2023\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_boxes_siblings\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;31m# if we want to align labels from other axes:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_siblings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2005\u001b[0;31m             \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2006\u001b[0m             \u001b[0mtlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtlb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tick_bboxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticks_to_draw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m             \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtlb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mtick\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m         \u001b[0mminor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minorticklocs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mminor_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0mminor_ticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_minor_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminor_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mget_minorticklocs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;34m\"\"\"Get the array of minor tick locations in data coordinates.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0;31m# Remove minor ticks duplicating major ticks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1353\u001b[0;31m         \u001b[0mmajor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1354\u001b[0m         \u001b[0mminor_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtick_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36mtick_values\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2209\u001b[0m         vmin, vmax = mtransforms.nonsingular(\n\u001b[1;32m   2210\u001b[0m             vmin, vmax, expander=1e-13, tiny=1e-14)\n\u001b[0;32m-> 2211\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2213\u001b[0m         \u001b[0mprune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/ticker.py\u001b[0m in \u001b[0;36m_raw_ticks\u001b[0;34m(self, vmin, vmax)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nbins\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2150\u001b[0;31m                 nbins = np.clip(self.axis.get_tick_space(),\n\u001b[0m\u001b[1;32m   2151\u001b[0m                                 max(1, self._min_n_ticks - 1), 9)\n\u001b[1;32m   2152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m     \"\"\"\n\u001b[0;32m-> 2115\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             um.maximum, a, min, out=out, casting=casting, **kwargs)\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         return _clip_dep_invoke_with_casting(\n\u001b[0m\u001b[1;32m    160\u001b[0m             um.clip, a, min, max, out=out, casting=casting, **kwargs)\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[0;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# try to deal with broken casting rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_UFuncOutputCastingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# Numpy 1.17.0, 2019-02-24\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "modelCLS = myModelFC()\n",
        "mytrainStep2(modelCLS,criterion1,criterion2,modelEmb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKLSFaQg_wTk"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VVlLPa_5-VcV"
      },
      "outputs": [],
      "source": [
        "def binarize(y, threshold=0.5):\n",
        "    y = np.array(y)\n",
        "    y = np.ma.fix_invalid(y, fill_value=threshold)\n",
        "    y[y >= threshold] = 1\n",
        "    y[y < threshold] = 0\n",
        "\n",
        "    return y\n",
        "\n",
        "\n",
        "def auc(true_y, pred_y):\n",
        "    \"\"\"\n",
        "    Calculates the AUC score (Area Under the Curve), a well-known\n",
        "    scalar evaluation score for binary classifiers. This score\n",
        "    also considers \"unanswered\" problem, where score = 0.5.\n",
        "    Parameters\n",
        "    ----------\n",
        "    prediction_scores : array [n_problems]\n",
        "        The predictions outputted by a verification system.\n",
        "        Assumes `0 >= prediction <=1`.\n",
        "    ground_truth_scores : array [n_problems]\n",
        "        The gold annotations provided for each problem.\n",
        "        Will typically be `0` or `1`.\n",
        "    Returns\n",
        "    ----------\n",
        "    auc = the Area Under the Curve.\n",
        "    References\n",
        "    ----------\n",
        "        E. Stamatatos, et al. Overview of the Author Identification\n",
        "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return roc_auc_score(true_y, pred_y)\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def c_at_1(true_y, pred_y, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates the c@1 score, an evaluation method specific to the\n",
        "    PAN competition. This method rewards predictions which leave\n",
        "    some problems unanswered (score = 0.5). See:\n",
        "        A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
        "        In Proc. of the 49th Annual Meeting of the Association for\n",
        "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n",
        "    Parameters\n",
        "    ----------\n",
        "    prediction_scores : array [n_problems]\n",
        "        The predictions outputted by a verification system.\n",
        "        Assumes `0 >= prediction <=1`.\n",
        "    ground_truth_scores : array [n_problems]\n",
        "        The gold annotations provided for each problem.\n",
        "        Will always be `0` or `1`.\n",
        "    Returns\n",
        "    ----------\n",
        "    c@1 = the c@1 measure (which accounts for unanswered\n",
        "        problems.)\n",
        "    References\n",
        "    ----------\n",
        "        - E. Stamatatos, et al. Overview of the Author Identification\n",
        "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
        "        - A. Peñas and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n",
        "        In Proc. of the 49th Annual Meeting of the Association for\n",
        "        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n",
        "    \"\"\"\n",
        "\n",
        "    n = float(len(pred_y))\n",
        "    nc, nu = 0.0, 0.0\n",
        "\n",
        "    for gt_score, pred_score in zip(true_y, pred_y):\n",
        "        if pred_score == 0.5 or (pred_score >= 0.50 and pred_score <= 0.51):\n",
        "            nu += 1\n",
        "        elif (pred_score > 0.5) == (gt_score > 0.5):\n",
        "            nc += 1.0\n",
        "\n",
        "    return (1 / n) * (nc + (nu * nc / n))\n",
        "\n",
        "\n",
        "def f1(true_y, pred_y):\n",
        "    \"\"\"\n",
        "    Assesses verification performance, assuming that every\n",
        "    `score > 0.5` represents a same-author pair decision.\n",
        "    Note that all non-decisions (scores == 0.5) are ignored\n",
        "    by this metric.\n",
        "    Parameters\n",
        "    ----------\n",
        "    prediction_scores : array [n_problems]\n",
        "        The predictions outputted by a verification system.\n",
        "        Assumes `0 >= prediction <=1`.\n",
        "    ground_truth_scores : array [n_problems]\n",
        "        The gold annotations provided for each problem.\n",
        "        Will typically be `0` or `1`.\n",
        "    Returns\n",
        "    ----------\n",
        "    acc = The number of correct attributions.\n",
        "    References\n",
        "    ----------\n",
        "        E. Stamatatos, et al. Overview of the Author Identification\n",
        "        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n",
        "    \"\"\"\n",
        "    true_y_filtered, pred_y_filtered = [], []\n",
        "\n",
        "    for true, pred in zip(true_y, pred_y):\n",
        "        if pred != 0.5:\n",
        "            true_y_filtered.append(true)\n",
        "            pred_y_filtered.append(pred)\n",
        "\n",
        "    pred_y_filtered = binarize(pred_y_filtered)\n",
        "\n",
        "    return f1_score(true_y_filtered, pred_y_filtered)\n",
        "\n",
        "\n",
        "def f_05_u_score(true_y, pred_y, pos_label=1, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Return F0.5u score of prediction.\n",
        "    :param true_y: true labels\n",
        "    :param pred_y: predicted labels\n",
        "    :param threshold: indication for non-decisions (default = 0.5)\n",
        "    :param pos_label: positive class label (default = 1)\n",
        "    :return: F0.5u score\n",
        "    \"\"\"\n",
        "\n",
        "    pred_y = binarize(pred_y)\n",
        "\n",
        "    n_tp = 0\n",
        "    n_fn = 0\n",
        "    n_fp = 0\n",
        "    n_u = 0\n",
        "\n",
        "    for i, pred in enumerate(pred_y):\n",
        "        if pred == threshold:\n",
        "            n_u += 1\n",
        "        elif pred == pos_label and pred == true_y[i]:\n",
        "            n_tp += 1\n",
        "        elif pred == pos_label and pred != true_y[i]:\n",
        "            n_fp += 1\n",
        "        elif true_y[i] == pos_label and pred != true_y[i]:\n",
        "            n_fn += 1\n",
        "\n",
        "    return (1.25 * n_tp) / (1.25 * n_tp + 0.25 * (n_fn + n_u) + n_fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jW0LzST-TES",
        "outputId": "6841cde2-97dd-43b6-d2f1-3bb86c68995d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pytz<2023 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2022.7.1)\n",
            "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.9/dist-packages (from mlflow) (23.0)\n",
            "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.17.6)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.2.2)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.7.1)\n",
            "Requirement already satisfied: pyarrow<12,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (9.0.0)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.27.1)\n",
            "Requirement already satisfied: Flask<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.2.3)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (8.1.3)\n",
            "Requirement already satisfied: gunicorn<21 in /usr/local/lib/python3.9/dist-packages (from mlflow) (20.1.0)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.4.47)\n",
            "Requirement already satisfied: docker<7,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (6.0.1)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.4.4)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.1.31)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.4)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.22.4)\n",
            "Requirement already satisfied: shap<1,>=0.40 in /usr/local/lib/python3.9/dist-packages (from mlflow) (0.41.0)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.10.1)\n",
            "Requirement already satisfied: querystring-parser<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.2.4)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (6.1.0)\n",
            "Requirement already satisfied: cloudpickle<3 in /usr/local/lib/python3.9/dist-packages (from mlflow) (2.2.1)\n",
            "Requirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from mlflow) (3.20.3)\n",
            "Requirement already satisfied: alembic<2 in /usr/local/lib/python3.9/dist-packages (from mlflow) (1.10.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.9/dist-packages (from mlflow) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.9/dist-packages (from alembic<2->mlflow) (4.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.9/dist-packages (from alembic<2->mlflow) (1.2.4)\n",
            "Requirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (2.6.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n",
            "Requirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (3.2.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.9/dist-packages (from databricks-cli<1,>=0.8.7->mlflow) (0.8.10)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from docker<7,>=4.0.0->mlflow) (1.5.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.9/dist-packages (from docker<7,>=4.0.0->mlflow) (1.26.15)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.9/dist-packages (from Flask<3->mlflow) (2.2.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask<3->mlflow) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from gitpython<4,>=2.1.0->mlflow) (4.0.10)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.9/dist-packages (from gunicorn<21->mlflow) (67.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2<4,>=2.11->mlflow) (2.1.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (0.11.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib<4->mlflow) (1.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.17.3->mlflow) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2->mlflow) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn<2->mlflow) (1.1.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from shap<1,>=0.40->mlflow) (0.56.4)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.9/dist-packages (from shap<1,>=0.40->mlflow) (0.0.7)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.9/dist-packages (from shap<1,>=0.40->mlflow) (4.65.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.9/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow) (5.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->shap<1,>=0.40->mlflow) (0.39.1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "import tabulate\n",
        "!pip install mlflow\n",
        "import mlflow\n",
        "class ContrastiveChunkerEvaluator:\n",
        "\n",
        "    def __init__(self, test_dataset, distance_metric,\n",
        "                 threshold: float = 0.5, find_threshold: bool = False):\n",
        "        self.test_dataset = test_dataset\n",
        "        self.distance_metric = distance_metric  # lambda x, y: 1-F.cosine_similarity(x, y)\n",
        "        \n",
        "        self.threshold = threshold\n",
        "        self.find_threshold = find_threshold\n",
        "        \n",
        "\n",
        "    def call(self, model, epoch: int = -1, steps: int = -1) -> (float,float):\n",
        "\n",
        "        # just encode all of the samples, then use some comparison to determine if they are the same or different?\n",
        "        # Can I just wrap this with the contrastive loss thing somehow? I think so, try it\n",
        "\n",
        "      \n",
        "        print('evaluating')\n",
        "        \n",
        "        truth = []\n",
        "        emb1, emb2 = [], []\n",
        "        ids = []\n",
        "        with torch.no_grad():\n",
        "            for input1, mask1, input2, mask2,target1,id in tqdm(self.test_dataset):\n",
        "                b_input_ids1 = input1[0].to(device)\n",
        "                b_input_mask1 = mask1[0].to(device)\n",
        "                label = target1[0].type(torch.LongTensor)\n",
        "                b_input_ids2 = input2[0].to(device)\n",
        "                b_input_mask2 = mask2[0].to(device)\n",
        "                b_labels = label.to(device)\n",
        "                h = modelEmb.init_hidden(b_input_ids1.size(0))\n",
        "                FC11 = modelEmb(b_input_ids1, b_input_mask1,h)\n",
        "                FC22 = modelEmb(b_input_ids2, b_input_mask2,h)\n",
        "                FC11=FC11.mean(0)\n",
        "                FC22=FC22.mean(0)\n",
        "                emb1.append(FC11)\n",
        "                emb2.append(FC22)\n",
        "                truth.append(b_labels[0].cpu().data.numpy())\n",
        "                ids.append(id)\n",
        "\n",
        "            # at this point, we have two lists of embeddings. We want to compare each embeddings, elementwise, to\n",
        "            # determine if they were written by the same author or not (i.e. get the distance between them)\n",
        "            # we also want to keep track of all of them for a ranking?\n",
        "            \n",
        "            predictions = self.distance_metric(torch.stack(emb1), torch.stack(emb2))\n",
        "            print(predictions)\n",
        "            print(ids)\n",
        "        # need to add some calibration here - especially using eucledian as the output magnitude will be quite different\n",
        "        # the predictions here are good I think, but they need to be changed to a similarity score, for now just do this by max(x) - x\n",
        "        similarities =  predictions #torch.max(predictions) -\n",
        "        normalized_similarities = similarities/torch.max(similarities)\n",
        "        \n",
        "        \n",
        "        # normalized_similarities = (1 + similarities) / 2\n",
        "        # print(normalized_similarities)\n",
        "        normalized_similarities = normalized_similarities.cpu().data.numpy()\n",
        "        # for now, just assume 50% same auth and 50% different auth - this seems like cheating but what can you do?\n",
        "        # threshold = sorted(normalized_similarities)[int(len(normalized_similarities)/2)]\n",
        "        threshold = self.threshold\n",
        "\n",
        "        if self.find_threshold:\n",
        "            # save this numpy array for testing later\n",
        "            # print('saving similairities and truth values')\n",
        "            # np.save('sims' + self.name + '.npy', normalized_similarities)\n",
        "\n",
        "            # np.save('truth' + self.name + '.npy', np.array(truth))\n",
        "            # print('predictions saved')\n",
        "\n",
        "            # first plot the score distribution: how do we make a histogram here?\n",
        "            pos_data, neg_data = [], []\n",
        "            for sim, lbl in zip(normalized_similarities, truth):\n",
        "                if lbl == 1:\n",
        "                    pos_data.append(sim)\n",
        "                elif lbl == 0:\n",
        "                    neg_data.append(sim)\n",
        "                else:\n",
        "                    assert False, 'problems'\n",
        "\n",
        "            plt.hist(pos_data, bins=25, alpha=0.5, label='same_author')\n",
        "            plt.hist(neg_data, bins=25, alpha=0.5, label='diff_author')\n",
        "            plt.title('Score distributions for same and different author pairs')\n",
        "            plt.xlabel('score')\n",
        "            plt.ylabel('count')\n",
        "            plt.show()\n",
        "            # plt.savefig('hist_differences')\n",
        "            fpr, tpr, threshold = metrics.roc_curve(truth, normalized_similarities,pos_label=1)\n",
        "            roc_auc = metrics.auc(fpr, tpr)\n",
        "            # print(f'AUC: {roc_auc:.4f}')\n",
        "            # Plot ROC AUC\n",
        "            # plt.title('Receiver Operating Characteristic')\n",
        "            plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "            plt.legend(loc = 'lower right')\n",
        "            plt.plot([0, 1], [0, 1],'r--')\n",
        "            plt.xlim([0, 1])\n",
        "            plt.ylim([0, 1])\n",
        "            plt.ylabel('True Positive Rate')\n",
        "            plt.xlabel('False Positive Rate')\n",
        "            plt.show()\n",
        "            # set some ranges to sweep, and shuffle them - maybe see more interesting results before hand?\n",
        "            all_thresholds = [x for x in np.linspace(0.0001, 1, 100)]\n",
        "            all_possible_scores = []\n",
        "            for threshold in all_thresholds:\n",
        "                results = {\n",
        "                    'auc': metrics.roc_auc_score(truth, normalized_similarities),\n",
        "                    'c@1': c_at_1(truth, normalized_similarities, threshold),\n",
        "                    'f_05_u': f_05_u_score(truth, normalized_similarities, threshold=threshold),\n",
        "                    'F1': f1(truth, normalized_similarities)\n",
        "                }\n",
        "                \n",
        "                results['overall'] = np.mean(list(results.values()))\n",
        "                binarized_predictions = binarize(normalized_similarities, threshold)\n",
        "                correct_predictions = [1 if x == y else 0 for x, y in zip(truth, binarized_predictions)]\n",
        "                results['accuracy'] = sum(correct_predictions) / len(correct_predictions)\n",
        "\n",
        "                all_possible_scores.append((threshold, results))\n",
        "            # now log all of the results to mlflow\n",
        "            best_auc = [0, 0]\n",
        "            best_acc = [0, 0]\n",
        "            best_overall = [0, 0]\n",
        "            best_f05 = [0, 0]\n",
        "            best_ca1 = [0, 0]\n",
        "            best_f1 = [0, 0]\n",
        "            for thresh, res in all_possible_scores:\n",
        "                mlflow.log_metric('threshtest/accuracy', res['accuracy'], step=int(1000*thresh))\n",
        "                mlflow.log_metric('threshtest/auc', res['auc'], step=int(1000*thresh))\n",
        "                mlflow.log_metric('threshtest/ca1', res['c@1'], step=int(1000*thresh))\n",
        "                mlflow.log_metric('threshtest/f_05_u', res['f_05_u'], step=int(1000*thresh))\n",
        "                mlflow.log_metric('threshtest/F1', res['F1'], step=int(1000*thresh))\n",
        "                mlflow.log_metric('threshtest/overall', res['overall'], step=int(1000*thresh))\n",
        "\n",
        "                # track optimal measures\n",
        "                best_auc = best_auc if best_auc[0] > res['auc'] else [res['auc'], thresh]\n",
        "                best_acc = best_acc if best_acc[0] > res['accuracy'] else [res['accuracy'], thresh]\n",
        "                best_overall = best_overall if best_overall[0] > res['overall'] else [res['overall'], thresh]\n",
        "                best_f05 = best_f05 if best_f05[0] > res['f_05_u'] else [res['f_05_u'], thresh]\n",
        "                best_ca1 = best_ca1 if best_ca1[0] > res['c@1'] else [res['c@1'], thresh]\n",
        "                best_f1 = best_f1 if best_f1[0] > res['F1'] else [res['F1'], thresh]\n",
        "\n",
        "            mlflow.log_metric('threshtest/opt_accuracy', best_acc[0], step=int(1000*best_acc[1]))\n",
        "            mlflow.log_metric('threshtest/opt_auc', best_auc[0], step=int(1000*best_auc[1]))\n",
        "            mlflow.log_metric('threshtest/opt_ca1', best_ca1[0], step=int(1000*best_ca1[1]))\n",
        "            mlflow.log_metric('threshtest/opt_f_05_u', best_f05[0], step=int(1000*best_f05[1]))\n",
        "            mlflow.log_metric('threshtest/opt_F1', best_f1[0], step=int(1000*best_f1[1]))\n",
        "            mlflow.log_metric('threshtest/opt_overall', best_overall[0], step=int(1000*best_overall[1]))\n",
        "            print(f\"accuracy: {best_acc}, auc: {best_auc}, c@1: {best_ca1}, f_05_u: {best_f05}, F1: {best_f1}, overall: {best_overall}\")\n",
        "\n",
        "            return best_acc,best_f1#best_auc\n",
        "\n",
        "        # now select best threshold for final reporting?\n",
        "\n",
        "        binarized_predictions = binarize(normalized_similarities, threshold)\n",
        "\n",
        "        # this doesn't work with raw scores\n",
        "        # results = eval_metrics.evaluate_all(truth, binarized_predictions)\n",
        "        # for now just give raw distances as input\n",
        "        results = {\n",
        "            'auc': metrics.roc_auc_score(truth, normalized_similarities),\n",
        "            'c@1': c_at_1(truth, normalized_similarities, threshold),\n",
        "            'f_05_u': f_05_u_score(truth, normalized_similarities, threshold=threshold),\n",
        "            'F1': f1(truth, normalized_similarities)\n",
        "        }\n",
        "        results['overall'] = np.mean(list(results.values()))\n",
        "\n",
        "        correct_predictions = [1 if x == y else 0 for x, y in zip(truth, binarized_predictions)]\n",
        "        results['accuracy'] = sum(correct_predictions)/len(correct_predictions)\n",
        "        \n",
        "        # print(f'Evaluation took {time.time() - start_time} seconds. Scores at step {steps} and epoch {epoch} are:')\n",
        "        \n",
        "        print(f\"accuracy: {results['accuracy']}, auc: {results['auc']}, c@1: {results['c@1']}, f_05_u: {results['f_05_u']}, F1: {results['F1']}, overall: {results['overall']}\")\n",
        "\n",
        "        try:\n",
        "            if epoch > -1:\n",
        "                log_step = epoch\n",
        "            elif isinstance(steps, int):\n",
        "                log_step = steps\n",
        "            elif isinstance(steps, list):\n",
        "                log_step = steps[-1]\n",
        "            else:\n",
        "                log_step = -1\n",
        "\n",
        "            mlflow.log_metric('accuracy', results['accuracy'], step=log_step)\n",
        "            mlflow.log_metric('auc', results['auc'], step=log_step)\n",
        "            mlflow.log_metric('ca1', results['c@1'], step=log_step)\n",
        "            mlflow.log_metric('f_05_u', results['f_05_u'], step=log_step)\n",
        "            mlflow.log_metric('F1', results['F1'], step=log_step)\n",
        "            mlflow.log_metric('overall', results['overall'], step=log_step)\n",
        "\n",
        "        except Exception as e:\n",
        "            print('exception trying to log metrics')\n",
        "            print(e)\n",
        "\n",
        "        return results#results['auc'],results['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "unkWnOw2CUzc",
        "outputId": "7c22ed3c-70be-4172-c9b2-bb78045b99cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluating\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [00:41<00:00,  6.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8566, 0.8138, 0.9269, 0.8471, 0.7485, 0.7225, 0.8393, 0.8662, 0.6347,\n",
            "        0.7895, 0.8566, 0.8383, 0.9263, 0.7675, 0.9231, 0.8855, 0.7580, 0.8874,\n",
            "        0.6617, 0.7136, 0.7444, 0.8199, 0.7534, 0.7647, 0.8758, 0.7780, 0.8395,\n",
            "        0.8422, 0.8948, 0.8121, 0.9142, 0.8939, 0.7480, 0.6938, 0.8676, 0.8531,\n",
            "        0.9065, 0.7115, 0.8184, 0.8814, 0.7507, 0.7345, 0.9176, 0.8070, 0.9077,\n",
            "        0.8608, 0.9099, 0.8510, 0.8017, 0.9176, 0.8121, 0.8669, 0.8859, 0.7639,\n",
            "        0.7358, 0.8412, 0.8335, 0.9142, 0.7856, 0.8883, 0.8062, 0.7239, 0.7410,\n",
            "        0.8434, 0.8471, 0.8124, 0.8186, 0.6936, 0.7897, 0.7811, 0.7480, 0.8103,\n",
            "        0.8183, 0.8183, 0.8838, 0.7462, 0.7548, 0.8919, 0.7233, 0.9076, 0.8221,\n",
            "        0.8526, 0.6938, 0.8814, 0.7609, 0.7997, 0.9176, 0.7315, 0.8501, 0.7895,\n",
            "        0.7580, 0.7896, 0.6857, 0.7111, 0.7864, 0.7680, 0.7315, 0.8855, 0.8782,\n",
            "        0.8183, 0.8731, 0.7835, 0.7214, 0.7055, 0.6922, 0.7499, 0.6857, 0.8108,\n",
            "        0.7462, 0.7913, 0.7896, 0.8477, 0.7225, 0.7780, 0.7499, 0.8662, 0.8233,\n",
            "        0.8186, 0.7460, 0.7876, 0.8838, 0.8343, 0.7301, 0.8343, 0.8062, 0.7529,\n",
            "        0.8247, 0.7895, 0.8662, 0.7574, 0.9176, 0.7574, 0.8383, 0.8247, 0.6936,\n",
            "        0.7542, 0.6885, 0.8782, 0.8778, 0.6779, 0.8250, 0.7680, 0.7974, 0.6938,\n",
            "        0.8079, 0.7780, 0.7805, 0.8948, 0.9099, 0.8033, 0.7945, 0.9078, 0.8233,\n",
            "        0.8943, 0.8477, 0.7248, 0.7680, 0.8393, 0.8186, 0.8312, 0.8943, 0.8188,\n",
            "        0.7537, 0.8986, 0.7853, 0.7776, 0.7509, 0.7843, 0.8268, 0.7974, 0.8662,\n",
            "        0.7033, 0.7495, 0.7489, 0.8343, 0.8666, 0.8859, 0.7574, 0.8277, 0.7017,\n",
            "        0.8138, 0.9185, 0.7625, 0.7345, 0.7843, 0.9142, 0.7639, 0.7856, 0.8233,\n",
            "        0.7508, 0.8861, 0.6779, 0.9176, 0.8489, 0.8268, 0.7534, 0.8070, 0.9078,\n",
            "        0.7403, 0.8782, 0.8814, 0.8943, 0.8335, 0.8298, 0.8782, 0.9078, 0.9307,\n",
            "        0.7470, 0.8917, 0.6360, 0.9176, 0.8855, 0.8687, 0.7917, 0.7609, 0.7548,\n",
            "        0.8642, 0.7805, 0.7136, 0.8929, 0.8917, 0.8055, 0.8709, 0.9076, 0.9451,\n",
            "        0.7936, 0.7936, 0.8608, 0.8748, 0.7945, 0.8114, 0.7641, 0.8919, 0.7233,\n",
            "        0.6885, 0.7895, 0.7674, 0.7864, 0.7509, 0.7017, 0.7033, 0.9099, 0.7674,\n",
            "        0.7674, 0.7507, 0.8393, 0.7776, 0.8154, 0.8939, 0.8676, 0.8268, 0.7012,\n",
            "        0.8676, 0.7485, 0.8138, 0.8778, 0.7499, 0.8919, 0.8221, 0.7345, 0.7993,\n",
            "        0.9056, 0.7560, 0.7149, 0.8890, 0.8400, 0.8586, 0.8943, 0.6936, 0.8055,\n",
            "        0.7352, 0.8110, 0.8471, 0.8935, 0.7627, 0.9071, 0.7045, 0.8669, 0.8627,\n",
            "        0.9185, 0.7895, 0.8210, 0.7876, 0.8939, 0.8434, 0.8284, 0.8935, 0.8731,\n",
            "        0.6936], device='cuda:0')\n",
            "[tensor([162]), tensor([171]), tensor([218]), tensor([53]), tensor([235]), tensor([65]), tensor([95]), tensor([164]), tensor([213]), tensor([282]), tensor([162]), tensor([285]), tensor([183]), tensor([42]), tensor([101]), tensor([207]), tensor([14]), tensor([51]), tensor([221]), tensor([147]), tensor([248]), tensor([175]), tensor([243]), tensor([140]), tensor([224]), tensor([244]), tensor([86]), tensor([257]), tensor([72]), tensor([114]), tensor([242]), tensor([201]), tensor([105]), tensor([137]), tensor([188]), tensor([196]), tensor([1]), tensor([268]), tensor([287]), tensor([151]), tensor([96]), tensor([141]), tensor([173]), tensor([68]), tensor([19]), tensor([275]), tensor([182]), tensor([35]), tensor([166]), tensor([173]), tensor([114]), tensor([93]), tensor([204]), tensor([186]), tensor([15]), tensor([90]), tensor([24]), tensor([242]), tensor([167]), tensor([62]), tensor([66]), tensor([55]), tensor([57]), tensor([215]), tensor([53]), tensor([284]), tensor([219]), tensor([208]), tensor([61]), tensor([50]), tensor([105]), tensor([33]), tensor([85]), tensor([85]), tensor([103]), tensor([135]), tensor([260]), tensor([132]), tensor([172]), tensor([238]), tensor([274]), tensor([63]), tensor([137]), tensor([151]), tensor([106]), tensor([129]), tensor([173]), tensor([88]), tensor([254]), tensor([282]), tensor([14]), tensor([181]), tensor([3]), tensor([251]), tensor([136]), tensor([84]), tensor([88]), tensor([207]), tensor([193]), tensor([85]), tensor([107]), tensor([265]), tensor([271]), tensor([190]), tensor([40]), tensor([110]), tensor([3]), tensor([2]), tensor([135]), tensor([112]), tensor([181]), tensor([216]), tensor([65]), tensor([244]), tensor([110]), tensor([164]), tensor([138]), tensor([219]), tensor([109]), tensor([115]), tensor([103]), tensor([29]), tensor([197]), tensor([225]), tensor([66]), tensor([232]), tensor([131]), tensor([148]), tensor([164]), tensor([256]), tensor([173]), tensor([256]), tensor([285]), tensor([131]), tensor([208]), tensor([161]), tensor([10]), tensor([193]), tensor([217]), tensor([269]), tensor([199]), tensor([247]), tensor([126]), tensor([137]), tensor([206]), tensor([244]), tensor([20]), tensor([72]), tensor([182]), tensor([255]), tensor([234]), tensor([64]), tensor([138]), tensor([125]), tensor([216]), tensor([44]), tensor([247]), tensor([95]), tensor([219]), tensor([194]), tensor([125]), tensor([262]), tensor([145]), tensor([233]), tensor([180]), tensor([76]), tensor([223]), tensor([16]), tensor([36]), tensor([126]), tensor([164]), tensor([184]), tensor([263]), tensor([52]), tensor([225]), tensor([276]), tensor([204]), tensor([256]), tensor([163]), tensor([178]), tensor([171]), tensor([192]), tensor([83]), tensor([141]), tensor([16]), tensor([242]), tensor([186]), tensor([167]), tensor([138]), tensor([37]), tensor([46]), tensor([269]), tensor([173]), tensor([144]), tensor([267]), tensor([243]), tensor([68]), tensor([64]), tensor([278]), tensor([193]), tensor([151]), tensor([125]), tensor([24]), tensor([120]), tensor([193]), tensor([64]), tensor([168]), tensor([195]), tensor([13]), tensor([91]), tensor([173]), tensor([207]), tensor([258]), tensor([17]), tensor([106]), tensor([260]), tensor([259]), tensor([20]), tensor([147]), tensor([6]), tensor([13]), tensor([239]), tensor([41]), tensor([238]), tensor([45]), tensor([31]), tensor([31]), tensor([275]), tensor([11]), tensor([234]), tensor([191]), tensor([32]), tensor([132]), tensor([172]), tensor([10]), tensor([148]), tensor([70]), tensor([136]), tensor([189]), tensor([178]), tensor([184]), tensor([182]), tensor([70]), tensor([70]), tensor([96]), tensor([95]), tensor([76]), tensor([236]), tensor([201]), tensor([188]), tensor([36]), tensor([117]), tensor([188]), tensor([235]), tensor([171]), tensor([217]), tensor([110]), tensor([132]), tensor([214]), tensor([134]), tensor([222]), tensor([241]), tensor([78]), tensor([149]), tensor([28]), tensor([74]), tensor([79]), tensor([125]), tensor([208]), tensor([239]), tensor([226]), tensor([202]), tensor([53]), tensor([246]), tensor([26]), tensor([30]), tensor([98]), tensor([93]), tensor([25]), tensor([192]), tensor([282]), tensor([56]), tensor([115]), tensor([201]), tensor([215]), tensor([152]), tensor([246]), tensor([107]), tensor([208])]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABJvklEQVR4nO3deVxU9eL/8feAMpAKlrK6i7lmWmbmlhuJXCu1cuFabmnmUtfMLG/fXCtudUu7rtV1Ka1raabdNJdMtBRzr9QyFxQrwRURTFD5/P7wx9zGAURgYPC8no/HPB6ez/mcz/mcz5wz8+Yso80YYwQAAGAhXsXdAQAAgKJGAAIAAJZDAAIAAJZDAAIAAJZDAAIAAJZDAAIAAJZDAAIAAJZDAAIAAJZDAAIAAJZDAILbtG3bVm3btnVMHz58WDabTfPmzXP7uufNmyebzabDhw87yqpXr67777/f7euWpNjYWNlsNsXGxhbJ+q5XamqqBg4cqJCQENlsNo0YMaK4u4SrXH38XC+bzabx48c7prM7JiTpjTfeUM2aNeXt7a3GjRtLki5duqTRo0erSpUq8vLyUteuXfPdDytr27atbrvttuLuRqHy9M+260EAKiY//vijHnnkEVWrVk2+vr6qVKmS7rvvPk2dOrW4u+ZxZsyYUSShKT88uW+5efXVVzVv3jwNGTJE8+fP12OPPVbcXUIxWL16tUaPHq2WLVtq7ty5evXVVyVJc+bM0RtvvKFHHnlE77//vp555pli7mnOVqxY4RT0itrvv/+u8ePHa9euXcXWB+SPjf8LrOht2rRJ7dq1U9WqVdW3b1+FhITo6NGj2rx5sw4ePKgDBw4UdxcLRdZfr1l/KRhjlJ6ertKlS8vb2zvP7dx2222qWLHidf3FcfnyZV28eFF2u102m03SlTNAt912m7744os8t5PfvmVmZiojI0M+Pj7y8vK8vzPuuecelSpVSt9++21xdwU5uPr4uV42m03jxo1zhIPsjokXXnhBb7zxhv744w/5+Pg4lu3Vq5e+/fZb/frrrwXZhCIxfPhwTZ8+XcX1VbZt2zY1bdpUc+fOVb9+/ZzmtW3bVidPntTu3buLpW/u4OmfbdejVHF3wIpeeeUVBQQEaOvWrSpfvrzTvOPHjxdpX86fP6+bbrqpSNZls9nk6+vr1nWkpaWpTJky8vb2vq6QVdi8vLzcvq0Fcfz4cdWvX7/Q2rt06ZIyMzOdvkThWbI7Jo4fPy4/Pz+X9+348eMun00FYYzRhQsX5OfnV2ht4oqiPvby+tlWlN8t+VWy41sJdfDgQTVo0CDbD5igoCCXsgULFujuu+/WTTfdpJtvvln33nuvVq9e7VRnxowZatCggex2u8LCwjRs2DAlJyc71cm6Hr19+3bde++9uummm/T3v/9dkpSenq5x48apVq1astvtqlKlikaPHq309PQ8bdO7776r8PBw+fn56e6779Y333zjUie7e4ASExPVv39/Va5cWXa7XaGhoerSpYvjPoXq1atrz549Wr9+vWw2m2w2m+Mv46x7GtavX6+hQ4cqKChIlStXdpp39f0O0pXT/o0bN5avr6/q16+vJUuWOM0fP3684y/kP7u6zdz6ltN18kWLFqlJkyby8/NTxYoV9eijj+q3335zqtOvXz+VLVtWv/32m7p27aqyZcsqMDBQo0aN0uXLl53qLly4UE2aNFG5cuXk7++vhg0b6u2333bpe5asfsXHx2v58uWOfmdt0/Hjx/X4448rODhYvr6+atSokd5//32nNrLex3/+85+aMmWKwsPDZbfbtXfv3hzXu2bNGrVq1Urly5dX2bJlVadOHce+J0kZGRkaO3asmjRpooCAAJUpU0atW7fWunXrclz39OnTVbNmTd10003q2LGjjh49KmOMJk2apMqVK8vPz09dunTR6dOnXfrz5ZdfqnXr1ipTpozKlSunzp07a8+ePTn2P8vp06c1atQoNWzYUGXLlpW/v7+ioqL0/fffZzvOn3zyiV555RVVrlxZvr6+6tChQ7ZnePNy/OQkPT1dzzzzjAIDA1WuXDk9+OCD2Z65uXr/tdlsmjt3rtLS0hz7QVaddevWac+ePY7yrP04MzNTU6ZMUYMGDeTr66vg4GANHjxYZ86ccVpX1v12q1at0l133SU/Pz+98847kqTk5GSNGDFCVapUkd1uV61atfTaa68pMzPTsfyf3+essbHb7WratKm2bt3qqNevXz9Nnz7dsT1Zr9wsW7ZMnTt3VlhYmOx2u8LDwzVp0iSXY6t69eouZ3Qk53uzYmNj1bRpU0lS//79ncbxz/bu3at27drppptuUqVKlfT666+7tOuuY89ms2n48OH68MMPVadOHfn6+qpJkybasGGDU70jR45o6NChqlOnjvz8/FShQgV1797d5TM0u8+23L5btm3bpsjISFWsWFF+fn6qUaOGBgwYkGN/ixJngIpBtWrVFBcXp927d1/zBrkJEyZo/PjxatGihSZOnCgfHx999913+vrrr9WxY0dJV76wJ0yYoIiICA0ZMkT79u3TzJkztXXrVm3cuFGlS5d2tHfq1ClFRUWpV69eevTRRxUcHKzMzEw9+OCD+vbbb/XEE0+oXr16+vHHHzV58mT98ssvWrp0aa59nD17tgYPHqwWLVpoxIgROnTokB588EHdcsstqlKlSq7LPvzww9qzZ4+eeuopVa9eXcePH9eaNWuUkJCg6tWra8qUKXrqqadUtmxZvfjii5Kk4OBgpzaGDh2qwMBAjR07Vmlpabmub//+/erZs6eefPJJ9e3bV3PnzlX37t21cuVK3Xfffbkue7W89O3P5s2bp/79+6tp06aKiYlRUlKS3n77bW3cuFE7d+50CsSXL19WZGSkmjVrpn/+85/66quv9Oabbyo8PFxDhgyRdCVUREdHq0OHDnrttdckST/99JM2btyov/3tb9n2oV69epo/f76eeeYZVa5cWc8++6wkKTAwUH/88Yfatm2rAwcOaPjw4apRo4YWLVqkfv36KTk52aXNuXPn6sKFC3riiSdkt9t1yy23ZLvOPXv26P7779ftt9+uiRMnym6368CBA9q4caOjTkpKiv79738rOjpagwYN0rlz5zR79mxFRkZqy5Ytjptzs3z44YfKyMjQU089pdOnT+v1119Xjx491L59e8XGxur555/XgQMHNHXqVI0aNUpz5sxxLDt//nz17dtXkZGReu2113T+/HnNnDlTrVq10s6dO1W9evUc38NDhw5p6dKl6t69u2rUqKGkpCS98847atOmjfbu3auwsDCn+v/4xz/k5eWlUaNG6ezZs3r99dfVu3dvfffdd446BTl+JGngwIFasGCB/vrXv6pFixb6+uuv1blz52suN3/+fL377rvasmWL/v3vf0uS7rjjDs2fP1+vvPKKUlNTFRMTI+nKfiNJgwcPduzHTz/9tOLj4zVt2jTt3LnT5bNm3759io6O1uDBgzVo0CDVqVNH58+fV5s2bfTbb79p8ODBqlq1qjZt2qQxY8bo2LFjmjJlilMfP/roI507d06DBw+WzWbT66+/roceekiHDh1S6dKlNXjwYP3+++9as2aN5s+ff81tlq4ch2XLltXIkSNVtmxZff311xo7dqxSUlL0xhtv5KmNLPXq1dPEiRM1duxYPfHEE2rdurUkqUWLFo46Z86cUadOnfTQQw+pR48eWrx4sZ5//nk1bNhQUVFRkuS2Yy/L+vXr9fHHH+vpp5+W3W7XjBkz1KlTJ23ZssXxHbR161Zt2rRJvXr1UuXKlXX48GHNnDlTbdu21d69e695Nie775bjx4+rY8eOCgwM1AsvvKDy5cvr8OHDLn90FhuDIrd69Wrj7e1tvL29TfPmzc3o0aPNqlWrTEZGhlO9/fv3Gy8vL9OtWzdz+fJlp3mZmZnGGGOOHz9ufHx8TMeOHZ3qTJs2zUgyc+bMcZS1adPGSDKzZs1yamv+/PnGy8vLfPPNN07ls2bNMpLMxo0bc9yWjIwMExQUZBo3bmzS09Md5e+++66RZNq0aeMoi4+PN5LM3LlzjTHGnDlzxkgyb7zxRi6jZUyDBg2c2skyd+5cI8m0atXKXLp0Kdt58fHxjrJq1aoZSebTTz91lJ09e9aEhoaaO+64w1E2btw4k92hkV2bOfVt3bp1RpJZt26dMeZ/43TbbbeZP/74w1Hviy++MJLM2LFjHWV9+/Y1kszEiROd2rzjjjtMkyZNHNN/+9vfjL+/v8u250W1atVM586dncqmTJliJJkFCxY4yjIyMkzz5s1N2bJlTUpKijHmf++jv7+/OX78+DXXNXnyZCPJnDhxIsc6ly5dctp/jLmyfwQHB5sBAwY4yrLWHRgYaJKTkx3lY8aMMZJMo0aNzMWLFx3l0dHRxsfHx1y4cMEYY8y5c+dM+fLlzaBBg5zWlZiYaAICAlzKr3bhwgWXYzE+Pt7Y7Xan9yvr/a9Xr57Tdr399ttGkvnxxx+NMdd3/GRn165dRpIZOnSoU/lf//pXI8mMGzfOUZbd/tu3b19TpkwZl3bbtGljGjRo4FT2zTffGEnmww8/dCpfuXKlS3nWsbZy5UqnupMmTTJlypQxv/zyi1P5Cy+8YLy9vU1CQoIx5n/vc4UKFczp06cd9ZYtW2Ykmf/+97+OsmHDhmV7vObk/PnzLmWDBw82N910k2M/ydqGvn37utRt06aN0/uydetWp8+1q+tKMh988IGjLD093YSEhJiHH37YUeauY88YYyQZSWbbtm2OsiNHjhhfX1/TrVs3R1l24xIXF+fS/6s/2/68nVd/t3z22WdGktm6dWue+lrUuARWDO677z7FxcXpwQcf1Pfff6/XX39dkZGRqlSpkj7//HNHvaVLlyozM1Njx451udks6zTvV199pYyMDI0YMcKpzqBBg+Tv76/ly5c7LWe329W/f3+nskWLFqlevXqqW7euTp486Xi1b99eklwuQ/zZtm3bdPz4cT355JNO16D79eungICAXMch696D2NhYl1Po12PQoEF5vt8nLCxM3bp1c0z7+/urT58+2rlzpxITE/Pdh2vJGqehQ4c6XT/v3Lmz6tat6/I+SdKTTz7pNN26dWsdOnTIMV2+fHmlpaVpzZo1hdLHFStWKCQkRNHR0Y6y0qVL6+mnn1ZqaqrWr1/vVP/hhx9WYGDgNdvNOrO1bNkyp8scf+bt7e3YfzIzM3X69GldunRJd911l3bs2OFSv3v37k77V7NmzSRJjz76qEqVKuVUnpGR4bjMuGbNGiUnJys6OtppX/f29lazZs1y3delK8dP1nF2+fJlnTp1ynFJL7t+9u/f3+m4yDpDkPU+FuT4ka68Z5L09NNPO5W742cNFi1apICAAN13331OY9ekSROVLVvWZexq1KihyMhIlzZat26tm2++2amNiIgIXb582eWyTM+ePXXzzTc7pq8ev/z4831I586d08mTJ9W6dWudP39eP//8c77bzUnZsmX16KOPOqZ9fHx09913O22Du469LM2bN1eTJk0c01WrVlWXLl20atUqx6W/P4/LxYsXderUKdWqVUvly5fPdt++WnbfLVnH/hdffKGLFy/mub9FhQBUTJo2baolS5bozJkz2rJli8aMGaNz587pkUcecVzPPXjwoLy8vHK9WfXIkSOSpDp16jiV+/j4qGbNmo75WSpVquRys9z+/fu1Z88eBQYGOr1q164tKfcbs7Pav/XWW53KS5curZo1a+Y2BLLb7Xrttdf05ZdfKjg4WPfee69ef/316w4iNWrUyHPdWrVqudwjkLWd2d0vVFhyep8kqW7dui7vk6+vr8sH3M033+wUFIcOHaratWsrKipKlStX1oABA7Ry5coC9fHWW291CdtZlz+u7mNex71nz55q2bKlBg4cqODgYPXq1UuffPKJSxh6//33dfvtt8vX11cVKlRQYGCgli9frrNnz7q0WbVqVafprLBw9SWjrPKscdu/f78kqX379i77++rVq6/5EEJmZqYmT56sW2+9VXa7XRUrVlRgYKB++OGHPPUz68s8qz8FOX6ylvfy8lJ4eLhTeXb7WUHt379fZ8+eVVBQkMvYpaamuoxddvvH/v37tXLlSpflIyIiJLl+1lxr/PJjz5496tatmwICAuTv76/AwEBHQMnuPSyoypUru3zmXH0su+vYy3L1/iVd+dw7f/68Tpw4IenKZbixY8c67s3K2reTk5PzNC7Zfbe0adNGDz/8sCZMmKCKFSuqS5cumjt3bp7vLXU37gEqZj4+PmratKmaNm2q2rVrq3///lq0aJHGjRvnlvVl9xRGZmamGjZsqLfeeivbZfJyH0J+jRgxQg888ICWLl2qVatW6aWXXlJMTIy+/vpr3XHHHXlqo7CfLMnpJsqrb5J0p7yc0QoKCtKuXbu0atUqffnll/ryyy81d+5c9enTx+XmSXfI67j7+flpw4YNWrdunZYvX66VK1fq448/Vvv27bV69Wp5e3trwYIF6tevn7p27arnnntOQUFB8vb2VkxMjA4ePOjSZk7jk1O5+f+PSGeFrvnz5yskJMSl3p/PHmXn1Vdf1UsvvaQBAwZo0qRJuuWWW+Tl5aURI0Zke3brWv0pSTIzMxUUFKQPP/ww2/lXB/acPmvuu+8+jR49Ots2sv4YyVLY45ecnKw2bdrI399fEydOVHh4uHx9fbVjxw49//zzTu9hbp8D1/OEqTv2AXc8TffUU09p7ty5GjFihJo3b66AgADZbDb16tUrxzO31+qTzWbT4sWLtXnzZv33v//VqlWrNGDAAL355pvavHmzypYtW+jbcT0IQB7krrvukiQdO3ZMkhQeHq7MzEzt3bvX5SbQLNWqVZN05YbDP//FmJGRofj4eMdfVrkJDw/X999/rw4dOlzzCYqc1r9//37HJTPpyinU+Ph4NWrUKE/rf/bZZ/Xss89q//79aty4sd58800tWLBAUs4fRPlx4MABGWOc2vzll18kyXHza9ZfmcnJyU43Jl/9V9j19O3P79OfxymrLGv+9fLx8dEDDzygBx54QJmZmRo6dKjeeecdvfTSS6pVq9Z1tVWtWjX98MMPyszMdPpLNOuyQH77KF15dLZDhw7q0KGD3nrrLb366qt68cUXtW7dOkVERGjx4sWqWbOmlixZ4jSmhf2HQNaZkqCgoDwdG1dbvHix2rVrp9mzZzuVJycnq2LFitfdXkGPn2rVqikzM1MHDx50Ouuzb9++6+7LtYSHh+urr75Sy5Yt8/0FHB4ertTU1HyNfU6u5/MhNjZWp06d0pIlS3Tvvfc6yuPj413q3nzzzS5P0kpXPgf+/FlbGJ9P7jz2pP+d+fyzX375RTfddJMjuC5evFh9+/bVm2++6ahz4cKFbMfget1zzz2655579Morr+ijjz5S7969tXDhQg0cOLDAbRcEl8CKwbp167JN/1nX87M+yLp27SovLy9NnDjRJYFnLR8RESEfHx/961//cmpz9uzZOnv2bJ6eBunRo4d+++03vffeey7z/vjjj1yfrLrrrrsUGBioWbNmKSMjw1E+b968ax4458+f14ULF5zKwsPDVa5cOadTpGXKlCmUg1C68qutn332mWM6JSVFH3zwgRo3buw4I5D1Jfnn+xHS0tKyPauS177dddddCgoK0qxZs5y27csvv9RPP/2Up/fpaqdOnXKa9vLy0u233y5J+TrF/Je//EWJiYn6+OOPHWWXLl3S1KlTVbZsWbVp0+a625SU7WPoWYE+q59ZfyX/eR/+7rvvFBcXl6915iQyMlL+/v569dVXs70nIetyQE68vb1djt1Fixa5/JRBXhXk+JHkeIroX//6l1P51U9TFYYePXro8uXLmjRpksu8S5cu5am/PXr0UFxcnFatWuUyLzk5WZcuXbrufpUpU8ax/LVkt59lZGRoxowZLnXDw8O1efNmp/fliy++0NGjR/O9/py469jLEhcX53Qfz9GjR7Vs2TJ17NjRMSbZ7dtTp04t0JnvM2fOuLR59bFfnDgDVAyeeuopnT9/Xt26dVPdunWVkZGhTZs26eOPP1b16tUdN5LVqlVLL774oiZNmqTWrVvroYcekt1u19atWxUWFqaYmBgFBgZqzJgxmjBhgjp16qQHH3xQ+/bt04wZM9S0aVOnm+9y8thjj+mTTz7Rk08+qXXr1qlly5a6fPmyfv75Z33yySeO3/LITunSpfXyyy9r8ODBat++vXr27Kn4+HjNnTv3mvcw/PLLL+rQoYN69Oih+vXrq1SpUvrss8+UlJSkXr16Oeo1adJEM2fO1Msvv6xatWopKCjI5SxKXtWuXVuPP/64tm7dquDgYM2ZM0dJSUmaO3euo07Hjh1VtWpVPf7443ruuefk7e2tOXPmKDAwUAkJCU7t5bVvpUuX1muvvab+/furTZs2io6OdjwGX7169Xz9VwMDBw7U6dOn1b59e1WuXFlHjhzR1KlT1bhxY8e9A9fjiSee0DvvvKN+/fpp+/btql69uhYvXqyNGzdqypQpKleu3HW3KUkTJ07Uhg0b1LlzZ1WrVk3Hjx/XjBkzVLlyZbVq1UqSdP/992vJkiXq1q2bOnfurPj4eM2aNUv169dXampqvtabHX9/f82cOVOPPfaY7rzzTvXq1cvxvi5fvlwtW7bUtGnTclz+/vvv18SJE9W/f3+1aNFCP/74oz788MM83a+TnYIcP9KVL5Po6GjNmDFDZ8+eVYsWLbR27Vq3/Jp8mzZtNHjwYMXExGjXrl3q2LGjSpcurf3792vRokV6++239cgjj+TaxnPPPafPP/9c999/v/r166cmTZooLS1NP/74oxYvXqzDhw9f95m0rJt7n376aUVGRsrb29vp8+PPWrRooZtvvll9+/bV008/LZvNpvnz52f7B+nAgQO1ePFiderUST169NDBgwe1YMECl/utwsPDVb58ec2aNUvlypVTmTJl1KxZs+u6T8ddx16W2267TZGRkU6PwUtXfmYly/3336/58+crICBA9evXV1xcnL766itVqFAh3+t9//33NWPGDHXr1k3h4eE6d+6c3nvvPfn7++svf/lLgbapUBTHo2dW9+WXX5oBAwaYunXrmrJlyxofHx9Tq1Yt89RTT5mkpCSX+nPmzDF33HGHsdvt5uabbzZt2rQxa9ascaozbdo0U7duXVO6dGkTHBxshgwZYs6cOeNUJ7tHW7NkZGSY1157zTRo0MCxniZNmpgJEyaYs2fPXnObZsyYYWrUqGHsdru56667zIYNG1weF736MfiTJ0+aYcOGmbp165oyZcqYgIAA06xZM/PJJ584tZ2YmGg6d+5sypUr5/RocNZjvdk9YpnTY/CdO3c2q1atMrfffrux2+2mbt26ZtGiRS7Lb9++3TRr1sz4+PiYqlWrmrfeeivbNnPqW3aPihpjzMcff+x4L2+55RbTu3dv8+uvvzrVyenR5Ksfz1+8eLHp2LGjCQoKcvRz8ODB5tixYy7LXi27x+CNMSYpKcn079/fVKxY0fj4+JiGDRu6PN6b9T5e6+cLsqxdu9Z06dLFhIWFGR8fHxMWFmaio6OdHoXOzMw0r776qqlWrZqx2+3mjjvuMF988YXp27evqVat2jXXnTXeV7+XOe0j69atM5GRkSYgIMD4+vqa8PBw069fP6dHhbNz4cIF8+yzz5rQ0FDj5+dnWrZsaeLi4lz29Zz6c/UxkCUvx09O/vjjD/P000+bChUqmDJlypgHHnjAHD16tNAfg8/y7rvvmiZNmhg/Pz9Trlw507BhQzN69Gjz+++/O+rktH8Zc+WnCMaMGWNq1aplfHx8TMWKFU2LFi3MP//5T8dPgeS2j129XZcuXTJPPfWUCQwMNDab7ZqPxG/cuNHcc889xs/Pz4SFhTl+hiS74/XNN980lSpVMna73bRs2dJs27Yt2/dl2bJlpn79+qZUqVJO729O43j1fm2Me449Y66M17Bhw8yCBQvMrbfe6ji+rt7WM2fOONZftmxZExkZaX7++WeXnwPI6TH47LZzx44dJjo62lStWtXY7XYTFBRk7r///mseZ0WF/wsMAIAblM1m07Bhw3I9s2lV3AMEAAAshwAEAAAshwAEAAAsh6fAAAC4QXGbb844AwQAACyHAAQAACyHS2DZyMzM1O+//65y5coV6n/DAAAA3McYo3PnziksLMzlP5e9GgEoG7///rtb/wNQAADgPkePHlXlypVzrUMAykbWz44fPXpU/v7+xdwbAACQFykpKapSpUqe/vsQAlA2si57+fv7E4AAAChh8nL7CjdBAwAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyylV3B0AAMCTTF7zS6G088x9tQulHbgHZ4AAAIDlEIAAAIDlEIAAAIDlEIAAAIDlFGsA2rBhgx544AGFhYXJZrNp6dKlTvNtNlu2rzfeeCPHNsePH+9Sv27dum7eEgAAUJIUawBKS0tTo0aNNH369GznHzt2zOk1Z84c2Ww2Pfzww7m226BBA6flvv32W3d0HwAAlFDF+hh8VFSUoqKicpwfEhLiNL1s2TK1a9dONWvWzLXdUqVKuSwLAACQpcTcA5SUlKTly5fr8ccfv2bd/fv3KywsTDVr1lTv3r2VkJBQBD0EAAAlRYn5IcT3339f5cqV00MPPZRrvWbNmmnevHmqU6eOjh07pgkTJqh169bavXu3ypUrl+0y6enpSk9Pd0ynpKQUat8BAIBnKTEBaM6cOerdu7d8fX1zrffnS2q33367mjVrpmrVqumTTz7J8exRTEyMJkyYUKj9BQAAnqtEXAL75ptvtG/fPg0cOPC6ly1fvrxq166tAwcO5FhnzJgxOnv2rON19OjRgnQXAAB4uBIRgGbPnq0mTZqoUaNG171samqqDh48qNDQ0Bzr2O12+fv7O70AAMCNq1gDUGpqqnbt2qVdu3ZJkuLj47Vr1y6nm5ZTUlK0aNGiHM/+dOjQQdOmTXNMjxo1SuvXr9fhw4e1adMmdevWTd7e3oqOjnbrtgAAgJKjWO8B2rZtm9q1a+eYHjlypCSpb9++mjdvniRp4cKFMsbkGGAOHjyokydPOqZ//fVXRUdH69SpUwoMDFSrVq20efNmBQYGum9DAABAiWIzxpji7oSnSUlJUUBAgM6ePcvlMACwmMlrfimUdp65r3ahtIO8u57v7xJxDxAAAEBhIgABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLKVXcHQAAANlYF1PwNtqNKXgbNyjOAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMsp1gC0YcMGPfDAAwoLC5PNZtPSpUud5vfr1082m83p1alTp2u2O336dFWvXl2+vr5q1qyZtmzZ4qYtAAAAJVGxBqC0tDQ1atRI06dPz7FOp06ddOzYMcfrP//5T65tfvzxxxo5cqTGjRunHTt2qFGjRoqMjNTx48cLu/sAAKCEKlWcK4+KilJUVFSudex2u0JCQvLc5ltvvaVBgwapf//+kqRZs2Zp+fLlmjNnjl544YUC9RcAANwYPP4eoNjYWAUFBalOnToaMmSITp06lWPdjIwMbd++XREREY4yLy8vRUREKC4uLsfl0tPTlZKS4vQCAAA3Lo8OQJ06ddIHH3ygtWvX6rXXXtP69esVFRWly5cvZ1v/5MmTunz5soKDg53Kg4ODlZiYmON6YmJiFBAQ4HhVqVKlULcDAAB4lmK9BHYtvXr1cvy7YcOGuv322xUeHq7Y2Fh16NCh0NYzZswYjRw50jGdkpJCCAIA4Abm0WeArlazZk1VrFhRBw4cyHZ+xYoV5e3traSkJKfypKSkXO8jstvt8vf3d3oBAIAbV4kKQL/++qtOnTql0NDQbOf7+PioSZMmWrt2raMsMzNTa9euVfPmzYuqmwAAwMMVawBKTU3Vrl27tGvXLklSfHy8du3apYSEBKWmpuq5557T5s2bdfjwYa1du1ZdunRRrVq1FBkZ6WijQ4cOmjZtmmN65MiReu+99/T+++/rp59+0pAhQ5SWluZ4KgwAAKBY7wHatm2b2rVr55jOug+nb9++mjlzpn744Qe9//77Sk5OVlhYmDp27KhJkybJbrc7ljl48KBOnjzpmO7Zs6dOnDihsWPHKjExUY0bN9bKlStdbowGAADWVawBqG3btjLG5Dh/1apV12zj8OHDLmXDhw/X8OHDC9I1AABwAytR9wABAAAUBgIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwHAIQAACwnFLF3QEAsKR1MQVvo92YgrcBWBRngAAAgOUQgAAAgOUQgAAAgOUQgAAAgOUUawDasGGDHnjgAYWFhclms2np0qWOeRcvXtTzzz+vhg0bqkyZMgoLC1OfPn30+++/59rm+PHjZbPZnF5169Z185YAAICSpFgDUFpamho1aqTp06e7zDt//rx27Nihl156STt27NCSJUu0b98+Pfjgg9dst0GDBjp27Jjj9e2337qj+wAAoIQq1sfgo6KiFBUVle28gIAArVmzxqls2rRpuvvuu5WQkKCqVavm2G6pUqUUEhJSqH0FAAA3jhJ1D9DZs2dls9lUvnz5XOvt379fYWFhqlmzpnr37q2EhIRc66enpyslJcXpBQAAblwlJgBduHBBzz//vKKjo+Xv759jvWbNmmnevHlauXKlZs6cqfj4eLVu3Vrnzp3LcZmYmBgFBAQ4XlWqVHHHJgAAAA9RIgLQxYsX1aNHDxljNHPmzFzrRkVFqXv37rr99tsVGRmpFStWKDk5WZ988kmOy4wZM0Znz551vI4ePVrYmwAAADyIx/9XGFnh58iRI/r6669zPfuTnfLly6t27do6cOBAjnXsdrvsdntBuwoAAEoIjz4DlBV+9u/fr6+++koVKlS47jZSU1N18OBBhYaGuqGHAACgJCrWAJSamqpdu3Zp165dkqT4+Hjt2rVLCQkJunjxoh555BFt27ZNH374oS5fvqzExEQlJiYqIyPD0UaHDh00bdo0x/SoUaO0fv16HT58WJs2bVK3bt3k7e2t6Ojoot48AADgoYr1Eti2bdvUrl07x/TIkSMlSX379tX48eP1+eefS5IaN27stNy6devUtm1bSdLBgwd18uRJx7xff/1V0dHROnXqlAIDA9WqVStt3rxZgYGB7t0YAABQYhRrAGrbtq2MMTnOz21elsOHDztNL1y4sKDdAgAANziPvgcIAADAHQhAAADAcghAAADAcjz+d4AAuM/kNb8USjvP3Fe7UNqBha2LKZx22o0pnHZww+MMEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsBwCEAAAsJxSxd0BAJ7tnoR3r1knbnbu8zdXfSJP63rmvtp5qgcABcUZIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDkEIAAAYDn5CkDt27dXcnKyS3lKSorat29f0D4BAAC4Vb4CUGxsrDIyMlzKL1y4oG+++abAnQIAAHCn6wpAP/zwg3744QdJ0t69ex3TP/zwg3bu3KnZs2erUqVKeW5vw4YNeuCBBxQWFiabzaalS5c6zTfGaOzYsQoNDZWfn58iIiK0f//+a7Y7ffp0Va9eXb6+vmrWrJm2bNlyPZsJAABucKWup3Ljxo1ls9lks9myvdTl5+enqVOn5rm9tLQ0NWrUSAMGDNBDDz3kMv/111/Xv/71L73//vuqUaOGXnrpJUVGRmrv3r3y9fXNts2PP/5YI0eO1KxZs9SsWTNNmTJFkZGR2rdvn4KCgvK+sQAA4IZ1XQEoPj5exhjVrFlTW7ZsUWBgoGOej4+PgoKC5O3tnef2oqKiFBUVle08Y4ymTJmi//u//1OXLl0kSR988IGCg4O1dOlS9erVK9vl3nrrLQ0aNEj9+/eXJM2aNUvLly/XnDlz9MILL+S5bwAA4MZ1XQGoWrVqkqTMzEy3dObP4uPjlZiYqIiICEdZQECAmjVrpri4uGwDUEZGhrZv364xY8Y4yry8vBQREaG4uLgc15Wenq709HTHdEpKSiFtBQAA8ETXFYD+bP/+/Vq3bp2OHz/uEojGjh1b4I4lJiZKkoKDg53Kg4ODHfOudvLkSV2+fDnbZX7++ecc1xUTE6MJEyYUsMcAPN66mIK30W7Mteu4SdyhU07Tmy/9kq92nrmvdmF0xzMVwnt8T8Kpa1e6hs1VnyhwG3CvfAWg9957T0OGDFHFihUVEhIim83mmGez2QolABWlMWPGaOTIkY7plJQUValSpRh7BAAA3ClfAejll1/WK6+8oueff76w++MQEhIiSUpKSlJoaKijPCkpSY0bN852mYoVK8rb21tJSUlO5UlJSY72smO322W32wveaQAAUCLk63eAzpw5o+7duxd2X5zUqFFDISEhWrt2raMsJSVF3333nZo3b57tMj4+PmrSpInTMpmZmVq7dm2OywAAAOvJVwDq3r27Vq9eXeCVp6amateuXdq1a5ekKzc+79q1SwkJCbLZbBoxYoRefvllff755/rxxx/Vp08fhYWFqWvXro42OnTooGnTpjmmR44cqffee0/vv/++fvrpJw0ZMkRpaWmOp8IAAADydQmsVq1aeumll7R582Y1bNhQpUuXdpr/9NNP56mdbdu2qV27do7prPtw+vbtq3nz5mn06NFKS0vTE088oeTkZLVq1UorV650+g2ggwcP6uTJk47pnj176sSJExo7dqwSExPVuHFjrVy50uXGaAAAYF35CkDvvvuuypYtq/Xr12v9+vVO82w2W54DUNu2bWWMyXG+zWbTxIkTNXHixBzrHD582KVs+PDhGj58eJ76AAAArCdfASg+Pr6w+wEAAFBk8nUPEAAAQEmWrzNAAwYMyHX+nDlz8tUZAACAopCvAHTmzBmn6YsXL2r37t1KTk7O9j9JBQAA8CT5CkCfffaZS1lmZqaGDBmi8PDwAncKAADAnQrtHiAvLy+NHDlSkydPLqwmAQAA3KJQb4I+ePCgLl26VJhNAgAAFLp8XQL7838cKknGGB07dkzLly9X3759C6VjAAAA7pKvALRz506naS8vLwUGBurNN9+85hNiAAAAxS1fAWjdunWF3Q8AAIAik68AlOXEiRPat2+fJKlOnToKDAwslE4BAAC4U75ugk5LS9OAAQMUGhqqe++9V/fee6/CwsL0+OOP6/z584XdRwAAgEKVrwA0cuRIrV+/Xv/973+VnJys5ORkLVu2TOvXr9ezzz5b2H0EAAAoVPm6BPbpp59q8eLFatu2raPsL3/5i/z8/NSjRw/NnDmzsPoHANdt8ppfsi2/J+HUdbXTvGaFwugOAA+UrzNA58+fV3BwsEt5UFAQl8AAAIDHy1cAat68ucaNG6cLFy44yv744w9NmDBBzZs3L7TOAQAAuEO+LoFNmTJFnTp1UuXKldWoUSNJ0vfffy+73a7Vq1cXagcBAAAKW74CUMOGDbV//359+OGH+vnnnyVJ0dHR6t27t/z8/Aq1gwAAAIUtXwEoJiZGwcHBGjRokFP5nDlzdOLECT3//POF0jkAAAB3yNc9QO+8847q1q3rUt6gQQPNmjWrwJ0CAABwp3wFoMTERIWGhrqUBwYG6tixYwXuFAAAgDvlKwBVqVJFGzdudCnfuHGjwsLCCtwpAAAAd8rXPUCDBg3SiBEjdPHiRbVv316StHbtWo0ePZpfggYAAB4vXwHoueee06lTpzR06FBlZGRIknx9ffX8889rzJgxhdpBAACAwpavAGSz2fTaa6/ppZde0k8//SQ/Pz/deuutstvthd0/AACAQpevAJSlbNmyatq0aWH1BQAAoEjk6yZoAACAkowABAAALIcABAAALIcABAAALIcABAAALIcABAAALKdAj8ED7jJ5zS+F0s4z99UulHZKpHUx16xyT8KpIugIstufGXugeHEGCAAAWA4BCAAAWA4BCAAAWA4BCAAAWA4BCAAAWI7HB6Dq1avLZrO5vIYNG5Zt/Xnz5rnU9fX1LeJeAwAAT+bxj8Fv3bpVly9fdkzv3r1b9913n7p3757jMv7+/tq3b59j2mazubWPAACgZPH4ABQYGOg0/Y9//EPh4eFq06ZNjsvYbDaFhIS4u2sAAKCE8vhLYH+WkZGhBQsWaMCAAbme1UlNTVW1atVUpUoVdenSRXv27Mm13fT0dKWkpDi9AADAjatEBaClS5cqOTlZ/fr1y7FOnTp1NGfOHC1btkwLFixQZmamWrRooV9//TXHZWJiYhQQEOB4ValSxQ29BwAAnqJEBaDZs2crKipKYWFhOdZp3ry5+vTpo8aNG6tNmzZasmSJAgMD9c477+S4zJgxY3T27FnH6+jRo+7oPgAA8BAefw9QliNHjuirr77SkiVLrmu50qVL64477tCBAwdyrGO322W32wvaRQAAUEKUmDNAc+fOVVBQkDp37nxdy12+fFk//vijQkND3dQzAABQ0pSIAJSZmam5c+eqb9++KlXK+aRVnz59NGbMGMf0xIkTtXr1ah06dEg7duzQo48+qiNHjmjgwIFF3W0AAOChSsQlsK+++koJCQkaMGCAy7yEhAR5ef0vx505c0aDBg1SYmKibr75ZjVp0kSbNm1S/fr1i7LLAADAg5WIANSxY0cZY7KdFxsb6zQ9efJkTZ48uQh6BQAASqoScQkMAACgMBGAAACA5RCAAACA5ZSIe4CAG8XkNb8USjvP3Fe7UNrxNLmNzz0J7+a5nXsKozOS4g6dci08NOq62yms/ri0ex1j8mdxswtn/c1rVrjyj3Zjcq8IeCDOAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMshAAEAAMspVdwdAADgRjR5zS8FWv6ehFOSpOY1KxRGd3AVzgABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADL8egANH78eNlsNqdX3bp1c11m0aJFqlu3rnx9fdWwYUOtWLGiiHoLAABKCo8OQJLUoEEDHTt2zPH69ttvc6y7adMmRUdH6/HHH9fOnTvVtWtXde3aVbt37y7CHgMAAE/n8QGoVKlSCgkJcbwqVqyYY923335bnTp10nPPPad69epp0qRJuvPOOzVt2rQi7DEAAPB0Hh+A9u/fr7CwMNWsWVO9e/dWQkJCjnXj4uIUERHhVBYZGam4uLhc15Genq6UlBSnFwAAuHGVKu4O5KZZs2aaN2+e6tSpo2PHjmnChAlq3bq1du/erXLlyrnUT0xMVHBwsFNZcHCwEhMTc11PTEyMJkyYUKh9R9G7J+Fd18J1FYq+I+3GuH0Vk9f8cs069ySccns/YG1xh/7/PnZoVPF25P9rXrMYjneUWB59BigqKkrdu3fX7bffrsjISK1YsULJycn65JNPCnU9Y8aM0dmzZx2vo0ePFmr7AADAs3j0GaCrlS9fXrVr19aBAweynR8SEqKkpCSnsqSkJIWEhOTart1ul91uL7R+AgAAz+bRZ4CulpqaqoMHDyo0NDTb+c2bN9fatWudytasWaPmzZsXRfcAAEAJ4dEBaNSoUVq/fr0OHz6sTZs2qVu3bvL29lZ0dLQkqU+fPhoz5n/3W/ztb3/TypUr9eabb+rnn3/W+PHjtW3bNg0fPry4NgEAAHggj74E9uuvvyo6OlqnTp1SYGCgWrVqpc2bNyswMFCSlJCQIC+v/2W4Fi1a6KOPPtL//d//6e9//7tuvfVWLV26VLfddltxbQIAAPBAHh2AFi5cmOv82NhYl7Lu3bure/fubuoRAAC4EXj0JTAAAAB3IAABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADLIQABAADL8ej/DR4WsS7GpeiehFPF0BEAgFVwBggAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFhOqeLuAHDDWReT46x7Ek4VYUcAa4k7xPHlIpfPo+vSbkzhtONBOAMEAAAshwAEAAAshwAEAAAshwAEAAAshwAEAAAsx6MDUExMjJo2bapy5copKChIXbt21b59+3JdZt68ebLZbE4vX1/fIuoxAAAoCTw6AK1fv17Dhg3T5s2btWbNGl28eFEdO3ZUWlparsv5+/vr2LFjjteRI0eKqMcAAKAk8OjfAVq5cqXT9Lx58xQUFKTt27fr3nvvzXE5m82mkJAQd3cPAACUUB59BuhqZ8+elSTdcsstudZLTU1VtWrVVKVKFXXp0kV79uzJtX56erpSUlKcXgAA4MZVYgJQZmamRowYoZYtW+q2227LsV6dOnU0Z84cLVu2TAsWLFBmZqZatGihX3/9NcdlYmJiFBAQ4HhVqVLFHZsAAAA8RIkJQMOGDdPu3bu1cOHCXOs1b95cffr0UePGjdWmTRstWbJEgYGBeuedd3JcZsyYMTp79qzjdfTo0cLuPgAA8CAefQ9QluHDh+uLL77Qhg0bVLly5etatnTp0rrjjjt04MCBHOvY7XbZ7faCdhMAAJQQHn0GyBij4cOH67PPPtPXX3+tGjVqXHcbly9f1o8//qjQ0FA39BAAAJREHn0GaNiwYfroo4+0bNkylStXTomJiZKkgIAA+fn5SZL69OmjSpUqKSbmyv94O3HiRN1zzz2qVauWkpOT9cYbb+jIkSMaOHBgsW0HAADwLB4dgGbOnClJatu2rVP53Llz1a9fP0lSQkKCvLz+dyLrzJkzGjRokBITE3XzzTerSZMm2rRpk+rXr19U3QYAAB7OowOQMeaadWJjY52mJ0+erMmTJ7upRwAA4Ebg0fcAAQAAuAMBCAAAWA4BCAAAWI5H3wOEEmBdTHH3IFdxh04Vdxcg6Z6Ed4u7C0CRYp/3fJwBAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAlkMAAgAAllOquDtgSetiFHfoVIGa2Fz1CUnSM/fVLlA/AADWUJDvnc2XfinEnhTwu6uQcAYIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYDgEIAABYTokIQNOnT1f16tXl6+urZs2aacuWLbnWX7RokerWrStfX181bNhQK1asKKKeAgCAksDjA9DHH3+skSNHaty4cdqxY4caNWqkyMhIHT9+PNv6mzZtUnR0tB5//HHt3LlTXbt2VdeuXbV79+4i7jkAAPBUHh+A3nrrLQ0aNEj9+/dX/fr1NWvWLN10002aM2dOtvXffvttderUSc8995zq1aunSZMm6c4779S0adOKuOcAAMBTeXQAysjI0Pbt2xUREeEo8/LyUkREhOLi4rJdJi4uzqm+JEVGRuZYHwAAWE+p4u5Abk6ePKnLly8rODjYqTw4OFg///xztsskJiZmWz8xMTHH9aSnpys9Pd0xffbsWUlSSkpKfrueu7QLSvsj/dr1cnEhLVVSAfuYdqFAfXCngo4PANwoUgrps7ogn6tZ3zmFxV3fr1ntGmOuWdejA1BRiYmJ0YQJE1zKq1SpUgy9yasrl/T+Xsy9AABYQeHeRuLu765z584pICAg1zoeHYAqVqwob29vJSUlOZUnJSUpJCQk22VCQkKuq74kjRkzRiNHjnRMZ2Zm6vTp06pQoYJsNlsBtqBkSUlJUZUqVXT06FH5+/sXd3eKHePhijFxxZg4YzxcMSbO3DkexhidO3dOYWFh16zr0QHIx8dHTZo00dq1a9W1a1dJV8LJ2rVrNXz48GyXad68udauXasRI0Y4ytasWaPmzZvnuB673S673e5UVr58+YJ2v8Ty9/fnIP0TxsMVY+KKMXHGeLhiTJy5azyudeYni0cHIEkaOXKk+vbtq7vuukt33323pkyZorS0NPXv31+S1KdPH1WqVEkxMTGSpL/97W9q06aN3nzzTXXu3FkLFy7Utm3b9O677xbnZgAAAA/i8QGoZ8+eOnHihMaOHavExEQ1btxYK1eudNzonJCQIC+v/z3M1qJFC3300Uf6v//7P/3973/XrbfeqqVLl+q2224rrk0AAAAexuMDkCQNHz48x0tesbGxLmXdu3dX9+7d3dyrG4/dbte4ceNcLgdaFePhijFxxZg4YzxcMSbOPGU8bCYvz4oBAADcQDz6hxABAADcgQAEAAAshwAEAAAshwAEAAAshwB0A5s+fbqqV68uX19fNWvWTFu2bMmxbtu2bWWz2VxenTt3dtQxxmjs2LEKDQ2Vn5+fIiIitH///qLYlEJT2GPSr18/l/mdOnUqik0pFNczHpI0ZcoU1alTR35+fqpSpYqeeeYZXbjg/P8UXW+bnqawx2T8+PEu+0jdunXdvRmF6nrG5OLFi5o4caLCw8Pl6+urRo0aaeXKlQVq09MU9niU9H1kw4YNeuCBBxQWFiabzaalS5dec5nY2FjdeeedstvtqlWrlubNm+dSx+37iMENaeHChcbHx8fMmTPH7NmzxwwaNMiUL1/eJCUlZVv/1KlT5tixY47X7t27jbe3t5k7d66jzj/+8Q8TEBBgli5dar7//nvz4IMPmho1apg//vijiLaqYNwxJn379jWdOnVyqnf69Oki2qKCud7x+PDDD43dbjcffvihiY+PN6tWrTKhoaHmmWeeyXebnsYdYzJu3DjToEEDp33kxIkTRbVJBXa9YzJ69GgTFhZmli9fbg4ePGhmzJhhfH19zY4dO/Ldpidxx3iU9H1kxYoV5sUXXzRLliwxksxnn32Wa/1Dhw6Zm266yYwcOdLs3bvXTJ061Xh7e5uVK1c66hTFPkIAukHdfffdZtiwYY7py5cvm7CwMBMTE5On5SdPnmzKlStnUlNTjTHGZGZmmpCQEPPGG2846iQnJxu73W7+85//FG7n3aSwx8SYKwGoS5cuhd3VInG94zFs2DDTvn17p7KRI0eali1b5rtNT+OOMRk3bpxp1KiRW/pbFK53TEJDQ820adOcyh566CHTu3fvfLfpSdwxHiV9H/mzvASg0aNHmwYNGjiV9ezZ00RGRjqmi2If4RLYDSgjI0Pbt29XRESEo8zLy0sRERGKi4vLUxuzZ89Wr169VKZMGUlSfHy8EhMTndoMCAhQs2bN8txmcXLHmGSJjY1VUFCQ6tSpoyFDhujUqVOF2nd3yM94tGjRQtu3b3echj506JBWrFihv/zlL/lu05O4Y0yy7N+/X2FhYapZs6Z69+6thIQE921IIcrPmKSnp8vX19epzM/PT99++22+2/QU7hiPLCV1H8mPuLg4pzGUpMjISMcYFtU+QgC6AZ08eVKXL192/HchWYKDg5WYmHjN5bds2aLdu3dr4MCBjrKs5fLbZnFzx5hIUqdOnfTBBx9o7dq1eu2117R+/XpFRUXp8uXLhdr/wpaf8fjrX/+qiRMnqlWrVipdurTCw8PVtm1b/f3vf893m57EHWMiSc2aNdO8efO0cuVKzZw5U/Hx8WrdurXOnTvn1u0pDPkZk8jISL311lvav3+/MjMztWbNGi1ZskTHjh3Ld5uewh3jIZXsfSQ/EhMTsx3DlJQU/fHHH0W2jxCA4GL27Nlq2LCh7r777uLuisfIaUx69eqlBx98UA0bNlTXrl31xRdfaOvWrdn+Fy0lXWxsrF599VXNmDFDO3bs0JIlS7R8+XJNmjSpuLtWbPIyJlFRUerevbtuv/12RUZGasWKFUpOTtYnn3xSjD13n7ffflu33nqr6tatKx8fHw0fPlz9+/d3+j8brSQv42G1fcRTWHOPvMFVrFhR3t7eSkpKcipPSkpSSEhIrsumpaVp4cKFevzxx53Ks5bLT5uewB1jkp2aNWuqYsWKOnDgQIH66275GY+XXnpJjz32mAYOHKiGDRuqW7duevXVVxUTE6PMzMwCjbEncMeYZKd8+fKqXbu2x+8jUv7GJDAwUEuXLlVaWpqOHDmin3/+WWXLllXNmjXz3aancMd4ZKck7SP5ERISku0Y+vv7y8/Pr8j2EQLQDcjHx0dNmjTR2rVrHWWZmZlau3atmjdvnuuyixYtUnp6uh599FGn8ho1aigkJMSpzZSUFH333XfXbNMTuGNMsvPrr7/q1KlTCg0NLXCf3Sk/43H+/HmXv+K9vb0lXfmJhIKMsSdwx5hkJzU1VQcPHvT4fUQq2HHj6+urSpUq6dKlS/r000/VpUuXArdZ3NwxHtkpSftIfjRv3txpDCVpzZo1jjEssn2k0G6nhkdZuHChsdvtZt68eWbv3r3miSeeMOXLlzeJiYnGGGMee+wx88ILL7gs16pVK9OzZ89s2/zHP/5hypcvb5YtW2Z++OEH06VLlxL3GHxhjsm5c+fMqFGjTFxcnImPjzdfffWVufPOO82tt95qLly44PbtKajrHY9x48aZcuXKmf/85z/m0KFDZvXq1SY8PNz06NEjz216OneMybPPPmtiY2NNfHy82bhxo4mIiDAVK1Y0x48fL/Lty4/rHZPNmzebTz/91Bw8eNBs2LDBtG/f3tSoUcOcOXMmz216MneMR0nfR86dO2d27txpdu7caSSZt956y+zcudMcOXLEGGPMCy+8YB577DFH/azH4J977jnz008/menTp2f7GLy79xEC0A1s6tSppmrVqsbHx8fcfffdZvPmzY55bdq0MX379nWq//PPPxtJZvXq1dm2l5mZaV566SUTHBxs7Ha76dChg9m3b587N6HQFeaYnD9/3nTs2NEEBgaa0qVLm2rVqplBgwaViA/xLNczHhcvXjTjx4834eHhxtfX11SpUsUMHTrU6YP8Wm2WBIU9Jj179jShoaHGx8fHVKpUyfTs2dMcOHCgCLeo4K5nTGJjY029evWM3W43FSpUMI899pj57bffrqtNT1fY41HS95F169YZSS6vrHHo27evadOmjcsyjRs3Nj4+PqZmzZpOv6+Wxd37iM2YHM7TAgAA3KC4BwgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAAFgOAQgAspGRkVHcXQDgRgQgACXK4sWL1bBhQ/n5+alChQqKiIhQWlqaJGnOnDlq0KCB7Ha7QkNDNXz4cMdyCQkJ6tKli8qWLSt/f3/16NFDSUlJjvnjx49X48aN9e9//1s1atSQr6+vJCk5OVkDBw5UYGCg/P391b59e33//fdFu9EACh0BCECJcezYMUVHR2vAgAH66aefFBsbq4ceekjGGM2cOVPDhg3TE088oR9//FGff/65atWqJUnKzMxUly5ddPr0aa1fv15r1qzRoUOH1LNnT6f2Dxw4oE8//VRLlizRrl27JEndu3fX8ePH9eWXX2r79u2688471aFDB50+fbqoNx9AIeI/QwVQYuzYsUNNmjTR4cOHVa1aNad5lSpVUv/+/fXyyy+7LLdmzRpFRUUpPj5eVapUkSTt3btXDRo00JYtW9S0aVONHz9er776qn777TcFBgZKkr799lt17txZx48fl91ud7RXq1YtjR49Wk888YQbtxaAO5Uq7g4AQF41atRIHTp0UMOGDRUZGamOHTvqkUce0cWLF/X777+rQ4cO2S73008/qUqVKo7wI0n169dX+fLl9dNPP6lp06aSpGrVqjnCjyR9//33Sk1NVYUKFZza++OPP3Tw4EE3bCGAokIAAlBieHt7a82aNdq0aZNWr16tqVOn6sUXX9TatWsLpf0yZco4Taempio0NFSxsbEudcuXL18o6wRQPAhAAEoUm82mli1bqmXLlho7dqyqVaumNWvWqHr16lq7dq3atWvnsky9evV09OhRHT161OkSWHJysurXr5/juu68804lJiaqVKlSql69urs2CUAxIAABKDG+++47rV27Vh07dlRQUJC+++47nThxQvXq1dP48eP15JNPKigoSFFRUTp37pw2btyop556ShEREWrYsKF69+6tKVOm6NKlSxo6dKjatGmju+66K8f1RUREqHnz5uratatef/111a5dW7///ruWL1+ubt265bosAM9GAAJQYvj7+2vDhg2aMmWKUlJSVK1aNb355puKioqSJF24cEGTJ0/WqFGjVLFiRT3yyCOSrpw1WrZsmZ566inde++98vLyUqdOnTR16tRc12ez2bRixQq9+OKL6t+/v06cOKGQkBDde++9Cg4Odvv2AnAfngIDAACWw+8AAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAyyEAAQAAy/l/710MoFu8+rIAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: [0.5363321799307958, 1.0], auc: [0.4494949494949495, 1.0], c@1: [0.4671280276816609, 1.0], f_05_u: [0.52285050348567, 0.9899], F1: [0.6367924528301887, 1.0], overall: [0.5190664833731172, 0.9899]\n",
            "([0.5363321799307958, 1.0], [0.6367924528301887, 1.0])\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# modelEmb.cuda()\n",
        "evaluator = ContrastiveChunkerEvaluator(test_dataloader,torch.nn.CosineSimilarity(),find_threshold=True,threshold=0.80)\n",
        "all_results = evaluator.call(modelEmb)\n",
        "print(all_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1OBUM551UZ7h",
        "outputId": "433cf631-e007-441d-bc4b-0bd79d543d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 201/201 [00:57<00:00,  3.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.1884,  0.5195,  0.9705,  0.9163, -0.3123,  0.7654,  0.7308,  0.5057,\n",
            "        -0.1642,  0.0794,  0.6466, -0.2823,  0.6979,  0.0540,  0.4567,  0.4132,\n",
            "        -0.2784,  0.5461, -0.1018, -0.1326,  0.9096,  0.9173,  0.4475,  0.2384,\n",
            "         0.2709,  0.5235,  0.9401, -0.1455,  0.3382, -0.2723,  0.2054, -0.2824,\n",
            "         0.2159,  0.1219,  0.2785, -0.2670,  0.5083,  0.2246,  0.5927,  0.4044,\n",
            "         0.0129,  0.6228, -0.3231,  0.1413,  0.7412,  0.0964,  0.3845,  0.1082,\n",
            "         0.0587, -0.2091,  0.7739,  0.6461,  0.5741, -0.1842,  0.2684, -0.1410,\n",
            "         0.8726,  0.1487,  0.2178,  0.7117,  0.3322,  0.3915,  0.1807, -0.0976,\n",
            "         0.3309,  0.2163,  0.4321,  0.4526,  0.6603,  0.3251,  0.8365, -0.2743,\n",
            "        -0.1953, -0.3932,  0.3394,  0.7096,  0.3351, -0.1405, -0.0304, -0.2603,\n",
            "         0.1418,  0.2490,  0.6338,  0.3732, -0.1104, -0.2126,  0.9414,  0.5836,\n",
            "         0.2912,  0.2019,  0.7411,  0.9771,  0.4062, -0.3014,  0.6244,  0.4269,\n",
            "         0.8443,  0.0090,  0.6897,  0.4104,  0.1052, -0.0584,  0.5409,  0.4341,\n",
            "         0.3829,  0.4211,  0.1778, -0.0776,  0.0318, -0.2694,  0.6495,  0.5157,\n",
            "         0.8405,  0.5037,  0.2672, -0.2406,  0.1122, -0.2662,  0.3719, -0.2067,\n",
            "         0.7673,  0.3177,  0.0720,  0.7109,  0.6050,  0.8103, -0.5259,  0.9158,\n",
            "         0.5698,  0.1363, -0.0926, -0.0970, -0.1460, -0.0011,  0.1815,  0.2296,\n",
            "         0.2938, -0.1778, -0.0960,  0.2463,  0.6009,  0.4918, -0.2911,  0.2297,\n",
            "        -0.3485,  0.5100, -0.2684,  0.4366,  0.0788,  0.4353,  0.6720, -0.1733,\n",
            "         0.5310,  0.2234,  0.4795,  0.9437,  0.4124, -0.2551, -0.2215,  0.3664,\n",
            "         0.5273,  0.5061,  0.5326,  0.7630,  0.2219,  0.2745,  0.4400, -0.0808,\n",
            "         0.6549,  0.6750,  0.8258,  0.4278, -0.0076,  0.5203,  0.7397,  0.2271,\n",
            "         0.3088,  0.1463,  0.4043,  0.1317, -0.2604, -0.1405, -0.0896,  0.9466,\n",
            "         0.3933, -0.0033,  0.1579,  0.8232,  0.4008,  0.3931,  0.4106,  0.3842,\n",
            "         0.3647,  0.2366,  0.2406,  0.3005,  0.5074, -0.2673,  0.9412, -0.1788,\n",
            "        -0.1780], device='cuda:0')\n",
            "[('87a79a17-4d07-50f2-8fc5-739a6cc3e615',), ('066602b1-e276-5cf2-9c6a-910dac1a0521',), ('b9edc138-4f93-594d-94a3-b22575c98329',), ('cf9c5afd-5809-50ae-8edf-26a6b92ffde5',), ('95a84b41-a3a3-5efa-963b-951fd0db111c',), ('4ac59721-f531-5ee9-a719-980460cd74e6',), ('b2b559c6-1e24-5eec-8ceb-b84a34de7bf8',), ('629943a2-9ad5-57ee-8df6-a681905285b6',), ('87a79a17-4d07-50f2-8fc5-739a6cc3e615',), ('11e27f6d-b203-5749-9d1e-cc81713a9b86',), ('fcce4af9-6f1f-5b69-b935-7ea62e491774',), ('2b2a80ad-692e-5115-8ee5-63ce12fc4a37',), ('07dfb1a5-58d9-544f-8d9e-807cb557ce9d',), ('3be1acef-5e60-5213-9759-1992c46d265c',), ('d78e6deb-b618-54be-92cc-36903c38f095',), ('081f6233-700f-5011-b57f-0ca3398398c1',), ('7c048d40-0329-5135-906c-dd0879cccbbb',), ('74a4d9da-c3aa-5efd-9989-74390df979af',), ('d7a84449-93c2-5eaf-a961-0378530b7a5f',), ('24becf43-f5ff-59e8-9459-2ba40ec5c276',), ('f01dfeab-4622-53c8-8272-4c8b384d6412',), ('f3ab2879-25fe-5934-a460-a48b0778e2f0',), ('d78e6deb-b618-54be-92cc-36903c38f095',), ('90a73d68-2a93-52eb-a167-249669fdd591',), ('22cb3360-11e7-5cc2-9703-db7761385421',), ('c8271fcb-9bf0-5113-9d3e-a2edf849fc47',), ('32fef089-abad-523d-976f-26715a899c7f',), ('5f28d974-317e-5240-b1af-3d71a5a87d00',), ('426d8094-960c-5f37-8d6a-259a97f5157b',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('6fa0ed7b-1dfd-5231-8a75-01e757197db2',), ('47a91096-a950-5611-b1c2-42d0b35ed996',), ('90a73d68-2a93-52eb-a167-249669fdd591',), ('459f12ad-6078-577f-92d9-ade6edf8fbea',), ('306f3a48-b49d-5c9c-84b3-ec64b5133ec2',), ('5e8dd88c-8ec8-5441-9c04-f9ca7c9fdbde',), ('fe25d384-8512-55ad-ba3a-71b3c7e17351',), ('c25df04c-73ae-58d3-8ea4-787bfd39aad8',), ('23b0cad1-42a6-551f-9ec4-9022f27f5bbd',), ('bc9a7911-eabd-5101-8533-dd35aacef865',), ('1a47e559-1a50-5e80-ad93-408d0d55553c',), ('0f46177e-9a3a-5be6-8aca-9516d5941915',), ('741fce2a-aa12-5bce-9a38-5b27a407c309',), ('e26348c2-bd4f-5ff9-98e6-0a94f7fe9711',), ('a74cd5dc-ad8f-57ef-8270-5c6d25c6cb96',), ('87c3ba13-c4d6-5144-b01c-6f15434fc3dc',), ('a5b6b010-4078-50c8-94b6-0b902f194836',), ('87c3ba13-c4d6-5144-b01c-6f15434fc3dc',), ('3be1acef-5e60-5213-9759-1992c46d265c',), ('427ce81f-9152-5702-a9cb-c1506b53739b',), ('0e3b6611-d233-5b95-9013-eed3cc05206c',), ('6ee859aa-cffb-5471-8063-dac4fde02f1f',), ('6a6a4413-faaf-5d72-923a-37fa65edda68',), ('33682728-2a74-54cd-9220-124d4b82ed59',), ('9edf450f-f14a-5654-ac51-e98f82fc809c',), ('24becf43-f5ff-59e8-9459-2ba40ec5c276',), ('8c7c1a34-a42a-5b91-ab8f-4fd1a6a5d9d7',), ('22ad2c98-f085-5da6-83a3-83ab25dad939',), ('6fa0ed7b-1dfd-5231-8a75-01e757197db2',), ('78702a36-9f6f-5b84-95fa-db287db2e41b',), ('426d8094-960c-5f37-8d6a-259a97f5157b',), ('a5b6b010-4078-50c8-94b6-0b902f194836',), ('fd542599-efc1-5acb-b32b-b040ab204f35',), ('9e259fe1-a966-5ceb-9946-a980d71e8eb4',), ('22ce00f1-6eff-58c9-9714-a3be8b256d57',), ('e4f6d187-8612-5654-b6ac-fa7953f00b36',), ('889ab7b1-17d7-589a-b248-c7a85cf1fd5c',), ('56d7c101-ff35-527e-8222-d7583584f0f3',), ('d0bcd781-a26e-5cf0-8753-7798af653efd',), ('091809eb-e1b3-5045-85b2-065f802c6075',), ('609482c3-f1a5-583c-858e-2c63ee222df6',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('37d101b9-cbc5-59b1-83dd-d2becdc1a249',), ('1ef1e912-4638-576c-836e-26106280ee37',), ('22ce00f1-6eff-58c9-9714-a3be8b256d57',), ('b370b9b6-7ea0-5f82-a848-4ffc42d5a0d8',), ('51a330fc-d8b9-517d-a11a-b2aeb669bf16',), ('0cf91c65-1523-5f66-b2c8-40a2ea7197e5',), ('be4c0e4d-4f52-5596-867c-afde6fe13cfb',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('e26348c2-bd4f-5ff9-98e6-0a94f7fe9711',), ('22cb3360-11e7-5cc2-9703-db7761385421',), ('d0bcd781-a26e-5cf0-8753-7798af653efd',), ('bc8b9690-bcda-52bc-987e-134e82d615a9',), ('dae55351-369d-5d80-aa55-3dfc9e322a5a',), ('96884382-c15a-5245-bb5f-a66d6a0fe5fb',), ('32fef089-abad-523d-976f-26715a899c7f',), ('56ca41c5-c5d2-5421-a715-992309522e1a',), ('52427e60-e022-597e-bbd0-6771da39f12d',), ('6fa0ed7b-1dfd-5231-8a75-01e757197db2',), ('b2b559c6-1e24-5eec-8ceb-b84a34de7bf8',), ('07e30a6f-45dd-5239-80bc-cef3af9be71b',), ('8006be38-8dfe-5463-a9b8-e120565cc9a1',), ('95a84b41-a3a3-5efa-963b-951fd0db111c',), ('fcce4af9-6f1f-5b69-b935-7ea62e491774',), ('5c8871e9-d384-504c-b338-41bebd551487',), ('7a5b0c34-38db-5da5-808f-67a78747ff53',), ('1a47e559-1a50-5e80-ad93-408d0d55553c',), ('846c77f5-42f8-509b-ab3e-65a70cd335ea',), ('44e1f95e-4fe2-575a-b7ab-82d9c0cf59d6',), ('1a3a141c-011a-5504-897c-e746d6c5f75d',), ('2c59ebda-477e-5f22-aa7e-3da39dd167a4',), ('b0a9ac7d-2a88-566d-90ba-5fd41d66845d',), ('bc9a7911-eabd-5101-8533-dd35aacef865',), ('a5b6b010-4078-50c8-94b6-0b902f194836',), ('243abd1b-8f09-5e8e-af05-3d25c22657e1',), ('f07f67d4-a602-54ce-bf58-718de3fbeefc',), ('aadc202a-a531-5e08-8c3a-7438cc6cc058',), ('1a47e559-1a50-5e80-ad93-408d0d55553c',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('6ee859aa-cffb-5471-8063-dac4fde02f1f',), ('c8271fcb-9bf0-5113-9d3e-a2edf849fc47',), ('7a5b0c34-38db-5da5-808f-67a78747ff53',), ('e03f911d-6567-5934-8578-0d4820a6255c',), ('e4f6d187-8612-5654-b6ac-fa7953f00b36',), ('88aab503-be2c-5233-949c-a4a652ab79eb',), ('edc69dd5-055a-54cf-8b0c-028e0591c573',), ('1ddcc4d9-f39b-5b27-8482-83fed6544550',), ('a5e9a289-0999-5764-b597-dc1bf8c21ede',), ('25db7f95-77e9-57b8-adf9-0e2a54be8bd9',), ('4ac59721-f531-5ee9-a719-980460cd74e6',), ('091809eb-e1b3-5045-85b2-065f802c6075',), ('7c9056e9-3516-5b7b-af36-3170a9507e25',), ('44bca4d0-80d6-5df6-a1a9-98be67017ec7',), ('6a6a4413-faaf-5d72-923a-37fa65edda68',), ('bfe0849e-edec-58f0-85e8-c85f11984981',), ('84583d17-6c43-5780-8477-fa85a74776c6',), ('492bbc18-1344-5a44-a590-072786d978da',), ('56ca41c5-c5d2-5421-a715-992309522e1a',), ('652b1be5-8d09-5ebc-822b-5873567e582f',), ('9e259fe1-a966-5ceb-9946-a980d71e8eb4',), ('9e259fe1-a966-5ceb-9946-a980d71e8eb4',), ('769a84ad-2446-5e52-84fc-beaa3f19351e',), ('742c563d-6616-5f20-beae-e54b8b07d5e0',), ('fd542599-efc1-5acb-b32b-b040ab204f35',), ('4258f949-3208-5869-b0db-c184e1de3892',), ('52427e60-e022-597e-bbd0-6771da39f12d',), ('734150a2-3e67-5c72-bba8-e41d50427450',), ('9e259fe1-a966-5ceb-9946-a980d71e8eb4',), ('e4f6d187-8612-5654-b6ac-fa7953f00b36',), ('dbd00a0c-0646-5121-b312-f150f6f3b5c9',), ('c3d888ca-9149-5553-8603-7fec83b22691',), ('2b2a80ad-692e-5115-8ee5-63ce12fc4a37',), ('c25df04c-73ae-58d3-8ea4-787bfd39aad8',), ('ab2248c9-0484-5233-b84b-811b08c18b37',), ('fe25d384-8512-55ad-ba3a-71b3c7e17351',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('bc87ecaa-3472-5e4c-854b-a2018cd3c0c6',), ('e2ac4453-bf54-53f2-bf68-6caae6aacded',), ('bc87ecaa-3472-5e4c-854b-a2018cd3c0c6',), ('d7271dd5-8c6a-59cc-9000-11b95be209e1',), ('e02af6ec-a25b-57a5-886f-516cf69ea45e',), ('89865d3f-5f33-54a2-942d-100d292a4baf',), ('841bb85b-c365-541c-991f-405a77c57bd8',), ('e03f911d-6567-5934-8578-0d4820a6255c',), ('32fef089-abad-523d-976f-26715a899c7f',), ('44e1f95e-4fe2-575a-b7ab-82d9c0cf59d6',), ('6a661a9d-e9b8-5912-8e65-66f4183fdd0e',), ('71583e0d-4014-53d1-9fbf-8e7cd3e3cce0',), ('72ff1bfd-96f5-5243-b152-ffc8749bc289',), ('2fb0cb2f-eac3-5d2c-8bbc-325ecc75b3f8',), ('fe25d384-8512-55ad-ba3a-71b3c7e17351',), ('b0a9ac7d-2a88-566d-90ba-5fd41d66845d',), ('0b2046f6-6e3a-5f91-ab25-934c8a371fb3',), ('90a73d68-2a93-52eb-a167-249669fdd591',), ('9edf450f-f14a-5654-ac51-e98f82fc809c',), ('3f6f2e3e-e948-56c2-b494-3d4f4714b038',), ('9e259fe1-a966-5ceb-9946-a980d71e8eb4',), ('34285edf-7536-581a-830a-b2c4ac9cd804',), ('2191927b-8563-592b-9906-41d8e12bf6b6',), ('fc7a4795-d5cb-5e87-a948-f03d36f01204',), ('f743bf27-19f6-546f-a25a-52205f738ae5',), ('31024f0c-5b05-549a-affc-42c891c72431',), ('066602b1-e276-5cf2-9c6a-910dac1a0521',), ('44bca4d0-80d6-5df6-a1a9-98be67017ec7',), ('4258f949-3208-5869-b0db-c184e1de3892',), ('c6c7bfd2-2487-5890-8b62-de1bf94cfed2',), ('e26348c2-bd4f-5ff9-98e6-0a94f7fe9711',), ('243abd1b-8f09-5e8e-af05-3d25c22657e1',), ('cb4054b1-d422-58d6-a137-dcfc70100df6',), ('d710842b-5ec3-58c8-803e-7edd22f0556b',), ('0cf91c65-1523-5f66-b2c8-40a2ea7197e5',), ('f7f07940-a4a0-5552-b997-3916a47fb539',), ('32fef089-abad-523d-976f-26715a899c7f',), ('a5b6b010-4078-50c8-94b6-0b902f194836',), ('52087bd7-6d4d-582a-b602-47eeebad50ab',), ('fd542599-efc1-5acb-b32b-b040ab204f35',), ('609482c3-f1a5-583c-858e-2c63ee222df6',), ('312548a6-dbe3-564d-a977-fe728d2e86d2',), ('081f6233-700f-5011-b57f-0ca3398398c1',), ('312548a6-dbe3-564d-a977-fe728d2e86d2',), ('5600adb2-9761-5d84-bb1f-ea452c43530a',), ('9829748b-9b85-50ff-bc98-66ade15c4dd3',), ('d578f8e1-1a26-56db-9a6a-1fbed616454c',), ('d578f8e1-1a26-56db-9a6a-1fbed616454c',), ('306f3a48-b49d-5c9c-84b3-ec64b5133ec2',), ('0900afe8-2f98-5abe-93c9-85be50921e11',), ('c82a3e47-d7d3-5ff0-a09f-f464592893c7',), ('b9326101-6352-56dd-9d1b-1f41466897b7',), ('87a79a17-4d07-50f2-8fc5-739a6cc3e615',), ('2a510395-b263-5412-a3f2-73b305a8fb0e',)]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy: [0.8159203980099502, 0.45459999999999995], auc: [0.8964964964964965, 1.0], c@1: [0.7960199004975124, 1.0], f_05_u: [0.8187134502923976, 0.9899], F1: [0.7320261437908496, 1.0], overall: [0.8108139977693141, 0.9899]\n",
            "([0.8159203980099502, 0.45459999999999995], [0.7320261437908496, 1.0])\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "modelEmb.cuda()\n",
        "evaluator2 = ContrastiveChunkerEvaluator(test_dataloader2,torch.nn.CosineSimilarity(),find_threshold=False)\n",
        "all_results = evaluator2.call(modelEmb)\n",
        "print(all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyu7_MnPqZKB",
        "outputId": "77d2a438-4f95-4f2f-ac74-c1106392f656"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "([0.7549668874172185, 0.5657], [0.7826086956521738, 1.0])\n"
          ]
        }
      ],
      "source": [
        "metr = getPickleFileInDict('test_results_pan20')\n",
        "print(all_results)\n",
        "# print(roc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQCMK6wWNrJy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds_probs = probs[:, 1].cpu().data.numpy()\n",
        "    print(probs)\n",
        "    preds = probs.argmax(1).cpu().data.numpy()\n",
        "    y_true = y_true.cpu().data.numpy()\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds_probs,pos_label=1)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "    print(f'F1-score: {f1*100:.2f}%')\n",
        "    # Plot ROC AUC\n",
        "    # plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUXJjSNshqzn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import brier_score_loss\n",
        "def calcBrier(probs,y_true):\n",
        "  # predict probabilities\n",
        "  preds = probs[:, 1]\n",
        "  # keep the predictions for class 1 only\n",
        "  \n",
        "  # calculate bier score\n",
        "  loss = brier_score_loss(y_true, preds)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTQvb_XKsLKv"
      },
      "outputs": [],
      "source": [
        "def getTensors(cosine,label):\n",
        "  print(cosine)\n",
        "  pred = torch.relu(cosine)\n",
        "  # print(pred)\n",
        "  # print(torch.sigmoid(cosine))\n",
        "  # print(label)\n",
        "  if label==1:\n",
        "    #  gt = torch.cat((1-label,label),dim=1).to(device)\n",
        "     if pred == torch.Tensor([0.0]).to(device): #means negative cosine\n",
        "        # if torch.sigmoid(cosine) > (1-torch.sigmoid(cosine)):\n",
        "        #    pred = torch.cat((torch.sigmoid(cosine),1-torch.sigmoid(cosine)),dim=0) \n",
        "        # else:\n",
        "        #    pred = torch.cat((1-torch.sigmoid(cosine),torch.sigmoid(cosine)),dim=0)\n",
        "        pred = torch.cat((1-torch.sigmoid(cosine),torch.sigmoid(cosine)),dim=0)\n",
        "        \n",
        "     else:\n",
        "        # if torch.sigmoid(cosine) > (1-torch.sigmoid(cosine)):\n",
        "        #    pred = torch.cat((1-cosine,cosine),dim=0)  \n",
        "        # else:\n",
        "        #    pred = torch.cat((cosine,1-cosine),dim=0)\n",
        "        pred = torch.cat((1-torch.sigmoid(torch.Tensor([1])),torch.sigmoid(torch.Tensor([1]))),dim=0)\n",
        "  else:\n",
        "    #  gt = torch.cat((label,1-label),dim=1).to(device)\n",
        "     if pred == torch.Tensor([0.0]).to(device): #means negative cosine\n",
        "        # if torch.sigmoid(cosine) > (1-torch.sigmoid(cosine)):\n",
        "        #    pred = torch.cat((cosine,1-cosine),dim=0) \n",
        "        # else:\n",
        "        #    pred = torch.cat((1-cosine,cosine),dim=0)\n",
        "        pred = torch.cat((torch.sigmoid(torch.Tensor([1])),(1-torch.sigmoid(torch.Tensor([1])) ) ),dim=0)\n",
        "        \n",
        "     else:\n",
        "        # if torch.sigmoid(cosine) > (1-torch.sigmoid(cosine)):\n",
        "        #    pred = torch.cat((1-torch.sigmoid(cosine),torch.sigmoid(cosine)),dim=0)  \n",
        "        # else:\n",
        "        #    pred = torch.cat((torch.sigmoid(cosine),1-torch.sigmoid(cosine)),dim=0)\n",
        "        pred = torch.cat((torch.sigmoid(cosine),1-torch.sigmoid(cosine)),dim=0)\n",
        "  \n",
        "  return pred,label\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def calc_accuracy_f1(preds,labels):\n",
        "    labels = labels.cpu().data.numpy()\n",
        "    pred = preds.argmax(1).cpu().data.numpy()\n",
        "    accuracy = accuracy_score(labels, pred)#(torch.sum(preds == labels) )/len(preds)\n",
        "\n",
        "    f1 = f1_score(labels, pred, average='macro')\n",
        "    return accuracy,f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxa7hwDQ_eQL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "# f1scores = list()\n",
        "def bert_predict( modelEmb, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    # modelCLS.cuda()\n",
        "    modelEmb.cuda()\n",
        "    modelEmb.eval()\n",
        "    # modelCLS.eval()\n",
        "    all_logits = []\n",
        "    f1scores = 0\n",
        "    # For each batch in our test set...\n",
        "    step = 0\n",
        "    target_label = []\n",
        "    accs = 0\n",
        "    for input1, mask1, input2, mask2,target1 in test_dataloader:#validation_dataloader:\n",
        "        # print(batch[0].permute(0,2,1).squeeze(2).shape)\n",
        "        # print(batch[1])\n",
        "        # print(batch[2])\n",
        "        step+=1\n",
        "        # print(input1)\n",
        "        # Load batch to GPU\n",
        "        b_input_ids1 = input1[0].to(device)\n",
        "        b_input_mask1 = mask1[0].to(device)\n",
        "        label = target1[0].type(torch.LongTensor)\n",
        "        b_input_ids2 = input2[0].to(device)\n",
        "        b_input_mask2 = mask2[0].to(device)\n",
        "        # label = label.type(torch.LongTensor)\n",
        "        b_labels = label.to(device)\n",
        "        # print(b_labels)\n",
        "        # print(b_input_ids1)\n",
        "        h = modelEmb.init_hidden(b_input_ids1.size(0))\n",
        "        # b_labels = torch.squeeze(b_labels,1)\n",
        "        # b_input_ids2 = input_id2.to(device)\n",
        "        # b_input_mask2 = mask2.to(device)\n",
        "        # step += 1\n",
        "        # print(\"Batch \",str(step), \" from \", str(len(test_dataloader)))\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            FC11 = modelEmb(b_input_ids1, b_input_mask1,h)\n",
        "            FC22 = modelEmb(b_input_ids2, b_input_mask2,h)\n",
        "            # out = modelCLS(out1,out2)\n",
        "        FC11 = FC11.mean(0)\n",
        "        FC22 = FC22.mean(0)\n",
        "       \n",
        "        out = F.cosine_similarity(FC11,FC22,dim=0)\n",
        "        print(out)\n",
        "        print(b_labels)\n",
        "        # pred,target = getTensors(out.unsqueeze(0),b_labels[0])\n",
        "        \n",
        "        # all_logits.append(pred.to(device))\n",
        "        # target_label.append(target.to(device))\n",
        "        # f1 = calcF1score(F.sigmoid(out.unsqueeze(0)), b_labels[0].unsqueeze(0))\n",
        "    #     acc = calcAccuracy(out.unsqueeze(0), b_labels[0].unsqueeze(0))\n",
        "    #     # f1scores += f1\n",
        "    #     accs+=acc\n",
        "    #     # print(\"Batch \",str(step), \" from \", str(len(test_dataloader)), \" ---> \",str(f1),\"------- Acc= \",str(acc) )\n",
        "    #     print(\"Batch \",str(step), \" from \", str(len(test_dataloader)), \" ---> ------- Acc= \",str(acc) )\n",
        "    # # Concatenate logits from each batch\n",
        "    # all_logits = torch.stack(all_logits)\n",
        "    # target_label = torch.stack(target_label)\n",
        "    # print(accs/len(test_dataloader))\n",
        "    # avg_acc = accs/len(test_dataloader)\n",
        "    # Apply softmax to calculate probabilities\n",
        "    # probs = all_logits.cpu().numpy()\n",
        "    # avg_f1score = f1scores /len(test_dataloader)\n",
        "    return all_logits,target_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td9_DcXaCIt2"
      },
      "source": [
        "# PAN 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leltW4TNCL6X"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EQGX4pBLCcVB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "class AuthorshipDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset for Author Verification on the IMDB62 Dataset.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dict_per_auth_ids,\n",
        "                 dict_per_auth_masks,\n",
        "                 pan22\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_file (string): the path to the IMDB62 Dataset txt file\n",
        "        \"\"\"\n",
        "        # get the dataset, then break it up into dict key'd on authors with values a list of texts.\n",
        "        self.per_author_dataset = dict_per_auth_ids\n",
        "        self.per_author_dataset_masks = dict_per_auth_masks\n",
        "        # self.base_rate = base_rate\n",
        "        self.authors = list(self.per_author_dataset.keys())\n",
        "        self.PAN22 = pan22\n",
        "    def __len__(self):\n",
        "        #return sum([len(self.per_author_dataset[a]) for a in self.per_author_dataset.keys()])\n",
        "        if self.PAN22: \n",
        "          return sum([len(self.per_author_dataset[a][k][j]) for a in self.per_author_dataset.keys() for k in self.per_author_dataset[a].keys() for j in self.per_author_dataset[a][k].keys()])\n",
        "        else:\n",
        "          return sum([len(self.per_author_dataset[a][k]) for a in self.per_author_dataset.keys() for k in self.per_author_dataset[a].keys()])\n",
        "    def __getitem__(self, idx):\n",
        "        n_auth = len(self.authors)\n",
        "        \n",
        "        auth = self.authors[idx%n_auth]\n",
        "\n",
        "        # print(auth)\n",
        "        textid1 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "        textid2 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "\n",
        "        counter = 0\n",
        "        while textid1 == textid2:\n",
        "            counter+=1\n",
        "            if counter > 5:\n",
        "               break \n",
        "            textid2 = random.choice(range(0,len(self.per_author_dataset[auth])))\n",
        "        \n",
        "        if self.PAN22==True:\n",
        "          type1 = random.choice(list(self.per_author_dataset[auth][textid1].keys()))\n",
        "          type2 = random.choice(list(self.per_author_dataset[auth][textid2].keys()))\n",
        "          \n",
        "          cnt = 0\n",
        "          while type1 == type2:\n",
        "              cnt+=1\n",
        "              if cnt == 10:\n",
        "                  break\n",
        "              type2 = random.choice(list(self.per_author_dataset[auth][textid2].keys()))\n",
        "          chunkid1 = random.choice(range(0,len(self.per_author_dataset[auth][textid1][type1])))\n",
        "          chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2][type2])))\n",
        "          text1 = self.per_author_dataset[auth][textid1][type1][chunkid1]\n",
        "          mask1 = self.per_author_dataset_masks[auth][textid1][type1][chunkid1]\n",
        "          text2 = self.per_author_dataset[auth][textid2][type2][chunkid2]\n",
        "          mask2 = self.per_author_dataset_masks[auth][textid2][type2][chunkid2]\n",
        "          cnt=0\n",
        "          # print(text1)\n",
        "          while torch.equal(text1,text2):\n",
        "                cnt+=1\n",
        "              \n",
        "                if cnt > 50:\n",
        "                  break\n",
        "                chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2][type2])))    \n",
        "                text2 = self.per_author_dataset[auth][textid2][type2][chunkid2]\n",
        "                mask2 = self.per_author_dataset_masks[auth][textid2][type2][chunkid2]\n",
        "        else:\n",
        "            chunkid1 = random.choice(range(0,len(self.per_author_dataset[auth][textid1])))\n",
        "            chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2])))\n",
        "            text1 = self.per_author_dataset[auth][textid1][chunkid1]\n",
        "            mask1 = self.per_author_dataset_masks[auth][textid1][chunkid1]\n",
        "            text2 = self.per_author_dataset[auth][textid2][chunkid2]\n",
        "            mask2 = self.per_author_dataset_masks[auth][textid2][chunkid2]\n",
        "            cnt=0\n",
        "            # print(text1)\n",
        "            while torch.equal(text1,text2):\n",
        "                  cnt+=1\n",
        "                \n",
        "                  if cnt > 50:\n",
        "                    break\n",
        "                  chunkid2 = random.choice(range(0,len(self.per_author_dataset[auth][textid2])))    \n",
        "                  text2 = self.per_author_dataset[auth][textid2][chunkid2]\n",
        "                  mask2 = self.per_author_dataset_masks[auth][textid2][chunkid2]\n",
        "        return text1,mask1,text2,mask2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Aa2pQ8ChUn",
        "outputId": "86eed0e7-dd2b-42b3-b4dc-9b8651841591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5561\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "train_ids = base_path+'/PAN23/train/PAN23_370_train_overlap_uncased_ids_openset_withPOS_concat'\n",
        "train_masks = base_path+'/PAN23/train/PAN23_370_train_overlap_uncased_masks_openset_withPOS_concat'\n",
        "val_ids = base_path+'/PAN23/val/PAN23_370_val_overlap_uncased_ids_openset_withPOS_concat'\n",
        "val_masks = base_path+'/PAN23/val/PAN23_370_val_overlap_uncased_masks_openset_withPOS_concat'\n",
        "\n",
        "trainIds,valIds,trainMasks,valMasks = getPickleFileInDict(train_ids),getPickleFileInDict(val_ids),getPickleFileInDict(train_masks),getPickleFileInDict(val_masks)\n",
        "\n",
        "dataset_train = AuthorshipDataset(trainIds,trainMasks,True)\n",
        "train_dataloader = torch.utils.data.DataLoader(dataset_train, batch_size=16,shuffle=True)\n",
        "\n",
        "dataset_val = AuthorshipDataset(valIds,valMasks,True)\n",
        "validation_dataloader = torch.utils.data.DataLoader(dataset_val, batch_size=16,shuffle=True)\n",
        "\n",
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3ZPRzSBj8Jj"
      },
      "source": [
        "## Read Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zd_YEmyhj-jH"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "def getPickleFileInDict(dataset):\n",
        "    with open( dataset, 'rb') as f:\n",
        "        dict_dataset = pickle.load(f)\n",
        "    return dict_dataset\n",
        "\n",
        "class MyDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self,\n",
        "                 data_pos,\n",
        "                 base_rate: float = 0.5\n",
        "                 ):\n",
        "\n",
        "       \n",
        "        self.per_pair_dataset = data_pos\n",
        "        self.used = {}\n",
        "        self.base_rate = base_rate\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # return sum([len(self.per_pair_dataset[x]) for x in self.per_pair_dataset.keys()])\n",
        "        return len(list(self.per_pair_dataset.keys()))#+len(list(self.per_pair_dataset2.keys()))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        id = random.choice(list(self.per_pair_dataset.keys()))\n",
        "        # if id not in self.used.keys():\n",
        "        x = self.per_pair_dataset[id]\n",
        "        #    self.used[id]=[]\n",
        "        # else:\n",
        "        #    while id in self.used.keys():\n",
        "        #         id = random.choice(list(self.per_pair_dataset.keys()))\n",
        "                \n",
        "        #         if id not in self.used.keys():\n",
        "        #            self.used[id]=[]\n",
        "        #            x = self.per_pair_dataset[id]\n",
        "        #            break\n",
        "\n",
        "        batchText1 = []\n",
        "        batchText2 = []\n",
        "        batchMask1 = []\n",
        "        batchMask2 = []\n",
        "        labels = []\n",
        "       \n",
        "        for item in x:\n",
        "            \n",
        "            batchText1.append(item[0])\n",
        "            batchMask1.append(item[1])\n",
        "            batchText2.append(item[2])\n",
        "            batchMask2.append(item[3])\n",
        "\n",
        "            labels.append(item[4])\n",
        "\n",
        "        return torch.stack(batchText1), torch.stack(batchMask1), torch.stack(batchText2), torch.stack(batchMask2), torch.LongTensor(labels),id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHb0f2WlkCnx",
        "outputId": "311208c3-1fa7-4c5c-f2d0-75474f8b95b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289\n"
          ]
        }
      ],
      "source": [
        "dataset_test = base_path+'/PAN23/PAN23_370_overlap_uncased_test_openset-pos_concat'\n",
        "\n",
        "dataset_test = getPickleFileInDict(dataset_test)\n",
        "dataset_test = MyDatasetTest(dataset_test,0.5)\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=1,shuffle=False)\n",
        "print(len(test_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Qg9MDve_8ES",
        "outputId": "3fe2cd9f-fbb3-4c74-b098-56e2b0650580"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(1)\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "cnt2 = 0\n",
        "for input1, mask1, input2, mask2,label,_ in test_dataloader:\n",
        "    print(label[0][0])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5NF1xFfCRYL"
      },
      "source": [
        "## BERT + Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-EhRDbDyCrkU"
      },
      "outputs": [],
      "source": [
        "class myModelEmbeddings(nn.Module):\n",
        "  def __init__(self,bert_emb_layer,startLayer,endLayer,bertModel,groupLayersMode = (False,False)):#(True,True)-> Grouping and Summing | #(True,False)-> Grouping and Concat\n",
        "      super(myModelEmbeddings, self).__init__()\n",
        "      self.bert_emb_layer = bert_emb_layer\n",
        "      self.startLayer = startLayer\n",
        "      self.endLayer = endLayer\n",
        "      self.groupLayersMode = groupLayersMode\n",
        "      self.bertModel = bertModel\n",
        "      \n",
        "     \n",
        "      inputFeatures = 0\n",
        "      if self.groupLayersMode == (True,False):\n",
        "        inputFeatures = (endLayer - startLayer)*768 \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        inputFeatures = 768\n",
        "      else:\n",
        "        inputFeatures = 768\n",
        "      self.bilstm = nn.LSTM(input_size=768, hidden_size=768,batch_first=True,bidirectional=True)#num_layers=3,dropout=0.2,\n",
        "\n",
        "  def getSpecificLayerOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][1:] \n",
        "      layerOutput = hidden_states[self.bert_emb_layer] # get specific Layer (from 0 to 11) for all tuples (batch_size, sequence_length, hidden_size)\n",
        "      \n",
        "      return  layerOutput\n",
        "  \n",
        "  def concatSpecificLayersOfBERT(self,bertOutputs):\n",
        "      hidden_states = bertOutputs[2][0:] \n",
        "      concatEmbeddingLayers = torch.cat([hidden_states[i] for i in range(self.startLayer,self.endLayer)], dim=-1)\n",
        "      \n",
        "      return concatEmbeddingLayers\n",
        "  def getCLSEmbeddings(self,bertOutputs ):\n",
        "      embeddings = bertOutputs[0] #last hidden states\n",
        "      #embeddings = bertOutputs[1] # pooler\n",
        "      return embeddings\n",
        "  def getCLSEmbeddingsFromLayers(self,bertOutputs ):\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      \n",
        "      \n",
        "      # Extract the hidden state for the [CLS] token from last four encode layers\n",
        "      last_layer_hidden_states = hidden_states[2:13]\n",
        "      cls = []\n",
        "      for layer in last_layer_hidden_states:\n",
        "          cls.append(layer[:,0,:])\n",
        "      cls_embeddings = torch.stack(cls, dim=1)\n",
        "      del cls\n",
        "\n",
        "      return cls_embeddings\n",
        "  def sumSpecificLayersOfBERT(self,bertOutputs):\n",
        "      #Number of layers: 13   (initial embeddings + 12 BERT layers) - So we need [2][1:] 1 and onwards\n",
        "      hidden_states = bertOutputs[2][0:]\n",
        "      # `hidden_states` is a Python list.\n",
        "     \n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[self.startLayer:self.endLayer]).sum(0)\n",
        "      sumEmbeddingLayers = torch.stack(hidden_states[-4:]).sum(0)\n",
        "      # sumEmbeddingLayers = torch.stack(hidden_states[-4:]).mean(dim=0)\n",
        "      del hidden_states\n",
        "\n",
        "      return sumEmbeddingLayers\n",
        "  def pooling(self,token_embeddings, mask, strategy='avg'):\n",
        "      if strategy == 'max':\n",
        "        #  avg_setence_embeddings = torch.mean(token_embeddings,dim=1)\n",
        "        #  print(avg_setence_embeddings.shape)\n",
        "         input_mask_expanded = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
        "         max_setence_embeddings = torch.max(token_embeddings, 1)[0]\n",
        "         return max_setence_embeddings\n",
        "      elif strategy == 'avg':\n",
        "         in_mask = mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "         # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "         avg_setence_embeddings = torch.sum(token_embeddings * in_mask, 1) / torch.clamp(in_mask.sum(1), min=1e-9)\n",
        "         return avg_setence_embeddings\n",
        "      elif strategy == 'sum':\n",
        "        sum_setence_embeddings = torch.sum(token_embeddings[0:len(token_embeddings)],1)\n",
        "        return sum_setence_embeddings\n",
        "\n",
        "  def forwardOnce(self, sent_id, mask):\n",
        "      outputs =  self.bertModel(input_ids=sent_id, attention_mask=mask)#,decoder_input_ids=sent_id\n",
        "    \n",
        "      if self.groupLayersMode == (True,False):\n",
        "        embeddings = self.concatSpecificLayersOfBERT(outputs)\n",
        "        return  embeddings \n",
        "      elif self.groupLayersMode == (True,True):\n",
        "        embeddings = self.sumSpecificLayersOfBERT(outputs)\n",
        "        # embeddings = self.getCLSEmbeddingsFromLayers(outputs)\n",
        "        return embeddings \n",
        "      else:\n",
        "        # embeddings = self.getSpecificLayerOfBERT(outputs)\n",
        "        # embeddings = self.getCLSEmbeddings(outputs )\n",
        "        embeddings = self.getCLSEmbeddingsFromLayers(outputs)\n",
        "        return embeddings \n",
        "      \n",
        "  def init_hidden(self, batch_size):\n",
        "        #Initialization of the LSTM hidden and cell states\n",
        "        h0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        c0 = torch.zeros((2*1, batch_size, 768)).detach().to(device)\n",
        "        hidden = (h0, c0)\n",
        "        return hidden\n",
        "  def forward(self, sent_id1, mask1,hidden):\n",
        "\n",
        "      # forward pass of input 1\n",
        "      output1 = self.forwardOnce(sent_id1, mask1)\n",
        "      out1, (hidden1,cell1) = self.bilstm(output1,hidden)\n",
        "      out_split1 = out1.view(sent_id1.shape[0], 11, 2, 768)\n",
        "      out_forward1 = out_split1[:, :, 0, :]\n",
        "      out_backward1 = out_split1[:, :, 1, :]\n",
        "      batch_indices = torch.arange(0, sent_id1.shape[0], device=device)\n",
        "      seq_indices = 11 - 1\n",
        "      direction_full1 = torch.cat([out_split1[batch_indices, seq_indices, 0], out_split1[batch_indices, 0, 1]], dim=-1)\n",
        "      return direction_full1#F.normalize(direction_full1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zA_guYVDx31"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k65z9sCsC_qM"
      },
      "outputs": [],
      "source": [
        "def validation(model,epoch,criterion1,validation_dataloader,modelFC=None,criterion2 = None):\n",
        "    \n",
        "      # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    # model.bertModel.eval()\n",
        " \n",
        "    model.eval()\n",
        "    if modelFC is not None:\n",
        "        modelFC.eval()\n",
        "    # Tracking variables \n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    step = 0\n",
        "    concatAll = []\n",
        "    totalF1 = 0\n",
        "    totalAcc = 0\n",
        "    avg_val_accuracy = 0\n",
        "\n",
        "    for input1, mask1, input2, mask2 in validation_dataloader:\n",
        "        step += 1\n",
        "        # if step == 71:\n",
        "        #    break\n",
        "        b_input_ids1 = input1.to(device)\n",
        "       \n",
        "        b_input_mask1 = mask1.to(device)\n",
        "        # target1 = target1.type(torch.FloatTensor)\n",
        "        # b_labels = target1.to(device)\n",
        "        h = model.init_hidden(b_input_ids1.shape[0])\n",
        "        b_input_ids2 = input2.to(device)\n",
        "        b_input_mask2 = mask2.to(device)\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            if modelFC==None:\n",
        "\n",
        "              FC11 = model(b_input_ids1, b_input_mask1,h)             \n",
        "              FC22 = model(b_input_ids2, b_input_mask2,h)             \n",
        "              cos = nn.CosineSimilarity(dim=1)\n",
        "              output2 = cos(FC11, FC22)\n",
        "              # embeddings = torch.cat((FC11, FC22))\n",
        "              # indices = torch.arange(0, FC11.size(0), device=device)\n",
        "              # labels = torch.cat((indices, indices))\n",
        "              # loss1 = criterion1(embeddings,labels)\n",
        "              # acc = 0\n",
        "              loss1,acc = criterion1(FC11,FC22)\n",
        "              # loss1= criterion1(FC11, labels, ref_emb=FC22, ref_labels=labels)\n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "              \n",
        "              #output11, output22,FC11,FC22,avg1,avg2,concatenated,concatenatedfc,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2)\n",
        "              FC11,FC22,_ = model(b_input_ids1, b_input_mask1,b_input_ids2, b_input_mask2,h)\n",
        "              out = modelFC(FC11,FC22)\n",
        "              loss2 = criterion2(out,b_labels)\n",
        "            if modelFC == None:\n",
        "                total_eval_loss += loss1.item()\n",
        "                totalAcc+=0#acc.cpu().item()\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 1 AVG. val Loss \"+str(loss1.item()),\"acc =\",str(acc))\n",
        "                # label_ids = b_labels.to('cpu').numpy()\n",
        "                \n",
        "            elif modelFC is not None and criterion2 is not None:\n",
        "            \n",
        "                total_eval_loss += loss2.item()\n",
        "                f1 = calcF1score(out,b_labels)\n",
        "                totalF1 += f1\n",
        "                probs = F.softmax(out, dim=1).cpu().numpy()\n",
        "                accuracy = calcAccuracy(probs,b_labels)\n",
        "                totalAcc+=accuracy\n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2 Probs\")\n",
        "                print(probs) \n",
        "                print(\"========== Epoch \"+str(epoch)+ \" Batch \"+str(step)+\"==== Step 2  val Loss \"+str(loss2.item()), \"==== \",str(f1),\"===== Acc = \",str(accuracy))\n",
        "                modelFC.train()\n",
        "    if modelFC==None:\n",
        "      avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "      avg_f1_val = totalF1/len(validation_dataloader)\n",
        "      avg_val_accuracy = totalAcc/len(validation_dataloader)\n",
        "    else:\n",
        "       avg_val_loss = total_eval_loss/len(validation_dataloader)\n",
        "       avg_f1_val = totalF1/len(validation_dataloader)\n",
        "       avg_val_accuracy = totalAcc/len(validation_dataloader)\n",
        "    \n",
        "    \n",
        "    if modelFC==None:\n",
        "       model.bertModel.train()\n",
        "       model.train()\n",
        "    else:\n",
        "       model.bertModel.eval()\n",
        "       model.eval()\n",
        "       modelFC.train()  \n",
        "    return  avg_val_loss,avg_f1_val ,avg_val_accuracy, #avg_val_f1,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlAefWYBDvE0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R4sHMmS8DVdr"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "def mytrainStep1(model,criterion1,criterion2):\n",
        "\n",
        "      if torch.cuda.is_available():\n",
        "          model.to(device)\n",
        "      for param in model.bertModel.parameters():\n",
        "              param.requires_grad = False\n",
        "      # modules = [model.bertModel.embeddings, *model.bertModel.encoder.layer[:8]] #Replace 8 by what you want\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      # modules = [model.bertModel.decoder,*model.bertModel.encoder.block[:10]]\n",
        "      # for module in modules:\n",
        "      #     for param in module.parameters():\n",
        "      #         param.requires_grad = False\n",
        "      # optimizer = torch.optim.Adam(model.parameters(),\n",
        "      #                               lr=0.0001)\n",
        "      optimizer = AdamW(model.parameters(),\n",
        "                                    lr=0.002,#0.003, #5e-5, 3e-5, 2e-5 #0.001 0.00003\n",
        "                                    #weight_decay=1e-5, \n",
        "                                    correct_bias=False) #eps=1e-8,len(train_dataloader)\n",
        "      #scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=6*len(train_dataloader))\n",
        "      # Set the seed value all over the place to make this reproducible.\n",
        "      scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "      seed_val = 42\n",
        "\n",
        "      random.seed(seed_val)\n",
        "      np.random.seed(seed_val)\n",
        "      torch.manual_seed(seed_val)\n",
        "      torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "      # We'll store a number of quantities such as training and validation loss, \n",
        "      # validation accuracy, and timings.\n",
        "      training_stats = []\n",
        "\n",
        "      # Measure the total training time for the whole run.\n",
        "      total_t0 = time.time()\n",
        "\n",
        "      # For each epoch...\n",
        "      listOflossesTrain = list()\n",
        "      listOfF1Train = list()\n",
        "      listOflossesValid = list()\n",
        "      listOfF1Valid = list()\n",
        "      epoch_stop = 0\n",
        "      model.bertModel.train()\n",
        "      model.train()\n",
        "      totalAcc = 0\n",
        "      for epoch_i in range(0, 6):\n",
        "\n",
        "          print(\"\")\n",
        "          print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, n_epochs))\n",
        "          print('Training...')\n",
        "\n",
        "          # Measure how long the training epoch takes.\n",
        "          t0 = time.time()\n",
        "\n",
        "          # Reset the total loss for this epoch.\n",
        "          total_train_loss = 0\n",
        "          # model.bertModel.train()\n",
        "          \n",
        "\n",
        "          # For each batch of training data...\n",
        "          step = 0\n",
        "\n",
        "          accum_iter = 4\n",
        "\n",
        "          for batch_idx,(input1, mask1, input2, mask2) in  enumerate(train_dataloader):\n",
        "              step+=1\n",
        "              # if step==151:\n",
        "              #    break\n",
        "              # # Progress update every 40 batches.\n",
        "              # h = model.init_hidden(target1.size(0))\n",
        "              if step % 100 == 0 and not step == 0:\n",
        "                  # Calculate elapsed time in minutes.\n",
        "                  elapsed = format_time(time.time() - t0)\n",
        "                  # Report progress.\n",
        "                  print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "              # if torch.cuda.is_available():\n",
        "              b_input_ids1 = input1.to(device)\n",
        "              b_input_mask1 = mask1.to(device)\n",
        "\n",
        "\n",
        "              h = model.init_hidden(b_input_ids1.shape[0])\n",
        "\n",
        "              b_input_ids2 = input2.to(device)\n",
        "              b_input_mask2 = mask2.to(device)\n",
        "              model.zero_grad()\n",
        "              FC11 = model(b_input_ids1, b_input_mask1,h)             \n",
        "              FC22 = model(b_input_ids2, b_input_mask2,h)\n",
        "\n",
        "              # embeddings = torch.cat((FC11, FC22))\n",
        "              # indices = torch.arange(0, FC11.size(0), device=device)\n",
        "              # labels = torch.cat((indices, indices))\n",
        "              loss1,acc = criterion1(FC11,FC22)\n",
        "              # loss1 = criterion1(embeddings,labels)\n",
        "              acc = 0\n",
        "              total_train_loss += loss1.item()\n",
        "              print(\"========== Epoch \"+str(epoch_i)+ \" Batch \"+str(step)+\"==== Step 1  Train Loss \"+str(loss1.item()),\"acc =\",str(acc))\n",
        "              loss1.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "                      \n",
        "          avg_train_loss = total_train_loss /len(train_dataloader)\n",
        "          print(\"========== Epoch \"+str(epoch_i)+ \" ==== Step 1 AVG. Train Loss \"+str(avg_train_loss))            \n",
        "          listOflossesTrain.append(avg_train_loss)\n",
        "          training_time = format_time(time.time() - t0)\n",
        "\n",
        "          print(\"\")\n",
        "          # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "          print(\"  Training epoch took: {:}\".format(training_time))\n",
        "          \n",
        "          # print(\"  Accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\n",
        "          # Calculate the average loss over all of the batches.avg_val_accuracy, avg_val_f1,\n",
        "          avg_val_loss,avgf1,avgAcc = validation(model,epoch_i,criterion1,validation_dataloader)\n",
        "          scheduler.step(avg_val_loss)\n",
        "          listOflossesValid.append(avg_val_loss)\n",
        "          # listOfF1Valid.append(avg_val_f1)\n",
        "          # Measure how long the validation run took.\n",
        "          validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "          print(\"  Average Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "          print(\"  Average Validation Accuracy: {0:.2f}\".format(avgAcc))\n",
        "          print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "       \n",
        "          early_stopping1(avg_val_loss, model)\n",
        "          epoch_stop = epoch_i+1\n",
        "          if early_stopping1.early_stop:\n",
        "              print(\"Early stopping\")\n",
        "              # break  \n",
        "\n",
        "      print(\"\")\n",
        "      print(\"Training complete!\")\n",
        "\n",
        "      print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "      createPlot(listOflossesTrain,listOflossesValid,6)\n",
        "      return listOflossesValid, listOflossesTrain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmGm4QvXEgQ6"
      },
      "source": [
        "### Start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l2vYIgLGD81P"
      },
      "outputs": [],
      "source": [
        "class ContrastiveLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, margin_pos=1.0, margin_neg=0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin_pos = margin_pos\n",
        "        self.margin_neg = margin_neg\n",
        "    def sim_matrix(self,a, b, eps=1e-8):\n",
        "        \"\"\"\n",
        "        added eps for numerical stability\n",
        "        \"\"\"\n",
        "        a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "        a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "        b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "        sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "        return sim_mt.to(device)  \n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        batch_size = x1.shape[0]\n",
        "        similarity_matrix = self.sim_matrix(x1, x2)\n",
        "        # print(similarity_matrix)\n",
        "        diagonal = torch.diagonal(similarity_matrix)#.to(device) #diagonal are the positives\n",
        "        gt_pos = F.pad(torch.LongTensor(), (0,len(diagonal)), \"constant\", 1).to(device)#.unsqueeze(1)#.to(device)\n",
        "        \n",
        "        # gt_pos = torch.cat((1-gt_pos,gt_pos),dim=1).to(device)\n",
        "        #negatives = similarity_matrix.flatten()[1:].view(batch_size-1, batch_size+1)[:,:-1].reshape(batch_size, batch_size-1).flatten()#.to(device)\n",
        "        negatives_above = similarity_matrix.triu(diagonal=1)\n",
        "        negatives = negatives_above[negatives_above.nonzero(as_tuple=True)]\n",
        "        # print(negatives)\n",
        "        lab_neg = F.pad(torch.LongTensor(), (0,len(negatives)), \"constant\", 0).to(device)\n",
        "        loss_contrastive_pos =torch.mean(torch.pow(self.margin_pos - diagonal, 2))\n",
        "        loss_contrastive_neg =torch.mean(torch.pow(torch.relu(negatives - self.margin_neg), 2))\n",
        "        print(\"pos\",str(loss_contrastive_pos))\n",
        "        print(\"neg\",str(loss_contrastive_neg))\n",
        "        # if torch.isnan(loss_contrastive_neg) == False:  \n",
        "        loss_contrastive = (loss_contrastive_pos + loss_contrastive_neg)/2\n",
        "        # else:\n",
        "          # loss_contrastive = (loss_contrastive_pos + 0.0)/2\n",
        "        # print(loss_contrastive_pos)\n",
        "        # print(loss_contrastive_neg)\n",
        "        return loss_contrastive,0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LuSv4fbTEabx"
      },
      "outputs": [],
      "source": [
        "from pytorch_metric_learning.losses import NTXentLoss\n",
        "modelEmb = myModelEmbeddings(bert_emb_layer=10,startLayer=6,endLayer=10,bertModel=bertModel)\n",
        "criterion1 = ContrastiveLoss()\n",
        "# criterion1 = NTXentLoss(temperature=0.07) \n",
        "modelEmb.load_state_dict(torch.load('/content/drive/My Drive/Thesis/PAN23/checkpointEmbUncased_PAN23_gen.pt'))#/content/drive/My Drive/Thesis/PAN20/checkpointEmbv2.pt'))\n",
        "# criterion1 = InfoNCE()\n",
        "modelEmb.cuda()\n",
        "modelEmb.eval()\n",
        "# criterion1 = losses.ContrastiveLoss(distance = distances.CosineSimilarity(),reducer=MeanReducer(),pos_margin=0.92, neg_margin=-0.1) #distance = distances.CosineSimilarity()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "early_stopping1 = EarlyStopping(patience=4, path='checkpointEmbUncased_PAN23_gen.pt',verbose=True)\n",
        "early_stopping2 = EarlyStopping(patience=2, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4V8FIQyi3vv7",
        "outputId": "dc3b22f3-f125-4b0f-ffd0-61afb7d96478"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mΗ έξοδος ροής περικόπηκε στις τελευταίες 5000 γραμμές.\u001b[0m\n",
            "========== Epoch 0 Batch 4238==== Step 1  Train Loss 0.2769983410835266 acc = 0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4239==== Step 1  Train Loss 0.27094465494155884 acc = 0\n",
            "pos tensor(0.2125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4240==== Step 1  Train Loss 0.2782735824584961 acc = 0\n",
            "pos tensor(0.2656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4241==== Step 1  Train Loss 0.285760760307312 acc = 0\n",
            "pos tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4242==== Step 1  Train Loss 0.28732800483703613 acc = 0\n",
            "pos tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4243==== Step 1  Train Loss 0.29757875204086304 acc = 0\n",
            "pos tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4244==== Step 1  Train Loss 0.269901305437088 acc = 0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4245==== Step 1  Train Loss 0.29230695962905884 acc = 0\n",
            "pos tensor(0.1791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4246==== Step 1  Train Loss 0.27253299951553345 acc = 0\n",
            "pos tensor(0.2334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4247==== Step 1  Train Loss 0.2987444996833801 acc = 0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4248==== Step 1  Train Loss 0.2896611988544464 acc = 0\n",
            "pos tensor(0.1999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4249==== Step 1  Train Loss 0.2762184143066406 acc = 0\n",
            "pos tensor(0.2739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4250==== Step 1  Train Loss 0.29986822605133057 acc = 0\n",
            "pos tensor(0.1753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4251==== Step 1  Train Loss 0.25874197483062744 acc = 0\n",
            "pos tensor(0.1749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4252==== Step 1  Train Loss 0.2623245418071747 acc = 0\n",
            "pos tensor(0.1832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4253==== Step 1  Train Loss 0.27691707015037537 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4254==== Step 1  Train Loss 0.27220433950424194 acc = 0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4255==== Step 1  Train Loss 0.300333708524704 acc = 0\n",
            "pos tensor(0.2303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4256==== Step 1  Train Loss 0.28073573112487793 acc = 0\n",
            "pos tensor(0.1287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4257==== Step 1  Train Loss 0.2798171937465668 acc = 0\n",
            "pos tensor(0.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4258==== Step 1  Train Loss 0.2831682562828064 acc = 0\n",
            "pos tensor(0.2391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4259==== Step 1  Train Loss 0.289273202419281 acc = 0\n",
            "pos tensor(0.1851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4260==== Step 1  Train Loss 0.2542518973350525 acc = 0\n",
            "pos tensor(0.1787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4261==== Step 1  Train Loss 0.27535849809646606 acc = 0\n",
            "pos tensor(0.2008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4262==== Step 1  Train Loss 0.28086191415786743 acc = 0\n",
            "pos tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4263==== Step 1  Train Loss 0.3074474334716797 acc = 0\n",
            "pos tensor(0.1606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4264==== Step 1  Train Loss 0.25197094678878784 acc = 0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4265==== Step 1  Train Loss 0.28651535511016846 acc = 0\n",
            "pos tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4266==== Step 1  Train Loss 0.28626468777656555 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4267==== Step 1  Train Loss 0.2759728729724884 acc = 0\n",
            "pos tensor(0.1892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4268==== Step 1  Train Loss 0.27297016978263855 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4269==== Step 1  Train Loss 0.28897836804389954 acc = 0\n",
            "pos tensor(0.2645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4270==== Step 1  Train Loss 0.3082905411720276 acc = 0\n",
            "pos tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4271==== Step 1  Train Loss 0.29775822162628174 acc = 0\n",
            "pos tensor(0.2426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4272==== Step 1  Train Loss 0.30497127771377563 acc = 0\n",
            "pos tensor(0.1803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4273==== Step 1  Train Loss 0.26206713914871216 acc = 0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3413, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4274==== Step 1  Train Loss 0.2574083209037781 acc = 0\n",
            "pos tensor(0.2017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4275==== Step 1  Train Loss 0.2733769416809082 acc = 0\n",
            "pos tensor(0.2221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4276==== Step 1  Train Loss 0.2829563021659851 acc = 0\n",
            "pos tensor(0.1976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4277==== Step 1  Train Loss 0.2869637608528137 acc = 0\n",
            "pos tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4278==== Step 1  Train Loss 0.26807641983032227 acc = 0\n",
            "pos tensor(0.1976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4279==== Step 1  Train Loss 0.2790049612522125 acc = 0\n",
            "pos tensor(0.1482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4280==== Step 1  Train Loss 0.2600495219230652 acc = 0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4281==== Step 1  Train Loss 0.3028634786605835 acc = 0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4282==== Step 1  Train Loss 0.28119781613349915 acc = 0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4283==== Step 1  Train Loss 0.25974026322364807 acc = 0\n",
            "pos tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4284==== Step 1  Train Loss 0.2796819508075714 acc = 0\n",
            "pos tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4285==== Step 1  Train Loss 0.2608043849468231 acc = 0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4286==== Step 1  Train Loss 0.27219894528388977 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4287==== Step 1  Train Loss 0.3006855249404907 acc = 0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4288==== Step 1  Train Loss 0.2620232105255127 acc = 0\n",
            "pos tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4289==== Step 1  Train Loss 0.291934609413147 acc = 0\n",
            "pos tensor(0.2336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4290==== Step 1  Train Loss 0.2969851493835449 acc = 0\n",
            "pos tensor(0.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4291==== Step 1  Train Loss 0.2629033923149109 acc = 0\n",
            "pos tensor(0.1449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4292==== Step 1  Train Loss 0.24646367132663727 acc = 0\n",
            "pos tensor(0.1921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4293==== Step 1  Train Loss 0.25845617055892944 acc = 0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4294==== Step 1  Train Loss 0.2710329294204712 acc = 0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4295==== Step 1  Train Loss 0.2643837034702301 acc = 0\n",
            "pos tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4296==== Step 1  Train Loss 0.26365920901298523 acc = 0\n",
            "pos tensor(0.2264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4297==== Step 1  Train Loss 0.2936632037162781 acc = 0\n",
            "pos tensor(0.1614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4298==== Step 1  Train Loss 0.2415136843919754 acc = 0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4299==== Step 1  Train Loss 0.2903398275375366 acc = 0\n",
            "  Batch 4,300  of  5,561.    Elapsed: 0:15:17.\n",
            "pos tensor(0.1742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4300==== Step 1  Train Loss 0.2553164064884186 acc = 0\n",
            "pos tensor(0.2156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4301==== Step 1  Train Loss 0.2766876518726349 acc = 0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4302==== Step 1  Train Loss 0.2602936029434204 acc = 0\n",
            "pos tensor(0.2499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4303==== Step 1  Train Loss 0.299356609582901 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4304==== Step 1  Train Loss 0.2787584364414215 acc = 0\n",
            "pos tensor(0.1842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4305==== Step 1  Train Loss 0.24918776750564575 acc = 0\n",
            "pos tensor(0.1679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4306==== Step 1  Train Loss 0.2666400969028473 acc = 0\n",
            "pos tensor(0.1714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4307==== Step 1  Train Loss 0.2630443572998047 acc = 0\n",
            "pos tensor(0.1582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4308==== Step 1  Train Loss 0.2714141607284546 acc = 0\n",
            "pos tensor(0.2190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4309==== Step 1  Train Loss 0.2796170711517334 acc = 0\n",
            "pos tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4310==== Step 1  Train Loss 0.27400118112564087 acc = 0\n",
            "pos tensor(0.2215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4311==== Step 1  Train Loss 0.2983228266239166 acc = 0\n",
            "pos tensor(0.1983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4312==== Step 1  Train Loss 0.2619132101535797 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4313==== Step 1  Train Loss 0.2514631152153015 acc = 0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4314==== Step 1  Train Loss 0.27156364917755127 acc = 0\n",
            "pos tensor(0.2188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4315==== Step 1  Train Loss 0.28608569502830505 acc = 0\n",
            "pos tensor(0.2013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4316==== Step 1  Train Loss 0.26651710271835327 acc = 0\n",
            "pos tensor(0.1536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4317==== Step 1  Train Loss 0.24658872187137604 acc = 0\n",
            "pos tensor(0.2285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4318==== Step 1  Train Loss 0.2767144441604614 acc = 0\n",
            "pos tensor(0.2203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4319==== Step 1  Train Loss 0.29633045196533203 acc = 0\n",
            "pos tensor(0.1890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4320==== Step 1  Train Loss 0.2615661025047302 acc = 0\n",
            "pos tensor(0.2139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4321==== Step 1  Train Loss 0.27199587225914 acc = 0\n",
            "pos tensor(0.2513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4322==== Step 1  Train Loss 0.2905804514884949 acc = 0\n",
            "pos tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4323==== Step 1  Train Loss 0.28237491846084595 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4324==== Step 1  Train Loss 0.2718837261199951 acc = 0\n",
            "pos tensor(0.2096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4325==== Step 1  Train Loss 0.26629406213760376 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4326==== Step 1  Train Loss 0.27029821276664734 acc = 0\n",
            "pos tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4327==== Step 1  Train Loss 0.2872488796710968 acc = 0\n",
            "pos tensor(0.1541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4328==== Step 1  Train Loss 0.25768589973449707 acc = 0\n",
            "pos tensor(0.2050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4329==== Step 1  Train Loss 0.28015825152397156 acc = 0\n",
            "pos tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4330==== Step 1  Train Loss 0.2542594075202942 acc = 0\n",
            "pos tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4331==== Step 1  Train Loss 0.2695338726043701 acc = 0\n",
            "pos tensor(0.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4332==== Step 1  Train Loss 0.2889304757118225 acc = 0\n",
            "pos tensor(0.2410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4333==== Step 1  Train Loss 0.29130449891090393 acc = 0\n",
            "pos tensor(0.2231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4334==== Step 1  Train Loss 0.2616806924343109 acc = 0\n",
            "pos tensor(0.2097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4335==== Step 1  Train Loss 0.27938154339790344 acc = 0\n",
            "pos tensor(0.2320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3535, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4336==== Step 1  Train Loss 0.2927214503288269 acc = 0\n",
            "pos tensor(0.1354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4337==== Step 1  Train Loss 0.24189725518226624 acc = 0\n",
            "pos tensor(0.1714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4338==== Step 1  Train Loss 0.2565537691116333 acc = 0\n",
            "pos tensor(0.1903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4339==== Step 1  Train Loss 0.26511433720588684 acc = 0\n",
            "pos tensor(0.2279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4340==== Step 1  Train Loss 0.288219153881073 acc = 0\n",
            "pos tensor(0.2250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4341==== Step 1  Train Loss 0.28616827726364136 acc = 0\n",
            "pos tensor(0.2206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4342==== Step 1  Train Loss 0.28897297382354736 acc = 0\n",
            "pos tensor(0.2158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4343==== Step 1  Train Loss 0.2838340997695923 acc = 0\n",
            "pos tensor(0.1647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4344==== Step 1  Train Loss 0.2633291184902191 acc = 0\n",
            "pos tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4345==== Step 1  Train Loss 0.2696419060230255 acc = 0\n",
            "pos tensor(0.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4346==== Step 1  Train Loss 0.25937849283218384 acc = 0\n",
            "pos tensor(0.2476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4347==== Step 1  Train Loss 0.2734754979610443 acc = 0\n",
            "pos tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4348==== Step 1  Train Loss 0.2834087908267975 acc = 0\n",
            "pos tensor(0.2401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4349==== Step 1  Train Loss 0.29995015263557434 acc = 0\n",
            "pos tensor(0.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4350==== Step 1  Train Loss 0.2588459849357605 acc = 0\n",
            "pos tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4351==== Step 1  Train Loss 0.28153616189956665 acc = 0\n",
            "pos tensor(0.2530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4352==== Step 1  Train Loss 0.29329001903533936 acc = 0\n",
            "pos tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4353==== Step 1  Train Loss 0.26837506890296936 acc = 0\n",
            "pos tensor(0.1956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4354==== Step 1  Train Loss 0.2774695158004761 acc = 0\n",
            "pos tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4355==== Step 1  Train Loss 0.290862500667572 acc = 0\n",
            "pos tensor(0.1531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4356==== Step 1  Train Loss 0.2510358393192291 acc = 0\n",
            "pos tensor(0.1729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4357==== Step 1  Train Loss 0.27950870990753174 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4358==== Step 1  Train Loss 0.2717631459236145 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4359==== Step 1  Train Loss 0.2836564779281616 acc = 0\n",
            "pos tensor(0.2167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4360==== Step 1  Train Loss 0.2919943332672119 acc = 0\n",
            "pos tensor(0.1374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4361==== Step 1  Train Loss 0.25933876633644104 acc = 0\n",
            "pos tensor(0.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4362==== Step 1  Train Loss 0.26731932163238525 acc = 0\n",
            "pos tensor(0.2256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4363==== Step 1  Train Loss 0.2906590700149536 acc = 0\n",
            "pos tensor(0.2033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4364==== Step 1  Train Loss 0.28652703762054443 acc = 0\n",
            "pos tensor(0.2291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4365==== Step 1  Train Loss 0.2823184132575989 acc = 0\n",
            "pos tensor(0.1650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4366==== Step 1  Train Loss 0.2541211247444153 acc = 0\n",
            "pos tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4367==== Step 1  Train Loss 0.25585901737213135 acc = 0\n",
            "pos tensor(0.1896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4368==== Step 1  Train Loss 0.2709731459617615 acc = 0\n",
            "pos tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4369==== Step 1  Train Loss 0.28282561898231506 acc = 0\n",
            "pos tensor(0.2045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4370==== Step 1  Train Loss 0.29492372274398804 acc = 0\n",
            "pos tensor(0.1813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4371==== Step 1  Train Loss 0.27140551805496216 acc = 0\n",
            "pos tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4372==== Step 1  Train Loss 0.3000507354736328 acc = 0\n",
            "pos tensor(0.1957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4373==== Step 1  Train Loss 0.27868640422821045 acc = 0\n",
            "pos tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4374==== Step 1  Train Loss 0.2724704444408417 acc = 0\n",
            "pos tensor(0.1672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4375==== Step 1  Train Loss 0.2587054371833801 acc = 0\n",
            "pos tensor(0.2216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4376==== Step 1  Train Loss 0.2918623387813568 acc = 0\n",
            "pos tensor(0.1766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4377==== Step 1  Train Loss 0.2616341710090637 acc = 0\n",
            "pos tensor(0.2488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4378==== Step 1  Train Loss 0.305447518825531 acc = 0\n",
            "pos tensor(0.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4379==== Step 1  Train Loss 0.2677677273750305 acc = 0\n",
            "pos tensor(0.1662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4380==== Step 1  Train Loss 0.27091896533966064 acc = 0\n",
            "pos tensor(0.2101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4381==== Step 1  Train Loss 0.30807605385780334 acc = 0\n",
            "pos tensor(0.1622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4382==== Step 1  Train Loss 0.264423668384552 acc = 0\n",
            "pos tensor(0.2132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4383==== Step 1  Train Loss 0.2759602963924408 acc = 0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4384==== Step 1  Train Loss 0.30232205986976624 acc = 0\n",
            "pos tensor(0.1761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4385==== Step 1  Train Loss 0.275691419839859 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4386==== Step 1  Train Loss 0.29401931166648865 acc = 0\n",
            "pos tensor(0.1839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4387==== Step 1  Train Loss 0.2706809639930725 acc = 0\n",
            "pos tensor(0.1316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4388==== Step 1  Train Loss 0.24500635266304016 acc = 0\n",
            "pos tensor(0.1739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4389==== Step 1  Train Loss 0.2731782793998718 acc = 0\n",
            "pos tensor(0.2200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4390==== Step 1  Train Loss 0.28080034255981445 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4391==== Step 1  Train Loss 0.2731636166572571 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4392==== Step 1  Train Loss 0.2761288583278656 acc = 0\n",
            "pos tensor(0.1619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4393==== Step 1  Train Loss 0.2901226580142975 acc = 0\n",
            "pos tensor(0.1516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4394==== Step 1  Train Loss 0.2538449764251709 acc = 0\n",
            "pos tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4395==== Step 1  Train Loss 0.28273066878318787 acc = 0\n",
            "pos tensor(0.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4396==== Step 1  Train Loss 0.26648715138435364 acc = 0\n",
            "pos tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4397==== Step 1  Train Loss 0.2775475084781647 acc = 0\n",
            "pos tensor(0.1568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4398==== Step 1  Train Loss 0.25257208943367004 acc = 0\n",
            "pos tensor(0.2351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4399==== Step 1  Train Loss 0.30216705799102783 acc = 0\n",
            "  Batch 4,400  of  5,561.    Elapsed: 0:15:38.\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4400==== Step 1  Train Loss 0.29125547409057617 acc = 0\n",
            "pos tensor(0.2037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4401==== Step 1  Train Loss 0.28182241320610046 acc = 0\n",
            "pos tensor(0.2061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4402==== Step 1  Train Loss 0.30049630999565125 acc = 0\n",
            "pos tensor(0.2411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4403==== Step 1  Train Loss 0.30020081996917725 acc = 0\n",
            "pos tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4404==== Step 1  Train Loss 0.27785009145736694 acc = 0\n",
            "pos tensor(0.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4405==== Step 1  Train Loss 0.2646334767341614 acc = 0\n",
            "pos tensor(0.2439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4406==== Step 1  Train Loss 0.30335986614227295 acc = 0\n",
            "pos tensor(0.1916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4407==== Step 1  Train Loss 0.2905013859272003 acc = 0\n",
            "pos tensor(0.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4408==== Step 1  Train Loss 0.2941444516181946 acc = 0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4409==== Step 1  Train Loss 0.283011257648468 acc = 0\n",
            "pos tensor(0.1527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4410==== Step 1  Train Loss 0.2745284140110016 acc = 0\n",
            "pos tensor(0.1951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4411==== Step 1  Train Loss 0.2852329611778259 acc = 0\n",
            "pos tensor(0.1720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4412==== Step 1  Train Loss 0.281936913728714 acc = 0\n",
            "pos tensor(0.1793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4413==== Step 1  Train Loss 0.3029579222202301 acc = 0\n",
            "pos tensor(0.2363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4414==== Step 1  Train Loss 0.31358373165130615 acc = 0\n",
            "pos tensor(0.2118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4415==== Step 1  Train Loss 0.3006473481655121 acc = 0\n",
            "pos tensor(0.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4416==== Step 1  Train Loss 0.2780477702617645 acc = 0\n",
            "pos tensor(0.2031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4417==== Step 1  Train Loss 0.2948148548603058 acc = 0\n",
            "pos tensor(0.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4418==== Step 1  Train Loss 0.2903596758842468 acc = 0\n",
            "pos tensor(0.1731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4419==== Step 1  Train Loss 0.26305854320526123 acc = 0\n",
            "pos tensor(0.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4420==== Step 1  Train Loss 0.26128077507019043 acc = 0\n",
            "pos tensor(0.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4421==== Step 1  Train Loss 0.2870992422103882 acc = 0\n",
            "pos tensor(0.1865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4422==== Step 1  Train Loss 0.2924646735191345 acc = 0\n",
            "pos tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4423==== Step 1  Train Loss 0.26375335454940796 acc = 0\n",
            "pos tensor(0.1830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4424==== Step 1  Train Loss 0.25756892561912537 acc = 0\n",
            "pos tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4425==== Step 1  Train Loss 0.26797744631767273 acc = 0\n",
            "pos tensor(0.1718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4426==== Step 1  Train Loss 0.26555684208869934 acc = 0\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4427==== Step 1  Train Loss 0.2869154214859009 acc = 0\n",
            "pos tensor(0.2261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4428==== Step 1  Train Loss 0.29184091091156006 acc = 0\n",
            "pos tensor(0.2095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4429==== Step 1  Train Loss 0.29977571964263916 acc = 0\n",
            "pos tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4430==== Step 1  Train Loss 0.29457104206085205 acc = 0\n",
            "pos tensor(0.1867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4431==== Step 1  Train Loss 0.27154725790023804 acc = 0\n",
            "pos tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4432==== Step 1  Train Loss 0.28001558780670166 acc = 0\n",
            "pos tensor(0.2682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4433==== Step 1  Train Loss 0.3080119490623474 acc = 0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4434==== Step 1  Train Loss 0.2821585237979889 acc = 0\n",
            "pos tensor(0.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4435==== Step 1  Train Loss 0.2565675973892212 acc = 0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4436==== Step 1  Train Loss 0.26846230030059814 acc = 0\n",
            "pos tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4437==== Step 1  Train Loss 0.28243348002433777 acc = 0\n",
            "pos tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4438==== Step 1  Train Loss 0.25971221923828125 acc = 0\n",
            "pos tensor(0.1882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4439==== Step 1  Train Loss 0.26712650060653687 acc = 0\n",
            "pos tensor(0.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4440==== Step 1  Train Loss 0.2864207625389099 acc = 0\n",
            "pos tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4441==== Step 1  Train Loss 0.3025333881378174 acc = 0\n",
            "pos tensor(0.1495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4442==== Step 1  Train Loss 0.2967945635318756 acc = 0\n",
            "pos tensor(0.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4443==== Step 1  Train Loss 0.30082911252975464 acc = 0\n",
            "pos tensor(0.1622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4444==== Step 1  Train Loss 0.29064637422561646 acc = 0\n",
            "pos tensor(0.1887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4445==== Step 1  Train Loss 0.29428887367248535 acc = 0\n",
            "pos tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4446==== Step 1  Train Loss 0.29034721851348877 acc = 0\n",
            "pos tensor(0.1605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4447==== Step 1  Train Loss 0.30204954743385315 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4448==== Step 1  Train Loss 0.2746504843235016 acc = 0\n",
            "pos tensor(0.1718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4449==== Step 1  Train Loss 0.25932732224464417 acc = 0\n",
            "pos tensor(0.1527, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4450==== Step 1  Train Loss 0.2600712776184082 acc = 0\n",
            "pos tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4451==== Step 1  Train Loss 0.27494630217552185 acc = 0\n",
            "pos tensor(0.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4452==== Step 1  Train Loss 0.2897849380970001 acc = 0\n",
            "pos tensor(0.1831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4453==== Step 1  Train Loss 0.2789117693901062 acc = 0\n",
            "pos tensor(0.2208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4454==== Step 1  Train Loss 0.2911987900733948 acc = 0\n",
            "pos tensor(0.1646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4455==== Step 1  Train Loss 0.2535972595214844 acc = 0\n",
            "pos tensor(0.1429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4456==== Step 1  Train Loss 0.24050231277942657 acc = 0\n",
            "pos tensor(0.2251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4457==== Step 1  Train Loss 0.2861446440219879 acc = 0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4458==== Step 1  Train Loss 0.25942885875701904 acc = 0\n",
            "pos tensor(0.2007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4459==== Step 1  Train Loss 0.27705585956573486 acc = 0\n",
            "pos tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4460==== Step 1  Train Loss 0.28051382303237915 acc = 0\n",
            "pos tensor(0.2830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4461==== Step 1  Train Loss 0.3004816174507141 acc = 0\n",
            "pos tensor(0.1874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4462==== Step 1  Train Loss 0.28437942266464233 acc = 0\n",
            "pos tensor(0.2001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4463==== Step 1  Train Loss 0.2695656418800354 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4464==== Step 1  Train Loss 0.26886194944381714 acc = 0\n",
            "pos tensor(0.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4465==== Step 1  Train Loss 0.28878721594810486 acc = 0\n",
            "pos tensor(0.2888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4466==== Step 1  Train Loss 0.3082799017429352 acc = 0\n",
            "pos tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4467==== Step 1  Train Loss 0.276580274105072 acc = 0\n",
            "pos tensor(0.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4468==== Step 1  Train Loss 0.26321542263031006 acc = 0\n",
            "pos tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4469==== Step 1  Train Loss 0.26350101828575134 acc = 0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4470==== Step 1  Train Loss 0.2594841420650482 acc = 0\n",
            "pos tensor(0.2647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4471==== Step 1  Train Loss 0.30863842368125916 acc = 0\n",
            "pos tensor(0.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4472==== Step 1  Train Loss 0.2692946791648865 acc = 0\n",
            "pos tensor(0.1768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4473==== Step 1  Train Loss 0.2800009250640869 acc = 0\n",
            "pos tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4474==== Step 1  Train Loss 0.28322702646255493 acc = 0\n",
            "pos tensor(0.2564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4475==== Step 1  Train Loss 0.29156607389450073 acc = 0\n",
            "pos tensor(0.2872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4476==== Step 1  Train Loss 0.3099434971809387 acc = 0\n",
            "pos tensor(0.2396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4477==== Step 1  Train Loss 0.28470203280448914 acc = 0\n",
            "pos tensor(0.1586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4478==== Step 1  Train Loss 0.24746841192245483 acc = 0\n",
            "pos tensor(0.1718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4479==== Step 1  Train Loss 0.26970309019088745 acc = 0\n",
            "pos tensor(0.1896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4480==== Step 1  Train Loss 0.2502140998840332 acc = 0\n",
            "pos tensor(0.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4481==== Step 1  Train Loss 0.28744539618492126 acc = 0\n",
            "pos tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4482==== Step 1  Train Loss 0.27055132389068604 acc = 0\n",
            "pos tensor(0.1955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4483==== Step 1  Train Loss 0.2700435519218445 acc = 0\n",
            "pos tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4484==== Step 1  Train Loss 0.26133298873901367 acc = 0\n",
            "pos tensor(0.1685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4485==== Step 1  Train Loss 0.2932489216327667 acc = 0\n",
            "pos tensor(0.1892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4486==== Step 1  Train Loss 0.26518985629081726 acc = 0\n",
            "pos tensor(0.1715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4487==== Step 1  Train Loss 0.2648078203201294 acc = 0\n",
            "pos tensor(0.2177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4488==== Step 1  Train Loss 0.2765044569969177 acc = 0\n",
            "pos tensor(0.1976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4489==== Step 1  Train Loss 0.2730087339878082 acc = 0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4490==== Step 1  Train Loss 0.28489047288894653 acc = 0\n",
            "pos tensor(0.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4491==== Step 1  Train Loss 0.25702086091041565 acc = 0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4492==== Step 1  Train Loss 0.29717719554901123 acc = 0\n",
            "pos tensor(0.1648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4493==== Step 1  Train Loss 0.251494437456131 acc = 0\n",
            "pos tensor(0.2095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4494==== Step 1  Train Loss 0.27320396900177 acc = 0\n",
            "pos tensor(0.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4495==== Step 1  Train Loss 0.26596859097480774 acc = 0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4496==== Step 1  Train Loss 0.26963651180267334 acc = 0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4497==== Step 1  Train Loss 0.26101040840148926 acc = 0\n",
            "pos tensor(0.1946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4498==== Step 1  Train Loss 0.2622838318347931 acc = 0\n",
            "pos tensor(0.2413, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4499==== Step 1  Train Loss 0.2947545647621155 acc = 0\n",
            "  Batch 4,500  of  5,561.    Elapsed: 0:15:59.\n",
            "pos tensor(0.2375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4500==== Step 1  Train Loss 0.29275184869766235 acc = 0\n",
            "pos tensor(0.2422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4501==== Step 1  Train Loss 0.26818472146987915 acc = 0\n",
            "pos tensor(0.2101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4502==== Step 1  Train Loss 0.27127841114997864 acc = 0\n",
            "pos tensor(0.2269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4503==== Step 1  Train Loss 0.29139775037765503 acc = 0\n",
            "pos tensor(0.1986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4504==== Step 1  Train Loss 0.2770574390888214 acc = 0\n",
            "pos tensor(0.1375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4505==== Step 1  Train Loss 0.25872015953063965 acc = 0\n",
            "pos tensor(0.1909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4506==== Step 1  Train Loss 0.25951510667800903 acc = 0\n",
            "pos tensor(0.2278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4507==== Step 1  Train Loss 0.28422778844833374 acc = 0\n",
            "pos tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4508==== Step 1  Train Loss 0.26316070556640625 acc = 0\n",
            "pos tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4509==== Step 1  Train Loss 0.2889275550842285 acc = 0\n",
            "pos tensor(0.1804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4510==== Step 1  Train Loss 0.25422781705856323 acc = 0\n",
            "pos tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4511==== Step 1  Train Loss 0.2726503014564514 acc = 0\n",
            "pos tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4512==== Step 1  Train Loss 0.25400668382644653 acc = 0\n",
            "pos tensor(0.2028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4513==== Step 1  Train Loss 0.2887566387653351 acc = 0\n",
            "pos tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4514==== Step 1  Train Loss 0.2714950442314148 acc = 0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4515==== Step 1  Train Loss 0.2725679278373718 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4516==== Step 1  Train Loss 0.2752766013145447 acc = 0\n",
            "pos tensor(0.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4517==== Step 1  Train Loss 0.27938780188560486 acc = 0\n",
            "pos tensor(0.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4518==== Step 1  Train Loss 0.24592196941375732 acc = 0\n",
            "pos tensor(0.2416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4519==== Step 1  Train Loss 0.27919507026672363 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4520==== Step 1  Train Loss 0.2638283967971802 acc = 0\n",
            "pos tensor(0.2219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4521==== Step 1  Train Loss 0.28108662366867065 acc = 0\n",
            "pos tensor(0.2803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4522==== Step 1  Train Loss 0.3017384707927704 acc = 0\n",
            "pos tensor(0.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4523==== Step 1  Train Loss 0.26178157329559326 acc = 0\n",
            "pos tensor(0.1696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4524==== Step 1  Train Loss 0.24713557958602905 acc = 0\n",
            "pos tensor(0.2704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4525==== Step 1  Train Loss 0.2954666018486023 acc = 0\n",
            "pos tensor(0.2393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4526==== Step 1  Train Loss 0.27710121870040894 acc = 0\n",
            "pos tensor(0.2376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4527==== Step 1  Train Loss 0.29654595255851746 acc = 0\n",
            "pos tensor(0.2162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4528==== Step 1  Train Loss 0.27573344111442566 acc = 0\n",
            "pos tensor(0.2233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4529==== Step 1  Train Loss 0.28825461864471436 acc = 0\n",
            "pos tensor(0.1863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4530==== Step 1  Train Loss 0.26643359661102295 acc = 0\n",
            "pos tensor(0.2276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4531==== Step 1  Train Loss 0.26638975739479065 acc = 0\n",
            "pos tensor(0.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4532==== Step 1  Train Loss 0.28505274653434753 acc = 0\n",
            "pos tensor(0.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4533==== Step 1  Train Loss 0.27465230226516724 acc = 0\n",
            "pos tensor(0.1784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4534==== Step 1  Train Loss 0.2722511291503906 acc = 0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4535==== Step 1  Train Loss 0.27377068996429443 acc = 0\n",
            "pos tensor(0.2013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4536==== Step 1  Train Loss 0.27684545516967773 acc = 0\n",
            "pos tensor(0.1616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4537==== Step 1  Train Loss 0.264448881149292 acc = 0\n",
            "pos tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4538==== Step 1  Train Loss 0.2746998965740204 acc = 0\n",
            "pos tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4539==== Step 1  Train Loss 0.26758527755737305 acc = 0\n",
            "pos tensor(0.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4540==== Step 1  Train Loss 0.27001437544822693 acc = 0\n",
            "pos tensor(0.2064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4541==== Step 1  Train Loss 0.24913010001182556 acc = 0\n",
            "pos tensor(0.2480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4542==== Step 1  Train Loss 0.2998560965061188 acc = 0\n",
            "pos tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4543==== Step 1  Train Loss 0.25506600737571716 acc = 0\n",
            "pos tensor(0.2337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4544==== Step 1  Train Loss 0.27989619970321655 acc = 0\n",
            "pos tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4545==== Step 1  Train Loss 0.259509801864624 acc = 0\n",
            "pos tensor(0.2683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4546==== Step 1  Train Loss 0.30401960015296936 acc = 0\n",
            "pos tensor(0.2411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4547==== Step 1  Train Loss 0.3007161319255829 acc = 0\n",
            "pos tensor(0.2473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4548==== Step 1  Train Loss 0.2762278616428375 acc = 0\n",
            "pos tensor(0.2424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4549==== Step 1  Train Loss 0.2896464467048645 acc = 0\n",
            "pos tensor(0.2136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4550==== Step 1  Train Loss 0.26336535811424255 acc = 0\n",
            "pos tensor(0.1605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4551==== Step 1  Train Loss 0.24459929764270782 acc = 0\n",
            "pos tensor(0.2201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4552==== Step 1  Train Loss 0.2758830189704895 acc = 0\n",
            "pos tensor(0.2121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4553==== Step 1  Train Loss 0.2892262637615204 acc = 0\n",
            "pos tensor(0.1844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4554==== Step 1  Train Loss 0.26037102937698364 acc = 0\n",
            "pos tensor(0.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4555==== Step 1  Train Loss 0.26449447870254517 acc = 0\n",
            "pos tensor(0.2091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4556==== Step 1  Train Loss 0.2535412013530731 acc = 0\n",
            "pos tensor(0.2239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4557==== Step 1  Train Loss 0.28238630294799805 acc = 0\n",
            "pos tensor(0.2043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4558==== Step 1  Train Loss 0.2810284495353699 acc = 0\n",
            "pos tensor(0.1731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4559==== Step 1  Train Loss 0.24267932772636414 acc = 0\n",
            "pos tensor(0.1586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4560==== Step 1  Train Loss 0.2614767849445343 acc = 0\n",
            "pos tensor(0.2414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4561==== Step 1  Train Loss 0.27750951051712036 acc = 0\n",
            "pos tensor(0.1996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4562==== Step 1  Train Loss 0.26249295473098755 acc = 0\n",
            "pos tensor(0.2045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4563==== Step 1  Train Loss 0.27527573704719543 acc = 0\n",
            "pos tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4564==== Step 1  Train Loss 0.272434264421463 acc = 0\n",
            "pos tensor(0.1615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4565==== Step 1  Train Loss 0.24972409009933472 acc = 0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4566==== Step 1  Train Loss 0.2681230902671814 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4567==== Step 1  Train Loss 0.2680964767932892 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4568==== Step 1  Train Loss 0.2678992450237274 acc = 0\n",
            "pos tensor(0.2094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4569==== Step 1  Train Loss 0.2841240167617798 acc = 0\n",
            "pos tensor(0.2381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4570==== Step 1  Train Loss 0.27741098403930664 acc = 0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4571==== Step 1  Train Loss 0.27386942505836487 acc = 0\n",
            "pos tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4572==== Step 1  Train Loss 0.3049451410770416 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4573==== Step 1  Train Loss 0.284390389919281 acc = 0\n",
            "pos tensor(0.2218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4574==== Step 1  Train Loss 0.2714269459247589 acc = 0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4575==== Step 1  Train Loss 0.2682046592235565 acc = 0\n",
            "pos tensor(0.1672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4576==== Step 1  Train Loss 0.27764952182769775 acc = 0\n",
            "pos tensor(0.2383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4577==== Step 1  Train Loss 0.2925594747066498 acc = 0\n",
            "pos tensor(0.1838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4578==== Step 1  Train Loss 0.2797132134437561 acc = 0\n",
            "pos tensor(0.1999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4579==== Step 1  Train Loss 0.30028530955314636 acc = 0\n",
            "pos tensor(0.2047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4580==== Step 1  Train Loss 0.26512038707733154 acc = 0\n",
            "pos tensor(0.1600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4581==== Step 1  Train Loss 0.23790131509304047 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4582==== Step 1  Train Loss 0.26359912753105164 acc = 0\n",
            "pos tensor(0.2140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4583==== Step 1  Train Loss 0.2746915817260742 acc = 0\n",
            "pos tensor(0.2270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4584==== Step 1  Train Loss 0.27485978603363037 acc = 0\n",
            "pos tensor(0.2172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4585==== Step 1  Train Loss 0.2934459447860718 acc = 0\n",
            "pos tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4586==== Step 1  Train Loss 0.24377259612083435 acc = 0\n",
            "pos tensor(0.2123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4587==== Step 1  Train Loss 0.26505351066589355 acc = 0\n",
            "pos tensor(0.2431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4588==== Step 1  Train Loss 0.3024325966835022 acc = 0\n",
            "pos tensor(0.1916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4589==== Step 1  Train Loss 0.26289698481559753 acc = 0\n",
            "pos tensor(0.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4590==== Step 1  Train Loss 0.29669320583343506 acc = 0\n",
            "pos tensor(0.2521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4591==== Step 1  Train Loss 0.2999586760997772 acc = 0\n",
            "pos tensor(0.2043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4592==== Step 1  Train Loss 0.27246949076652527 acc = 0\n",
            "pos tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4593==== Step 1  Train Loss 0.281852126121521 acc = 0\n",
            "pos tensor(0.2257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4594==== Step 1  Train Loss 0.2719789743423462 acc = 0\n",
            "pos tensor(0.1974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4595==== Step 1  Train Loss 0.2705385386943817 acc = 0\n",
            "pos tensor(0.1824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4596==== Step 1  Train Loss 0.26711660623550415 acc = 0\n",
            "pos tensor(0.2434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4597==== Step 1  Train Loss 0.2956950068473816 acc = 0\n",
            "pos tensor(0.1463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4598==== Step 1  Train Loss 0.2334008812904358 acc = 0\n",
            "pos tensor(0.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4599==== Step 1  Train Loss 0.2913820147514343 acc = 0\n",
            "  Batch 4,600  of  5,561.    Elapsed: 0:16:21.\n",
            "pos tensor(0.2221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4600==== Step 1  Train Loss 0.2850940525531769 acc = 0\n",
            "pos tensor(0.2152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4601==== Step 1  Train Loss 0.25745612382888794 acc = 0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4602==== Step 1  Train Loss 0.2654403746128082 acc = 0\n",
            "pos tensor(0.2096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4603==== Step 1  Train Loss 0.27481532096862793 acc = 0\n",
            "pos tensor(0.2107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4604==== Step 1  Train Loss 0.2735174894332886 acc = 0\n",
            "pos tensor(0.2350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4605==== Step 1  Train Loss 0.27729982137680054 acc = 0\n",
            "pos tensor(0.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4606==== Step 1  Train Loss 0.28079521656036377 acc = 0\n",
            "pos tensor(0.1730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4607==== Step 1  Train Loss 0.25386491417884827 acc = 0\n",
            "pos tensor(0.2313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4608==== Step 1  Train Loss 0.2970404028892517 acc = 0\n",
            "pos tensor(0.2389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4609==== Step 1  Train Loss 0.2881731390953064 acc = 0\n",
            "pos tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4610==== Step 1  Train Loss 0.23924113810062408 acc = 0\n",
            "pos tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4611==== Step 1  Train Loss 0.2790060341358185 acc = 0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4612==== Step 1  Train Loss 0.26341351866722107 acc = 0\n",
            "pos tensor(0.2496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4613==== Step 1  Train Loss 0.3054739534854889 acc = 0\n",
            "pos tensor(0.2250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4614==== Step 1  Train Loss 0.27533233165740967 acc = 0\n",
            "pos tensor(0.2141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4615==== Step 1  Train Loss 0.2939133942127228 acc = 0\n",
            "pos tensor(0.2135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4616==== Step 1  Train Loss 0.28529268503189087 acc = 0\n",
            "pos tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4617==== Step 1  Train Loss 0.25900352001190186 acc = 0\n",
            "pos tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4618==== Step 1  Train Loss 0.26467299461364746 acc = 0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4619==== Step 1  Train Loss 0.25785550475120544 acc = 0\n",
            "pos tensor(0.1608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4620==== Step 1  Train Loss 0.25810837745666504 acc = 0\n",
            "pos tensor(0.2590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4621==== Step 1  Train Loss 0.3025658130645752 acc = 0\n",
            "pos tensor(0.2673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4622==== Step 1  Train Loss 0.3094862103462219 acc = 0\n",
            "pos tensor(0.1749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4623==== Step 1  Train Loss 0.24577681720256805 acc = 0\n",
            "pos tensor(0.2168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4624==== Step 1  Train Loss 0.28011810779571533 acc = 0\n",
            "pos tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4625==== Step 1  Train Loss 0.2615300118923187 acc = 0\n",
            "pos tensor(0.1437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4626==== Step 1  Train Loss 0.260158896446228 acc = 0\n",
            "pos tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4627==== Step 1  Train Loss 0.2663494348526001 acc = 0\n",
            "pos tensor(0.2152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4628==== Step 1  Train Loss 0.29240867495536804 acc = 0\n",
            "pos tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4629==== Step 1  Train Loss 0.24226093292236328 acc = 0\n",
            "pos tensor(0.1769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4630==== Step 1  Train Loss 0.2545347809791565 acc = 0\n",
            "pos tensor(0.2024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4631==== Step 1  Train Loss 0.28325915336608887 acc = 0\n",
            "pos tensor(0.1582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4632==== Step 1  Train Loss 0.2780423164367676 acc = 0\n",
            "pos tensor(0.2055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4633==== Step 1  Train Loss 0.2633294463157654 acc = 0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4634==== Step 1  Train Loss 0.2785143256187439 acc = 0\n",
            "pos tensor(0.2886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4635==== Step 1  Train Loss 0.3063569962978363 acc = 0\n",
            "pos tensor(0.2114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4636==== Step 1  Train Loss 0.2788739800453186 acc = 0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4637==== Step 1  Train Loss 0.2861928641796112 acc = 0\n",
            "pos tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4638==== Step 1  Train Loss 0.2563173770904541 acc = 0\n",
            "pos tensor(0.1787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4639==== Step 1  Train Loss 0.27336129546165466 acc = 0\n",
            "pos tensor(0.2414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4640==== Step 1  Train Loss 0.2814602255821228 acc = 0\n",
            "pos tensor(0.2386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4641==== Step 1  Train Loss 0.2926212549209595 acc = 0\n",
            "pos tensor(0.2421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4642==== Step 1  Train Loss 0.2973586320877075 acc = 0\n",
            "pos tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4643==== Step 1  Train Loss 0.2759608030319214 acc = 0\n",
            "pos tensor(0.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4644==== Step 1  Train Loss 0.25460565090179443 acc = 0\n",
            "pos tensor(0.1459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4645==== Step 1  Train Loss 0.25847405195236206 acc = 0\n",
            "pos tensor(0.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4646==== Step 1  Train Loss 0.28613710403442383 acc = 0\n",
            "pos tensor(0.1681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4647==== Step 1  Train Loss 0.2784331738948822 acc = 0\n",
            "pos tensor(0.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4648==== Step 1  Train Loss 0.2802617847919464 acc = 0\n",
            "pos tensor(0.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4649==== Step 1  Train Loss 0.29763150215148926 acc = 0\n",
            "pos tensor(0.1996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4650==== Step 1  Train Loss 0.2837396562099457 acc = 0\n",
            "pos tensor(0.2180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4651==== Step 1  Train Loss 0.2773400545120239 acc = 0\n",
            "pos tensor(0.1689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4652==== Step 1  Train Loss 0.26251912117004395 acc = 0\n",
            "pos tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4653==== Step 1  Train Loss 0.2671624720096588 acc = 0\n",
            "pos tensor(0.2320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4654==== Step 1  Train Loss 0.29875797033309937 acc = 0\n",
            "pos tensor(0.1615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4655==== Step 1  Train Loss 0.24194665253162384 acc = 0\n",
            "pos tensor(0.2373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4656==== Step 1  Train Loss 0.2724582850933075 acc = 0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4657==== Step 1  Train Loss 0.2617425322532654 acc = 0\n",
            "pos tensor(0.2524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4658==== Step 1  Train Loss 0.2851240038871765 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4659==== Step 1  Train Loss 0.2609306573867798 acc = 0\n",
            "pos tensor(0.1916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4660==== Step 1  Train Loss 0.25665318965911865 acc = 0\n",
            "pos tensor(0.2275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4661==== Step 1  Train Loss 0.28042906522750854 acc = 0\n",
            "pos tensor(0.2080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4662==== Step 1  Train Loss 0.2676863968372345 acc = 0\n",
            "pos tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4663==== Step 1  Train Loss 0.2565957307815552 acc = 0\n",
            "pos tensor(0.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4664==== Step 1  Train Loss 0.29380470514297485 acc = 0\n",
            "pos tensor(0.1758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4665==== Step 1  Train Loss 0.26125234365463257 acc = 0\n",
            "pos tensor(0.2355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4666==== Step 1  Train Loss 0.2941977381706238 acc = 0\n",
            "pos tensor(0.2187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4667==== Step 1  Train Loss 0.2765592038631439 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4668==== Step 1  Train Loss 0.2774251103401184 acc = 0\n",
            "pos tensor(0.1831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4669==== Step 1  Train Loss 0.27784109115600586 acc = 0\n",
            "pos tensor(0.1761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4670==== Step 1  Train Loss 0.2655016779899597 acc = 0\n",
            "pos tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4671==== Step 1  Train Loss 0.26961368322372437 acc = 0\n",
            "pos tensor(0.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4672==== Step 1  Train Loss 0.2821366786956787 acc = 0\n",
            "pos tensor(0.2201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4673==== Step 1  Train Loss 0.2947404980659485 acc = 0\n",
            "pos tensor(0.2054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4674==== Step 1  Train Loss 0.2746492028236389 acc = 0\n",
            "pos tensor(0.2372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4675==== Step 1  Train Loss 0.2919536828994751 acc = 0\n",
            "pos tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4676==== Step 1  Train Loss 0.2878308594226837 acc = 0\n",
            "pos tensor(0.1734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4677==== Step 1  Train Loss 0.2657783031463623 acc = 0\n",
            "pos tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4678==== Step 1  Train Loss 0.2822743356227875 acc = 0\n",
            "pos tensor(0.2054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4679==== Step 1  Train Loss 0.2687743604183197 acc = 0\n",
            "pos tensor(0.2545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4680==== Step 1  Train Loss 0.29815709590911865 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4681==== Step 1  Train Loss 0.2844292223453522 acc = 0\n",
            "pos tensor(0.2065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4682==== Step 1  Train Loss 0.2739826440811157 acc = 0\n",
            "pos tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4683==== Step 1  Train Loss 0.28402554988861084 acc = 0\n",
            "pos tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4684==== Step 1  Train Loss 0.2826564311981201 acc = 0\n",
            "pos tensor(0.2505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4685==== Step 1  Train Loss 0.2839832901954651 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4686==== Step 1  Train Loss 0.28579258918762207 acc = 0\n",
            "pos tensor(0.2140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4687==== Step 1  Train Loss 0.2877691984176636 acc = 0\n",
            "pos tensor(0.2195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4688==== Step 1  Train Loss 0.26687613129615784 acc = 0\n",
            "pos tensor(0.1783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4689==== Step 1  Train Loss 0.28864219784736633 acc = 0\n",
            "pos tensor(0.2193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4690==== Step 1  Train Loss 0.2735668122768402 acc = 0\n",
            "pos tensor(0.2239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4691==== Step 1  Train Loss 0.28169944882392883 acc = 0\n",
            "pos tensor(0.1981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4692==== Step 1  Train Loss 0.2609442472457886 acc = 0\n",
            "pos tensor(0.1595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4693==== Step 1  Train Loss 0.2524673342704773 acc = 0\n",
            "pos tensor(0.2012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4694==== Step 1  Train Loss 0.2817865312099457 acc = 0\n",
            "pos tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4695==== Step 1  Train Loss 0.26275742053985596 acc = 0\n",
            "pos tensor(0.1851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4696==== Step 1  Train Loss 0.2735140323638916 acc = 0\n",
            "pos tensor(0.1669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4697==== Step 1  Train Loss 0.2576292157173157 acc = 0\n",
            "pos tensor(0.2287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4698==== Step 1  Train Loss 0.27782776951789856 acc = 0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4699==== Step 1  Train Loss 0.26646190881729126 acc = 0\n",
            "  Batch 4,700  of  5,561.    Elapsed: 0:16:42.\n",
            "pos tensor(0.1627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4700==== Step 1  Train Loss 0.2734217047691345 acc = 0\n",
            "pos tensor(0.2385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4701==== Step 1  Train Loss 0.2800312638282776 acc = 0\n",
            "pos tensor(0.1876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4702==== Step 1  Train Loss 0.259885311126709 acc = 0\n",
            "pos tensor(0.1956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4703==== Step 1  Train Loss 0.2915537357330322 acc = 0\n",
            "pos tensor(0.2017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4704==== Step 1  Train Loss 0.26820534467697144 acc = 0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4705==== Step 1  Train Loss 0.26402491331100464 acc = 0\n",
            "pos tensor(0.1676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4706==== Step 1  Train Loss 0.2651308476924896 acc = 0\n",
            "pos tensor(0.2135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4707==== Step 1  Train Loss 0.2847806215286255 acc = 0\n",
            "pos tensor(0.2394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4708==== Step 1  Train Loss 0.28342553973197937 acc = 0\n",
            "pos tensor(0.1877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4709==== Step 1  Train Loss 0.28199300169944763 acc = 0\n",
            "pos tensor(0.1373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4710==== Step 1  Train Loss 0.23717227578163147 acc = 0\n",
            "pos tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4711==== Step 1  Train Loss 0.2720595896244049 acc = 0\n",
            "pos tensor(0.1422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4712==== Step 1  Train Loss 0.25847434997558594 acc = 0\n",
            "pos tensor(0.2082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4713==== Step 1  Train Loss 0.2742399573326111 acc = 0\n",
            "pos tensor(0.2307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4714==== Step 1  Train Loss 0.28502047061920166 acc = 0\n",
            "pos tensor(0.1591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4715==== Step 1  Train Loss 0.24663281440734863 acc = 0\n",
            "pos tensor(0.2043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4716==== Step 1  Train Loss 0.2942163348197937 acc = 0\n",
            "pos tensor(0.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4717==== Step 1  Train Loss 0.28338998556137085 acc = 0\n",
            "pos tensor(0.1694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4718==== Step 1  Train Loss 0.255105584859848 acc = 0\n",
            "pos tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4719==== Step 1  Train Loss 0.2799687683582306 acc = 0\n",
            "pos tensor(0.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4720==== Step 1  Train Loss 0.248790442943573 acc = 0\n",
            "pos tensor(0.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4721==== Step 1  Train Loss 0.25591522455215454 acc = 0\n",
            "pos tensor(0.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4722==== Step 1  Train Loss 0.2919563353061676 acc = 0\n",
            "pos tensor(0.1730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4723==== Step 1  Train Loss 0.25043827295303345 acc = 0\n",
            "pos tensor(0.2335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4724==== Step 1  Train Loss 0.28931841254234314 acc = 0\n",
            "pos tensor(0.2270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4725==== Step 1  Train Loss 0.2895619869232178 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4726==== Step 1  Train Loss 0.2833482027053833 acc = 0\n",
            "pos tensor(0.2217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4727==== Step 1  Train Loss 0.28173744678497314 acc = 0\n",
            "pos tensor(0.1955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4728==== Step 1  Train Loss 0.256797194480896 acc = 0\n",
            "pos tensor(0.1717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4729==== Step 1  Train Loss 0.2516154646873474 acc = 0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4730==== Step 1  Train Loss 0.27695417404174805 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4731==== Step 1  Train Loss 0.2796410322189331 acc = 0\n",
            "pos tensor(0.1734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4732==== Step 1  Train Loss 0.2473846971988678 acc = 0\n",
            "pos tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4733==== Step 1  Train Loss 0.2567763328552246 acc = 0\n",
            "pos tensor(0.2829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4734==== Step 1  Train Loss 0.3026283383369446 acc = 0\n",
            "pos tensor(0.2465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4735==== Step 1  Train Loss 0.2847079038619995 acc = 0\n",
            "pos tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4736==== Step 1  Train Loss 0.2755528688430786 acc = 0\n",
            "pos tensor(0.2250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4737==== Step 1  Train Loss 0.2770037055015564 acc = 0\n",
            "pos tensor(0.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4738==== Step 1  Train Loss 0.2644693851470947 acc = 0\n",
            "pos tensor(0.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4739==== Step 1  Train Loss 0.2666315734386444 acc = 0\n",
            "pos tensor(0.2280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4740==== Step 1  Train Loss 0.2873179018497467 acc = 0\n",
            "pos tensor(0.2273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4741==== Step 1  Train Loss 0.2698941230773926 acc = 0\n",
            "pos tensor(0.2270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4742==== Step 1  Train Loss 0.2844691276550293 acc = 0\n",
            "pos tensor(0.1613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4743==== Step 1  Train Loss 0.24944043159484863 acc = 0\n",
            "pos tensor(0.2087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4744==== Step 1  Train Loss 0.26653680205345154 acc = 0\n",
            "pos tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4745==== Step 1  Train Loss 0.2856784462928772 acc = 0\n",
            "pos tensor(0.1835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4746==== Step 1  Train Loss 0.2619217336177826 acc = 0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4747==== Step 1  Train Loss 0.2686956822872162 acc = 0\n",
            "pos tensor(0.2555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4748==== Step 1  Train Loss 0.3026573657989502 acc = 0\n",
            "pos tensor(0.2269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4749==== Step 1  Train Loss 0.28838881850242615 acc = 0\n",
            "pos tensor(0.1880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4750==== Step 1  Train Loss 0.2542648911476135 acc = 0\n",
            "pos tensor(0.2279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4751==== Step 1  Train Loss 0.2725864052772522 acc = 0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4752==== Step 1  Train Loss 0.2563282251358032 acc = 0\n",
            "pos tensor(0.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4753==== Step 1  Train Loss 0.26118025183677673 acc = 0\n",
            "pos tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4754==== Step 1  Train Loss 0.2733546197414398 acc = 0\n",
            "pos tensor(0.1640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4755==== Step 1  Train Loss 0.24760903418064117 acc = 0\n",
            "pos tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4756==== Step 1  Train Loss 0.2621425986289978 acc = 0\n",
            "pos tensor(0.2330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4757==== Step 1  Train Loss 0.2835882604122162 acc = 0\n",
            "pos tensor(0.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4758==== Step 1  Train Loss 0.26976096630096436 acc = 0\n",
            "pos tensor(0.2370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4759==== Step 1  Train Loss 0.29246705770492554 acc = 0\n",
            "pos tensor(0.2579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4760==== Step 1  Train Loss 0.29271894693374634 acc = 0\n",
            "pos tensor(0.2416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4761==== Step 1  Train Loss 0.29680678248405457 acc = 0\n",
            "pos tensor(0.1625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4762==== Step 1  Train Loss 0.2669031023979187 acc = 0\n",
            "pos tensor(0.2253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4763==== Step 1  Train Loss 0.2837285101413727 acc = 0\n",
            "pos tensor(0.1749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4764==== Step 1  Train Loss 0.2676999866962433 acc = 0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4765==== Step 1  Train Loss 0.264833927154541 acc = 0\n",
            "pos tensor(0.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4766==== Step 1  Train Loss 0.26814955472946167 acc = 0\n",
            "pos tensor(0.2135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4767==== Step 1  Train Loss 0.27371323108673096 acc = 0\n",
            "pos tensor(0.2206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4768==== Step 1  Train Loss 0.2790994942188263 acc = 0\n",
            "pos tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4769==== Step 1  Train Loss 0.26723712682724 acc = 0\n",
            "pos tensor(0.2554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4770==== Step 1  Train Loss 0.2962856888771057 acc = 0\n",
            "pos tensor(0.2110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4771==== Step 1  Train Loss 0.2738259434700012 acc = 0\n",
            "pos tensor(0.1835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4772==== Step 1  Train Loss 0.2639089524745941 acc = 0\n",
            "pos tensor(0.2021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4773==== Step 1  Train Loss 0.27260786294937134 acc = 0\n",
            "pos tensor(0.1738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4774==== Step 1  Train Loss 0.25715911388397217 acc = 0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4775==== Step 1  Train Loss 0.2639152705669403 acc = 0\n",
            "pos tensor(0.2490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4776==== Step 1  Train Loss 0.32302403450012207 acc = 0\n",
            "pos tensor(0.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4777==== Step 1  Train Loss 0.2760774493217468 acc = 0\n",
            "pos tensor(0.1696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4778==== Step 1  Train Loss 0.28337058424949646 acc = 0\n",
            "pos tensor(0.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4779==== Step 1  Train Loss 0.27591872215270996 acc = 0\n",
            "pos tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4780==== Step 1  Train Loss 0.24625201523303986 acc = 0\n",
            "pos tensor(0.2301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4781==== Step 1  Train Loss 0.2845982611179352 acc = 0\n",
            "pos tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4782==== Step 1  Train Loss 0.24795448780059814 acc = 0\n",
            "pos tensor(0.2036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4783==== Step 1  Train Loss 0.2784775495529175 acc = 0\n",
            "pos tensor(0.1804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4784==== Step 1  Train Loss 0.27335667610168457 acc = 0\n",
            "pos tensor(0.2324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4785==== Step 1  Train Loss 0.27967971563339233 acc = 0\n",
            "pos tensor(0.2762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4786==== Step 1  Train Loss 0.31051939725875854 acc = 0\n",
            "pos tensor(0.2342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4787==== Step 1  Train Loss 0.26629289984703064 acc = 0\n",
            "pos tensor(0.2200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4788==== Step 1  Train Loss 0.28645575046539307 acc = 0\n",
            "pos tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4789==== Step 1  Train Loss 0.2840883135795593 acc = 0\n",
            "pos tensor(0.2180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4790==== Step 1  Train Loss 0.2880324125289917 acc = 0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4791==== Step 1  Train Loss 0.28456035256385803 acc = 0\n",
            "pos tensor(0.1736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4792==== Step 1  Train Loss 0.2895938456058502 acc = 0\n",
            "pos tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4793==== Step 1  Train Loss 0.2656087577342987 acc = 0\n",
            "pos tensor(0.2263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4794==== Step 1  Train Loss 0.28433334827423096 acc = 0\n",
            "pos tensor(0.1854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4795==== Step 1  Train Loss 0.27058589458465576 acc = 0\n",
            "pos tensor(0.1926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4796==== Step 1  Train Loss 0.27101969718933105 acc = 0\n",
            "pos tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4797==== Step 1  Train Loss 0.2600908577442169 acc = 0\n",
            "pos tensor(0.1738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4798==== Step 1  Train Loss 0.269256055355072 acc = 0\n",
            "pos tensor(0.2038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4799==== Step 1  Train Loss 0.2794668972492218 acc = 0\n",
            "  Batch 4,800  of  5,561.    Elapsed: 0:17:03.\n",
            "pos tensor(0.1367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4800==== Step 1  Train Loss 0.2449571043252945 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4801==== Step 1  Train Loss 0.2829011082649231 acc = 0\n",
            "pos tensor(0.2063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4802==== Step 1  Train Loss 0.2879025638103485 acc = 0\n",
            "pos tensor(0.2009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4803==== Step 1  Train Loss 0.2733847498893738 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4804==== Step 1  Train Loss 0.26592767238616943 acc = 0\n",
            "pos tensor(0.2237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4805==== Step 1  Train Loss 0.294291615486145 acc = 0\n",
            "pos tensor(0.2230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4806==== Step 1  Train Loss 0.28704598546028137 acc = 0\n",
            "pos tensor(0.2469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4807==== Step 1  Train Loss 0.28372323513031006 acc = 0\n",
            "pos tensor(0.1574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4808==== Step 1  Train Loss 0.27142757177352905 acc = 0\n",
            "pos tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4809==== Step 1  Train Loss 0.2845585346221924 acc = 0\n",
            "pos tensor(0.2238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4810==== Step 1  Train Loss 0.2827097177505493 acc = 0\n",
            "pos tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4811==== Step 1  Train Loss 0.24759340286254883 acc = 0\n",
            "pos tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4812==== Step 1  Train Loss 0.27283352613449097 acc = 0\n",
            "pos tensor(0.1612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4813==== Step 1  Train Loss 0.269224613904953 acc = 0\n",
            "pos tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4814==== Step 1  Train Loss 0.27316176891326904 acc = 0\n",
            "pos tensor(0.2268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4815==== Step 1  Train Loss 0.28492745757102966 acc = 0\n",
            "pos tensor(0.2615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4816==== Step 1  Train Loss 0.30950483679771423 acc = 0\n",
            "pos tensor(0.1910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4817==== Step 1  Train Loss 0.28147223591804504 acc = 0\n",
            "pos tensor(0.1623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4818==== Step 1  Train Loss 0.2656964957714081 acc = 0\n",
            "pos tensor(0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4819==== Step 1  Train Loss 0.2847062051296234 acc = 0\n",
            "pos tensor(0.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4820==== Step 1  Train Loss 0.2879221439361572 acc = 0\n",
            "pos tensor(0.2179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4821==== Step 1  Train Loss 0.28470057249069214 acc = 0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4822==== Step 1  Train Loss 0.2872248589992523 acc = 0\n",
            "pos tensor(0.1999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4823==== Step 1  Train Loss 0.27220869064331055 acc = 0\n",
            "pos tensor(0.1601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4824==== Step 1  Train Loss 0.2522279620170593 acc = 0\n",
            "pos tensor(0.1766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4825==== Step 1  Train Loss 0.2793077826499939 acc = 0\n",
            "pos tensor(0.1512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4826==== Step 1  Train Loss 0.2481878697872162 acc = 0\n",
            "pos tensor(0.2308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4827==== Step 1  Train Loss 0.2796917259693146 acc = 0\n",
            "pos tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4828==== Step 1  Train Loss 0.2862418591976166 acc = 0\n",
            "pos tensor(0.2027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4829==== Step 1  Train Loss 0.27715441584587097 acc = 0\n",
            "pos tensor(0.1881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4830==== Step 1  Train Loss 0.27562254667282104 acc = 0\n",
            "pos tensor(0.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4831==== Step 1  Train Loss 0.29556089639663696 acc = 0\n",
            "pos tensor(0.1421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4832==== Step 1  Train Loss 0.2516324818134308 acc = 0\n",
            "pos tensor(0.1994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4833==== Step 1  Train Loss 0.2697007656097412 acc = 0\n",
            "pos tensor(0.1920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4834==== Step 1  Train Loss 0.3050130009651184 acc = 0\n",
            "pos tensor(0.2480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4835==== Step 1  Train Loss 0.30089271068573 acc = 0\n",
            "pos tensor(0.1867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4836==== Step 1  Train Loss 0.26377108693122864 acc = 0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4837==== Step 1  Train Loss 0.2794707417488098 acc = 0\n",
            "pos tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4838==== Step 1  Train Loss 0.27820008993148804 acc = 0\n",
            "pos tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4839==== Step 1  Train Loss 0.2814290523529053 acc = 0\n",
            "pos tensor(0.1971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4840==== Step 1  Train Loss 0.2837996482849121 acc = 0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4841==== Step 1  Train Loss 0.2624835669994354 acc = 0\n",
            "pos tensor(0.1682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4842==== Step 1  Train Loss 0.2593597173690796 acc = 0\n",
            "pos tensor(0.1694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4843==== Step 1  Train Loss 0.2693530321121216 acc = 0\n",
            "pos tensor(0.2008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4844==== Step 1  Train Loss 0.2909446358680725 acc = 0\n",
            "pos tensor(0.1763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4845==== Step 1  Train Loss 0.2806929647922516 acc = 0\n",
            "pos tensor(0.1862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4846==== Step 1  Train Loss 0.2930563986301422 acc = 0\n",
            "pos tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4847==== Step 1  Train Loss 0.29045772552490234 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4848==== Step 1  Train Loss 0.2650575637817383 acc = 0\n",
            "pos tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4849==== Step 1  Train Loss 0.2807506322860718 acc = 0\n",
            "pos tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4850==== Step 1  Train Loss 0.27427414059638977 acc = 0\n",
            "pos tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4851==== Step 1  Train Loss 0.2887968122959137 acc = 0\n",
            "pos tensor(0.1923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4852==== Step 1  Train Loss 0.28924304246902466 acc = 0\n",
            "pos tensor(0.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4853==== Step 1  Train Loss 0.2858530879020691 acc = 0\n",
            "pos tensor(0.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4854==== Step 1  Train Loss 0.2734271287918091 acc = 0\n",
            "pos tensor(0.1110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4855==== Step 1  Train Loss 0.2702978551387787 acc = 0\n",
            "pos tensor(0.1589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4856==== Step 1  Train Loss 0.26100432872772217 acc = 0\n",
            "pos tensor(0.1845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4857==== Step 1  Train Loss 0.2781097888946533 acc = 0\n",
            "pos tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4858==== Step 1  Train Loss 0.2684987783432007 acc = 0\n",
            "pos tensor(0.2102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4859==== Step 1  Train Loss 0.2787545621395111 acc = 0\n",
            "pos tensor(0.2121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4860==== Step 1  Train Loss 0.31659403443336487 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3538, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4861==== Step 1  Train Loss 0.2735172510147095 acc = 0\n",
            "pos tensor(0.2462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4862==== Step 1  Train Loss 0.3079262375831604 acc = 0\n",
            "pos tensor(0.2579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4863==== Step 1  Train Loss 0.3010154068470001 acc = 0\n",
            "pos tensor(0.1735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4864==== Step 1  Train Loss 0.26655852794647217 acc = 0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4865==== Step 1  Train Loss 0.2699672281742096 acc = 0\n",
            "pos tensor(0.2147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4866==== Step 1  Train Loss 0.27866870164871216 acc = 0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4867==== Step 1  Train Loss 0.25137564539909363 acc = 0\n",
            "pos tensor(0.2260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4868==== Step 1  Train Loss 0.28582119941711426 acc = 0\n",
            "pos tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4869==== Step 1  Train Loss 0.27296730875968933 acc = 0\n",
            "pos tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4870==== Step 1  Train Loss 0.2753618657588959 acc = 0\n",
            "pos tensor(0.2504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4871==== Step 1  Train Loss 0.3030751943588257 acc = 0\n",
            "pos tensor(0.2177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4872==== Step 1  Train Loss 0.2695002257823944 acc = 0\n",
            "pos tensor(0.2058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4873==== Step 1  Train Loss 0.26782870292663574 acc = 0\n",
            "pos tensor(0.1872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4874==== Step 1  Train Loss 0.25977128744125366 acc = 0\n",
            "pos tensor(0.1454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4875==== Step 1  Train Loss 0.2650673985481262 acc = 0\n",
            "pos tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4876==== Step 1  Train Loss 0.28670427203178406 acc = 0\n",
            "pos tensor(0.1550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4877==== Step 1  Train Loss 0.26895907521247864 acc = 0\n",
            "pos tensor(0.1704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4878==== Step 1  Train Loss 0.26347070932388306 acc = 0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4879==== Step 1  Train Loss 0.28565725684165955 acc = 0\n",
            "pos tensor(0.2168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4880==== Step 1  Train Loss 0.2778841555118561 acc = 0\n",
            "pos tensor(0.1676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4881==== Step 1  Train Loss 0.2644276022911072 acc = 0\n",
            "pos tensor(0.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4882==== Step 1  Train Loss 0.28048574924468994 acc = 0\n",
            "pos tensor(0.2419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4883==== Step 1  Train Loss 0.2945227026939392 acc = 0\n",
            "pos tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4884==== Step 1  Train Loss 0.2654914855957031 acc = 0\n",
            "pos tensor(0.2275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4885==== Step 1  Train Loss 0.2977932095527649 acc = 0\n",
            "pos tensor(0.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4886==== Step 1  Train Loss 0.2728492021560669 acc = 0\n",
            "pos tensor(0.2113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4887==== Step 1  Train Loss 0.2802101671695709 acc = 0\n",
            "pos tensor(0.1816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4888==== Step 1  Train Loss 0.2682711184024811 acc = 0\n",
            "pos tensor(0.2228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4889==== Step 1  Train Loss 0.28765150904655457 acc = 0\n",
            "pos tensor(0.2008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4890==== Step 1  Train Loss 0.2974582314491272 acc = 0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4891==== Step 1  Train Loss 0.2728129029273987 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4892==== Step 1  Train Loss 0.27229997515678406 acc = 0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4893==== Step 1  Train Loss 0.2634802460670471 acc = 0\n",
            "pos tensor(0.1973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4894==== Step 1  Train Loss 0.27733945846557617 acc = 0\n",
            "pos tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4895==== Step 1  Train Loss 0.2847585380077362 acc = 0\n",
            "pos tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4896==== Step 1  Train Loss 0.29862532019615173 acc = 0\n",
            "pos tensor(0.2215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4897==== Step 1  Train Loss 0.30418556928634644 acc = 0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4898==== Step 1  Train Loss 0.25566723942756653 acc = 0\n",
            "pos tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4899==== Step 1  Train Loss 0.2671249806880951 acc = 0\n",
            "  Batch 4,900  of  5,561.    Elapsed: 0:17:24.\n",
            "pos tensor(0.1893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4900==== Step 1  Train Loss 0.2849390506744385 acc = 0\n",
            "pos tensor(0.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4901==== Step 1  Train Loss 0.2877950072288513 acc = 0\n",
            "pos tensor(0.2165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4902==== Step 1  Train Loss 0.2929069995880127 acc = 0\n",
            "pos tensor(0.1953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4903==== Step 1  Train Loss 0.2764231562614441 acc = 0\n",
            "pos tensor(0.2040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4904==== Step 1  Train Loss 0.278145968914032 acc = 0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4905==== Step 1  Train Loss 0.2924986481666565 acc = 0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4906==== Step 1  Train Loss 0.25971823930740356 acc = 0\n",
            "pos tensor(0.1940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4907==== Step 1  Train Loss 0.2714383006095886 acc = 0\n",
            "pos tensor(0.1868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4908==== Step 1  Train Loss 0.27502840757369995 acc = 0\n",
            "pos tensor(0.2037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4909==== Step 1  Train Loss 0.27254003286361694 acc = 0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4910==== Step 1  Train Loss 0.2743091881275177 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4911==== Step 1  Train Loss 0.2555084228515625 acc = 0\n",
            "pos tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4912==== Step 1  Train Loss 0.2955230474472046 acc = 0\n",
            "pos tensor(0.1933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4913==== Step 1  Train Loss 0.26415809988975525 acc = 0\n",
            "pos tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4914==== Step 1  Train Loss 0.27864575386047363 acc = 0\n",
            "pos tensor(0.2156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4915==== Step 1  Train Loss 0.27776816487312317 acc = 0\n",
            "pos tensor(0.2324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4916==== Step 1  Train Loss 0.298480361700058 acc = 0\n",
            "pos tensor(0.2627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4917==== Step 1  Train Loss 0.29137086868286133 acc = 0\n",
            "pos tensor(0.1908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4918==== Step 1  Train Loss 0.27698439359664917 acc = 0\n",
            "pos tensor(0.2027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4919==== Step 1  Train Loss 0.2753802537918091 acc = 0\n",
            "pos tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4920==== Step 1  Train Loss 0.2551819980144501 acc = 0\n",
            "pos tensor(0.2415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4921==== Step 1  Train Loss 0.2925577163696289 acc = 0\n",
            "pos tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4922==== Step 1  Train Loss 0.2810846269130707 acc = 0\n",
            "pos tensor(0.2173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4923==== Step 1  Train Loss 0.30370792746543884 acc = 0\n",
            "pos tensor(0.2515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4924==== Step 1  Train Loss 0.2891642451286316 acc = 0\n",
            "pos tensor(0.2183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4925==== Step 1  Train Loss 0.261666476726532 acc = 0\n",
            "pos tensor(0.1620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4926==== Step 1  Train Loss 0.24967941641807556 acc = 0\n",
            "pos tensor(0.2657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4927==== Step 1  Train Loss 0.3051058053970337 acc = 0\n",
            "pos tensor(0.1938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4928==== Step 1  Train Loss 0.26720744371414185 acc = 0\n",
            "pos tensor(0.1646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4929==== Step 1  Train Loss 0.24628841876983643 acc = 0\n",
            "pos tensor(0.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4930==== Step 1  Train Loss 0.2660369277000427 acc = 0\n",
            "pos tensor(0.1743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4931==== Step 1  Train Loss 0.25059592723846436 acc = 0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4932==== Step 1  Train Loss 0.26637083292007446 acc = 0\n",
            "pos tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4933==== Step 1  Train Loss 0.2891503870487213 acc = 0\n",
            "pos tensor(0.2001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4934==== Step 1  Train Loss 0.2613864243030548 acc = 0\n",
            "pos tensor(0.2017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4935==== Step 1  Train Loss 0.26863527297973633 acc = 0\n",
            "pos tensor(0.2291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4936==== Step 1  Train Loss 0.2871198058128357 acc = 0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4937==== Step 1  Train Loss 0.2725190818309784 acc = 0\n",
            "pos tensor(0.2144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4938==== Step 1  Train Loss 0.25993555784225464 acc = 0\n",
            "pos tensor(0.2327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4939==== Step 1  Train Loss 0.2886543869972229 acc = 0\n",
            "pos tensor(0.2064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4940==== Step 1  Train Loss 0.28245556354522705 acc = 0\n",
            "pos tensor(0.2299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4941==== Step 1  Train Loss 0.26653486490249634 acc = 0\n",
            "pos tensor(0.2342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4942==== Step 1  Train Loss 0.28189870715141296 acc = 0\n",
            "pos tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4943==== Step 1  Train Loss 0.26775071024894714 acc = 0\n",
            "pos tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4944==== Step 1  Train Loss 0.2713262140750885 acc = 0\n",
            "pos tensor(0.2048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4945==== Step 1  Train Loss 0.275983989238739 acc = 0\n",
            "pos tensor(0.1573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4946==== Step 1  Train Loss 0.26822569966316223 acc = 0\n",
            "pos tensor(0.2117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4947==== Step 1  Train Loss 0.2747322916984558 acc = 0\n",
            "pos tensor(0.1758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4948==== Step 1  Train Loss 0.2617151737213135 acc = 0\n",
            "pos tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4949==== Step 1  Train Loss 0.2871971130371094 acc = 0\n",
            "pos tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4950==== Step 1  Train Loss 0.2673434317111969 acc = 0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4951==== Step 1  Train Loss 0.27999642491340637 acc = 0\n",
            "pos tensor(0.2258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4952==== Step 1  Train Loss 0.2811200022697449 acc = 0\n",
            "pos tensor(0.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4953==== Step 1  Train Loss 0.29113197326660156 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4954==== Step 1  Train Loss 0.2832280397415161 acc = 0\n",
            "pos tensor(0.1881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4955==== Step 1  Train Loss 0.2869507670402527 acc = 0\n",
            "pos tensor(0.2008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4956==== Step 1  Train Loss 0.2852950692176819 acc = 0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4957==== Step 1  Train Loss 0.2728968858718872 acc = 0\n",
            "pos tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4958==== Step 1  Train Loss 0.27467769384384155 acc = 0\n",
            "pos tensor(0.1857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4959==== Step 1  Train Loss 0.2879134714603424 acc = 0\n",
            "pos tensor(0.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4960==== Step 1  Train Loss 0.2867269515991211 acc = 0\n",
            "pos tensor(0.1580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4961==== Step 1  Train Loss 0.25651267170906067 acc = 0\n",
            "pos tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4962==== Step 1  Train Loss 0.27844417095184326 acc = 0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4963==== Step 1  Train Loss 0.274993896484375 acc = 0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4964==== Step 1  Train Loss 0.27642497420310974 acc = 0\n",
            "pos tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4965==== Step 1  Train Loss 0.28182604908943176 acc = 0\n",
            "pos tensor(0.1846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4966==== Step 1  Train Loss 0.27210062742233276 acc = 0\n",
            "pos tensor(0.2106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4967==== Step 1  Train Loss 0.2979929447174072 acc = 0\n",
            "pos tensor(0.2197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4968==== Step 1  Train Loss 0.28246569633483887 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4969==== Step 1  Train Loss 0.2650587856769562 acc = 0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4970==== Step 1  Train Loss 0.2741740942001343 acc = 0\n",
            "pos tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4971==== Step 1  Train Loss 0.26469868421554565 acc = 0\n",
            "pos tensor(0.1624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4972==== Step 1  Train Loss 0.26070499420166016 acc = 0\n",
            "pos tensor(0.1999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4973==== Step 1  Train Loss 0.26649773120880127 acc = 0\n",
            "pos tensor(0.2081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4974==== Step 1  Train Loss 0.28394418954849243 acc = 0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4975==== Step 1  Train Loss 0.2774946987628937 acc = 0\n",
            "pos tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4976==== Step 1  Train Loss 0.2513798475265503 acc = 0\n",
            "pos tensor(0.2038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4977==== Step 1  Train Loss 0.28605151176452637 acc = 0\n",
            "pos tensor(0.2112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4978==== Step 1  Train Loss 0.2631605267524719 acc = 0\n",
            "pos tensor(0.2056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4979==== Step 1  Train Loss 0.2662433385848999 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4980==== Step 1  Train Loss 0.2856407165527344 acc = 0\n",
            "pos tensor(0.1946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4981==== Step 1  Train Loss 0.2514588236808777 acc = 0\n",
            "pos tensor(0.2348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4982==== Step 1  Train Loss 0.2813969850540161 acc = 0\n",
            "pos tensor(0.2367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4983==== Step 1  Train Loss 0.2923489212989807 acc = 0\n",
            "pos tensor(0.2194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4984==== Step 1  Train Loss 0.2874171733856201 acc = 0\n",
            "pos tensor(0.2139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4985==== Step 1  Train Loss 0.2786676585674286 acc = 0\n",
            "pos tensor(0.1740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4986==== Step 1  Train Loss 0.27302220463752747 acc = 0\n",
            "pos tensor(0.1836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4987==== Step 1  Train Loss 0.282558798789978 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4988==== Step 1  Train Loss 0.2834400534629822 acc = 0\n",
            "pos tensor(0.1955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4989==== Step 1  Train Loss 0.2619965374469757 acc = 0\n",
            "pos tensor(0.1780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4990==== Step 1  Train Loss 0.2689741849899292 acc = 0\n",
            "pos tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4991==== Step 1  Train Loss 0.2810710668563843 acc = 0\n",
            "pos tensor(0.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4992==== Step 1  Train Loss 0.24798665940761566 acc = 0\n",
            "pos tensor(0.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4993==== Step 1  Train Loss 0.29105573892593384 acc = 0\n",
            "pos tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4994==== Step 1  Train Loss 0.25947731733322144 acc = 0\n",
            "pos tensor(0.1782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4995==== Step 1  Train Loss 0.26093265414237976 acc = 0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4996==== Step 1  Train Loss 0.280984103679657 acc = 0\n",
            "pos tensor(0.1741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4997==== Step 1  Train Loss 0.2836739718914032 acc = 0\n",
            "pos tensor(0.1919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4998==== Step 1  Train Loss 0.2672545313835144 acc = 0\n",
            "pos tensor(0.1577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 4999==== Step 1  Train Loss 0.2582782506942749 acc = 0\n",
            "  Batch 5,000  of  5,561.    Elapsed: 0:17:46.\n",
            "pos tensor(0.1737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5000==== Step 1  Train Loss 0.2416015863418579 acc = 0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5001==== Step 1  Train Loss 0.2647380828857422 acc = 0\n",
            "pos tensor(0.2360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5002==== Step 1  Train Loss 0.2979733347892761 acc = 0\n",
            "pos tensor(0.2156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5003==== Step 1  Train Loss 0.260231614112854 acc = 0\n",
            "pos tensor(0.2176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5004==== Step 1  Train Loss 0.28298065066337585 acc = 0\n",
            "pos tensor(0.2214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5005==== Step 1  Train Loss 0.27547240257263184 acc = 0\n",
            "pos tensor(0.2469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5006==== Step 1  Train Loss 0.28265148401260376 acc = 0\n",
            "pos tensor(0.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5007==== Step 1  Train Loss 0.28749749064445496 acc = 0\n",
            "pos tensor(0.1872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5008==== Step 1  Train Loss 0.2723378539085388 acc = 0\n",
            "pos tensor(0.2483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5009==== Step 1  Train Loss 0.2881145775318146 acc = 0\n",
            "pos tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5010==== Step 1  Train Loss 0.242591992020607 acc = 0\n",
            "pos tensor(0.2176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5011==== Step 1  Train Loss 0.2880309820175171 acc = 0\n",
            "pos tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5012==== Step 1  Train Loss 0.27221646904945374 acc = 0\n",
            "pos tensor(0.1911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5013==== Step 1  Train Loss 0.2644643783569336 acc = 0\n",
            "pos tensor(0.2854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5014==== Step 1  Train Loss 0.29597264528274536 acc = 0\n",
            "pos tensor(0.2400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5015==== Step 1  Train Loss 0.2983091473579407 acc = 0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5016==== Step 1  Train Loss 0.2624691128730774 acc = 0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5017==== Step 1  Train Loss 0.27984902262687683 acc = 0\n",
            "pos tensor(0.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5018==== Step 1  Train Loss 0.27623119950294495 acc = 0\n",
            "pos tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5019==== Step 1  Train Loss 0.25229012966156006 acc = 0\n",
            "pos tensor(0.1951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5020==== Step 1  Train Loss 0.27353358268737793 acc = 0\n",
            "pos tensor(0.2493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5021==== Step 1  Train Loss 0.29218214750289917 acc = 0\n",
            "pos tensor(0.2262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5022==== Step 1  Train Loss 0.27290672063827515 acc = 0\n",
            "pos tensor(0.2046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5023==== Step 1  Train Loss 0.2866923213005066 acc = 0\n",
            "pos tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5024==== Step 1  Train Loss 0.27187350392341614 acc = 0\n",
            "pos tensor(0.2351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5025==== Step 1  Train Loss 0.2760443091392517 acc = 0\n",
            "pos tensor(0.1741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5026==== Step 1  Train Loss 0.2751629054546356 acc = 0\n",
            "pos tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5027==== Step 1  Train Loss 0.27495500445365906 acc = 0\n",
            "pos tensor(0.2440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5028==== Step 1  Train Loss 0.2823750078678131 acc = 0\n",
            "pos tensor(0.2179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5029==== Step 1  Train Loss 0.2635713219642639 acc = 0\n",
            "pos tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5030==== Step 1  Train Loss 0.26895686984062195 acc = 0\n",
            "pos tensor(0.1882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5031==== Step 1  Train Loss 0.2781417965888977 acc = 0\n",
            "pos tensor(0.1565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5032==== Step 1  Train Loss 0.2573986053466797 acc = 0\n",
            "pos tensor(0.1729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5033==== Step 1  Train Loss 0.2838635742664337 acc = 0\n",
            "pos tensor(0.1780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5034==== Step 1  Train Loss 0.2755075693130493 acc = 0\n",
            "pos tensor(0.1814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5035==== Step 1  Train Loss 0.2505679130554199 acc = 0\n",
            "pos tensor(0.1411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5036==== Step 1  Train Loss 0.253277450799942 acc = 0\n",
            "pos tensor(0.2110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5037==== Step 1  Train Loss 0.2759973406791687 acc = 0\n",
            "pos tensor(0.2121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3413, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5038==== Step 1  Train Loss 0.276677668094635 acc = 0\n",
            "pos tensor(0.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5039==== Step 1  Train Loss 0.2422899305820465 acc = 0\n",
            "pos tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5040==== Step 1  Train Loss 0.2872377932071686 acc = 0\n",
            "pos tensor(0.1649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5041==== Step 1  Train Loss 0.252999871969223 acc = 0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5042==== Step 1  Train Loss 0.26683053374290466 acc = 0\n",
            "pos tensor(0.2072, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5043==== Step 1  Train Loss 0.2677001357078552 acc = 0\n",
            "pos tensor(0.2417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5044==== Step 1  Train Loss 0.30211418867111206 acc = 0\n",
            "pos tensor(0.1936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5045==== Step 1  Train Loss 0.26036158204078674 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5046==== Step 1  Train Loss 0.27548083662986755 acc = 0\n",
            "pos tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5047==== Step 1  Train Loss 0.27189013361930847 acc = 0\n",
            "pos tensor(0.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5048==== Step 1  Train Loss 0.29040661454200745 acc = 0\n",
            "pos tensor(0.2063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5049==== Step 1  Train Loss 0.2709891200065613 acc = 0\n",
            "pos tensor(0.2546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5050==== Step 1  Train Loss 0.3104376792907715 acc = 0\n",
            "pos tensor(0.2596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5051==== Step 1  Train Loss 0.29273325204849243 acc = 0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5052==== Step 1  Train Loss 0.26235702633857727 acc = 0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5053==== Step 1  Train Loss 0.24929295480251312 acc = 0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5054==== Step 1  Train Loss 0.2774827480316162 acc = 0\n",
            "pos tensor(0.2589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5055==== Step 1  Train Loss 0.2876178026199341 acc = 0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5056==== Step 1  Train Loss 0.2554095685482025 acc = 0\n",
            "pos tensor(0.2556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5057==== Step 1  Train Loss 0.2847687005996704 acc = 0\n",
            "pos tensor(0.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5058==== Step 1  Train Loss 0.2902330756187439 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5059==== Step 1  Train Loss 0.2725493609905243 acc = 0\n",
            "pos tensor(0.2182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5060==== Step 1  Train Loss 0.2792208194732666 acc = 0\n",
            "pos tensor(0.2097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5061==== Step 1  Train Loss 0.26740777492523193 acc = 0\n",
            "pos tensor(0.2652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5062==== Step 1  Train Loss 0.29095837473869324 acc = 0\n",
            "pos tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5063==== Step 1  Train Loss 0.2748856544494629 acc = 0\n",
            "pos tensor(0.2432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5064==== Step 1  Train Loss 0.2524648904800415 acc = 0\n",
            "pos tensor(0.1936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5065==== Step 1  Train Loss 0.27793967723846436 acc = 0\n",
            "pos tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5066==== Step 1  Train Loss 0.27780717611312866 acc = 0\n",
            "pos tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5067==== Step 1  Train Loss 0.2772084176540375 acc = 0\n",
            "pos tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5068==== Step 1  Train Loss 0.2778269052505493 acc = 0\n",
            "pos tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5069==== Step 1  Train Loss 0.2896762788295746 acc = 0\n",
            "pos tensor(0.2512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5070==== Step 1  Train Loss 0.28973570466041565 acc = 0\n",
            "pos tensor(0.2596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5071==== Step 1  Train Loss 0.30024564266204834 acc = 0\n",
            "pos tensor(0.2203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5072==== Step 1  Train Loss 0.2775490880012512 acc = 0\n",
            "pos tensor(0.2125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5073==== Step 1  Train Loss 0.2896133065223694 acc = 0\n",
            "pos tensor(0.1746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5074==== Step 1  Train Loss 0.2793271541595459 acc = 0\n",
            "pos tensor(0.2022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5075==== Step 1  Train Loss 0.25663402676582336 acc = 0\n",
            "pos tensor(0.1718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5076==== Step 1  Train Loss 0.2528380751609802 acc = 0\n",
            "pos tensor(0.1628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5077==== Step 1  Train Loss 0.25561511516571045 acc = 0\n",
            "pos tensor(0.2330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5078==== Step 1  Train Loss 0.30380499362945557 acc = 0\n",
            "pos tensor(0.1675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5079==== Step 1  Train Loss 0.24987924098968506 acc = 0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5080==== Step 1  Train Loss 0.2709241509437561 acc = 0\n",
            "pos tensor(0.2539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5081==== Step 1  Train Loss 0.3054496645927429 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5082==== Step 1  Train Loss 0.2869318425655365 acc = 0\n",
            "pos tensor(0.2114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5083==== Step 1  Train Loss 0.2916393280029297 acc = 0\n",
            "pos tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5084==== Step 1  Train Loss 0.2613045275211334 acc = 0\n",
            "pos tensor(0.2168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5085==== Step 1  Train Loss 0.28766387701034546 acc = 0\n",
            "pos tensor(0.1902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5086==== Step 1  Train Loss 0.2734748125076294 acc = 0\n",
            "pos tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5087==== Step 1  Train Loss 0.27371370792388916 acc = 0\n",
            "pos tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5088==== Step 1  Train Loss 0.2796521484851837 acc = 0\n",
            "pos tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5089==== Step 1  Train Loss 0.24094343185424805 acc = 0\n",
            "pos tensor(0.1959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5090==== Step 1  Train Loss 0.2506542205810547 acc = 0\n",
            "pos tensor(0.2458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5091==== Step 1  Train Loss 0.30745336413383484 acc = 0\n",
            "pos tensor(0.2408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5092==== Step 1  Train Loss 0.30477583408355713 acc = 0\n",
            "pos tensor(0.1949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5093==== Step 1  Train Loss 0.2683545649051666 acc = 0\n",
            "pos tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5094==== Step 1  Train Loss 0.25897568464279175 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5095==== Step 1  Train Loss 0.27470746636390686 acc = 0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5096==== Step 1  Train Loss 0.2885857820510864 acc = 0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5097==== Step 1  Train Loss 0.274436891078949 acc = 0\n",
            "pos tensor(0.2055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5098==== Step 1  Train Loss 0.2879634499549866 acc = 0\n",
            "pos tensor(0.1870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5099==== Step 1  Train Loss 0.27128952741622925 acc = 0\n",
            "  Batch 5,100  of  5,561.    Elapsed: 0:18:07.\n",
            "pos tensor(0.2333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5100==== Step 1  Train Loss 0.29411476850509644 acc = 0\n",
            "pos tensor(0.1804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5101==== Step 1  Train Loss 0.2550326883792877 acc = 0\n",
            "pos tensor(0.2189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5102==== Step 1  Train Loss 0.28155800700187683 acc = 0\n",
            "pos tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5103==== Step 1  Train Loss 0.2526548504829407 acc = 0\n",
            "pos tensor(0.2373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5104==== Step 1  Train Loss 0.29071396589279175 acc = 0\n",
            "pos tensor(0.1450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5105==== Step 1  Train Loss 0.24816575646400452 acc = 0\n",
            "pos tensor(0.1956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5106==== Step 1  Train Loss 0.27467653155326843 acc = 0\n",
            "pos tensor(0.1402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5107==== Step 1  Train Loss 0.2847868800163269 acc = 0\n",
            "pos tensor(0.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5108==== Step 1  Train Loss 0.2821215093135834 acc = 0\n",
            "pos tensor(0.1827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3803, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5109==== Step 1  Train Loss 0.2815290093421936 acc = 0\n",
            "pos tensor(0.1886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5110==== Step 1  Train Loss 0.3078022003173828 acc = 0\n",
            "pos tensor(0.1995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5111==== Step 1  Train Loss 0.2761295437812805 acc = 0\n",
            "pos tensor(0.1464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5112==== Step 1  Train Loss 0.2611517906188965 acc = 0\n",
            "pos tensor(0.1599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5113==== Step 1  Train Loss 0.2457318902015686 acc = 0\n",
            "pos tensor(0.1940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5114==== Step 1  Train Loss 0.26119476556777954 acc = 0\n",
            "pos tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5115==== Step 1  Train Loss 0.28589245676994324 acc = 0\n",
            "pos tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5116==== Step 1  Train Loss 0.2840031385421753 acc = 0\n",
            "pos tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5117==== Step 1  Train Loss 0.27952685952186584 acc = 0\n",
            "pos tensor(0.2246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5118==== Step 1  Train Loss 0.28142091631889343 acc = 0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5119==== Step 1  Train Loss 0.2760853171348572 acc = 0\n",
            "pos tensor(0.2341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5120==== Step 1  Train Loss 0.29854822158813477 acc = 0\n",
            "pos tensor(0.2428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5121==== Step 1  Train Loss 0.2873494625091553 acc = 0\n",
            "pos tensor(0.2123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5122==== Step 1  Train Loss 0.2690868079662323 acc = 0\n",
            "pos tensor(0.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5123==== Step 1  Train Loss 0.2806800901889801 acc = 0\n",
            "pos tensor(0.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5124==== Step 1  Train Loss 0.2756956219673157 acc = 0\n",
            "pos tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5125==== Step 1  Train Loss 0.2934691309928894 acc = 0\n",
            "pos tensor(0.2212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5126==== Step 1  Train Loss 0.3006361722946167 acc = 0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5127==== Step 1  Train Loss 0.2668834924697876 acc = 0\n",
            "pos tensor(0.1764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5128==== Step 1  Train Loss 0.2671689987182617 acc = 0\n",
            "pos tensor(0.1849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5129==== Step 1  Train Loss 0.2749161124229431 acc = 0\n",
            "pos tensor(0.2109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5130==== Step 1  Train Loss 0.2800315022468567 acc = 0\n",
            "pos tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5131==== Step 1  Train Loss 0.27555716037750244 acc = 0\n",
            "pos tensor(0.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5132==== Step 1  Train Loss 0.2902945280075073 acc = 0\n",
            "pos tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5133==== Step 1  Train Loss 0.2640261650085449 acc = 0\n",
            "pos tensor(0.1768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5134==== Step 1  Train Loss 0.2701771557331085 acc = 0\n",
            "pos tensor(0.1893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5135==== Step 1  Train Loss 0.2572758197784424 acc = 0\n",
            "pos tensor(0.2213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5136==== Step 1  Train Loss 0.2956106662750244 acc = 0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5137==== Step 1  Train Loss 0.2767719328403473 acc = 0\n",
            "pos tensor(0.1520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5138==== Step 1  Train Loss 0.24642813205718994 acc = 0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5139==== Step 1  Train Loss 0.2767738699913025 acc = 0\n",
            "pos tensor(0.2229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5140==== Step 1  Train Loss 0.30033814907073975 acc = 0\n",
            "pos tensor(0.2035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5141==== Step 1  Train Loss 0.28176432847976685 acc = 0\n",
            "pos tensor(0.2325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5142==== Step 1  Train Loss 0.28217577934265137 acc = 0\n",
            "pos tensor(0.2131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5143==== Step 1  Train Loss 0.25604403018951416 acc = 0\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5144==== Step 1  Train Loss 0.27552056312561035 acc = 0\n",
            "pos tensor(0.2124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5145==== Step 1  Train Loss 0.2753468155860901 acc = 0\n",
            "pos tensor(0.2219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5146==== Step 1  Train Loss 0.2910955548286438 acc = 0\n",
            "pos tensor(0.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5147==== Step 1  Train Loss 0.278744637966156 acc = 0\n",
            "pos tensor(0.1673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5148==== Step 1  Train Loss 0.24945040047168732 acc = 0\n",
            "pos tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5149==== Step 1  Train Loss 0.26053985953330994 acc = 0\n",
            "pos tensor(0.1763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5150==== Step 1  Train Loss 0.26711416244506836 acc = 0\n",
            "pos tensor(0.2246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5151==== Step 1  Train Loss 0.2805003821849823 acc = 0\n",
            "pos tensor(0.2127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5152==== Step 1  Train Loss 0.27955105900764465 acc = 0\n",
            "pos tensor(0.2305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5153==== Step 1  Train Loss 0.2766675353050232 acc = 0\n",
            "pos tensor(0.1961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5154==== Step 1  Train Loss 0.2642885148525238 acc = 0\n",
            "pos tensor(0.2364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5155==== Step 1  Train Loss 0.3026139438152313 acc = 0\n",
            "pos tensor(0.2265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5156==== Step 1  Train Loss 0.2881617546081543 acc = 0\n",
            "pos tensor(0.2082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5157==== Step 1  Train Loss 0.2807081341743469 acc = 0\n",
            "pos tensor(0.1786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5158==== Step 1  Train Loss 0.25322824716567993 acc = 0\n",
            "pos tensor(0.2000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5159==== Step 1  Train Loss 0.2736494541168213 acc = 0\n",
            "pos tensor(0.1870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5160==== Step 1  Train Loss 0.2648453712463379 acc = 0\n",
            "pos tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5161==== Step 1  Train Loss 0.24700011312961578 acc = 0\n",
            "pos tensor(0.2354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5162==== Step 1  Train Loss 0.2707964777946472 acc = 0\n",
            "pos tensor(0.2159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5163==== Step 1  Train Loss 0.2621764540672302 acc = 0\n",
            "pos tensor(0.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5164==== Step 1  Train Loss 0.2631314992904663 acc = 0\n",
            "pos tensor(0.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5165==== Step 1  Train Loss 0.2891085147857666 acc = 0\n",
            "pos tensor(0.2187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5166==== Step 1  Train Loss 0.2635686993598938 acc = 0\n",
            "pos tensor(0.1844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5167==== Step 1  Train Loss 0.2631819546222687 acc = 0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5168==== Step 1  Train Loss 0.25324100255966187 acc = 0\n",
            "pos tensor(0.2024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5169==== Step 1  Train Loss 0.2723467946052551 acc = 0\n",
            "pos tensor(0.2522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5170==== Step 1  Train Loss 0.2782805562019348 acc = 0\n",
            "pos tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5171==== Step 1  Train Loss 0.2657904624938965 acc = 0\n",
            "pos tensor(0.1660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5172==== Step 1  Train Loss 0.25930318236351013 acc = 0\n",
            "pos tensor(0.1807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5173==== Step 1  Train Loss 0.24865451455116272 acc = 0\n",
            "pos tensor(0.2084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3462, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5174==== Step 1  Train Loss 0.27727270126342773 acc = 0\n",
            "pos tensor(0.2274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5175==== Step 1  Train Loss 0.29435020685195923 acc = 0\n",
            "pos tensor(0.2412, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5176==== Step 1  Train Loss 0.27149054408073425 acc = 0\n",
            "pos tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5177==== Step 1  Train Loss 0.27375268936157227 acc = 0\n",
            "pos tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5178==== Step 1  Train Loss 0.2772907018661499 acc = 0\n",
            "pos tensor(0.2387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5179==== Step 1  Train Loss 0.28328847885131836 acc = 0\n",
            "pos tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5180==== Step 1  Train Loss 0.2765549421310425 acc = 0\n",
            "pos tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5181==== Step 1  Train Loss 0.29199182987213135 acc = 0\n",
            "pos tensor(0.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5182==== Step 1  Train Loss 0.2844027876853943 acc = 0\n",
            "pos tensor(0.1547, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5183==== Step 1  Train Loss 0.26155439019203186 acc = 0\n",
            "pos tensor(0.2133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5184==== Step 1  Train Loss 0.2838556468486786 acc = 0\n",
            "pos tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5185==== Step 1  Train Loss 0.27431121468544006 acc = 0\n",
            "pos tensor(0.1669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5186==== Step 1  Train Loss 0.2617150545120239 acc = 0\n",
            "pos tensor(0.1352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5187==== Step 1  Train Loss 0.23507335782051086 acc = 0\n",
            "pos tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5188==== Step 1  Train Loss 0.2775379419326782 acc = 0\n",
            "pos tensor(0.2016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5189==== Step 1  Train Loss 0.277568519115448 acc = 0\n",
            "pos tensor(0.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5190==== Step 1  Train Loss 0.27432695031166077 acc = 0\n",
            "pos tensor(0.2087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5191==== Step 1  Train Loss 0.2844170033931732 acc = 0\n",
            "pos tensor(0.2182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5192==== Step 1  Train Loss 0.3010886311531067 acc = 0\n",
            "pos tensor(0.1947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5193==== Step 1  Train Loss 0.29489946365356445 acc = 0\n",
            "pos tensor(0.2057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5194==== Step 1  Train Loss 0.2662579119205475 acc = 0\n",
            "pos tensor(0.1724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5195==== Step 1  Train Loss 0.26845160126686096 acc = 0\n",
            "pos tensor(0.1553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3483, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5196==== Step 1  Train Loss 0.251801460981369 acc = 0\n",
            "pos tensor(0.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5197==== Step 1  Train Loss 0.3001428246498108 acc = 0\n",
            "pos tensor(0.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5198==== Step 1  Train Loss 0.27106624841690063 acc = 0\n",
            "pos tensor(0.2009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5199==== Step 1  Train Loss 0.26464271545410156 acc = 0\n",
            "  Batch 5,200  of  5,561.    Elapsed: 0:18:28.\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5200==== Step 1  Train Loss 0.256632536649704 acc = 0\n",
            "pos tensor(0.2091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5201==== Step 1  Train Loss 0.29128938913345337 acc = 0\n",
            "pos tensor(0.2078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5202==== Step 1  Train Loss 0.2729731798171997 acc = 0\n",
            "pos tensor(0.2061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5203==== Step 1  Train Loss 0.26479053497314453 acc = 0\n",
            "pos tensor(0.1690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5204==== Step 1  Train Loss 0.2535911798477173 acc = 0\n",
            "pos tensor(0.2310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5205==== Step 1  Train Loss 0.29313528537750244 acc = 0\n",
            "pos tensor(0.2011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5206==== Step 1  Train Loss 0.2826688587665558 acc = 0\n",
            "pos tensor(0.2478, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5207==== Step 1  Train Loss 0.2870226502418518 acc = 0\n",
            "pos tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5208==== Step 1  Train Loss 0.26846009492874146 acc = 0\n",
            "pos tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5209==== Step 1  Train Loss 0.2503277063369751 acc = 0\n",
            "pos tensor(0.1599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5210==== Step 1  Train Loss 0.26091641187667847 acc = 0\n",
            "pos tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5211==== Step 1  Train Loss 0.28511103987693787 acc = 0\n",
            "pos tensor(0.1881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5212==== Step 1  Train Loss 0.2665935158729553 acc = 0\n",
            "pos tensor(0.1701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5213==== Step 1  Train Loss 0.25142693519592285 acc = 0\n",
            "pos tensor(0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5214==== Step 1  Train Loss 0.24340523779392242 acc = 0\n",
            "pos tensor(0.2149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5215==== Step 1  Train Loss 0.2852323055267334 acc = 0\n",
            "pos tensor(0.2069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5216==== Step 1  Train Loss 0.28135383129119873 acc = 0\n",
            "pos tensor(0.2568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5217==== Step 1  Train Loss 0.3081786036491394 acc = 0\n",
            "pos tensor(0.2126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5218==== Step 1  Train Loss 0.2800349295139313 acc = 0\n",
            "pos tensor(0.2145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5219==== Step 1  Train Loss 0.2753864824771881 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5220==== Step 1  Train Loss 0.2701681852340698 acc = 0\n",
            "pos tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5221==== Step 1  Train Loss 0.25922757387161255 acc = 0\n",
            "pos tensor(0.2677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5222==== Step 1  Train Loss 0.29520314931869507 acc = 0\n",
            "pos tensor(0.1821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5223==== Step 1  Train Loss 0.25522488355636597 acc = 0\n",
            "pos tensor(0.1690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5224==== Step 1  Train Loss 0.2470865249633789 acc = 0\n",
            "pos tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5225==== Step 1  Train Loss 0.26739293336868286 acc = 0\n",
            "pos tensor(0.1644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5226==== Step 1  Train Loss 0.26193487644195557 acc = 0\n",
            "pos tensor(0.2031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5227==== Step 1  Train Loss 0.2813240885734558 acc = 0\n",
            "pos tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5228==== Step 1  Train Loss 0.27486538887023926 acc = 0\n",
            "pos tensor(0.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5229==== Step 1  Train Loss 0.24960052967071533 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5230==== Step 1  Train Loss 0.26452121138572693 acc = 0\n",
            "pos tensor(0.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5231==== Step 1  Train Loss 0.2789149284362793 acc = 0\n",
            "pos tensor(0.2611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5232==== Step 1  Train Loss 0.28032201528549194 acc = 0\n",
            "pos tensor(0.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5233==== Step 1  Train Loss 0.2721145749092102 acc = 0\n",
            "pos tensor(0.2465, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5234==== Step 1  Train Loss 0.291504442691803 acc = 0\n",
            "pos tensor(0.2734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5235==== Step 1  Train Loss 0.28345584869384766 acc = 0\n",
            "pos tensor(0.2263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5236==== Step 1  Train Loss 0.2792871594429016 acc = 0\n",
            "pos tensor(0.2222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5237==== Step 1  Train Loss 0.2838021218776703 acc = 0\n",
            "pos tensor(0.1906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5238==== Step 1  Train Loss 0.25441768765449524 acc = 0\n",
            "pos tensor(0.2209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5239==== Step 1  Train Loss 0.2779536247253418 acc = 0\n",
            "pos tensor(0.2355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5240==== Step 1  Train Loss 0.2891216576099396 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5241==== Step 1  Train Loss 0.26271671056747437 acc = 0\n",
            "pos tensor(0.1652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5242==== Step 1  Train Loss 0.25113359093666077 acc = 0\n",
            "pos tensor(0.2461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5243==== Step 1  Train Loss 0.27762681245803833 acc = 0\n",
            "pos tensor(0.2144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5244==== Step 1  Train Loss 0.27572101354599 acc = 0\n",
            "pos tensor(0.1356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5245==== Step 1  Train Loss 0.242875874042511 acc = 0\n",
            "pos tensor(0.1838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5246==== Step 1  Train Loss 0.2783049941062927 acc = 0\n",
            "pos tensor(0.1631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5247==== Step 1  Train Loss 0.26745402812957764 acc = 0\n",
            "pos tensor(0.1769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5248==== Step 1  Train Loss 0.27489665150642395 acc = 0\n",
            "pos tensor(0.2262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5249==== Step 1  Train Loss 0.2814663350582123 acc = 0\n",
            "pos tensor(0.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5250==== Step 1  Train Loss 0.29124531149864197 acc = 0\n",
            "pos tensor(0.1526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5251==== Step 1  Train Loss 0.2747045159339905 acc = 0\n",
            "pos tensor(0.2061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5252==== Step 1  Train Loss 0.2746136486530304 acc = 0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5253==== Step 1  Train Loss 0.2919032573699951 acc = 0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5254==== Step 1  Train Loss 0.2707098126411438 acc = 0\n",
            "pos tensor(0.2101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5255==== Step 1  Train Loss 0.2639108896255493 acc = 0\n",
            "pos tensor(0.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5256==== Step 1  Train Loss 0.26577624678611755 acc = 0\n",
            "pos tensor(0.1898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5257==== Step 1  Train Loss 0.2725965082645416 acc = 0\n",
            "pos tensor(0.1981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5258==== Step 1  Train Loss 0.2771770656108856 acc = 0\n",
            "pos tensor(0.1875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5259==== Step 1  Train Loss 0.2862260937690735 acc = 0\n",
            "pos tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5260==== Step 1  Train Loss 0.2668110728263855 acc = 0\n",
            "pos tensor(0.2123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5261==== Step 1  Train Loss 0.2769600749015808 acc = 0\n",
            "pos tensor(0.2180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5262==== Step 1  Train Loss 0.25831690430641174 acc = 0\n",
            "pos tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5263==== Step 1  Train Loss 0.29177534580230713 acc = 0\n",
            "pos tensor(0.1825, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5264==== Step 1  Train Loss 0.252204954624176 acc = 0\n",
            "pos tensor(0.2214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5265==== Step 1  Train Loss 0.29569002985954285 acc = 0\n",
            "pos tensor(0.2026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5266==== Step 1  Train Loss 0.27287155389785767 acc = 0\n",
            "pos tensor(0.2323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5267==== Step 1  Train Loss 0.28120946884155273 acc = 0\n",
            "pos tensor(0.1969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5268==== Step 1  Train Loss 0.2836883068084717 acc = 0\n",
            "pos tensor(0.1834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5269==== Step 1  Train Loss 0.2682695984840393 acc = 0\n",
            "pos tensor(0.2039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5270==== Step 1  Train Loss 0.26537925004959106 acc = 0\n",
            "pos tensor(0.2497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5271==== Step 1  Train Loss 0.29302358627319336 acc = 0\n",
            "pos tensor(0.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5272==== Step 1  Train Loss 0.26873642206192017 acc = 0\n",
            "pos tensor(0.1533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5273==== Step 1  Train Loss 0.2552899718284607 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5274==== Step 1  Train Loss 0.2736988663673401 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5275==== Step 1  Train Loss 0.27139750123023987 acc = 0\n",
            "pos tensor(0.1468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5276==== Step 1  Train Loss 0.24475109577178955 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5277==== Step 1  Train Loss 0.27110403776168823 acc = 0\n",
            "pos tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5278==== Step 1  Train Loss 0.2737146019935608 acc = 0\n",
            "pos tensor(0.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5279==== Step 1  Train Loss 0.27708858251571655 acc = 0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5280==== Step 1  Train Loss 0.2583466172218323 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5281==== Step 1  Train Loss 0.29073435068130493 acc = 0\n",
            "pos tensor(0.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5282==== Step 1  Train Loss 0.25004929304122925 acc = 0\n",
            "pos tensor(0.1669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5283==== Step 1  Train Loss 0.25156253576278687 acc = 0\n",
            "pos tensor(0.2567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5284==== Step 1  Train Loss 0.2964921295642853 acc = 0\n",
            "pos tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5285==== Step 1  Train Loss 0.2527983486652374 acc = 0\n",
            "pos tensor(0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5286==== Step 1  Train Loss 0.25310733914375305 acc = 0\n",
            "pos tensor(0.2159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5287==== Step 1  Train Loss 0.2765324115753174 acc = 0\n",
            "pos tensor(0.1938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5288==== Step 1  Train Loss 0.278328537940979 acc = 0\n",
            "pos tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5289==== Step 1  Train Loss 0.2640359103679657 acc = 0\n",
            "pos tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5290==== Step 1  Train Loss 0.26825985312461853 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5291==== Step 1  Train Loss 0.2623305916786194 acc = 0\n",
            "pos tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5292==== Step 1  Train Loss 0.2561764717102051 acc = 0\n",
            "pos tensor(0.2017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5293==== Step 1  Train Loss 0.2731836140155792 acc = 0\n",
            "pos tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5294==== Step 1  Train Loss 0.2767900824546814 acc = 0\n",
            "pos tensor(0.2265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5295==== Step 1  Train Loss 0.2807319164276123 acc = 0\n",
            "pos tensor(0.1841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5296==== Step 1  Train Loss 0.2540571689605713 acc = 0\n",
            "pos tensor(0.2451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5297==== Step 1  Train Loss 0.2802160382270813 acc = 0\n",
            "pos tensor(0.2357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5298==== Step 1  Train Loss 0.2875485420227051 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5299==== Step 1  Train Loss 0.28648290038108826 acc = 0\n",
            "  Batch 5,300  of  5,561.    Elapsed: 0:18:50.\n",
            "pos tensor(0.2011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5300==== Step 1  Train Loss 0.2809433043003082 acc = 0\n",
            "pos tensor(0.1748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5301==== Step 1  Train Loss 0.2642328143119812 acc = 0\n",
            "pos tensor(0.2689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5302==== Step 1  Train Loss 0.29663705825805664 acc = 0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5303==== Step 1  Train Loss 0.2613152265548706 acc = 0\n",
            "pos tensor(0.1990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5304==== Step 1  Train Loss 0.25747787952423096 acc = 0\n",
            "pos tensor(0.2073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5305==== Step 1  Train Loss 0.2782190442085266 acc = 0\n",
            "pos tensor(0.2537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5306==== Step 1  Train Loss 0.28964510560035706 acc = 0\n",
            "pos tensor(0.1846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5307==== Step 1  Train Loss 0.27068284153938293 acc = 0\n",
            "pos tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5308==== Step 1  Train Loss 0.26331397891044617 acc = 0\n",
            "pos tensor(0.1541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5309==== Step 1  Train Loss 0.27157270908355713 acc = 0\n",
            "pos tensor(0.1505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5310==== Step 1  Train Loss 0.24435240030288696 acc = 0\n",
            "pos tensor(0.1829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5311==== Step 1  Train Loss 0.2751568555831909 acc = 0\n",
            "pos tensor(0.2142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5312==== Step 1  Train Loss 0.2969300150871277 acc = 0\n",
            "pos tensor(0.2038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5313==== Step 1  Train Loss 0.2946535348892212 acc = 0\n",
            "pos tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5314==== Step 1  Train Loss 0.282597154378891 acc = 0\n",
            "pos tensor(0.2133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5315==== Step 1  Train Loss 0.28904345631599426 acc = 0\n",
            "pos tensor(0.1793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5316==== Step 1  Train Loss 0.28071683645248413 acc = 0\n",
            "pos tensor(0.1727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5317==== Step 1  Train Loss 0.2676953077316284 acc = 0\n",
            "pos tensor(0.1896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5318==== Step 1  Train Loss 0.26021021604537964 acc = 0\n",
            "pos tensor(0.2141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5319==== Step 1  Train Loss 0.2871917188167572 acc = 0\n",
            "pos tensor(0.2104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5320==== Step 1  Train Loss 0.2832597494125366 acc = 0\n",
            "pos tensor(0.2080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5321==== Step 1  Train Loss 0.2811526656150818 acc = 0\n",
            "pos tensor(0.1884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5322==== Step 1  Train Loss 0.2667279839515686 acc = 0\n",
            "pos tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5323==== Step 1  Train Loss 0.2496349960565567 acc = 0\n",
            "pos tensor(0.2041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5324==== Step 1  Train Loss 0.2696112394332886 acc = 0\n",
            "pos tensor(0.2330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5325==== Step 1  Train Loss 0.28303125500679016 acc = 0\n",
            "pos tensor(0.2263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5326==== Step 1  Train Loss 0.2867238223552704 acc = 0\n",
            "pos tensor(0.1893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5327==== Step 1  Train Loss 0.26583582162857056 acc = 0\n",
            "pos tensor(0.1856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5328==== Step 1  Train Loss 0.2709922194480896 acc = 0\n",
            "pos tensor(0.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5329==== Step 1  Train Loss 0.26631197333335876 acc = 0\n",
            "pos tensor(0.1551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5330==== Step 1  Train Loss 0.2510564625263214 acc = 0\n",
            "pos tensor(0.1950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5331==== Step 1  Train Loss 0.2672784924507141 acc = 0\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5332==== Step 1  Train Loss 0.27473533153533936 acc = 0\n",
            "pos tensor(0.1731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5333==== Step 1  Train Loss 0.2588028609752655 acc = 0\n",
            "pos tensor(0.2169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5334==== Step 1  Train Loss 0.2804027795791626 acc = 0\n",
            "pos tensor(0.1962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5335==== Step 1  Train Loss 0.26559022068977356 acc = 0\n",
            "pos tensor(0.2407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5336==== Step 1  Train Loss 0.2859252393245697 acc = 0\n",
            "pos tensor(0.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5337==== Step 1  Train Loss 0.2662346065044403 acc = 0\n",
            "pos tensor(0.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5338==== Step 1  Train Loss 0.25814568996429443 acc = 0\n",
            "pos tensor(0.1927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5339==== Step 1  Train Loss 0.2778988480567932 acc = 0\n",
            "pos tensor(0.1775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5340==== Step 1  Train Loss 0.2706369161605835 acc = 0\n",
            "pos tensor(0.2043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5341==== Step 1  Train Loss 0.2822457551956177 acc = 0\n",
            "pos tensor(0.2670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5342==== Step 1  Train Loss 0.30063146352767944 acc = 0\n",
            "pos tensor(0.2281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5343==== Step 1  Train Loss 0.28325504064559937 acc = 0\n",
            "pos tensor(0.1896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5344==== Step 1  Train Loss 0.26068314909935 acc = 0\n",
            "pos tensor(0.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5345==== Step 1  Train Loss 0.27325761318206787 acc = 0\n",
            "pos tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5346==== Step 1  Train Loss 0.24527084827423096 acc = 0\n",
            "pos tensor(0.2372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5347==== Step 1  Train Loss 0.29323405027389526 acc = 0\n",
            "pos tensor(0.1718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5348==== Step 1  Train Loss 0.27827298641204834 acc = 0\n",
            "pos tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5349==== Step 1  Train Loss 0.2574988007545471 acc = 0\n",
            "pos tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5350==== Step 1  Train Loss 0.2835225462913513 acc = 0\n",
            "pos tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5351==== Step 1  Train Loss 0.26715362071990967 acc = 0\n",
            "pos tensor(0.2392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3423, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5352==== Step 1  Train Loss 0.290749728679657 acc = 0\n",
            "pos tensor(0.2111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5353==== Step 1  Train Loss 0.26839473843574524 acc = 0\n",
            "pos tensor(0.2317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5354==== Step 1  Train Loss 0.28544363379478455 acc = 0\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5355==== Step 1  Train Loss 0.26044219732284546 acc = 0\n",
            "pos tensor(0.2152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5356==== Step 1  Train Loss 0.2755259871482849 acc = 0\n",
            "pos tensor(0.2003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5357==== Step 1  Train Loss 0.2737789750099182 acc = 0\n",
            "pos tensor(0.2071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5358==== Step 1  Train Loss 0.2634417414665222 acc = 0\n",
            "pos tensor(0.2292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5359==== Step 1  Train Loss 0.27735692262649536 acc = 0\n",
            "pos tensor(0.2085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5360==== Step 1  Train Loss 0.2628743648529053 acc = 0\n",
            "pos tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5361==== Step 1  Train Loss 0.291380375623703 acc = 0\n",
            "pos tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5362==== Step 1  Train Loss 0.25829923152923584 acc = 0\n",
            "pos tensor(0.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5363==== Step 1  Train Loss 0.27849698066711426 acc = 0\n",
            "pos tensor(0.1961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5364==== Step 1  Train Loss 0.2739795446395874 acc = 0\n",
            "pos tensor(0.1981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5365==== Step 1  Train Loss 0.26200100779533386 acc = 0\n",
            "pos tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5366==== Step 1  Train Loss 0.2910776734352112 acc = 0\n",
            "pos tensor(0.2156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5367==== Step 1  Train Loss 0.2922344207763672 acc = 0\n",
            "pos tensor(0.1776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5368==== Step 1  Train Loss 0.2532888352870941 acc = 0\n",
            "pos tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5369==== Step 1  Train Loss 0.2807115912437439 acc = 0\n",
            "pos tensor(0.2209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5370==== Step 1  Train Loss 0.28992122411727905 acc = 0\n",
            "pos tensor(0.1708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5371==== Step 1  Train Loss 0.24476945400238037 acc = 0\n",
            "pos tensor(0.1897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5372==== Step 1  Train Loss 0.26466992497444153 acc = 0\n",
            "pos tensor(0.2223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5373==== Step 1  Train Loss 0.26940080523490906 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5374==== Step 1  Train Loss 0.27784186601638794 acc = 0\n",
            "pos tensor(0.2034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5375==== Step 1  Train Loss 0.27403175830841064 acc = 0\n",
            "pos tensor(0.2155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5376==== Step 1  Train Loss 0.26440852880477905 acc = 0\n",
            "pos tensor(0.2464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5377==== Step 1  Train Loss 0.29234957695007324 acc = 0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5378==== Step 1  Train Loss 0.2741805613040924 acc = 0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5379==== Step 1  Train Loss 0.27430182695388794 acc = 0\n",
            "pos tensor(0.2134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5380==== Step 1  Train Loss 0.29990309476852417 acc = 0\n",
            "pos tensor(0.2271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5381==== Step 1  Train Loss 0.29026883840560913 acc = 0\n",
            "pos tensor(0.2226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5382==== Step 1  Train Loss 0.27658700942993164 acc = 0\n",
            "pos tensor(0.1794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5383==== Step 1  Train Loss 0.25560253858566284 acc = 0\n",
            "pos tensor(0.2163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5384==== Step 1  Train Loss 0.2593206763267517 acc = 0\n",
            "pos tensor(0.2318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5385==== Step 1  Train Loss 0.2915582060813904 acc = 0\n",
            "pos tensor(0.2152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5386==== Step 1  Train Loss 0.27005043625831604 acc = 0\n",
            "pos tensor(0.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5387==== Step 1  Train Loss 0.2615565359592438 acc = 0\n",
            "pos tensor(0.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5388==== Step 1  Train Loss 0.26297497749328613 acc = 0\n",
            "pos tensor(0.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5389==== Step 1  Train Loss 0.2418310046195984 acc = 0\n",
            "pos tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5390==== Step 1  Train Loss 0.2802375853061676 acc = 0\n",
            "pos tensor(0.2062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5391==== Step 1  Train Loss 0.27195197343826294 acc = 0\n",
            "pos tensor(0.1857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5392==== Step 1  Train Loss 0.27882644534111023 acc = 0\n",
            "pos tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5393==== Step 1  Train Loss 0.286501944065094 acc = 0\n",
            "pos tensor(0.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5394==== Step 1  Train Loss 0.26758715510368347 acc = 0\n",
            "pos tensor(0.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5395==== Step 1  Train Loss 0.3115229308605194 acc = 0\n",
            "pos tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5396==== Step 1  Train Loss 0.2599538564682007 acc = 0\n",
            "pos tensor(0.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5397==== Step 1  Train Loss 0.28973260521888733 acc = 0\n",
            "pos tensor(0.2716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5398==== Step 1  Train Loss 0.3049163520336151 acc = 0\n",
            "pos tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5399==== Step 1  Train Loss 0.2701948285102844 acc = 0\n",
            "  Batch 5,400  of  5,561.    Elapsed: 0:19:11.\n",
            "pos tensor(0.2249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5400==== Step 1  Train Loss 0.2796555161476135 acc = 0\n",
            "pos tensor(0.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5401==== Step 1  Train Loss 0.2557564675807953 acc = 0\n",
            "pos tensor(0.2032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5402==== Step 1  Train Loss 0.27453190088272095 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5403==== Step 1  Train Loss 0.2905597686767578 acc = 0\n",
            "pos tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5404==== Step 1  Train Loss 0.26346373558044434 acc = 0\n",
            "pos tensor(0.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5405==== Step 1  Train Loss 0.2523900270462036 acc = 0\n",
            "pos tensor(0.1899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5406==== Step 1  Train Loss 0.26961252093315125 acc = 0\n",
            "pos tensor(0.2092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5407==== Step 1  Train Loss 0.29915162920951843 acc = 0\n",
            "pos tensor(0.1631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5408==== Step 1  Train Loss 0.2504250407218933 acc = 0\n",
            "pos tensor(0.1766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5409==== Step 1  Train Loss 0.26619386672973633 acc = 0\n",
            "pos tensor(0.1782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5410==== Step 1  Train Loss 0.2502977252006531 acc = 0\n",
            "pos tensor(0.1936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5411==== Step 1  Train Loss 0.2661043405532837 acc = 0\n",
            "pos tensor(0.2475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5412==== Step 1  Train Loss 0.28950393199920654 acc = 0\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5413==== Step 1  Train Loss 0.28556719422340393 acc = 0\n",
            "pos tensor(0.2430, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5414==== Step 1  Train Loss 0.29840654134750366 acc = 0\n",
            "pos tensor(0.2633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5415==== Step 1  Train Loss 0.30974990129470825 acc = 0\n",
            "pos tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5416==== Step 1  Train Loss 0.26618626713752747 acc = 0\n",
            "pos tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5417==== Step 1  Train Loss 0.2877892851829529 acc = 0\n",
            "pos tensor(0.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5418==== Step 1  Train Loss 0.2807300090789795 acc = 0\n",
            "pos tensor(0.2219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5419==== Step 1  Train Loss 0.2801584303379059 acc = 0\n",
            "pos tensor(0.1862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5420==== Step 1  Train Loss 0.28732559084892273 acc = 0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5421==== Step 1  Train Loss 0.2697961926460266 acc = 0\n",
            "pos tensor(0.1800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5422==== Step 1  Train Loss 0.29192692041397095 acc = 0\n",
            "pos tensor(0.1443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5423==== Step 1  Train Loss 0.24513545632362366 acc = 0\n",
            "pos tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5424==== Step 1  Train Loss 0.2592552900314331 acc = 0\n",
            "pos tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5425==== Step 1  Train Loss 0.25585269927978516 acc = 0\n",
            "pos tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5426==== Step 1  Train Loss 0.2772005796432495 acc = 0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5427==== Step 1  Train Loss 0.31213051080703735 acc = 0\n",
            "pos tensor(0.1853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5428==== Step 1  Train Loss 0.2672962546348572 acc = 0\n",
            "pos tensor(0.2000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3084, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5429==== Step 1  Train Loss 0.254201203584671 acc = 0\n",
            "pos tensor(0.2175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5430==== Step 1  Train Loss 0.2939594089984894 acc = 0\n",
            "pos tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5431==== Step 1  Train Loss 0.2709304690361023 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5432==== Step 1  Train Loss 0.2615729570388794 acc = 0\n",
            "pos tensor(0.1449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5433==== Step 1  Train Loss 0.24077332019805908 acc = 0\n",
            "pos tensor(0.2267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5434==== Step 1  Train Loss 0.28755247592926025 acc = 0\n",
            "pos tensor(0.2301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5435==== Step 1  Train Loss 0.2790221571922302 acc = 0\n",
            "pos tensor(0.2179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5436==== Step 1  Train Loss 0.27342134714126587 acc = 0\n",
            "pos tensor(0.2069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5437==== Step 1  Train Loss 0.2608141601085663 acc = 0\n",
            "pos tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5438==== Step 1  Train Loss 0.2930392026901245 acc = 0\n",
            "pos tensor(0.2456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5439==== Step 1  Train Loss 0.2983659505844116 acc = 0\n",
            "pos tensor(0.2203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5440==== Step 1  Train Loss 0.28211715817451477 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5441==== Step 1  Train Loss 0.27350467443466187 acc = 0\n",
            "pos tensor(0.2329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5442==== Step 1  Train Loss 0.2783994972705841 acc = 0\n",
            "pos tensor(0.2064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5443==== Step 1  Train Loss 0.27018505334854126 acc = 0\n",
            "pos tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5444==== Step 1  Train Loss 0.2896555960178375 acc = 0\n",
            "pos tensor(0.2336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5445==== Step 1  Train Loss 0.29993170499801636 acc = 0\n",
            "pos tensor(0.1977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5446==== Step 1  Train Loss 0.2657272219657898 acc = 0\n",
            "pos tensor(0.2390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5447==== Step 1  Train Loss 0.28668662905693054 acc = 0\n",
            "pos tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5448==== Step 1  Train Loss 0.24610859155654907 acc = 0\n",
            "pos tensor(0.1800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5449==== Step 1  Train Loss 0.24495846033096313 acc = 0\n",
            "pos tensor(0.2158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5450==== Step 1  Train Loss 0.2827761769294739 acc = 0\n",
            "pos tensor(0.2717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5451==== Step 1  Train Loss 0.28863728046417236 acc = 0\n",
            "pos tensor(0.2306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5452==== Step 1  Train Loss 0.27410054206848145 acc = 0\n",
            "pos tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5453==== Step 1  Train Loss 0.27381470799446106 acc = 0\n",
            "pos tensor(0.2373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5454==== Step 1  Train Loss 0.2864935100078583 acc = 0\n",
            "pos tensor(0.1861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5455==== Step 1  Train Loss 0.2653687596321106 acc = 0\n",
            "pos tensor(0.2194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5456==== Step 1  Train Loss 0.28095555305480957 acc = 0\n",
            "pos tensor(0.2498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5457==== Step 1  Train Loss 0.296114981174469 acc = 0\n",
            "pos tensor(0.1623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5458==== Step 1  Train Loss 0.25762489438056946 acc = 0\n",
            "pos tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5459==== Step 1  Train Loss 0.2607654929161072 acc = 0\n",
            "pos tensor(0.2253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5460==== Step 1  Train Loss 0.2937373220920563 acc = 0\n",
            "pos tensor(0.1759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5461==== Step 1  Train Loss 0.272456556558609 acc = 0\n",
            "pos tensor(0.1791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5462==== Step 1  Train Loss 0.28473758697509766 acc = 0\n",
            "pos tensor(0.1800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.4052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5463==== Step 1  Train Loss 0.29260241985321045 acc = 0\n",
            "pos tensor(0.1949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5464==== Step 1  Train Loss 0.2890142500400543 acc = 0\n",
            "pos tensor(0.1438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5465==== Step 1  Train Loss 0.25418800115585327 acc = 0\n",
            "pos tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5466==== Step 1  Train Loss 0.26582300662994385 acc = 0\n",
            "pos tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5467==== Step 1  Train Loss 0.28295478224754333 acc = 0\n",
            "pos tensor(0.1987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5468==== Step 1  Train Loss 0.265996515750885 acc = 0\n",
            "pos tensor(0.2288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5469==== Step 1  Train Loss 0.2950761914253235 acc = 0\n",
            "pos tensor(0.1863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5470==== Step 1  Train Loss 0.28011026978492737 acc = 0\n",
            "pos tensor(0.2311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5471==== Step 1  Train Loss 0.27932268381118774 acc = 0\n",
            "pos tensor(0.2095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5472==== Step 1  Train Loss 0.26259249448776245 acc = 0\n",
            "pos tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5473==== Step 1  Train Loss 0.26187801361083984 acc = 0\n",
            "pos tensor(0.2361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5474==== Step 1  Train Loss 0.2820262014865875 acc = 0\n",
            "pos tensor(0.1801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5475==== Step 1  Train Loss 0.2672414183616638 acc = 0\n",
            "pos tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5476==== Step 1  Train Loss 0.2763022184371948 acc = 0\n",
            "pos tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5477==== Step 1  Train Loss 0.27662181854248047 acc = 0\n",
            "pos tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5478==== Step 1  Train Loss 0.2646061182022095 acc = 0\n",
            "pos tensor(0.2248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5479==== Step 1  Train Loss 0.28362858295440674 acc = 0\n",
            "pos tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5480==== Step 1  Train Loss 0.2588796019554138 acc = 0\n",
            "pos tensor(0.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5481==== Step 1  Train Loss 0.27691560983657837 acc = 0\n",
            "pos tensor(0.2264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5482==== Step 1  Train Loss 0.2759007215499878 acc = 0\n",
            "pos tensor(0.2212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5483==== Step 1  Train Loss 0.29864418506622314 acc = 0\n",
            "pos tensor(0.1982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5484==== Step 1  Train Loss 0.269609659910202 acc = 0\n",
            "pos tensor(0.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5485==== Step 1  Train Loss 0.2719855308532715 acc = 0\n",
            "pos tensor(0.1756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5486==== Step 1  Train Loss 0.26583391427993774 acc = 0\n",
            "pos tensor(0.1551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5487==== Step 1  Train Loss 0.25776469707489014 acc = 0\n",
            "pos tensor(0.2488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5488==== Step 1  Train Loss 0.29078441858291626 acc = 0\n",
            "pos tensor(0.1421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5489==== Step 1  Train Loss 0.26094934344291687 acc = 0\n",
            "pos tensor(0.2195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5490==== Step 1  Train Loss 0.2906970977783203 acc = 0\n",
            "pos tensor(0.1728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5491==== Step 1  Train Loss 0.2817394435405731 acc = 0\n",
            "pos tensor(0.1660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5492==== Step 1  Train Loss 0.26107046008110046 acc = 0\n",
            "pos tensor(0.1778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5493==== Step 1  Train Loss 0.26093053817749023 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5494==== Step 1  Train Loss 0.2877991199493408 acc = 0\n",
            "pos tensor(0.2112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5495==== Step 1  Train Loss 0.2638136148452759 acc = 0\n",
            "pos tensor(0.1863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5496==== Step 1  Train Loss 0.24930334091186523 acc = 0\n",
            "pos tensor(0.1922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5497==== Step 1  Train Loss 0.27220213413238525 acc = 0\n",
            "pos tensor(0.1838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5498==== Step 1  Train Loss 0.2657149136066437 acc = 0\n",
            "pos tensor(0.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5499==== Step 1  Train Loss 0.25358718633651733 acc = 0\n",
            "  Batch 5,500  of  5,561.    Elapsed: 0:19:32.\n",
            "pos tensor(0.2022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5500==== Step 1  Train Loss 0.28002920746803284 acc = 0\n",
            "pos tensor(0.2398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3303, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5501==== Step 1  Train Loss 0.285058856010437 acc = 0\n",
            "pos tensor(0.2476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5502==== Step 1  Train Loss 0.28166845440864563 acc = 0\n",
            "pos tensor(0.1662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5503==== Step 1  Train Loss 0.2500753402709961 acc = 0\n",
            "pos tensor(0.1762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5504==== Step 1  Train Loss 0.2713856101036072 acc = 0\n",
            "pos tensor(0.1895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5505==== Step 1  Train Loss 0.2556818425655365 acc = 0\n",
            "pos tensor(0.2365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5506==== Step 1  Train Loss 0.29059505462646484 acc = 0\n",
            "pos tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5507==== Step 1  Train Loss 0.270736962556839 acc = 0\n",
            "pos tensor(0.2479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5508==== Step 1  Train Loss 0.29807162284851074 acc = 0\n",
            "pos tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5509==== Step 1  Train Loss 0.261321097612381 acc = 0\n",
            "pos tensor(0.2592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5510==== Step 1  Train Loss 0.3061032295227051 acc = 0\n",
            "pos tensor(0.1995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5511==== Step 1  Train Loss 0.25978678464889526 acc = 0\n",
            "pos tensor(0.2182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5512==== Step 1  Train Loss 0.2893487811088562 acc = 0\n",
            "pos tensor(0.2018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5513==== Step 1  Train Loss 0.2793881893157959 acc = 0\n",
            "pos tensor(0.2030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5514==== Step 1  Train Loss 0.2780260443687439 acc = 0\n",
            "pos tensor(0.2262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5515==== Step 1  Train Loss 0.27493584156036377 acc = 0\n",
            "pos tensor(0.1958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5516==== Step 1  Train Loss 0.2696439027786255 acc = 0\n",
            "pos tensor(0.2271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5517==== Step 1  Train Loss 0.26767396926879883 acc = 0\n",
            "pos tensor(0.2166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5518==== Step 1  Train Loss 0.28737640380859375 acc = 0\n",
            "pos tensor(0.1579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5519==== Step 1  Train Loss 0.2577645480632782 acc = 0\n",
            "pos tensor(0.2091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5520==== Step 1  Train Loss 0.291964590549469 acc = 0\n",
            "pos tensor(0.2191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5521==== Step 1  Train Loss 0.2756307125091553 acc = 0\n",
            "pos tensor(0.1682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5522==== Step 1  Train Loss 0.2628696858882904 acc = 0\n",
            "pos tensor(0.2194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5523==== Step 1  Train Loss 0.3061233162879944 acc = 0\n",
            "pos tensor(0.2027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3438, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5524==== Step 1  Train Loss 0.2732517421245575 acc = 0\n",
            "pos tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5525==== Step 1  Train Loss 0.28264546394348145 acc = 0\n",
            "pos tensor(0.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5526==== Step 1  Train Loss 0.2738237977027893 acc = 0\n",
            "pos tensor(0.1347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5527==== Step 1  Train Loss 0.22783423960208893 acc = 0\n",
            "pos tensor(0.2162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5528==== Step 1  Train Loss 0.29616808891296387 acc = 0\n",
            "pos tensor(0.1852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5529==== Step 1  Train Loss 0.2901577353477478 acc = 0\n",
            "pos tensor(0.1967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5530==== Step 1  Train Loss 0.2864640951156616 acc = 0\n",
            "pos tensor(0.2052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5531==== Step 1  Train Loss 0.29745256900787354 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5532==== Step 1  Train Loss 0.27273625135421753 acc = 0\n",
            "pos tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5533==== Step 1  Train Loss 0.255034863948822 acc = 0\n",
            "pos tensor(0.1870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5534==== Step 1  Train Loss 0.2602097988128662 acc = 0\n",
            "pos tensor(0.2075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5535==== Step 1  Train Loss 0.27599233388900757 acc = 0\n",
            "pos tensor(0.2148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5536==== Step 1  Train Loss 0.2811887264251709 acc = 0\n",
            "pos tensor(0.2150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5537==== Step 1  Train Loss 0.27341896295547485 acc = 0\n",
            "pos tensor(0.2264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5538==== Step 1  Train Loss 0.27374446392059326 acc = 0\n",
            "pos tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5539==== Step 1  Train Loss 0.261007696390152 acc = 0\n",
            "pos tensor(0.1669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5540==== Step 1  Train Loss 0.25681447982788086 acc = 0\n",
            "pos tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5541==== Step 1  Train Loss 0.2725077271461487 acc = 0\n",
            "pos tensor(0.2153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5542==== Step 1  Train Loss 0.26559892296791077 acc = 0\n",
            "pos tensor(0.1963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5543==== Step 1  Train Loss 0.2663712501525879 acc = 0\n",
            "pos tensor(0.1608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5544==== Step 1  Train Loss 0.2608770728111267 acc = 0\n",
            "pos tensor(0.2321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5545==== Step 1  Train Loss 0.2914077639579773 acc = 0\n",
            "pos tensor(0.1980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5546==== Step 1  Train Loss 0.2679976522922516 acc = 0\n",
            "pos tensor(0.1989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5547==== Step 1  Train Loss 0.2724613547325134 acc = 0\n",
            "pos tensor(0.2060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5548==== Step 1  Train Loss 0.2702391743659973 acc = 0\n",
            "pos tensor(0.2323, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5549==== Step 1  Train Loss 0.2885332703590393 acc = 0\n",
            "pos tensor(0.2077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5550==== Step 1  Train Loss 0.2546427249908447 acc = 0\n",
            "pos tensor(0.2059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5551==== Step 1  Train Loss 0.2724783718585968 acc = 0\n",
            "pos tensor(0.1952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5552==== Step 1  Train Loss 0.2813831567764282 acc = 0\n",
            "pos tensor(0.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5553==== Step 1  Train Loss 0.256054162979126 acc = 0\n",
            "pos tensor(0.1737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5554==== Step 1  Train Loss 0.2745731472969055 acc = 0\n",
            "pos tensor(0.1966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5555==== Step 1  Train Loss 0.2745833396911621 acc = 0\n",
            "pos tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5556==== Step 1  Train Loss 0.2803956866264343 acc = 0\n",
            "pos tensor(0.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5557==== Step 1  Train Loss 0.3018094301223755 acc = 0\n",
            "pos tensor(0.2163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5558==== Step 1  Train Loss 0.2697889804840088 acc = 0\n",
            "pos tensor(0.2208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5559==== Step 1  Train Loss 0.27891796827316284 acc = 0\n",
            "pos tensor(0.1869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3500, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5560==== Step 1  Train Loss 0.2684575915336609 acc = 0\n",
            "pos tensor(0.1979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 0 Batch 5561==== Step 1  Train Loss 0.23455682396888733 acc = 0\n",
            "========== Epoch 0 ==== Step 1 AVG. Train Loss 0.28814088974554664\n",
            "\n",
            "  Training epoch took: 0:19:45\n",
            "\n",
            "Running Validation...\n",
            "pos tensor(0.1072, device='cuda:0')\n",
            "neg tensor(0.4711, device='cuda:0')\n",
            "========== Epoch 0 Batch 1==== Step 1 AVG. val Loss 0.2891482710838318 acc = 0.0\n",
            "pos tensor(0.1658, device='cuda:0')\n",
            "neg tensor(0.4426, device='cuda:0')\n",
            "========== Epoch 0 Batch 2==== Step 1 AVG. val Loss 0.30420762300491333 acc = 0.0\n",
            "pos tensor(0.1405, device='cuda:0')\n",
            "neg tensor(0.4763, device='cuda:0')\n",
            "========== Epoch 0 Batch 3==== Step 1 AVG. val Loss 0.30837398767471313 acc = 0.0\n",
            "pos tensor(0.1279, device='cuda:0')\n",
            "neg tensor(0.4695, device='cuda:0')\n",
            "========== Epoch 0 Batch 4==== Step 1 AVG. val Loss 0.29867273569107056 acc = 0.0\n",
            "pos tensor(0.1056, device='cuda:0')\n",
            "neg tensor(0.4560, device='cuda:0')\n",
            "========== Epoch 0 Batch 5==== Step 1 AVG. val Loss 0.280812531709671 acc = 0.0\n",
            "pos tensor(0.1359, device='cuda:0')\n",
            "neg tensor(0.4603, device='cuda:0')\n",
            "========== Epoch 0 Batch 6==== Step 1 AVG. val Loss 0.29812437295913696 acc = 0.0\n",
            "pos tensor(0.1343, device='cuda:0')\n",
            "neg tensor(0.4615, device='cuda:0')\n",
            "========== Epoch 0 Batch 7==== Step 1 AVG. val Loss 0.2978884279727936 acc = 0.0\n",
            "pos tensor(0.1481, device='cuda:0')\n",
            "neg tensor(0.4522, device='cuda:0')\n",
            "========== Epoch 0 Batch 8==== Step 1 AVG. val Loss 0.30017778277397156 acc = 0.0\n",
            "pos tensor(0.1504, device='cuda:0')\n",
            "neg tensor(0.4454, device='cuda:0')\n",
            "========== Epoch 0 Batch 9==== Step 1 AVG. val Loss 0.29789507389068604 acc = 0.0\n",
            "pos tensor(0.1255, device='cuda:0')\n",
            "neg tensor(0.4486, device='cuda:0')\n",
            "========== Epoch 0 Batch 10==== Step 1 AVG. val Loss 0.2870447635650635 acc = 0.0\n",
            "pos tensor(0.1502, device='cuda:0')\n",
            "neg tensor(0.4688, device='cuda:0')\n",
            "========== Epoch 0 Batch 11==== Step 1 AVG. val Loss 0.3094899654388428 acc = 0.0\n",
            "pos tensor(0.1308, device='cuda:0')\n",
            "neg tensor(0.4225, device='cuda:0')\n",
            "========== Epoch 0 Batch 12==== Step 1 AVG. val Loss 0.27663952112197876 acc = 0.0\n",
            "pos tensor(0.1167, device='cuda:0')\n",
            "neg tensor(0.4415, device='cuda:0')\n",
            "========== Epoch 0 Batch 13==== Step 1 AVG. val Loss 0.27909034490585327 acc = 0.0\n",
            "pos tensor(0.1814, device='cuda:0')\n",
            "neg tensor(0.4492, device='cuda:0')\n",
            "========== Epoch 0 Batch 14==== Step 1 AVG. val Loss 0.31531092524528503 acc = 0.0\n",
            "pos tensor(0.1255, device='cuda:0')\n",
            "neg tensor(0.4826, device='cuda:0')\n",
            "========== Epoch 0 Batch 15==== Step 1 AVG. val Loss 0.30403098464012146 acc = 0.0\n",
            "pos tensor(0.1302, device='cuda:0')\n",
            "neg tensor(0.4450, device='cuda:0')\n",
            "========== Epoch 0 Batch 16==== Step 1 AVG. val Loss 0.2876327633857727 acc = 0.0\n",
            "pos tensor(0.1266, device='cuda:0')\n",
            "neg tensor(0.4612, device='cuda:0')\n",
            "========== Epoch 0 Batch 17==== Step 1 AVG. val Loss 0.29392191767692566 acc = 0.0\n",
            "pos tensor(0.1170, device='cuda:0')\n",
            "neg tensor(0.4746, device='cuda:0')\n",
            "========== Epoch 0 Batch 18==== Step 1 AVG. val Loss 0.2957671284675598 acc = 0.0\n",
            "pos tensor(0.0995, device='cuda:0')\n",
            "neg tensor(0.4123, device='cuda:0')\n",
            "========== Epoch 0 Batch 19==== Step 1 AVG. val Loss 0.25592976808547974 acc = 0.0\n",
            "pos tensor(0.1472, device='cuda:0')\n",
            "neg tensor(0.4279, device='cuda:0')\n",
            "========== Epoch 0 Batch 20==== Step 1 AVG. val Loss 0.2875683605670929 acc = 0.0\n",
            "pos tensor(0.1223, device='cuda:0')\n",
            "neg tensor(0.4640, device='cuda:0')\n",
            "========== Epoch 0 Batch 21==== Step 1 AVG. val Loss 0.29316991567611694 acc = 0.0\n",
            "pos tensor(0.1109, device='cuda:0')\n",
            "neg tensor(0.4539, device='cuda:0')\n",
            "========== Epoch 0 Batch 22==== Step 1 AVG. val Loss 0.2823919355869293 acc = 0.0\n",
            "pos tensor(0.1133, device='cuda:0')\n",
            "neg tensor(0.4483, device='cuda:0')\n",
            "========== Epoch 0 Batch 23==== Step 1 AVG. val Loss 0.28084176778793335 acc = 0.0\n",
            "pos tensor(0.1280, device='cuda:0')\n",
            "neg tensor(0.4732, device='cuda:0')\n",
            "========== Epoch 0 Batch 24==== Step 1 AVG. val Loss 0.30059608817100525 acc = 0.0\n",
            "pos tensor(0.1630, device='cuda:0')\n",
            "neg tensor(0.4667, device='cuda:0')\n",
            "========== Epoch 0 Batch 25==== Step 1 AVG. val Loss 0.31483253836631775 acc = 0.0\n",
            "pos tensor(0.1523, device='cuda:0')\n",
            "neg tensor(0.3998, device='cuda:0')\n",
            "========== Epoch 0 Batch 26==== Step 1 AVG. val Loss 0.2760283946990967 acc = 0.0\n",
            "pos tensor(0.1651, device='cuda:0')\n",
            "neg tensor(0.4422, device='cuda:0')\n",
            "========== Epoch 0 Batch 27==== Step 1 AVG. val Loss 0.3036267161369324 acc = 0.0\n",
            "pos tensor(0.1180, device='cuda:0')\n",
            "neg tensor(0.4501, device='cuda:0')\n",
            "========== Epoch 0 Batch 28==== Step 1 AVG. val Loss 0.28404363989830017 acc = 0.0\n",
            "pos tensor(0.1388, device='cuda:0')\n",
            "neg tensor(0.4787, device='cuda:0')\n",
            "========== Epoch 0 Batch 29==== Step 1 AVG. val Loss 0.3087107241153717 acc = 0.0\n",
            "pos tensor(0.1526, device='cuda:0')\n",
            "neg tensor(0.4523, device='cuda:0')\n",
            "========== Epoch 0 Batch 30==== Step 1 AVG. val Loss 0.3024492859840393 acc = 0.0\n",
            "pos tensor(0.1369, device='cuda:0')\n",
            "neg tensor(0.4007, device='cuda:0')\n",
            "========== Epoch 0 Batch 31==== Step 1 AVG. val Loss 0.26881280541419983 acc = 0.0\n",
            "pos tensor(0.1200, device='cuda:0')\n",
            "neg tensor(0.4093, device='cuda:0')\n",
            "========== Epoch 0 Batch 32==== Step 1 AVG. val Loss 0.26466381549835205 acc = 0.0\n",
            "pos tensor(0.1379, device='cuda:0')\n",
            "neg tensor(0.4253, device='cuda:0')\n",
            "========== Epoch 0 Batch 33==== Step 1 AVG. val Loss 0.28160345554351807 acc = 0.0\n",
            "pos tensor(0.1465, device='cuda:0')\n",
            "neg tensor(0.4286, device='cuda:0')\n",
            "========== Epoch 0 Batch 34==== Step 1 AVG. val Loss 0.2875491976737976 acc = 0.0\n",
            "pos tensor(0.1409, device='cuda:0')\n",
            "neg tensor(0.4309, device='cuda:0')\n",
            "========== Epoch 0 Batch 35==== Step 1 AVG. val Loss 0.2859196066856384 acc = 0.0\n",
            "pos tensor(0.1114, device='cuda:0')\n",
            "neg tensor(0.4381, device='cuda:0')\n",
            "========== Epoch 0 Batch 36==== Step 1 AVG. val Loss 0.27474722266197205 acc = 0.0\n",
            "pos tensor(0.1705, device='cuda:0')\n",
            "neg tensor(0.3818, device='cuda:0')\n",
            "========== Epoch 0 Batch 37==== Step 1 AVG. val Loss 0.2761687636375427 acc = 0.0\n",
            "pos tensor(0.1379, device='cuda:0')\n",
            "neg tensor(0.4760, device='cuda:0')\n",
            "========== Epoch 0 Batch 38==== Step 1 AVG. val Loss 0.30695462226867676 acc = 0.0\n",
            "pos tensor(0.1196, device='cuda:0')\n",
            "neg tensor(0.4248, device='cuda:0')\n",
            "========== Epoch 0 Batch 39==== Step 1 AVG. val Loss 0.27217578887939453 acc = 0.0\n",
            "pos tensor(0.1156, device='cuda:0')\n",
            "neg tensor(0.4335, device='cuda:0')\n",
            "========== Epoch 0 Batch 40==== Step 1 AVG. val Loss 0.27454817295074463 acc = 0.0\n",
            "pos tensor(0.1476, device='cuda:0')\n",
            "neg tensor(0.4681, device='cuda:0')\n",
            "========== Epoch 0 Batch 41==== Step 1 AVG. val Loss 0.30783170461654663 acc = 0.0\n",
            "pos tensor(0.1335, device='cuda:0')\n",
            "neg tensor(0.4491, device='cuda:0')\n",
            "========== Epoch 0 Batch 42==== Step 1 AVG. val Loss 0.29134446382522583 acc = 0.0\n",
            "pos tensor(0.1026, device='cuda:0')\n",
            "neg tensor(0.4664, device='cuda:0')\n",
            "========== Epoch 0 Batch 43==== Step 1 AVG. val Loss 0.2844959795475006 acc = 0.0\n",
            "pos tensor(0.1237, device='cuda:0')\n",
            "neg tensor(0.4226, device='cuda:0')\n",
            "========== Epoch 0 Batch 44==== Step 1 AVG. val Loss 0.27317407727241516 acc = 0.0\n",
            "pos tensor(0.1222, device='cuda:0')\n",
            "neg tensor(0.4364, device='cuda:0')\n",
            "========== Epoch 0 Batch 45==== Step 1 AVG. val Loss 0.2792792320251465 acc = 0.0\n",
            "pos tensor(0.1195, device='cuda:0')\n",
            "neg tensor(0.4317, device='cuda:0')\n",
            "========== Epoch 0 Batch 46==== Step 1 AVG. val Loss 0.2756064534187317 acc = 0.0\n",
            "pos tensor(0.1357, device='cuda:0')\n",
            "neg tensor(0.4573, device='cuda:0')\n",
            "========== Epoch 0 Batch 47==== Step 1 AVG. val Loss 0.29646211862564087 acc = 0.0\n",
            "pos tensor(0.1658, device='cuda:0')\n",
            "neg tensor(0.4233, device='cuda:0')\n",
            "========== Epoch 0 Batch 48==== Step 1 AVG. val Loss 0.29452162981033325 acc = 0.0\n",
            "pos tensor(0.1355, device='cuda:0')\n",
            "neg tensor(0.4293, device='cuda:0')\n",
            "========== Epoch 0 Batch 49==== Step 1 AVG. val Loss 0.28239887952804565 acc = 0.0\n",
            "pos tensor(0.1658, device='cuda:0')\n",
            "neg tensor(0.4485, device='cuda:0')\n",
            "========== Epoch 0 Batch 50==== Step 1 AVG. val Loss 0.30713337659835815 acc = 0.0\n",
            "pos tensor(0.1291, device='cuda:0')\n",
            "neg tensor(0.4567, device='cuda:0')\n",
            "========== Epoch 0 Batch 51==== Step 1 AVG. val Loss 0.29290157556533813 acc = 0.0\n",
            "pos tensor(0.1254, device='cuda:0')\n",
            "neg tensor(0.4789, device='cuda:0')\n",
            "========== Epoch 0 Batch 52==== Step 1 AVG. val Loss 0.3021352291107178 acc = 0.0\n",
            "pos tensor(0.1713, device='cuda:0')\n",
            "neg tensor(0.4374, device='cuda:0')\n",
            "========== Epoch 0 Batch 53==== Step 1 AVG. val Loss 0.30437323451042175 acc = 0.0\n",
            "pos tensor(0.1503, device='cuda:0')\n",
            "neg tensor(0.4552, device='cuda:0')\n",
            "========== Epoch 0 Batch 54==== Step 1 AVG. val Loss 0.3027458190917969 acc = 0.0\n",
            "pos tensor(0.0997, device='cuda:0')\n",
            "neg tensor(0.4306, device='cuda:0')\n",
            "========== Epoch 0 Batch 55==== Step 1 AVG. val Loss 0.2651667594909668 acc = 0.0\n",
            "pos tensor(0.1419, device='cuda:0')\n",
            "neg tensor(0.4027, device='cuda:0')\n",
            "========== Epoch 0 Batch 56==== Step 1 AVG. val Loss 0.2722862958908081 acc = 0.0\n",
            "pos tensor(0.1413, device='cuda:0')\n",
            "neg tensor(0.4102, device='cuda:0')\n",
            "========== Epoch 0 Batch 57==== Step 1 AVG. val Loss 0.2757733464241028 acc = 0.0\n",
            "pos tensor(0.1485, device='cuda:0')\n",
            "neg tensor(0.4311, device='cuda:0')\n",
            "========== Epoch 0 Batch 58==== Step 1 AVG. val Loss 0.28979799151420593 acc = 0.0\n",
            "pos tensor(0.1429, device='cuda:0')\n",
            "neg tensor(0.4525, device='cuda:0')\n",
            "========== Epoch 0 Batch 59==== Step 1 AVG. val Loss 0.2977135479450226 acc = 0.0\n",
            "pos tensor(0.1609, device='cuda:0')\n",
            "neg tensor(0.4635, device='cuda:0')\n",
            "========== Epoch 0 Batch 60==== Step 1 AVG. val Loss 0.3122393786907196 acc = 0.0\n",
            "pos tensor(0.1247, device='cuda:0')\n",
            "neg tensor(0.4667, device='cuda:0')\n",
            "========== Epoch 0 Batch 61==== Step 1 AVG. val Loss 0.29571962356567383 acc = 0.0\n",
            "pos tensor(0.1384, device='cuda:0')\n",
            "neg tensor(0.4373, device='cuda:0')\n",
            "========== Epoch 0 Batch 62==== Step 1 AVG. val Loss 0.2878526449203491 acc = 0.0\n",
            "pos tensor(0.1523, device='cuda:0')\n",
            "neg tensor(0.4271, device='cuda:0')\n",
            "========== Epoch 0 Batch 63==== Step 1 AVG. val Loss 0.2896631360054016 acc = 0.0\n",
            "pos tensor(0.1469, device='cuda:0')\n",
            "neg tensor(0.4559, device='cuda:0')\n",
            "========== Epoch 0 Batch 64==== Step 1 AVG. val Loss 0.3013748228549957 acc = 0.0\n",
            "pos tensor(0.1242, device='cuda:0')\n",
            "neg tensor(0.4259, device='cuda:0')\n",
            "========== Epoch 0 Batch 65==== Step 1 AVG. val Loss 0.27507761120796204 acc = 0.0\n",
            "pos tensor(0.1654, device='cuda:0')\n",
            "neg tensor(0.4063, device='cuda:0')\n",
            "========== Epoch 0 Batch 66==== Step 1 AVG. val Loss 0.2858780026435852 acc = 0.0\n",
            "pos tensor(0.1003, device='cuda:0')\n",
            "neg tensor(0.4793, device='cuda:0')\n",
            "========== Epoch 0 Batch 67==== Step 1 AVG. val Loss 0.28977298736572266 acc = 0.0\n",
            "pos tensor(0.1378, device='cuda:0')\n",
            "neg tensor(0.4511, device='cuda:0')\n",
            "========== Epoch 0 Batch 68==== Step 1 AVG. val Loss 0.29447728395462036 acc = 0.0\n",
            "pos tensor(0.1206, device='cuda:0')\n",
            "neg tensor(0.4989, device='cuda:0')\n",
            "========== Epoch 0 Batch 69==== Step 1 AVG. val Loss 0.3097630739212036 acc = 0.0\n",
            "pos tensor(0.1320, device='cuda:0')\n",
            "neg tensor(0.4631, device='cuda:0')\n",
            "========== Epoch 0 Batch 70==== Step 1 AVG. val Loss 0.2975574731826782 acc = 0.0\n",
            "pos tensor(0.1210, device='cuda:0')\n",
            "neg tensor(0.4775, device='cuda:0')\n",
            "========== Epoch 0 Batch 71==== Step 1 AVG. val Loss 0.29925110936164856 acc = 0.0\n",
            "pos tensor(0.1558, device='cuda:0')\n",
            "neg tensor(0.4327, device='cuda:0')\n",
            "========== Epoch 0 Batch 72==== Step 1 AVG. val Loss 0.29421794414520264 acc = 0.0\n",
            "pos tensor(0.1340, device='cuda:0')\n",
            "neg tensor(0.4848, device='cuda:0')\n",
            "========== Epoch 0 Batch 73==== Step 1 AVG. val Loss 0.3094152510166168 acc = 0.0\n",
            "pos tensor(0.0958, device='cuda:0')\n",
            "neg tensor(0.4363, device='cuda:0')\n",
            "========== Epoch 0 Batch 74==== Step 1 AVG. val Loss 0.26602524518966675 acc = 0.0\n",
            "pos tensor(0.1503, device='cuda:0')\n",
            "neg tensor(0.4319, device='cuda:0')\n",
            "========== Epoch 0 Batch 75==== Step 1 AVG. val Loss 0.29107990860939026 acc = 0.0\n",
            "pos tensor(0.1169, device='cuda:0')\n",
            "neg tensor(0.4303, device='cuda:0')\n",
            "========== Epoch 0 Batch 76==== Step 1 AVG. val Loss 0.27358901500701904 acc = 0.0\n",
            "pos tensor(0.1142, device='cuda:0')\n",
            "neg tensor(0.4491, device='cuda:0')\n",
            "========== Epoch 0 Batch 77==== Step 1 AVG. val Loss 0.2816377580165863 acc = 0.0\n",
            "pos tensor(0.1243, device='cuda:0')\n",
            "neg tensor(0.4444, device='cuda:0')\n",
            "========== Epoch 0 Batch 78==== Step 1 AVG. val Loss 0.2843426465988159 acc = 0.0\n",
            "pos tensor(0.1334, device='cuda:0')\n",
            "neg tensor(0.4778, device='cuda:0')\n",
            "========== Epoch 0 Batch 79==== Step 1 AVG. val Loss 0.30561619997024536 acc = 0.0\n",
            "pos tensor(0.1423, device='cuda:0')\n",
            "neg tensor(0.4306, device='cuda:0')\n",
            "========== Epoch 0 Batch 80==== Step 1 AVG. val Loss 0.28644630312919617 acc = 0.0\n",
            "pos tensor(0.1549, device='cuda:0')\n",
            "neg tensor(0.4113, device='cuda:0')\n",
            "========== Epoch 0 Batch 81==== Step 1 AVG. val Loss 0.2830725908279419 acc = 0.0\n",
            "pos tensor(0.1351, device='cuda:0')\n",
            "neg tensor(0.4076, device='cuda:0')\n",
            "========== Epoch 0 Batch 82==== Step 1 AVG. val Loss 0.2713499665260315 acc = 0.0\n",
            "pos tensor(0.1046, device='cuda:0')\n",
            "neg tensor(0.4809, device='cuda:0')\n",
            "========== Epoch 0 Batch 83==== Step 1 AVG. val Loss 0.29277247190475464 acc = 0.0\n",
            "pos tensor(0.1164, device='cuda:0')\n",
            "neg tensor(0.4452, device='cuda:0')\n",
            "========== Epoch 0 Batch 84==== Step 1 AVG. val Loss 0.2807958126068115 acc = 0.0\n",
            "pos tensor(0.1070, device='cuda:0')\n",
            "neg tensor(0.4173, device='cuda:0')\n",
            "========== Epoch 0 Batch 85==== Step 1 AVG. val Loss 0.2621256709098816 acc = 0.0\n",
            "pos tensor(0.1422, device='cuda:0')\n",
            "neg tensor(0.4855, device='cuda:0')\n",
            "========== Epoch 0 Batch 86==== Step 1 AVG. val Loss 0.3138113021850586 acc = 0.0\n",
            "pos tensor(0.1282, device='cuda:0')\n",
            "neg tensor(0.4628, device='cuda:0')\n",
            "========== Epoch 0 Batch 87==== Step 1 AVG. val Loss 0.2955204248428345 acc = 0.0\n",
            "pos tensor(0.1396, device='cuda:0')\n",
            "neg tensor(0.4431, device='cuda:0')\n",
            "========== Epoch 0 Batch 88==== Step 1 AVG. val Loss 0.2913265526294708 acc = 0.0\n",
            "pos tensor(0.1244, device='cuda:0')\n",
            "neg tensor(0.4658, device='cuda:0')\n",
            "========== Epoch 0 Batch 89==== Step 1 AVG. val Loss 0.2951078414916992 acc = 0.0\n",
            "pos tensor(0.1504, device='cuda:0')\n",
            "neg tensor(0.4450, device='cuda:0')\n",
            "========== Epoch 0 Batch 90==== Step 1 AVG. val Loss 0.29770129919052124 acc = 0.0\n",
            "pos tensor(0.1568, device='cuda:0')\n",
            "neg tensor(0.4289, device='cuda:0')\n",
            "========== Epoch 0 Batch 91==== Step 1 AVG. val Loss 0.292855441570282 acc = 0.0\n",
            "pos tensor(0.1391, device='cuda:0')\n",
            "neg tensor(0.4162, device='cuda:0')\n",
            "========== Epoch 0 Batch 92==== Step 1 AVG. val Loss 0.27765703201293945 acc = 0.0\n",
            "pos tensor(0.1508, device='cuda:0')\n",
            "neg tensor(0.4348, device='cuda:0')\n",
            "========== Epoch 0 Batch 93==== Step 1 AVG. val Loss 0.29282063245773315 acc = 0.0\n",
            "pos tensor(0.1596, device='cuda:0')\n",
            "neg tensor(0.4411, device='cuda:0')\n",
            "========== Epoch 0 Batch 94==== Step 1 AVG. val Loss 0.3003225326538086 acc = 0.0\n",
            "pos tensor(0.1626, device='cuda:0')\n",
            "neg tensor(0.4338, device='cuda:0')\n",
            "========== Epoch 0 Batch 95==== Step 1 AVG. val Loss 0.29816314578056335 acc = 0.0\n",
            "pos tensor(0.1327, device='cuda:0')\n",
            "neg tensor(0.4766, device='cuda:0')\n",
            "========== Epoch 0 Batch 96==== Step 1 AVG. val Loss 0.3046753704547882 acc = 0.0\n",
            "pos tensor(0.1357, device='cuda:0')\n",
            "neg tensor(0.4188, device='cuda:0')\n",
            "========== Epoch 0 Batch 97==== Step 1 AVG. val Loss 0.27724185585975647 acc = 0.0\n",
            "pos tensor(0.1502, device='cuda:0')\n",
            "neg tensor(0.4500, device='cuda:0')\n",
            "========== Epoch 0 Batch 98==== Step 1 AVG. val Loss 0.300087034702301 acc = 0.0\n",
            "pos tensor(0.1289, device='cuda:0')\n",
            "neg tensor(0.4493, device='cuda:0')\n",
            "========== Epoch 0 Batch 99==== Step 1 AVG. val Loss 0.28909438848495483 acc = 0.0\n",
            "pos tensor(0.1612, device='cuda:0')\n",
            "neg tensor(0.4381, device='cuda:0')\n",
            "========== Epoch 0 Batch 100==== Step 1 AVG. val Loss 0.2996251583099365 acc = 0.0\n",
            "pos tensor(0.1398, device='cuda:0')\n",
            "neg tensor(0.4299, device='cuda:0')\n",
            "========== Epoch 0 Batch 101==== Step 1 AVG. val Loss 0.28486573696136475 acc = 0.0\n",
            "pos tensor(0.1359, device='cuda:0')\n",
            "neg tensor(0.4605, device='cuda:0')\n",
            "========== Epoch 0 Batch 102==== Step 1 AVG. val Loss 0.2982272505760193 acc = 0.0\n",
            "pos tensor(0.1085, device='cuda:0')\n",
            "neg tensor(0.4645, device='cuda:0')\n",
            "========== Epoch 0 Batch 103==== Step 1 AVG. val Loss 0.2865011990070343 acc = 0.0\n",
            "pos tensor(0.1018, device='cuda:0')\n",
            "neg tensor(0.4358, device='cuda:0')\n",
            "========== Epoch 0 Batch 104==== Step 1 AVG. val Loss 0.26880916953086853 acc = 0.0\n",
            "pos tensor(0.0925, device='cuda:0')\n",
            "neg tensor(0.4405, device='cuda:0')\n",
            "========== Epoch 0 Batch 105==== Step 1 AVG. val Loss 0.26649755239486694 acc = 0.0\n",
            "pos tensor(0.1106, device='cuda:0')\n",
            "neg tensor(0.4583, device='cuda:0')\n",
            "========== Epoch 0 Batch 106==== Step 1 AVG. val Loss 0.2844700813293457 acc = 0.0\n",
            "pos tensor(0.0923, device='cuda:0')\n",
            "neg tensor(0.4451, device='cuda:0')\n",
            "========== Epoch 0 Batch 107==== Step 1 AVG. val Loss 0.26872342824935913 acc = 0.0\n",
            "pos tensor(0.1547, device='cuda:0')\n",
            "neg tensor(0.4767, device='cuda:0')\n",
            "========== Epoch 0 Batch 108==== Step 1 AVG. val Loss 0.3157244622707367 acc = 0.0\n",
            "pos tensor(0.0789, device='cuda:0')\n",
            "neg tensor(0.4231, device='cuda:0')\n",
            "========== Epoch 0 Batch 109==== Step 1 AVG. val Loss 0.2510135769844055 acc = 0.0\n",
            "pos tensor(0.1563, device='cuda:0')\n",
            "neg tensor(0.4175, device='cuda:0')\n",
            "========== Epoch 0 Batch 110==== Step 1 AVG. val Loss 0.286901593208313 acc = 0.0\n",
            "pos tensor(0.0956, device='cuda:0')\n",
            "neg tensor(0.4910, device='cuda:0')\n",
            "========== Epoch 0 Batch 111==== Step 1 AVG. val Loss 0.2932828664779663 acc = 0.0\n",
            "pos tensor(0.1300, device='cuda:0')\n",
            "neg tensor(0.4181, device='cuda:0')\n",
            "========== Epoch 0 Batch 112==== Step 1 AVG. val Loss 0.27405476570129395 acc = 0.0\n",
            "pos tensor(0.1606, device='cuda:0')\n",
            "neg tensor(0.4332, device='cuda:0')\n",
            "========== Epoch 0 Batch 113==== Step 1 AVG. val Loss 0.2968818247318268 acc = 0.0\n",
            "pos tensor(0.1351, device='cuda:0')\n",
            "neg tensor(0.4719, device='cuda:0')\n",
            "========== Epoch 0 Batch 114==== Step 1 AVG. val Loss 0.3035088777542114 acc = 0.0\n",
            "pos tensor(0.1121, device='cuda:0')\n",
            "neg tensor(0.4581, device='cuda:0')\n",
            "========== Epoch 0 Batch 115==== Step 1 AVG. val Loss 0.2850772738456726 acc = 0.0\n",
            "pos tensor(0.1421, device='cuda:0')\n",
            "neg tensor(0.4569, device='cuda:0')\n",
            "========== Epoch 0 Batch 116==== Step 1 AVG. val Loss 0.2994932532310486 acc = 0.0\n",
            "pos tensor(0.1234, device='cuda:0')\n",
            "neg tensor(0.4865, device='cuda:0')\n",
            "========== Epoch 0 Batch 117==== Step 1 AVG. val Loss 0.30492454767227173 acc = 0.0\n",
            "pos tensor(0.1426, device='cuda:0')\n",
            "neg tensor(0.4534, device='cuda:0')\n",
            "========== Epoch 0 Batch 118==== Step 1 AVG. val Loss 0.29800495505332947 acc = 0.0\n",
            "pos tensor(0.1401, device='cuda:0')\n",
            "neg tensor(0.4061, device='cuda:0')\n",
            "========== Epoch 0 Batch 119==== Step 1 AVG. val Loss 0.2730797231197357 acc = 0.0\n",
            "pos tensor(0.1070, device='cuda:0')\n",
            "neg tensor(0.4397, device='cuda:0')\n",
            "========== Epoch 0 Batch 120==== Step 1 AVG. val Loss 0.27334171533584595 acc = 0.0\n",
            "pos tensor(0.1307, device='cuda:0')\n",
            "neg tensor(0.4624, device='cuda:0')\n",
            "========== Epoch 0 Batch 121==== Step 1 AVG. val Loss 0.29652005434036255 acc = 0.0\n",
            "pos tensor(0.1184, device='cuda:0')\n",
            "neg tensor(0.4381, device='cuda:0')\n",
            "========== Epoch 0 Batch 122==== Step 1 AVG. val Loss 0.27823489904403687 acc = 0.0\n",
            "pos tensor(0.1503, device='cuda:0')\n",
            "neg tensor(0.4478, device='cuda:0')\n",
            "========== Epoch 0 Batch 123==== Step 1 AVG. val Loss 0.2990558445453644 acc = 0.0\n",
            "pos tensor(0.1297, device='cuda:0')\n",
            "neg tensor(0.4741, device='cuda:0')\n",
            "========== Epoch 0 Batch 124==== Step 1 AVG. val Loss 0.30190911889076233 acc = 0.0\n",
            "pos tensor(0.1118, device='cuda:0')\n",
            "neg tensor(0.4659, device='cuda:0')\n",
            "========== Epoch 0 Batch 125==== Step 1 AVG. val Loss 0.2888365685939789 acc = 0.0\n",
            "pos tensor(0.1033, device='cuda:0')\n",
            "neg tensor(0.4582, device='cuda:0')\n",
            "========== Epoch 0 Batch 126==== Step 1 AVG. val Loss 0.2807193696498871 acc = 0.0\n",
            "pos tensor(0.1362, device='cuda:0')\n",
            "neg tensor(0.4049, device='cuda:0')\n",
            "========== Epoch 0 Batch 127==== Step 1 AVG. val Loss 0.27056756615638733 acc = 0.0\n",
            "pos tensor(0.1308, device='cuda:0')\n",
            "neg tensor(0.4438, device='cuda:0')\n",
            "========== Epoch 0 Batch 128==== Step 1 AVG. val Loss 0.2872999310493469 acc = 0.0\n",
            "pos tensor(0.1092, device='cuda:0')\n",
            "neg tensor(0.4580, device='cuda:0')\n",
            "========== Epoch 0 Batch 129==== Step 1 AVG. val Loss 0.28358304500579834 acc = 0.0\n",
            "pos tensor(0.0981, device='cuda:0')\n",
            "neg tensor(0.4794, device='cuda:0')\n",
            "========== Epoch 0 Batch 130==== Step 1 AVG. val Loss 0.28877145051956177 acc = 0.0\n",
            "pos tensor(0.1112, device='cuda:0')\n",
            "neg tensor(0.4489, device='cuda:0')\n",
            "========== Epoch 0 Batch 131==== Step 1 AVG. val Loss 0.2800843417644501 acc = 0.0\n",
            "pos tensor(0.1151, device='cuda:0')\n",
            "neg tensor(0.4913, device='cuda:0')\n",
            "========== Epoch 0 Batch 132==== Step 1 AVG. val Loss 0.3031795620918274 acc = 0.0\n",
            "pos tensor(0.1219, device='cuda:0')\n",
            "neg tensor(0.4923, device='cuda:0')\n",
            "========== Epoch 0 Batch 133==== Step 1 AVG. val Loss 0.30707791447639465 acc = 0.0\n",
            "pos tensor(0.1312, device='cuda:0')\n",
            "neg tensor(0.4314, device='cuda:0')\n",
            "========== Epoch 0 Batch 134==== Step 1 AVG. val Loss 0.2812815308570862 acc = 0.0\n",
            "pos tensor(0.1194, device='cuda:0')\n",
            "neg tensor(0.4387, device='cuda:0')\n",
            "========== Epoch 0 Batch 135==== Step 1 AVG. val Loss 0.2790449261665344 acc = 0.0\n",
            "pos tensor(0.1113, device='cuda:0')\n",
            "neg tensor(0.4550, device='cuda:0')\n",
            "========== Epoch 0 Batch 136==== Step 1 AVG. val Loss 0.28314918279647827 acc = 0.0\n",
            "pos tensor(0.1090, device='cuda:0')\n",
            "neg tensor(0.4535, device='cuda:0')\n",
            "========== Epoch 0 Batch 137==== Step 1 AVG. val Loss 0.2812245488166809 acc = 0.0\n",
            "pos tensor(0.1271, device='cuda:0')\n",
            "neg tensor(0.4739, device='cuda:0')\n",
            "========== Epoch 0 Batch 138==== Step 1 AVG. val Loss 0.3005055785179138 acc = 0.0\n",
            "pos tensor(0.1379, device='cuda:0')\n",
            "neg tensor(0.4364, device='cuda:0')\n",
            "========== Epoch 0 Batch 139==== Step 1 AVG. val Loss 0.2871217131614685 acc = 0.0\n",
            "pos tensor(0.1306, device='cuda:0')\n",
            "neg tensor(0.4735, device='cuda:0')\n",
            "========== Epoch 0 Batch 140==== Step 1 AVG. val Loss 0.3020212650299072 acc = 0.0\n",
            "pos tensor(0.1520, device='cuda:0')\n",
            "neg tensor(0.4695, device='cuda:0')\n",
            "========== Epoch 0 Batch 141==== Step 1 AVG. val Loss 0.3107767105102539 acc = 0.0\n",
            "pos tensor(0.1325, device='cuda:0')\n",
            "neg tensor(0.4332, device='cuda:0')\n",
            "========== Epoch 0 Batch 142==== Step 1 AVG. val Loss 0.2828425467014313 acc = 0.0\n",
            "pos tensor(0.1551, device='cuda:0')\n",
            "neg tensor(0.4182, device='cuda:0')\n",
            "========== Epoch 0 Batch 143==== Step 1 AVG. val Loss 0.2866443395614624 acc = 0.0\n",
            "pos tensor(0.1559, device='cuda:0')\n",
            "neg tensor(0.4484, device='cuda:0')\n",
            "========== Epoch 0 Batch 144==== Step 1 AVG. val Loss 0.3021499514579773 acc = 0.0\n",
            "pos tensor(0.1597, device='cuda:0')\n",
            "neg tensor(0.4526, device='cuda:0')\n",
            "========== Epoch 0 Batch 145==== Step 1 AVG. val Loss 0.306141197681427 acc = 0.0\n",
            "pos tensor(0.1195, device='cuda:0')\n",
            "neg tensor(0.4328, device='cuda:0')\n",
            "========== Epoch 0 Batch 146==== Step 1 AVG. val Loss 0.27613702416419983 acc = 0.0\n",
            "pos tensor(0.1291, device='cuda:0')\n",
            "neg tensor(0.4695, device='cuda:0')\n",
            "========== Epoch 0 Batch 147==== Step 1 AVG. val Loss 0.2993040084838867 acc = 0.0\n",
            "pos tensor(0.1004, device='cuda:0')\n",
            "neg tensor(0.4811, device='cuda:0')\n",
            "========== Epoch 0 Batch 148==== Step 1 AVG. val Loss 0.2907560169696808 acc = 0.0\n",
            "pos tensor(0.1139, device='cuda:0')\n",
            "neg tensor(0.4476, device='cuda:0')\n",
            "========== Epoch 0 Batch 149==== Step 1 AVG. val Loss 0.2807709574699402 acc = 0.0\n",
            "pos tensor(0.1224, device='cuda:0')\n",
            "neg tensor(0.4284, device='cuda:0')\n",
            "========== Epoch 0 Batch 150==== Step 1 AVG. val Loss 0.27539730072021484 acc = 0.0\n",
            "pos tensor(0.1248, device='cuda:0')\n",
            "neg tensor(0.4070, device='cuda:0')\n",
            "========== Epoch 0 Batch 151==== Step 1 AVG. val Loss 0.2659228444099426 acc = 0.0\n",
            "pos tensor(0.1202, device='cuda:0')\n",
            "neg tensor(0.5004, device='cuda:0')\n",
            "========== Epoch 0 Batch 152==== Step 1 AVG. val Loss 0.31031835079193115 acc = 0.0\n",
            "pos tensor(0.1437, device='cuda:0')\n",
            "neg tensor(0.4409, device='cuda:0')\n",
            "========== Epoch 0 Batch 153==== Step 1 AVG. val Loss 0.29232949018478394 acc = 0.0\n",
            "pos tensor(0.1383, device='cuda:0')\n",
            "neg tensor(0.4237, device='cuda:0')\n",
            "========== Epoch 0 Batch 154==== Step 1 AVG. val Loss 0.28095826506614685 acc = 0.0\n",
            "pos tensor(0.1322, device='cuda:0')\n",
            "neg tensor(0.4390, device='cuda:0')\n",
            "========== Epoch 0 Batch 155==== Step 1 AVG. val Loss 0.2855972647666931 acc = 0.0\n",
            "pos tensor(0.1349, device='cuda:0')\n",
            "neg tensor(0.4391, device='cuda:0')\n",
            "========== Epoch 0 Batch 156==== Step 1 AVG. val Loss 0.2869930565357208 acc = 0.0\n",
            "pos tensor(0.1490, device='cuda:0')\n",
            "neg tensor(0.4426, device='cuda:0')\n",
            "========== Epoch 0 Batch 157==== Step 1 AVG. val Loss 0.2957995533943176 acc = 0.0\n",
            "pos tensor(0.1584, device='cuda:0')\n",
            "neg tensor(0.4255, device='cuda:0')\n",
            "========== Epoch 0 Batch 158==== Step 1 AVG. val Loss 0.29196521639823914 acc = 0.0\n",
            "pos tensor(0.1095, device='cuda:0')\n",
            "neg tensor(0.4449, device='cuda:0')\n",
            "========== Epoch 0 Batch 159==== Step 1 AVG. val Loss 0.27722933888435364 acc = 0.0\n",
            "pos tensor(0.1329, device='cuda:0')\n",
            "neg tensor(0.4323, device='cuda:0')\n",
            "========== Epoch 0 Batch 160==== Step 1 AVG. val Loss 0.2826072871685028 acc = 0.0\n",
            "pos tensor(0.1448, device='cuda:0')\n",
            "neg tensor(0.4563, device='cuda:0')\n",
            "========== Epoch 0 Batch 161==== Step 1 AVG. val Loss 0.300564169883728 acc = 0.0\n",
            "pos tensor(0.1421, device='cuda:0')\n",
            "neg tensor(0.4418, device='cuda:0')\n",
            "========== Epoch 0 Batch 162==== Step 1 AVG. val Loss 0.2919793128967285 acc = 0.0\n",
            "pos tensor(0.1527, device='cuda:0')\n",
            "neg tensor(0.4102, device='cuda:0')\n",
            "========== Epoch 0 Batch 163==== Step 1 AVG. val Loss 0.2814432978630066 acc = 0.0\n",
            "pos tensor(0.1285, device='cuda:0')\n",
            "neg tensor(0.4554, device='cuda:0')\n",
            "========== Epoch 0 Batch 164==== Step 1 AVG. val Loss 0.2919822633266449 acc = 0.0\n",
            "pos tensor(0.1569, device='cuda:0')\n",
            "neg tensor(0.4088, device='cuda:0')\n",
            "========== Epoch 0 Batch 165==== Step 1 AVG. val Loss 0.2828395962715149 acc = 0.0\n",
            "pos tensor(0.1486, device='cuda:0')\n",
            "neg tensor(0.4464, device='cuda:0')\n",
            "========== Epoch 0 Batch 166==== Step 1 AVG. val Loss 0.2975063621997833 acc = 0.0\n",
            "pos tensor(0.1756, device='cuda:0')\n",
            "neg tensor(0.4342, device='cuda:0')\n",
            "========== Epoch 0 Batch 167==== Step 1 AVG. val Loss 0.30485960841178894 acc = 0.0\n",
            "pos tensor(0.1340, device='cuda:0')\n",
            "neg tensor(0.4490, device='cuda:0')\n",
            "========== Epoch 0 Batch 168==== Step 1 AVG. val Loss 0.29146069288253784 acc = 0.0\n",
            "pos tensor(0.1218, device='cuda:0')\n",
            "neg tensor(0.4339, device='cuda:0')\n",
            "========== Epoch 0 Batch 169==== Step 1 AVG. val Loss 0.27786096930503845 acc = 0.0\n",
            "pos tensor(0.1845, device='cuda:0')\n",
            "neg tensor(0.4481, device='cuda:0')\n",
            "========== Epoch 0 Batch 170==== Step 1 AVG. val Loss 0.31630268692970276 acc = 0.0\n",
            "pos tensor(0.1573, device='cuda:0')\n",
            "neg tensor(0.4417, device='cuda:0')\n",
            "========== Epoch 0 Batch 171==== Step 1 AVG. val Loss 0.2995370030403137 acc = 0.0\n",
            "pos tensor(0.1104, device='cuda:0')\n",
            "neg tensor(0.4300, device='cuda:0')\n",
            "========== Epoch 0 Batch 172==== Step 1 AVG. val Loss 0.27017176151275635 acc = 0.0\n",
            "pos tensor(0.1243, device='cuda:0')\n",
            "neg tensor(0.4552, device='cuda:0')\n",
            "========== Epoch 0 Batch 173==== Step 1 AVG. val Loss 0.2897571325302124 acc = 0.0\n",
            "pos tensor(0.1143, device='cuda:0')\n",
            "neg tensor(0.4764, device='cuda:0')\n",
            "========== Epoch 0 Batch 174==== Step 1 AVG. val Loss 0.29537713527679443 acc = 0.0\n",
            "pos tensor(0.0984, device='cuda:0')\n",
            "neg tensor(0.4970, device='cuda:0')\n",
            "========== Epoch 0 Batch 175==== Step 1 AVG. val Loss 0.2977125346660614 acc = 0.0\n",
            "pos tensor(0.1081, device='cuda:0')\n",
            "neg tensor(0.4513, device='cuda:0')\n",
            "========== Epoch 0 Batch 176==== Step 1 AVG. val Loss 0.27969616651535034 acc = 0.0\n",
            "pos tensor(0.1499, device='cuda:0')\n",
            "neg tensor(0.4322, device='cuda:0')\n",
            "========== Epoch 0 Batch 177==== Step 1 AVG. val Loss 0.29105326533317566 acc = 0.0\n",
            "pos tensor(0.1006, device='cuda:0')\n",
            "neg tensor(0.4886, device='cuda:0')\n",
            "========== Epoch 0 Batch 178==== Step 1 AVG. val Loss 0.29458245635032654 acc = 0.0\n",
            "pos tensor(0.1407, device='cuda:0')\n",
            "neg tensor(0.4349, device='cuda:0')\n",
            "========== Epoch 0 Batch 179==== Step 1 AVG. val Loss 0.2877695560455322 acc = 0.0\n",
            "pos tensor(0.1240, device='cuda:0')\n",
            "neg tensor(0.4782, device='cuda:0')\n",
            "========== Epoch 0 Batch 180==== Step 1 AVG. val Loss 0.3010968863964081 acc = 0.0\n",
            "pos tensor(0.1195, device='cuda:0')\n",
            "neg tensor(0.4772, device='cuda:0')\n",
            "========== Epoch 0 Batch 181==== Step 1 AVG. val Loss 0.29831159114837646 acc = 0.0\n",
            "pos tensor(0.1307, device='cuda:0')\n",
            "neg tensor(0.4370, device='cuda:0')\n",
            "========== Epoch 0 Batch 182==== Step 1 AVG. val Loss 0.28389039635658264 acc = 0.0\n",
            "pos tensor(0.1207, device='cuda:0')\n",
            "neg tensor(0.4591, device='cuda:0')\n",
            "========== Epoch 0 Batch 183==== Step 1 AVG. val Loss 0.2899083197116852 acc = 0.0\n",
            "pos tensor(0.1267, device='cuda:0')\n",
            "neg tensor(0.4349, device='cuda:0')\n",
            "========== Epoch 0 Batch 184==== Step 1 AVG. val Loss 0.28077781200408936 acc = 0.0\n",
            "pos tensor(0.1718, device='cuda:0')\n",
            "neg tensor(0.4316, device='cuda:0')\n",
            "========== Epoch 0 Batch 185==== Step 1 AVG. val Loss 0.3017250895500183 acc = 0.0\n",
            "pos tensor(0.0904, device='cuda:0')\n",
            "neg tensor(0.4740, device='cuda:0')\n",
            "========== Epoch 0 Batch 186==== Step 1 AVG. val Loss 0.2822086215019226 acc = 0.0\n",
            "pos tensor(0.1062, device='cuda:0')\n",
            "neg tensor(0.4459, device='cuda:0')\n",
            "========== Epoch 0 Batch 187==== Step 1 AVG. val Loss 0.2760528028011322 acc = 0.0\n",
            "pos tensor(0.1290, device='cuda:0')\n",
            "neg tensor(0.4753, device='cuda:0')\n",
            "========== Epoch 0 Batch 188==== Step 1 AVG. val Loss 0.30215537548065186 acc = 0.0\n",
            "pos tensor(0.1540, device='cuda:0')\n",
            "neg tensor(0.4686, device='cuda:0')\n",
            "========== Epoch 0 Batch 189==== Step 1 AVG. val Loss 0.31129658222198486 acc = 0.0\n",
            "pos tensor(0.1076, device='cuda:0')\n",
            "neg tensor(0.4692, device='cuda:0')\n",
            "========== Epoch 0 Batch 190==== Step 1 AVG. val Loss 0.28836965560913086 acc = 0.0\n",
            "pos tensor(0.1207, device='cuda:0')\n",
            "neg tensor(0.4886, device='cuda:0')\n",
            "========== Epoch 0 Batch 191==== Step 1 AVG. val Loss 0.3046642243862152 acc = 0.0\n",
            "pos tensor(0.1409, device='cuda:0')\n",
            "neg tensor(0.4284, device='cuda:0')\n",
            "========== Epoch 0 Batch 192==== Step 1 AVG. val Loss 0.2846332788467407 acc = 0.0\n",
            "pos tensor(0.1117, device='cuda:0')\n",
            "neg tensor(0.4856, device='cuda:0')\n",
            "========== Epoch 0 Batch 193==== Step 1 AVG. val Loss 0.2986295521259308 acc = 0.0\n",
            "pos tensor(0.1409, device='cuda:0')\n",
            "neg tensor(0.4140, device='cuda:0')\n",
            "========== Epoch 0 Batch 194==== Step 1 AVG. val Loss 0.27745431661605835 acc = 0.0\n",
            "pos tensor(0.1479, device='cuda:0')\n",
            "neg tensor(0.4174, device='cuda:0')\n",
            "========== Epoch 0 Batch 195==== Step 1 AVG. val Loss 0.2826511263847351 acc = 0.0\n",
            "pos tensor(0.1382, device='cuda:0')\n",
            "neg tensor(0.4488, device='cuda:0')\n",
            "========== Epoch 0 Batch 196==== Step 1 AVG. val Loss 0.29349660873413086 acc = 0.0\n",
            "pos tensor(0.1634, device='cuda:0')\n",
            "neg tensor(0.4448, device='cuda:0')\n",
            "========== Epoch 0 Batch 197==== Step 1 AVG. val Loss 0.3040696382522583 acc = 0.0\n",
            "pos tensor(0.1056, device='cuda:0')\n",
            "neg tensor(0.4532, device='cuda:0')\n",
            "========== Epoch 0 Batch 198==== Step 1 AVG. val Loss 0.27940988540649414 acc = 0.0\n",
            "pos tensor(0.1155, device='cuda:0')\n",
            "neg tensor(0.4504, device='cuda:0')\n",
            "========== Epoch 0 Batch 199==== Step 1 AVG. val Loss 0.2829352021217346 acc = 0.0\n",
            "pos tensor(0.1153, device='cuda:0')\n",
            "neg tensor(0.4357, device='cuda:0')\n",
            "========== Epoch 0 Batch 200==== Step 1 AVG. val Loss 0.2754978537559509 acc = 0.0\n",
            "pos tensor(0.0947, device='cuda:0')\n",
            "neg tensor(0.5098, device='cuda:0')\n",
            "========== Epoch 0 Batch 201==== Step 1 AVG. val Loss 0.3022661805152893 acc = 0.0\n",
            "pos tensor(0.1446, device='cuda:0')\n",
            "neg tensor(0.5055, device='cuda:0')\n",
            "========== Epoch 0 Batch 202==== Step 1 AVG. val Loss 0.3250751197338104 acc = 0.0\n",
            "pos tensor(0.1352, device='cuda:0')\n",
            "neg tensor(0.4389, device='cuda:0')\n",
            "========== Epoch 0 Batch 203==== Step 1 AVG. val Loss 0.2870730459690094 acc = 0.0\n",
            "pos tensor(0.1049, device='cuda:0')\n",
            "neg tensor(0.4733, device='cuda:0')\n",
            "========== Epoch 0 Batch 204==== Step 1 AVG. val Loss 0.28910666704177856 acc = 0.0\n",
            "pos tensor(0.1320, device='cuda:0')\n",
            "neg tensor(0.4426, device='cuda:0')\n",
            "========== Epoch 0 Batch 205==== Step 1 AVG. val Loss 0.28730297088623047 acc = 0.0\n",
            "pos tensor(0.1065, device='cuda:0')\n",
            "neg tensor(0.4804, device='cuda:0')\n",
            "========== Epoch 0 Batch 206==== Step 1 AVG. val Loss 0.2934521436691284 acc = 0.0\n",
            "pos tensor(0.1573, device='cuda:0')\n",
            "neg tensor(0.4443, device='cuda:0')\n",
            "========== Epoch 0 Batch 207==== Step 1 AVG. val Loss 0.30079755187034607 acc = 0.0\n",
            "pos tensor(0.1863, device='cuda:0')\n",
            "neg tensor(0.4135, device='cuda:0')\n",
            "========== Epoch 0 Batch 208==== Step 1 AVG. val Loss 0.299924373626709 acc = 0.0\n",
            "pos tensor(0.1333, device='cuda:0')\n",
            "neg tensor(0.4293, device='cuda:0')\n",
            "========== Epoch 0 Batch 209==== Step 1 AVG. val Loss 0.28127557039260864 acc = 0.0\n",
            "pos tensor(0.1338, device='cuda:0')\n",
            "neg tensor(0.4533, device='cuda:0')\n",
            "========== Epoch 0 Batch 210==== Step 1 AVG. val Loss 0.2935863137245178 acc = 0.0\n",
            "pos tensor(0.1567, device='cuda:0')\n",
            "neg tensor(0.4511, device='cuda:0')\n",
            "========== Epoch 0 Batch 211==== Step 1 AVG. val Loss 0.3038826286792755 acc = 0.0\n",
            "pos tensor(0.1524, device='cuda:0')\n",
            "neg tensor(0.4579, device='cuda:0')\n",
            "========== Epoch 0 Batch 212==== Step 1 AVG. val Loss 0.30518674850463867 acc = 0.0\n",
            "pos tensor(0.1514, device='cuda:0')\n",
            "neg tensor(0.4233, device='cuda:0')\n",
            "========== Epoch 0 Batch 213==== Step 1 AVG. val Loss 0.2873527407646179 acc = 0.0\n",
            "pos tensor(0.1272, device='cuda:0')\n",
            "neg tensor(0.4311, device='cuda:0')\n",
            "========== Epoch 0 Batch 214==== Step 1 AVG. val Loss 0.2791558504104614 acc = 0.0\n",
            "pos tensor(0.0819, device='cuda:0')\n",
            "neg tensor(0.4605, device='cuda:0')\n",
            "========== Epoch 0 Batch 215==== Step 1 AVG. val Loss 0.27117350697517395 acc = 0.0\n",
            "pos tensor(0.1214, device='cuda:0')\n",
            "neg tensor(0.4572, device='cuda:0')\n",
            "========== Epoch 0 Batch 216==== Step 1 AVG. val Loss 0.28930479288101196 acc = 0.0\n",
            "pos tensor(0.1450, device='cuda:0')\n",
            "neg tensor(0.4465, device='cuda:0')\n",
            "========== Epoch 0 Batch 217==== Step 1 AVG. val Loss 0.2957717776298523 acc = 0.0\n",
            "pos tensor(0.1515, device='cuda:0')\n",
            "neg tensor(0.4466, device='cuda:0')\n",
            "========== Epoch 0 Batch 218==== Step 1 AVG. val Loss 0.29906731843948364 acc = 0.0\n",
            "pos tensor(0.1581, device='cuda:0')\n",
            "neg tensor(0.3925, device='cuda:0')\n",
            "========== Epoch 0 Batch 219==== Step 1 AVG. val Loss 0.27530425786972046 acc = 0.0\n",
            "pos tensor(0.1359, device='cuda:0')\n",
            "neg tensor(0.4454, device='cuda:0')\n",
            "========== Epoch 0 Batch 220==== Step 1 AVG. val Loss 0.29064232110977173 acc = 0.0\n",
            "pos tensor(0.1078, device='cuda:0')\n",
            "neg tensor(0.4796, device='cuda:0')\n",
            "========== Epoch 0 Batch 221==== Step 1 AVG. val Loss 0.29366040229797363 acc = 0.0\n",
            "pos tensor(0.1753, device='cuda:0')\n",
            "neg tensor(0.4112, device='cuda:0')\n",
            "========== Epoch 0 Batch 222==== Step 1 AVG. val Loss 0.29323166608810425 acc = 0.0\n",
            "pos tensor(0.1524, device='cuda:0')\n",
            "neg tensor(0.4701, device='cuda:0')\n",
            "========== Epoch 0 Batch 223==== Step 1 AVG. val Loss 0.31127238273620605 acc = 0.0\n",
            "pos tensor(0.1462, device='cuda:0')\n",
            "neg tensor(0.4441, device='cuda:0')\n",
            "========== Epoch 0 Batch 224==== Step 1 AVG. val Loss 0.29513096809387207 acc = 0.0\n",
            "pos tensor(0.1012, device='cuda:0')\n",
            "neg tensor(0.4340, device='cuda:0')\n",
            "========== Epoch 0 Batch 225==== Step 1 AVG. val Loss 0.2675817906856537 acc = 0.0\n",
            "pos tensor(0.1073, device='cuda:0')\n",
            "neg tensor(0.4702, device='cuda:0')\n",
            "========== Epoch 0 Batch 226==== Step 1 AVG. val Loss 0.2887856066226959 acc = 0.0\n",
            "pos tensor(0.1242, device='cuda:0')\n",
            "neg tensor(0.4710, device='cuda:0')\n",
            "========== Epoch 0 Batch 227==== Step 1 AVG. val Loss 0.29760482907295227 acc = 0.0\n",
            "pos tensor(0.1665, device='cuda:0')\n",
            "neg tensor(0.4504, device='cuda:0')\n",
            "========== Epoch 0 Batch 228==== Step 1 AVG. val Loss 0.3084670603275299 acc = 0.0\n",
            "pos tensor(0.1434, device='cuda:0')\n",
            "neg tensor(0.4519, device='cuda:0')\n",
            "========== Epoch 0 Batch 229==== Step 1 AVG. val Loss 0.29762059450149536 acc = 0.0\n",
            "pos tensor(0.1234, device='cuda:0')\n",
            "neg tensor(0.4465, device='cuda:0')\n",
            "========== Epoch 0 Batch 230==== Step 1 AVG. val Loss 0.28495290875434875 acc = 0.0\n",
            "pos tensor(0.1425, device='cuda:0')\n",
            "neg tensor(0.4736, device='cuda:0')\n",
            "========== Epoch 0 Batch 231==== Step 1 AVG. val Loss 0.30806511640548706 acc = 0.0\n",
            "pos tensor(0.1406, device='cuda:0')\n",
            "neg tensor(0.4426, device='cuda:0')\n",
            "========== Epoch 0 Batch 232==== Step 1 AVG. val Loss 0.2915741205215454 acc = 0.0\n",
            "pos tensor(0.1172, device='cuda:0')\n",
            "neg tensor(0.4255, device='cuda:0')\n",
            "========== Epoch 0 Batch 233==== Step 1 AVG. val Loss 0.27135229110717773 acc = 0.0\n",
            "pos tensor(0.1559, device='cuda:0')\n",
            "neg tensor(0.4383, device='cuda:0')\n",
            "========== Epoch 0 Batch 234==== Step 1 AVG. val Loss 0.2970956563949585 acc = 0.0\n",
            "pos tensor(0.1419, device='cuda:0')\n",
            "neg tensor(0.4409, device='cuda:0')\n",
            "========== Epoch 0 Batch 235==== Step 1 AVG. val Loss 0.29140621423721313 acc = 0.0\n",
            "pos tensor(0.1226, device='cuda:0')\n",
            "neg tensor(0.4526, device='cuda:0')\n",
            "========== Epoch 0 Batch 236==== Step 1 AVG. val Loss 0.28756529092788696 acc = 0.0\n",
            "pos tensor(0.1768, device='cuda:0')\n",
            "neg tensor(0.4720, device='cuda:0')\n",
            "========== Epoch 0 Batch 237==== Step 1 AVG. val Loss 0.3244141936302185 acc = 0.0\n",
            "pos tensor(0.1476, device='cuda:0')\n",
            "neg tensor(0.4334, device='cuda:0')\n",
            "========== Epoch 0 Batch 238==== Step 1 AVG. val Loss 0.2905222475528717 acc = 0.0\n",
            "pos tensor(0.1559, device='cuda:0')\n",
            "neg tensor(0.4671, device='cuda:0')\n",
            "========== Epoch 0 Batch 239==== Step 1 AVG. val Loss 0.3114713430404663 acc = 0.0\n",
            "pos tensor(0.1043, device='cuda:0')\n",
            "neg tensor(0.4723, device='cuda:0')\n",
            "========== Epoch 0 Batch 240==== Step 1 AVG. val Loss 0.28830426931381226 acc = 0.0\n",
            "pos tensor(0.1386, device='cuda:0')\n",
            "neg tensor(0.4476, device='cuda:0')\n",
            "========== Epoch 0 Batch 241==== Step 1 AVG. val Loss 0.2931135296821594 acc = 0.0\n",
            "pos tensor(0.1512, device='cuda:0')\n",
            "neg tensor(0.4502, device='cuda:0')\n",
            "========== Epoch 0 Batch 242==== Step 1 AVG. val Loss 0.30070871114730835 acc = 0.0\n",
            "pos tensor(0.1047, device='cuda:0')\n",
            "neg tensor(0.5040, device='cuda:0')\n",
            "========== Epoch 0 Batch 243==== Step 1 AVG. val Loss 0.3043261468410492 acc = 0.0\n",
            "pos tensor(0.1252, device='cuda:0')\n",
            "neg tensor(0.4582, device='cuda:0')\n",
            "========== Epoch 0 Batch 244==== Step 1 AVG. val Loss 0.2916962504386902 acc = 0.0\n",
            "pos tensor(0.1299, device='cuda:0')\n",
            "neg tensor(0.4546, device='cuda:0')\n",
            "========== Epoch 0 Batch 245==== Step 1 AVG. val Loss 0.2922550439834595 acc = 0.0\n",
            "pos tensor(0.1269, device='cuda:0')\n",
            "neg tensor(0.5040, device='cuda:0')\n",
            "========== Epoch 0 Batch 246==== Step 1 AVG. val Loss 0.31547635793685913 acc = 0.0\n",
            "pos tensor(0.1693, device='cuda:0')\n",
            "neg tensor(0.4221, device='cuda:0')\n",
            "========== Epoch 0 Batch 247==== Step 1 AVG. val Loss 0.29573875665664673 acc = 0.0\n",
            "pos tensor(0.1283, device='cuda:0')\n",
            "neg tensor(0.4754, device='cuda:0')\n",
            "========== Epoch 0 Batch 248==== Step 1 AVG. val Loss 0.3018317222595215 acc = 0.0\n",
            "pos tensor(0.1187, device='cuda:0')\n",
            "neg tensor(0.4704, device='cuda:0')\n",
            "========== Epoch 0 Batch 249==== Step 1 AVG. val Loss 0.2945529818534851 acc = 0.0\n",
            "pos tensor(0.1607, device='cuda:0')\n",
            "neg tensor(0.4224, device='cuda:0')\n",
            "========== Epoch 0 Batch 250==== Step 1 AVG. val Loss 0.2915891110897064 acc = 0.0\n",
            "pos tensor(0.1442, device='cuda:0')\n",
            "neg tensor(0.4762, device='cuda:0')\n",
            "========== Epoch 0 Batch 251==== Step 1 AVG. val Loss 0.31019818782806396 acc = 0.0\n",
            "pos tensor(0.1382, device='cuda:0')\n",
            "neg tensor(0.4641, device='cuda:0')\n",
            "========== Epoch 0 Batch 252==== Step 1 AVG. val Loss 0.30113518238067627 acc = 0.0\n",
            "pos tensor(0.1401, device='cuda:0')\n",
            "neg tensor(0.4688, device='cuda:0')\n",
            "========== Epoch 0 Batch 253==== Step 1 AVG. val Loss 0.3044726252555847 acc = 0.0\n",
            "pos tensor(0.1256, device='cuda:0')\n",
            "neg tensor(0.4191, device='cuda:0')\n",
            "========== Epoch 0 Batch 254==== Step 1 AVG. val Loss 0.27231091260910034 acc = 0.0\n",
            "pos tensor(0.1454, device='cuda:0')\n",
            "neg tensor(0.4691, device='cuda:0')\n",
            "========== Epoch 0 Batch 255==== Step 1 AVG. val Loss 0.30723944306373596 acc = 0.0\n",
            "pos tensor(0.0927, device='cuda:0')\n",
            "neg tensor(0.4471, device='cuda:0')\n",
            "========== Epoch 0 Batch 256==== Step 1 AVG. val Loss 0.2698848247528076 acc = 0.0\n",
            "pos tensor(0.1270, device='cuda:0')\n",
            "neg tensor(0.4759, device='cuda:0')\n",
            "========== Epoch 0 Batch 257==== Step 1 AVG. val Loss 0.30145466327667236 acc = 0.0\n",
            "pos tensor(0.1188, device='cuda:0')\n",
            "neg tensor(0.4587, device='cuda:0')\n",
            "========== Epoch 0 Batch 258==== Step 1 AVG. val Loss 0.28870970010757446 acc = 0.0\n",
            "pos tensor(0.1757, device='cuda:0')\n",
            "neg tensor(0.4269, device='cuda:0')\n",
            "========== Epoch 0 Batch 259==== Step 1 AVG. val Loss 0.30128008127212524 acc = 0.0\n",
            "pos tensor(0.1527, device='cuda:0')\n",
            "neg tensor(0.4645, device='cuda:0')\n",
            "========== Epoch 0 Batch 260==== Step 1 AVG. val Loss 0.30856627225875854 acc = 0.0\n",
            "pos tensor(0.1025, device='cuda:0')\n",
            "neg tensor(0.4587, device='cuda:0')\n",
            "========== Epoch 0 Batch 261==== Step 1 AVG. val Loss 0.28061312437057495 acc = 0.0\n",
            "pos tensor(0.1732, device='cuda:0')\n",
            "neg tensor(0.3997, device='cuda:0')\n",
            "========== Epoch 0 Batch 262==== Step 1 AVG. val Loss 0.2864415645599365 acc = 0.0\n",
            "pos tensor(0.1413, device='cuda:0')\n",
            "neg tensor(0.4396, device='cuda:0')\n",
            "========== Epoch 0 Batch 263==== Step 1 AVG. val Loss 0.2904514670372009 acc = 0.0\n",
            "pos tensor(0.1344, device='cuda:0')\n",
            "neg tensor(0.4644, device='cuda:0')\n",
            "========== Epoch 0 Batch 264==== Step 1 AVG. val Loss 0.29940229654312134 acc = 0.0\n",
            "pos tensor(0.1282, device='cuda:0')\n",
            "neg tensor(0.4227, device='cuda:0')\n",
            "========== Epoch 0 Batch 265==== Step 1 AVG. val Loss 0.27544596791267395 acc = 0.0\n",
            "pos tensor(0.1335, device='cuda:0')\n",
            "neg tensor(0.4635, device='cuda:0')\n",
            "========== Epoch 0 Batch 266==== Step 1 AVG. val Loss 0.298490434885025 acc = 0.0\n",
            "pos tensor(0.1014, device='cuda:0')\n",
            "neg tensor(0.4707, device='cuda:0')\n",
            "========== Epoch 0 Batch 267==== Step 1 AVG. val Loss 0.28602904081344604 acc = 0.0\n",
            "pos tensor(0.1064, device='cuda:0')\n",
            "neg tensor(0.4569, device='cuda:0')\n",
            "========== Epoch 0 Batch 268==== Step 1 AVG. val Loss 0.28167039155960083 acc = 0.0\n",
            "pos tensor(0.1256, device='cuda:0')\n",
            "neg tensor(0.4738, device='cuda:0')\n",
            "========== Epoch 0 Batch 269==== Step 1 AVG. val Loss 0.2997199296951294 acc = 0.0\n",
            "pos tensor(0.1428, device='cuda:0')\n",
            "neg tensor(0.4174, device='cuda:0')\n",
            "========== Epoch 0 Batch 270==== Step 1 AVG. val Loss 0.2800646722316742 acc = 0.0\n",
            "pos tensor(0.1031, device='cuda:0')\n",
            "neg tensor(0.4780, device='cuda:0')\n",
            "========== Epoch 0 Batch 271==== Step 1 AVG. val Loss 0.2905246615409851 acc = 0.0\n",
            "pos tensor(0.1322, device='cuda:0')\n",
            "neg tensor(0.4486, device='cuda:0')\n",
            "========== Epoch 0 Batch 272==== Step 1 AVG. val Loss 0.29038915038108826 acc = 0.0\n",
            "pos tensor(0.1341, device='cuda:0')\n",
            "neg tensor(0.4175, device='cuda:0')\n",
            "========== Epoch 0 Batch 273==== Step 1 AVG. val Loss 0.2758139669895172 acc = 0.0\n",
            "pos tensor(0.1118, device='cuda:0')\n",
            "neg tensor(0.4159, device='cuda:0')\n",
            "========== Epoch 0 Batch 274==== Step 1 AVG. val Loss 0.2638653814792633 acc = 0.0\n",
            "pos tensor(0.1307, device='cuda:0')\n",
            "neg tensor(0.4536, device='cuda:0')\n",
            "========== Epoch 0 Batch 275==== Step 1 AVG. val Loss 0.29212021827697754 acc = 0.0\n",
            "pos tensor(0.1246, device='cuda:0')\n",
            "neg tensor(0.4807, device='cuda:0')\n",
            "========== Epoch 0 Batch 276==== Step 1 AVG. val Loss 0.3026413321495056 acc = 0.0\n",
            "pos tensor(0.1125, device='cuda:0')\n",
            "neg tensor(0.4331, device='cuda:0')\n",
            "========== Epoch 0 Batch 277==== Step 1 AVG. val Loss 0.272786945104599 acc = 0.0\n",
            "pos tensor(0.1491, device='cuda:0')\n",
            "neg tensor(0.4606, device='cuda:0')\n",
            "========== Epoch 0 Batch 278==== Step 1 AVG. val Loss 0.3048647344112396 acc = 0.0\n",
            "pos tensor(0.1442, device='cuda:0')\n",
            "neg tensor(0.4354, device='cuda:0')\n",
            "========== Epoch 0 Batch 279==== Step 1 AVG. val Loss 0.28976982831954956 acc = 0.0\n",
            "pos tensor(0.1546, device='cuda:0')\n",
            "neg tensor(0.4480, device='cuda:0')\n",
            "========== Epoch 0 Batch 280==== Step 1 AVG. val Loss 0.3013076186180115 acc = 0.0\n",
            "pos tensor(0.1414, device='cuda:0')\n",
            "neg tensor(0.4511, device='cuda:0')\n",
            "========== Epoch 0 Batch 281==== Step 1 AVG. val Loss 0.2962341904640198 acc = 0.0\n",
            "pos tensor(0.0970, device='cuda:0')\n",
            "neg tensor(0.4747, device='cuda:0')\n",
            "========== Epoch 0 Batch 282==== Step 1 AVG. val Loss 0.28588759899139404 acc = 0.0\n",
            "pos tensor(0.1462, device='cuda:0')\n",
            "neg tensor(0.4562, device='cuda:0')\n",
            "========== Epoch 0 Batch 283==== Step 1 AVG. val Loss 0.30117371678352356 acc = 0.0\n",
            "pos tensor(0.1443, device='cuda:0')\n",
            "neg tensor(0.4363, device='cuda:0')\n",
            "========== Epoch 0 Batch 284==== Step 1 AVG. val Loss 0.2903275489807129 acc = 0.0\n",
            "pos tensor(0.1117, device='cuda:0')\n",
            "neg tensor(0.4382, device='cuda:0')\n",
            "========== Epoch 0 Batch 285==== Step 1 AVG. val Loss 0.2749446630477905 acc = 0.0\n",
            "pos tensor(0.1600, device='cuda:0')\n",
            "neg tensor(0.4134, device='cuda:0')\n",
            "========== Epoch 0 Batch 286==== Step 1 AVG. val Loss 0.2867254614830017 acc = 0.0\n",
            "pos tensor(0.2137, device='cuda:0')\n",
            "neg tensor(0.4255, device='cuda:0')\n",
            "========== Epoch 0 Batch 287==== Step 1 AVG. val Loss 0.31959712505340576 acc = 0.0\n",
            "pos tensor(0.1319, device='cuda:0')\n",
            "neg tensor(0.4442, device='cuda:0')\n",
            "========== Epoch 0 Batch 288==== Step 1 AVG. val Loss 0.2880507707595825 acc = 0.0\n",
            "pos tensor(0.1110, device='cuda:0')\n",
            "neg tensor(0.4530, device='cuda:0')\n",
            "========== Epoch 0 Batch 289==== Step 1 AVG. val Loss 0.28196612000465393 acc = 0.0\n",
            "pos tensor(0.1365, device='cuda:0')\n",
            "neg tensor(0.4206, device='cuda:0')\n",
            "========== Epoch 0 Batch 290==== Step 1 AVG. val Loss 0.27859818935394287 acc = 0.0\n",
            "pos tensor(0.1239, device='cuda:0')\n",
            "neg tensor(0.4570, device='cuda:0')\n",
            "========== Epoch 0 Batch 291==== Step 1 AVG. val Loss 0.2904292047023773 acc = 0.0\n",
            "pos tensor(0.1231, device='cuda:0')\n",
            "neg tensor(0.4711, device='cuda:0')\n",
            "========== Epoch 0 Batch 292==== Step 1 AVG. val Loss 0.29710519313812256 acc = 0.0\n",
            "pos tensor(0.1705, device='cuda:0')\n",
            "neg tensor(0.4205, device='cuda:0')\n",
            "========== Epoch 0 Batch 293==== Step 1 AVG. val Loss 0.2954810857772827 acc = 0.0\n",
            "pos tensor(0.1392, device='cuda:0')\n",
            "neg tensor(0.4563, device='cuda:0')\n",
            "========== Epoch 0 Batch 294==== Step 1 AVG. val Loss 0.2977076470851898 acc = 0.0\n",
            "pos tensor(0.1410, device='cuda:0')\n",
            "neg tensor(0.4591, device='cuda:0')\n",
            "========== Epoch 0 Batch 295==== Step 1 AVG. val Loss 0.3000144958496094 acc = 0.0\n",
            "pos tensor(0.1398, device='cuda:0')\n",
            "neg tensor(0.4256, device='cuda:0')\n",
            "========== Epoch 0 Batch 296==== Step 1 AVG. val Loss 0.2827204465866089 acc = 0.0\n",
            "pos tensor(0.1158, device='cuda:0')\n",
            "neg tensor(0.4232, device='cuda:0')\n",
            "========== Epoch 0 Batch 297==== Step 1 AVG. val Loss 0.2695068418979645 acc = 0.0\n",
            "pos tensor(0.1218, device='cuda:0')\n",
            "neg tensor(0.4374, device='cuda:0')\n",
            "========== Epoch 0 Batch 298==== Step 1 AVG. val Loss 0.2795836329460144 acc = 0.0\n",
            "pos tensor(0.1595, device='cuda:0')\n",
            "neg tensor(0.4450, device='cuda:0')\n",
            "========== Epoch 0 Batch 299==== Step 1 AVG. val Loss 0.3022516965866089 acc = 0.0\n",
            "pos tensor(0.1832, device='cuda:0')\n",
            "neg tensor(0.4501, device='cuda:0')\n",
            "========== Epoch 0 Batch 300==== Step 1 AVG. val Loss 0.3166316747665405 acc = 0.0\n",
            "  Average Validation Loss: 0.29\n",
            "  Average Validation Accuracy: 0.00\n",
            "  Validation took: 0:20:43\n",
            "Validation loss decreased (inf --> 0.290888).  Saving model ...\n",
            "\n",
            "======== Epoch 2 / 2 ========\n",
            "Training...\n",
            "pos tensor(0.1933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 1==== Step 1  Train Loss 0.2603130340576172 acc = 0\n",
            "pos tensor(0.1524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 2==== Step 1  Train Loss 0.2645972967147827 acc = 0\n",
            "pos tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 3==== Step 1  Train Loss 0.27013832330703735 acc = 0\n",
            "pos tensor(0.2122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 4==== Step 1  Train Loss 0.2610732316970825 acc = 0\n",
            "pos tensor(0.2170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 5==== Step 1  Train Loss 0.2719106078147888 acc = 0\n",
            "pos tensor(0.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 6==== Step 1  Train Loss 0.27049219608306885 acc = 0\n",
            "pos tensor(0.1612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 7==== Step 1  Train Loss 0.26533594727516174 acc = 0\n",
            "pos tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 8==== Step 1  Train Loss 0.2771913707256317 acc = 0\n",
            "pos tensor(0.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 9==== Step 1  Train Loss 0.2768280506134033 acc = 0\n",
            "pos tensor(0.1590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 10==== Step 1  Train Loss 0.23654210567474365 acc = 0\n",
            "pos tensor(0.2587, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.2915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 11==== Step 1  Train Loss 0.27512961626052856 acc = 0\n",
            "pos tensor(0.1991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 12==== Step 1  Train Loss 0.277055561542511 acc = 0\n",
            "pos tensor(0.2351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 13==== Step 1  Train Loss 0.2793964743614197 acc = 0\n",
            "pos tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 14==== Step 1  Train Loss 0.2629028558731079 acc = 0\n",
            "pos tensor(0.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 15==== Step 1  Train Loss 0.2654188275337219 acc = 0\n",
            "pos tensor(0.1920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 16==== Step 1  Train Loss 0.2486138790845871 acc = 0\n",
            "pos tensor(0.2431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 17==== Step 1  Train Loss 0.2953132092952728 acc = 0\n",
            "pos tensor(0.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 18==== Step 1  Train Loss 0.25876665115356445 acc = 0\n",
            "pos tensor(0.2375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 19==== Step 1  Train Loss 0.2710595726966858 acc = 0\n",
            "pos tensor(0.1816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 20==== Step 1  Train Loss 0.272477924823761 acc = 0\n",
            "pos tensor(0.2410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 21==== Step 1  Train Loss 0.2826496362686157 acc = 0\n",
            "pos tensor(0.1962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 22==== Step 1  Train Loss 0.2622540295124054 acc = 0\n",
            "pos tensor(0.1909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 23==== Step 1  Train Loss 0.2677445411682129 acc = 0\n",
            "pos tensor(0.2317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 24==== Step 1  Train Loss 0.2808295786380768 acc = 0\n",
            "pos tensor(0.2171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 25==== Step 1  Train Loss 0.2718559503555298 acc = 0\n",
            "pos tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 26==== Step 1  Train Loss 0.2715766727924347 acc = 0\n",
            "pos tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 27==== Step 1  Train Loss 0.2506321668624878 acc = 0\n",
            "pos tensor(0.1396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 28==== Step 1  Train Loss 0.22969479858875275 acc = 0\n",
            "pos tensor(0.1895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 29==== Step 1  Train Loss 0.2681734561920166 acc = 0\n",
            "pos tensor(0.2582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 30==== Step 1  Train Loss 0.2990432381629944 acc = 0\n",
            "pos tensor(0.2190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 31==== Step 1  Train Loss 0.29778411984443665 acc = 0\n",
            "pos tensor(0.2032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 32==== Step 1  Train Loss 0.2690540850162506 acc = 0\n",
            "pos tensor(0.2197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 33==== Step 1  Train Loss 0.2654387950897217 acc = 0\n",
            "pos tensor(0.1760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 34==== Step 1  Train Loss 0.2855435013771057 acc = 0\n",
            "pos tensor(0.2703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "neg tensor(0.3518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "========== Epoch 1 Batch 35==== Step 1  Train Loss 0.31102922558784485 acc = 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3d7a8eac969d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalLosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainLosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmytrainStep1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelEmb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-db2dbdb9aea1>\u001b[0m in \u001b[0;36mmytrainStep1\u001b[0;34m(model, criterion1, criterion2)\u001b[0m\n\u001b[1;32m     93\u001b[0m               \u001b[0;31m# indices = torch.arange(0, FC11.size(0), device=device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0;31m# labels = torch.cat((indices, indices))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m               \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFC11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFC22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m               \u001b[0;31m# loss1 = criterion1(embeddings,labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m               \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e533fff5ee45>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss_contrastive_pos\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_pos\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdiagonal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss_contrastive_neg\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegatives\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmargin_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pos\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_contrastive_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"neg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_contrastive_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# if torch.isnan(loss_contrastive_neg) == False:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "valLosses,trainLosses = mytrainStep1(modelEmb,criterion1,criterion2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.260270 --> 0.253662"
      ],
      "metadata": {
        "id": "02103Vs-vPxK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uUqNY6QMAvlj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "adbc4387-eb8a-41c4-ad03-ce4845c13a44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Thesis/PAN23/checkpointEmbUncased_PAN23_gen.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import shutil\n",
        "# torch.save(early_stopping1, 'earlyStopping/pt')\n",
        "shutil.copy('checkpointEmbUncased_PAN23_gen.pt','/content/drive/My Drive/Thesis/PAN23')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9yR3x0C4pnDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3Yt7yaDqvK"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oVfLShVA7VEo"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNwsvz6gX8RLnm69tBWsKKr",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d037b45aa644edfbe493fc43cb2de68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17930393d0ff4b2186dcd8217673d0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af4db6e4a20a48aeb9a2076c54107b33",
            "placeholder": "​",
            "style": "IPY_MODEL_3845dbd442ae4bd18924a378c7f77148",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "2919a03c29de4629943a3d36a65d9fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdcafddb16b44dc4afaf720582c4a707",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa0d7d1bf18d4f9198ba82d378f9eaa5",
            "value": 466062
          }
        },
        "30a6a50dd2944a7bafe7f332b118658b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9e93eabfd8a48d4824da8b1b21073d1",
              "IPY_MODEL_a25329cb016e4d1e96248d61a9435e89",
              "IPY_MODEL_f602361e43574417ace59246ba975d8a"
            ],
            "layout": "IPY_MODEL_9fda2a68db4b47299e85a89dd7c5437a"
          }
        },
        "3194dc2f5e4d49d4aaae2f55aed82c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "353776018a3048f8a92fc4498b44fd36": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3845dbd442ae4bd18924a378c7f77148": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3933840c51654e76ac71679e8814008a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a0978c9f484ab18396e073d2e8f5cf",
            "placeholder": "​",
            "style": "IPY_MODEL_ac3ea17989f84f77976656bb5e94e708",
            "value": " 466k/466k [00:00&lt;00:00, 3.40MB/s]"
          }
        },
        "48e722c84ca740308fce0332ad4db56e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7158da9b773740c290a6afff8024a373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7a0025a147124158a7f003eb3dae1c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b2ae757dd6b497682bb671a392417d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c4ea841b04b4364bc84f7e31548fa8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d49a7d3652941389d62dbc8a0d6c164": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9265ef91f4044b9c8ccfe2f8a2b3cf44",
              "IPY_MODEL_2919a03c29de4629943a3d36a65d9fca",
              "IPY_MODEL_3933840c51654e76ac71679e8814008a"
            ],
            "layout": "IPY_MODEL_48e722c84ca740308fce0332ad4db56e"
          }
        },
        "9265ef91f4044b9c8ccfe2f8a2b3cf44": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fa8cfbc351e4bd1941e5eb82ed78bf1",
            "placeholder": "​",
            "style": "IPY_MODEL_d6b7000f324d4eac9dfa399d1895a556",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "9fa8cfbc351e4bd1941e5eb82ed78bf1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fda2a68db4b47299e85a89dd7c5437a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a07f64a9ba864e7faba1ba38a1fd2224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a25329cb016e4d1e96248d61a9435e89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c4ea841b04b4364bc84f7e31548fa8a",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a07f64a9ba864e7faba1ba38a1fd2224",
            "value": 28
          }
        },
        "ac3ea17989f84f77976656bb5e94e708": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af4db6e4a20a48aeb9a2076c54107b33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be45071aa3ac47b1a77b0191c4a36b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8dadda9377a46edbfd562c778702f51",
            "placeholder": "​",
            "style": "IPY_MODEL_f0c49329a5224525b90d964842e558c3",
            "value": " 232k/232k [00:00&lt;00:00, 1.98MB/s]"
          }
        },
        "be9bd95f149c4ab1a638b4f75107b4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8dadda9377a46edbfd562c778702f51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdcafddb16b44dc4afaf720582c4a707": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6b7000f324d4eac9dfa399d1895a556": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efed44431ebc435688b3eb0e46eae96b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17930393d0ff4b2186dcd8217673d0c8",
              "IPY_MODEL_fff7628804744f54a5359f381dc192be",
              "IPY_MODEL_be45071aa3ac47b1a77b0191c4a36b6e"
            ],
            "layout": "IPY_MODEL_0d037b45aa644edfbe493fc43cb2de68"
          }
        },
        "f0c49329a5224525b90d964842e558c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f602361e43574417ace59246ba975d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3194dc2f5e4d49d4aaae2f55aed82c2b",
            "placeholder": "​",
            "style": "IPY_MODEL_7a0025a147124158a7f003eb3dae1c65",
            "value": " 28.0/28.0 [00:00&lt;00:00, 1.40kB/s]"
          }
        },
        "f7a0978c9f484ab18396e073d2e8f5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e93eabfd8a48d4824da8b1b21073d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be9bd95f149c4ab1a638b4f75107b4f0",
            "placeholder": "​",
            "style": "IPY_MODEL_8b2ae757dd6b497682bb671a392417d5",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "fa0d7d1bf18d4f9198ba82d378f9eaa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fff7628804744f54a5359f381dc192be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_353776018a3048f8a92fc4498b44fd36",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7158da9b773740c290a6afff8024a373",
            "value": 231508
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}